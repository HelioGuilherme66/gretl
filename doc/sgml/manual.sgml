<!DOCTYPE book SYSTEM "/usr/share/sgml/docbook-4.1/docbook.dtd"[
<!ENTITY % texmath SYSTEM "./dbtexmath.dtd">
<!ENTITY version "0.96">
<!ENTITY latex "LaTeX">
<!ENTITY tex "TeX">
<!ENTITY opto "<literal>[ -o ]</literal>">
<!ENTITY cmdcol "82pt">
<!ENTITY rsqu "<emphasis>R</emphasis><superscript>2</superscript>">
%texmath;
]>

<!-- =============Document Header ============================= -->

<book id="manual">

  <title>Gretl Manual</title>

  <bookinfo>

    <subtitle>Gnu Regression, Econometrics and Time-series Library</subtitle>

    <author>
      <firstname>Allin</firstname>
      <surname>Cottrell</surname>
      <affiliation>
	<orgname>Wake Forest University</orgname>
	<orgdiv>Department of Economics</orgdiv>
      </affiliation>
    </author>

    <graphic fileref="figures/gretl-logo.png"/>
    
    <copyright>
      <year>2001</year>
      <holder>Allin Cottrell</holder>
    </copyright>

    <date>October, 2001</date>

    <legalnotice id="legalnotice">
      <para>Permission is granted to copy, distribute and/or modify
	this document under the terms of the <ulink
	  url="gnome-help:fdl" type="help"><citetitle>GNU Free
	    Documentation License</citetitle></ulink>, Version 1.1 or
	any later version published by the Free Software Foundation.
      </para>
    </legalnotice>

    <releaseinfo>
      This is version 0.0 of the Gretl manual.
    </releaseinfo>

  </bookinfo>


  <chapter id="intro"><title>Introduction</title>

    <para>
      <application>gretl</application> is an econometrics package,
      built around a shared library which may be accessed using a
      command-line client program
      (<application>gretlcli</application>) or a graphical user
      interface (<application>gretl</application>). If you don't know
      what econometrics is but have some interest in the software
      anyway, please take a look at <xref linkend="app-a"/>.
    </para>

    <sect1 id="features"><title>Features at a glance</title>

      <variablelist>
	<varlistentry><term>User-friendly</term>
	  <listitem>
	    <para>
	      <application>gretl</application> offers an intuitive
	      user interface; it is very easy to get up and running
	      with econometric analysis. Thanks to its association
	      with Ramanathan's <citetitle>Introductory
		Econometrics</citetitle> the package offers many
	      practice data files and command scripts.  These are well
	      annotated and accessible.</para>
	  </listitem>
	</varlistentry>  
	<varlistentry><term>Flexible</term>
	  <listitem>
	    <para>
	      You can choose your preferred point on the spectrum from
	      interactive point-and-click to batch processing, and can
	      easily combine these approaches.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry><term>Cross-platform</term>
	  <listitem>
	    <para>
	      <application>gretl</application>'s home platform is
	      Linux, but it is also available for MS Windows.  I have
	      compiled it on AIX and it should work on any unix-like
	      system that has the appropriate basic libraries (see
	      <xref linkend="app-b"/>).
	    </para>
	  </listitem>
	</varlistentry>  
	<varlistentry><term>Open source</term>
	  <listitem>
	    <para>
	      The full source code for
	      <application>gretl</application> is available to anyone
	      who wants to critique it, patch it, or extend it.  The
	      author welcomes any bug reports.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry><term>Reasonably sophisticated</term>
	  <listitem>
	    <para>
	      <application>gretl</application> offers a full range of
	      least-squares based estimators, including Two-Stage
	      Least Squares.  It also offers (binomial) logit and
	      probit estimation, and has a loop construct for running
	      Monte Carlo analyses or iterated least squares
	      estimation of non-linear models.  While it does not
	      include all the estimators and tests that a professional
	      econometrician might require, it supports the export of
	      data to the formats of (<ulink
		url="http://www.r-project.org">GNU R</ulink>) and
	      (<ulink url="www.octave.org">GNU Octave</ulink>) for
	      further custom processing (see <xref linkend="app-c"/>).
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry><term>Internet ready</term>
	  <listitem>
	    <para>
	      <application>gretl</application> can access and fetch
	      databases from a server at Wake Forest University.  The
	      MS Windows version comes with an updater program which
	      will detect when a new version is available and offer
	      the option of auto-updating.
	    </para>
	  </listitem>
	</varlistentry>
      </variablelist>

    </sect1>

    <sect1 id="ack"><title>Acknowledgements</title>

      <para>
	My primary debt is to Professor Ramu Ramanathan of the
	University of California, San Diego.  A few years back he was
	kind enough to provide me with the source code for his program
	<application>ESL</application> (<quote>Econometrics Software
	  Library</quote>), which I ported to Linux, and since then I
	have been collaborating with him on updating and extending the
	program.  For the <application>gretl</application> project I
	have made extensive changes to the original
	<application>ESL</application> code.  New econometric
	functionality has been added, and the graphical interface is
	entirely new. Please note that Professor Ramanathan is not
	responsible for any bugs in <application>gretl</application>.
      </para>

      <para>
	I am also grateful to William Greene, author of
	<citetitle>Econometric Analysis</citetitle>, for permission to
	include in the <application>gretl</application> distribution
	some of the data files analysed in his text.
      </para>

      <para>I have benefitted greatly from the work of numerous
	developers of open-source software: for specifics please see
	<xref linkend="app-b"/> to this manual.</para>

      <para>My thanks are due to Richard Stallman of the Free Software
	Foundation, for his support of free software in general and
	for agreeing to <quote>adopt</quote>
	<application>gretl</application> as a GNU program in
	particular.</para>

    </sect1>

  <sect1 id="install"><title>Installing the programs</title>


    <sect2 id="linux-install"><title>Linux/unix</title>

      <para>
	On the Linux platform you have the choice of compiling the
	<application>gretl</application> code yourself or making use
	of a pre-built package. Ready-to-run packages are available in
	<application>rpm</application> format (suitable for Red Hat
	Linux and related systems) and also
	<application>deb</application> format (Debian GNU/Linux).  I
	am grateful to Dirk Eddelb&uuml;ttel for making the latter. If
	you prefer to compile your own (or are using a unix system for
	which pre-built packages are not available) here is what to
	do.
      </para>

      <orderedlist>
	<listitem><para>Download the latest
	    <application>gretl</application> source package from
	    <ulink url="http://ricardo.ecn.wfu.edu/gretl">Wake Forest
	      University</ulink>.
	  </para></listitem>
	
	<listitem><para>Unzip and untar the package.  On a system with
	    the GNU utilities available, the command would be
	    <command>tar -xvfz gretl-N.tar.gz</command> (replace
	    <command>N</command> with the specific version number of
	    the file you downloaded at step 1).</para></listitem>
	<listitem><para>Change directory to the gretl source directory
	    created at step 2 (e.g. <filename>gretl-0.70</filename>).
	    </para></listitem>  
	<listitem><para>The basic routine is then 

	    <programlisting>
	      ./configure 
	      make
	      make install 
	    </programlisting> 

	    However, you should probably do <command>./configure
	    --help</command> first to see what options are available.
	    One option you way wish to tweak is
	    <command>--prefix</command>.  By default the installation
	    goes under <filename>/usr/local</filename> but you can
	    change this.  For example <command>./configure
	    --prefix=/usr</command> will put everything under the
	    <filename>/usr</filename> tree.  In the event that a
	    required library is not found on your system, so that the
	    configure process fails, please take a look at <xref
	    linkend="app-b"/> of this manual.
	  </para>
	</listitem>
      </orderedlist>

    </sect2>

    <sect2 id="windows-install"><title>MS Windows</title>

      <para>
	The MS Windows version comes as a self-extracting executable.
	Installation is just a matter of downloading
	<filename>gretl_install.exe</filename> from <ulink
	  url="http://ricardo.ecn.wfu.edu/gretl/win32/">Wake Forest
	  University</ulink> and running this program. You will be
	prompted for a location to install the package (the default is
	<filename>c:\userdata\gretl</filename>).
      </para> 

    </sect2>

    <sect2 id="updating"><title>Updating</title>

      <para>
	If your computer is connected to the Internet, then on
	start-up <application>gretl</application> will query its home
	website at Wake Forest University to see if any program
	updates are available. If so, a window will open up informing
	you of that fact.  (If you want to supress this feature,
	uncheck the box marked <quote>Tell me about gretl
	  updates</quote> under <application>gretl</application>'s
	<quote>File, Preferences, General</quote> menu.)
      </para>

      <para>
	The MS Windows version of the program goes a step further: it
	tells you that you can update <application>gretl</application>
	automatically if you wish.  To do this, follow the
	instructions in the popup window: close
	<application>gretl</application> then run the program titled
	<quote>gretl updater</quote> (you should find this along with
	the main <application>gretl</application> program item, under
	the Programs heading in the Windows Start menu). Once the
	updater has completed its work you may restart
	<application>gretl</application>.
      </para>

    </sect2>
    </sect1>

  </chapter>

  <chapter id="getting-started"><title>Getting started</title>

    <para>
      Assuming the package has been successfully installed, starting
      from scratch is probably easiest with the graphical interface,
      <application>gretl</application>.<footnote><para>For convenience I
      will refer to the graphical client program simply as
      <application>gretl</application> in this manual.  Note, however,
      that the specific name of the program differs according to the
      computer platform.  On unix-like systems it is called
      <filename>gretl_x11</filename> while on MS Windows it is
      <filename>gretlw32.exe</filename>. On unix-like systems a
      wrapper script named <filename>gretl</filename> is also
      installed.</para>
      </footnote>
    </para>

    <para>This introduction is mostly angled towards the graphical
      client program; please see <xref linkend="cmdref"/> and
      <xref linkend="cli"/>  below for details on the command-line
      program, <application>gretlcli</application>.
    </para>

    <para>
      You can supply the name of a data file to open as an argument to
      <application>gretl</application>, but for the moment let's not
      do that: just fire up the program.  You should see a main window
      (which will hold information on the data set but which is at
      first blank) and various menus, some of them disabled at first.
    </para>

    <para>
      What can you do at this point?  You can browse the supplied data
      files (or databases), open a data file, create a new data file,
      read the help items, or open a command script.  For now let's
      browse the supplied data files.  Under the File menu choose
      <quote>Open data, sample file, Ramanathan&hellip;</quote>.  A
      second window should open, presenting a list of data files
      supplied with the package (see <xref linkend="fig-datafiles"/>).
      The
      numbering of the files corresponds to the chapter organization
      of Ramanathan (1998), which contains discussion of the analysis
      of these data. The data will be useful for practice purposes
      even without the text.
    </para>

    <figure id="fig-datafiles" float="1" pgwide="1">
      <title>Practice data files window</title>
      <screenshot>
	<screeninfo>Practice data files window</screeninfo>
	<graphic fileref="figures/datafiles.png" align="center"/>
      </screenshot>
    </figure>

    <para>
      If you select a row in this window and click on
      <quote>Info</quote> this pops open the the <quote>header
	file</quote> for the data set in question, which tells you
      something about the source and definition of the variables.  If
      you find a file that is of interest, you may open it by clicking
      on <quote>Open</quote>, or just double-clicking on the file
      name.  For the moment let's open <filename>data3-6</filename>.
    </para>

    <tip>
      <para>In <application>gretl</application> windows containing
	lists, double-clicking on a line launches a default action for
	the associated list entry: e.g. displaying the values of a
	data series, opening a file.</para>
    </tip>

    <para>
      This file contains data pertaining to a classic econometric
      <quote>chestnut</quote>, the consumption function.  The data
      window should now display the name of the current data file, the
      overall data range and sample range, and the names of the
      variables along with brief descriptive tags&mdash;see
      <xref linkend="fig-mainwin"/>.
    </para>

    <figure id="fig-mainwin" float="1" pgwide="1">
     <title>Main window, with a practice data file open</title>
     <screenshot>
	<screeninfo>Main window</screeninfo>
	<graphic fileref="figures/mainwin" align="center"/>
     </screenshot>
    </figure>

    <para>
      OK, what can we do now?  Hopefully the various menu options
      should be fairly self explanatory.  For now we'll dip into the
      Model menu; a brief tour of all the main window menus is given
      in <xref linkend="menus"/> below.
    </para>

    <para>
      <application>gretl</application>'s Model menu offers numerous
      various econometric estimation routines.  The simplest and most
      standard is Ordinary Least Squares (OLS).</para>

    <para>Selecting OLS pops up a dialog box calling for a
      <emphasis>model specification</emphasis>.  This takes the form
      of a list of variable names or numbers, separated by spaces.
      The first name or number represents the dependent variable, the
      remainder the independent variables.  It is usual to include a
      constant (ID number 0) among the independent variables
      (otherwise you are forcing the intercept to equal zero).
    </para>

    <tip><para>You can put a variable's ID number into the dialog box
	by clicking on that variable's row in the main
	window.</para></tip>

    <para> Thus, continuing the example of
      <filename>data3-6</filename>, the entry <command>2 0 3</command>
      (or equivalently <command>Ct 0 Yt</command>) specifies a
      regression of consumption (dependent) on income and a constant.
    </para>

    <para> You can specify a lagged value of an existing variable
      without explicitly adding this to the data set first.  Thus a
      variant on the above estimation command which includes the first
      lag of income on the right-hand side would be <command>2 0 3
	Yt(-1)</command> The lag is selected using a negative integer
      enclosed in parentheses. Note that in this context the name, not
      the number, of the variable in question must be used.
    </para>

    <sect1 id="est-output"><title>Estimation output</title>

    <para>
      Once you've specified a model, a window displaying the
      regression output will appear.  The output is reasonably
      comprehensive and in a standard format.
    </para>  

    <para>The output window contains menus that allow you to inspect
      or graph the residuals and fitted values, and to run various
      diagnostic tests on the model.</para>  

    <para>There is also an option to reprint the regression output in
      &latex; format.  This is not fully implemented yet, but works
      for OLS models.  You can print the results in a tabular format
      (similar to what's in the output window, but properly typeset)
      or as an equation, across the page.  For each of these options
      you can choose to preview the typeset product, or save the
      output to file for incorporation in a &latex; document.
      Previewing requires that you have a functioning &tex; system on
      your computer.</para> 

    <para>If you want to import <application>gretl</application>
      output into an editor or word processor there are two main
      options.  You can simply copy and paste from an output window
      (using its Edit menu) to the target program, or you can save the
      output to a file then import the file into the target program.
      When you finish a <application>gretl</application> session you
      are given the option of saving all the output from the session
      to a single file.</para> 

    <tip><para>When inserting <application>gretl</application> output
	into a word processor, select a monospaced or typewriter-style
	font (e.g. Courier) to preserve the output's tabular
	formatting.  Select a small font (10-point Courier should do)
	to prevent the output lines from being broken in the wrong
	place.</para> </tip>

    </sect1>
  </chapter>

  <chapter id="menus"><title>The main window menus</title>

    <para>
      Reading left to right along the main window's menu bar, we find
      the File, Session, Data, Sample, Variable, Model and Help menus
      (see <xref linkend="fig-mainwin"/>).</para>  

    <itemizedlist>
      <listitem><para><guimenu>File menu</guimenu></para>
	<itemizedlist>
	  <listitem><para><guimenuitem>Open data</guimenuitem>: Open a
	      native <application>gretl</application> data file or
	      import from other formats.  See
	      <xref linkend="datafiles"/>.</para></listitem>
	  <listitem><para><guimenuitem>Clear data set</guimenuitem>:
	      Clear the current data set out of memory.  Generally you
	      don't have to do this (since opening a new data file
	      automatically clears the old one) but sometimes it's
	      useful (see <xref
		linkend="scratch"/>).</para></listitem>
	  <listitem><para><guimenuitem>Browse databases</guimenuitem>:
	      See <xref linkend="dbase"/> and
	      <xref linkend="scratch"/>.</para></listitem>
	  <listitem><para><guimenuitem>Save data</guimenuitem> and
	      <guimenuitem>Export data</guimenuitem>: Write out the
	      current data set in native format, in Comma Separated
	      Values (CSV) format, or the formats of GNU R or GNU
	      Octave.  See <xref linkend="datafiles"/> and also 
	      <xref linkend="app-c"/>.</para></listitem>
	  <listitem><para><guimenuitem>Create data set</guimenuitem>:
	      Initialize the built-in spreadsheet for entering data
	      manually.  See <xref linkend="scratch"/>.</para></listitem>
	  <listitem><para><guimenuitem>Save last graph</guimenuitem>:
	      Just as it says.</para></listitem>
	  <listitem><para><guimenuitem>Open command
		file</guimenuitem>: Open a file of
	      <application>gretl</application> commands, either one
	      you have created yourself or one of the practice files
	      supplied with the package.  If you want to create a
	      command file from scratch use the next item,
	      <guimenuitem>New command
		file</guimenuitem>.</para></listitem>
	  <listitem><para><guimenuitem>Gretl console</guimenuitem>:
	      Open a <quote>console</quote> window into which you can
	      type commands as you would using the command-line
	      program, <application>gretlcli</application> (as opposed
	      to using point-and-click). See <xref linkend="cmdref"/>.
	      </para></listitem>
	  <listitem><para><guimenuitem>p-value finder</guimenuitem>:
	      Open a window which enables you to look up p-values from
	      the Gaussian, <emphasis>t</emphasis>,
	      &khgr;<superscript>2</superscript>,
	      <emphasis>F</emphasis> or gamma distributions. See also
	      the <command>pvalue</command> command in
	      <xref linkend="cmdref"/> below.</para></listitem>
	  <listitem><para><guimenuitem>statistical
		tables</guimenuitem>: Look up critical values for
	      commonly used distributions (Gaussian,
	      <emphasis>t</emphasis>,
	      &khgr;<superscript>2</superscript>,
	      <emphasis>F</emphasis> and
	      Durbin&ndash;Watson).</para></listitem>
	  <listitem><para><guimenuitem>test calculator</guimenuitem>:
	      Calculate test statistics and p-values for a range of
	      common hypothesis tests (population mean, variance and
	      proportion; difference of means, variances and
	      proportions).  The relevant sample statistics must be
	      already available for entry into the dialog box.  For
	      some simple tests that take as input data series rather
	      than pre-computed sample statistics, see
	      <quote>Difference of means</quote> and <quote>Difference
		of variances</quote> under the Data
	      menu.</para></listitem>
	  <listitem><para><guimenuitem>Preferences</guimenuitem>: Set
	      the paths to various files
	      <application>gretl</application> needs to access. Choose
	      the font in which gretl displays text output.  Select or
	      unselect <quote>expert mode</quote>. (If this mode is
	      selected various warning messages are suppressed.)
	      Activate or suppress <application>gretl</application>'s
	      messaging about the availability of program updates.
	      Configure or turn on/off the main-window
	      toolbar.</para></listitem>
	  <listitem><para><guimenuitem>Exit</guimenuitem>: Quit the
	      program.  If expert mode is not selected you'll be
	      prompted to save any unsaved work.</para></listitem>
	</itemizedlist>
      </listitem>

      <listitem><para><guimenu>Session menu</guimenu> This is discussed
	  separately below. Please see <xref linkend="session"/>.
	  </para></listitem>

      <listitem><para><guimenu>Data menu</guimenu></para>
	<itemizedlist>
	  <listitem><para><guimenuitem>Display values</guimenuitem>:
	      pops up a window with a simple (not editable) printout
	      of the values of the variables (either all of them or a
	      selected subset).</para></listitem>
	      <listitem><para><guimenuitem>Edit values</guimenuitem>:
	      pops up a spreadsheet window where you can make changes,
	      add new variables, and extend the number of
	      observations.  (The data matrix must remain rectangular,
	      with the same number of observations for each
	      series.)</para></listitem>
	      <listitem><para><guimenuitem>Graph specified
	      vars</guimenuitem>: Gives a choice between a time series
	      plot, a regular X&ndash;Y scatter plot, an X&ndash;Y
	      plot using impulses (vertical bars), and an X&ndash;Y
	      plot <quote>with factor separation</quote> (i.e. with
	      the points colored differently depending to the value of
	      a given dummy variable).  Serves up a dialog box where
	      you specify the variables to graph.  The simplest way to
	      fill out the dialog entry is to refer to the variables
	      by their ID numbers (shown in the leftmost column of the
	      main data window). Thus, having chosen the scatter plot
	      option, an entry of <quote>2 3</quote> will plot
	      variable number 2 (here, consumption) against variable
	      number 3 (income). The last referenced variable will be
	      on the <emphasis>x</emphasis> axis.  Gnuplot is used to
	      render the graph.</para></listitem>
	      <listitem><para><guimenuitem>Multiple
	      scatterplots</guimenuitem>: Show a collection of (at
	      most six) pairwise plots, with either a given variable
	      on the <emphasis>y</emphasis> axis plotted against
	      several different variables on the
	      <emphasis>x</emphasis> axis, or several
	      <emphasis>y</emphasis> variables plotted against a given
	      <emphasis>x</emphasis>. May be useful for exploratory
	      data analysis.</para></listitem>
	      <listitem><para><guimenuitem>Read info</guimenuitem>,
	      <guimenuitem>Edit header</guimenuitem>: <quote>Read
	      info</quote> just displays the header file information
	      for the current data file; <quote>Edit header</quote>
	      allows you to make changes to it (if you have permission
	      to do so).</para></listitem>
	      <listitem><para><guimenuitem>Summary
	      statistics</guimenuitem>: shows a fairly full set of
	      descriptive statistics for all variables in the data
	      set.</para></listitem>
	      <listitem><para><guimenuitem>Correlation
	      matrix</guimenuitem>: shows the pairwise correlation
	      coefficients for the variables in the data
	      set.</para></listitem>
	      <listitem><para><guimenuitem>Difference of
	      means</guimenuitem>: calculates the
	      <emphasis>t</emphasis> statistic for the null hypothesis
	      that the population means are equal for two selected
	      variables and shows its p-value.</para></listitem>
	      <listitem><para><guimenuitem>Difference of
	      variances</guimenuitem>: calculates the
	      <emphasis>F</emphasis> statistic for the null hypothesis
	      that the population variances are equal for two selected
	      variables and shows its p-value.</para></listitem>
	      <listitem><para><guimenuitem>Add variables</guimenuitem>
	      gives a sub-menu of standard transformations of
	      variables (logs, lags, squares, etc.) that you may wish
	      to add to the data set. Also gives the option of adding
	      random variables, and (for time-series data) adding
	      seasonal dummy variables (e.g. quarterly dummy variables
	      for quarterly data). Includes an item for seeding the
	      program's pseudo-random number
	      generator.</para></listitem>
	      <listitem><para><guimenuitem>Refresh
	      window</guimenuitem> Sometimes
	      <application>gretl</application> commands generate new
	      variables.  The <quote>refresh</quote> item ensures that
	      the listing of variables visible in the main data window
	      is in sync with the program's internal
	      state.</para></listitem>
	</itemizedlist>
      </listitem>
      <listitem><para><guimenu>Sample menu</guimenu></para>
	<itemizedlist>
	  <listitem><para><guimenuitem>Set range</guimenuitem>: Select
	      a different starting and/or ending point for the current
	      sample, within the range of data
	      available.</para></listitem> 
	  <listitem><para><guimenuitem>Restore full
		range</guimenuitem>
	      self-explanatory.</para></listitem>
	  <listitem><para><guimenuitem>Set frequency,
		startobs</guimenuitem>: Impose a particular
	      interpretation of the data in terms of frequency and
	      starting point.  This is primarily intended for use with
	      panel data; see <xref linkend="panel"/>
	      below.</para></listitem>  
	  <listitem><para><guimenuitem>Define, based on
		dummy</guimenuitem>: Given a dummy (indicator) variable
	      with values 0 or 1, this drops from the current sample
	      all observations for which the dummy variable has value
	      0.</para></listitem>
	  <listitem><para><guimenuitem>Restrict, based on
		criterion</guimenuitem>: Similar to the item above,
	      except that you don't need a pre-defined variable: you
	      supply a Boolean expression (e.g. <literal>sqft &gt;
		1400</literal>) and the sample is restricted to
	      observations satisfying that condition. See the help for
	      <command>genr</command> in
	      <xref linkend="cmdref"/> for details on the Boolean
	      operators that can be used.</para></listitem>
	  <listitem><para><guimenuitem>Drop all obs with missing
		values</guimenuitem>: Drop from the current sample all
	      observations for which at least one variable has a
	      missing value (see <xref linkend="missing-data"/>
	      below).</para></listitem>
	  <listitem><para><guimenuitem>Count missing
		values</guimenuitem>: Give a report on observations
	      where data values are missing. May be useful in
	      examining a panel data set, where it's quite common to
	      encounter missing values.</para></listitem>
	  <listitem><para><guimenuitem>Add case markers</guimenuitem>
	      Prompts for the name of a text file containing
	      <quote>case markers</quote> (short strings identifying
	      the individual observations) and adds this information
	      to the data set.  See <xref linkend="datafiles"/>
	      below.</para></listitem>
	</itemizedlist>
      </listitem>
      <listitem><para><guimenu>Variable menu</guimenu> Most items
	  under here operate on a single variable at a time.  The
	  <quote>active</quote> variable is set by highlighting it
	  (clicking on its row) in the main data window.  Most options
	  will be self-explanatory.  Note that you can rename a
	  variable, and can edit its descriptive label. You can also
	  <quote>Define a new variable</quote> via a formula (e.g.
	  involving some function of one or more existing variables).
	  For the syntax of such formulae, look at the online help for
	  <quote>Generate variable syntax</quote> or see the
	  <command>genr</command> command in 
	  <xref linkend="cmdref"/> below.  
	  One simple example:
	  <programlisting> 
	    foo = x1 * x2 
	  </programlisting> 
	  will create
	  a new variable <varname>foo</varname> as the product of the
	  existing variables <varname>x1</varname> and
	  <varname>x2</varname>.  In these formulae, variables must be
	  referenced by name, not number.</para></listitem> 

      <listitem><para><guimenu>Model menu</guimenu> This is introduced
	  in <xref linkend="getting-started"/>. For details on the
	  various estimators offered under this menu please consult
	  <xref linkend="estimators"/> and <xref
	    linkend="cmdref"/> below, and/or the online help under
	  <quote>Help, Estimation</quote>.</para></listitem>

      <listitem><para><guimenu>Help menu</guimenu>  Please use this as
	  needed! It gives details on the syntax required in various
	  dialog entries.</para></listitem>   

    </itemizedlist>

  </chapter>

  <chapter id="modes"><title>Modes of working</title>

  <sect1 id="scripts"><title>Command scripts</title>

    <para>
      As you execute commands in <application>gretl</application>,
      using the GUI and filling in dialog entries, those commands are
      recorded in the form of a <quote>script</quote>.  Such scripts
      can be edited and re-run, using either
      <application>gretl</application> or the command-line client,
      <application>gretlcli</application>.</para>

    <para>
      To view the current state of the script at any point in a
      <application>gretl</application> session, choose <quote>Command
	log</quote> under the File menu. This log is called
      <filename>session.inp</filename> and it is overwritten whenever
      you start a new session.  To preserve it, save the script under
      a different name.  Script files will be found most easily, using
      the GUI file selector, if you name them with the extension
      <quote><filename>.inp</filename></quote>.</para>

    <para>To open a script you have written independently, use the
      <quote>File, Open command file</quote> menu item.</para>

    <para>With a script window open, use its <quote>File, Save and
	Run</quote> menu item to run the commands.  All output is
      directed to a single window, where it can be edited, saved or
      copied to the clipboard.</para>

    <para>To learn more about the possibilities of scripting, take a
      look at the <application>gretl</application> Help item
      <quote>Script commands syntax,</quote> or start up the
      command-line program <application>gretlcli</application> and
      consult its help, or consult <xref linkend="cmdref"/> in this
      manual. In addition, the <application>gretl</application>
      package includes over 70 <quote>practice</quote> scripts.  Most
      of these relate to Ramanathan (1998), but they may also be used
      as a free-standing introduction to scripting in
      <application>gretl</application> and to various points of
      econometric theory.  You can explore the practice files under
      <quote>File, Open command file, practice file</quote>  There you
      will find a listing of the files along with a brief description
      of the points they illustrate and the data they employ.  Open
      any file and run it (<quote>File, Run</quote> in the resulting
      script window) to see the output.</para>

    <para>Note that long commands in a script can be broken over two
      or more lines, using backslash as a continuation
      character.</para>

    <para>You can, if you wish, use the GUI controls and the scripting
      approach in tandem, exploiting each method where it offers
      greater convenience.  Here are two suggestions.</para>

    <itemizedlist>
      <listitem><para>Open a data file in the GUI.  Explore the
	  data&mdash;generate graphs, run regressions, perform tests.
	  Then open the Command log, edit out any redundant commands,
	  and save it under a specific name. Run the script to
	  generate a single file containing a concise record of your
	  work.</para></listitem>
      <listitem><para>Start by establishing a new script file.  Type
	  in any commands that may be required to set up
	  transformations of the data (see the <command>genr</command> command
	  in <xref linkend="cmdref"/> below). Typically this sort of thing
	  can be accomplished more efficiently via commands assembled
	  with forethought rather than point-and-click. Then save and
	  run the script: the GUI data window will be updated
	  accordingly. Now you can carry out further exploration of
	  the data via the GUI. To revisit the data at a later point,
	  open and rerun the <quote>preparatory</quote> script
	  first.</para></listitem>
    </itemizedlist>

    <para>A further option is available for your computing
      convenience. Under <application>gretl</application>'s File menu
      you will find the item <quote>Gretl console</quote>.  This opens
      up a window in which you can type commands and execute them one
      by one (by pressing the Enter key) interactively.  This is
      essentially the same as <application>gretlcli</application>'s
      mode of operation, except that (a) the GUI is updated based on
      commands executed from the console, enabling you to work back
      and forth as you wish, and (b)
      <application>gretl</application>'s Monte Carlo loop routine (see
      <xref linkend="monte-carlo"/>) is not at present available in this
      mode.</para>

  </sect1>

  <sect1 id="session"><title>The <quote>session</quote>
      concept</title>

    <para>
      <application>gretl</application> offers the idea of a
      <quote>session</quote> as a way of keeping track of your work
      and revisiting it later.  This is experimental (and at present
      more likely to be buggy than the rest of the program): I would
      be interested in hearing people's reactions.</para>

    <para>
      The basic idea is to provide a little iconic space containing
      various objects pertaining to your current working session (see
      <xref linkend="fig-session"/>).  You can add objects (represented by
      icons) to this space as you go along.  If you save the session,
      these added objects should be available again if you re-open the
      session later.</para>

    <figure id="fig-session" float="1" pgwide="1">
      <title>Icon view: one model and one graph have been added to the
	default icons</title>
      <screenshot>
	<screeninfo>Session window</screeninfo>
	<graphic fileref="figures/session.png" align="center"/>
      </screenshot>
    </figure>

    <para>
      If you start <application>gretl</application> and open a data
      set, then select <quote>Icon view</quote> from the Session menu,
      you should see the basic default set of icons: these give you
      quick access to the command script, information on the data set
      (if any), correlation matrix and descriptive summary statistics.
      All of these are activated by double-clicking the relevant icon.
      The <quote>Data set</quote> icon is a little more complex:
      double-clicking opens up the data in the built-in spreadsheet,
      but you can also right-click on the icon for a menu of other
      actions.  </para>

    <tip><para>In many <application>gretl</application> windows, the
	right mouse button brings up a menu with common
	tasks.</para></tip>

    <para>Two sorts of objects can be added to the Icon View window:
      models and graphs.</para>  

    <para>To add a model, first estimate it using the Model menu. Then
      pull down the File menu in the model window and select
      <quote>Save to session as icon&hellip;</quote> or <quote>Save as icon
	and close</quote> (the first of these prompts you for a name
      for the model, while the second uses a default name, e.g.
      <quote>Model 1</quote>).</para>

    <para>To add a graph, first create it (under the Data menu,
      <quote>Graph specified vars</quote>, or via one of
      <application>gretl</application>'s other graph-generating
      commands), then choose <quote>Add last graph</quote> from the
      Session menu.</para>

    <para>Once a model or graph is added its icon should appear in the
      Icon View window.  Double-clicking on the icon redisplays the
      object, while right-clicking brings up a menu which lets you
      rename or delete the object.  This popup menu also gives you the
      option of editing graphs.</para>

    <para>If you create models or graphs that you think you may wish
      to re-examine later, then before quitting
      <application>gretl</application> select <quote>Save
	as&hellip;</quote>  from the Session menu and give a name under
      which to save the session.  To re-open the session later,
      either</para>

    <itemizedlist>
      <listitem><para>Start <application>gretl</application> then
	  re-open the session file by going to the <quote>Open</quote>
	  item under the Session menu, or</para></listitem>
      <listitem><para>From the command line, type <command>gretl -r</command>
	  <replaceable>sessionfile</replaceable>, where 
	  <replaceable>sessionfile</replaceable> is the name
	  under which the session was saved.</para></listitem>
    </itemizedlist>

    <para>Also under the Session menu is an option to launch a GNU R
      session.  R is a separate program (see <xref linkend="app-c"/>):
      if R is not installed on your computer this menu item will not
      accomplish anything.  This is a convenience function for anyone
      wishing to carry out further statistical analyses not available
      in <application>gretl</application>: when R is invoked in this
      way, it comes up loaded with a copy of the current
      <application>gretl</application> data set.</para>

    </sect1>

  <sect1 id="toolbar"><title>The gretl toolbar</title>

    <para>At the bottom left of the main window sits the toolbar.  The
      icons have the following functions, reading from left to
      right:</para>
    

    <orderedlist>
      <listitem><para>Launch a calculator program.  This is just a
	  convenience function in case you want quick access to a
	  calculator when you're working in
	  <application>gretl</application>.  The default program is
	  <filename>calc.exe</filename> under MS Windows, or
	  <filename>xcalc</filename> under the X window system.  You
	  can change the program under the <quote>File, Preferences,
	    General</quote> menu, <quote>Toolbar</quote>
	  tab.</para></listitem>
      <listitem><para>Launch an editor or word processor.  The default
	  is <filename>winword.exe</filename> under MS Windows,
	  <filename>emacs</filename> under X. This is configurable in
	  the same way as the calculator launcher.</para></listitem>
      <listitem><para>Open the gretl console.  A shortcut to the
	  <quote>Gretl console</quote> menu item (<xref linkend="menus"/>
	  above).</para></listitem>
      <listitem><para>Open the <application>gretl</application>
	  website in your web browser.  This will work only if you are
	  connected to the Internet and have a properly configured
	  browser.</para></listitem>
      <listitem><para>Open the current version of this manual, in PDF
	  format.  As with the previous item, this requires an
	  Internet connection; it also requires that your browser
	  knows how to handle PDF files.</para></listitem>
      <listitem><para>A shortcut to the help item for script commands
	  syntax (i.e. a listing with details of all available
	  commands).</para></listitem>
      <listitem><para>Shortcut to dialog box for defining a
	  graph.</para></listitem>
      <listitem><para>Shortcut to the listing of datasets associated
	  with Ramanathan's <citetitle>Introductory
	  Econometrics</citetitle>.</para></listitem>
    </orderedlist>

    <para>
      If you don't care to have the toolbar displayed, you can turn it
      off under the <quote>File, Preferences, General</quote> menu. Go
      to the Toolbar tab and uncheck the <quote>show gretl
	toolbar</quote> box.</para>

    </sect1>
  </chapter>

  <chapter id="datafiles"><title>Data files</title>

    <sect1 id="native"><title>Basic native format</title>

      <para>In <application>gretl</application>'s native data format,
	a data set is represented by two files.  One contains the
	actual data and the other information on how the data should
	be read.  To be more specific:</para>

      <orderedlist>
  
	<listitem><para><emphasis>Actual data</emphasis>: A
	    rectangular matrix of white-space separated numbers.  Each
	    column represents a variable, each row an observation on
	    each of the variables (spreadsheet style). Data columns
	    can be separated by spaces or tabs. The filename should
	    have either no suffix or the suffix
	    <filename>.dat</filename>.  By default the data file is
	    ASCII (plain text).  Optionally it can be gzip-compressed
	    to save disk space. You can insert comments into a data
	    file: if a line begins with the hash mark
	    (<literal>#</literal>) the entire line is ignored. This is
	    consistent with gnuplot and octave data
	    files.</para></listitem>

	<listitem><para><emphasis>Header</emphasis>: The data file
	    must be accompanied by a header file which has the same
	    basename as the data file plus the suffix
	    <filename>.hdr</filename>.  This file contains, in
	    order:</para>

	  <itemizedlist>
	    <listitem><para>(Optional) <emphasis>comments</emphasis>
		on the data, set off by the opening string
		<literal>(*</literal> and the closing string
		<literal>*)</literal>, each of these strings to occur
		on lines by themselves.</para></listitem>
	    <listitem><para>(Required) list of white-space separated
		<emphasis>names of the variables</emphasis> in the
		data file. Names are limited to 8 characters, must
		start with a letter, and are limited to alphanumeric
		characters plus the underscore.  The list may continue
		over more than one line; it is terminated with a
		semicolon, <literal>;</literal>.</para></listitem>
	    <listitem><para>(Required) <emphasis>observations</emphasis> line of
		the form <literal>1 1 85</literal>.  The first element
		gives the data frequency (1 for undated or annual
		data, 4 for quarterly, 12 for monthly).  The second
		and third elements give the starting and ending
		observations. Generally these will be 1 and the number
		of observations respectively, for undated data.  For
		time-series data one can use dates of the form
		<command>1959.1</command> (quarterly, one digit after
		the point) or <command>1967.03</command> (monthly, two
		digits after the point). See <xref linkend="panel"/> below
		for special use of this line in the case of panel
		data.</para></listitem>
	    <listitem><para>The keyword
		<literal>BYOBS</literal>.</para></listitem> 
	  </itemizedlist>
	</listitem>   
      </orderedlist> 

      <para>Here is an example of a well-formed data header file.

	<programlisting> 
	  (* 
	  DATA9-6: 
	  Data on log(money), log(income) and interest rate from US. 
	  Source: Stock and Watson (1993) Econometrica 
	  (unsmoothed data) Period is 1900-1989 (annual data). 
	  Data compiled by Graham Elliott. 
	  *) 
	  lmoney lincome intrate ; 
	  1 1900 1989 BYOBS 
	</programlisting>

	The corresponding data file contains three columns of data, each
	having 90 entries.</para>

    </sect1>

    <sect1 id="extensions"><title>Extensions to the basic data
	format</title>

      <para>
	The options available in <application>gretl</application> data
	files are broader than the setup just described, in three
	ways:
      </para>

      <orderedlist>
	<listitem><para>If the <literal>BYOBS</literal> keyword is
	    replaced by <literal>BYVAR</literal>, and followed by the
	    keyword <literal>BINARY</literal>, this indicates that the
	    corresponding data file is in binary format.  Such data
	    files can be written from
	    <application>gretlcli</application> using the
	    <command>store</command> command with the
	    <command>-s</command> flag (single precision) or the
	    <command>-o</command> flag (double
	    precision).</para></listitem>
	
	<listitem><para>If <literal>BYOBS</literal> is followed by the
	    keyword <literal>MARKERS</literal>,
	    <application>gretl</application> expects a data file in
	    which the <emphasis>first column</emphasis> contains strings (8
	    characters maximum) used to identify the observations.
	    This may be handy in the case of cross-sectional data
	    where the units of observation are identifiable:
	    countries, states, cities or whatever.  It can also be
	    useful for irregular time series data, such as daily stock
	    price data where some days are not trading days&mdash;in
	    this case the observations can be marked with a date
	    string such as <command>10/01/98</command>.  (Remember the
	    8-character maximum.)  Note that <command>BINARY</command> and
	    <command>MARKERS</command> are mutually exclusive flags. Also note
	    that the <quote>markers</quote> are not considered to be a
	    variable: this column does not have a corresponding entry
	    in the list of variable names in the header
	    file.</para></listitem>

	<listitem><para>If a file with the same base name as the data
	    file and header files, but with the suffix
	    <filename>.lbl</filename>, is found, this is read to fill
	    out the descriptive labels for the data series. The format
	    of the (plain text) label file is simple: each line
	    contains the name of one variable (as found in the header
	    file), followed by one or more spaces, followed by the
	    descriptive label. Here is an example: 

	    <literal>price  New car price index, 1982 base year</literal>

	    A label file of this sort is created automatically when
	    you save data from <application>gretl</application>, if
	    there is any descriptive information to be saved.  Such
	    information can be added under the <quote>Variable, Edit
	      label</quote> menu item.
	  </para>
	</listitem>
      </orderedlist>

    </sect1>

    <sect1 id="other-formats"><title>Other data file formats</title>

      <para>
	<application>gretl</application> will read various other data
	formats.
      </para>

      <itemizedlist>
  
	<listitem><para>Comma-Separated Values (CSV) files.  These can
	    be brought in using <application>gretl</application>'s
	    <quote>File, Open Data, Import CSV&hellip;</quote> menu
	    item, or the <command>import</command> script command.
	    The program expects a file that has (a) valid variable
	    names on the first row and (b) a rectangular block of data
	    beneath. Optionally the first column may contain strings
	    such as dates (8 characters max.): such a column should be
	    headed <quote><command>obs</command></quote> or
	    <quote><command>date</command></quote>, or its first row
	    cell may be left blank. There should be <emphasis>exactly
	      one</emphasis> non-data row at the top of the file.  See
	    also
	    <xref linkend="scratch"/> below.</para></listitem>
  
	<listitem><para>BOX1 format data.  Large amounts of micro data
	    are available (for free) in this format via the <ulink
	      url="http://www.census.gov/ftp/pub/DES/www/welcome.html">Data 
	      Extraction Service</ulink> of the US Bureau of the
	    Census. BOX1 data may be imported using the <quote>File,
	      Open Data, Import BOX&hellip;</quote> menu item or the
	    <command>import -o</command> script command.</para>
	</listitem>

      </itemizedlist>

      <para>When you import data from either of these two formats,
	<application>gretl</application> opens a
	<quote>diagnostic</quote> window, reporting on its progress in
	reading the data.  If you encounter a problem with
	ill-formatted data, the messages in this window should give
	you a handle on fixing the problem.</para>

      <para>For the convenience of anyone wanting to carry out more
	complex data analysis, <application>gretl</application> has a
	facility for writing out data in the native formats of GNU R
	and GNU Octave (see <xref linkend="app-c"/>).  In the GUI
	client this option is found under the <quote>File</quote>
	menu; in the command-line client use the
	<command>store</command> command with the flag
	<command>-r</command> (R) or <command>-m</command>
	(Octave).</para>

    </sect1>

  <sect1 id="dbase"><title>Binary databases</title>
  
    <para>For working with large amounts of data I have supplied
      <application>gretl</application> with a database-handling
      routine.  A <emphasis>database</emphasis>, as opposed to a
      <emphasis>data file</emphasis>, is not read directly into the
      program's workspace.  A database can contain series of mixed
      frequencies and sample ranges.  You open the database and select
      series to import into the working data set.  You can then save
      those series in a native format data file if you wish.
      Databases can be accessed via <application>gretl</application>'s
      menu item <quote>File, Browse databases</quote>.</para>
  
    <para>A <application>gretl</application> database consists of two
      parts: an ASCII index file (with filename suffix
      <filename>.idx</filename>) containing information on the series,
      and a binary file (suffix <filename>.bin</filename>) containing
      the actual data.  Two examples of the format for an entry in the
      <filename>idx</filename> file are shown below:</para>

    <programlisting>
      G0M910  Composite index of 11 leading indicators (1987=100) 
      M 1948.01 - 1995.11  n = 575
      currbal Balance of Payments: Balance on Current Account; SA 
      Q 1960.1 - 1999.4 n = 160
    </programlisting>
  
    <para>The first field is the series name.  The second is a
      description of the series (maximum 128 characters).  On the
      second line the first field is a frequency code:
      <literal>M</literal> for monthly, <literal>Q</literal> for
      quarterly and <literal>A</literal> for annual.  No other
      frequencies are accepted at present.  Then comes the starting
      date (N.B. with two digits following the point for monthly data,
      one for quarterly data, none for annual), a space, a hyphen,
      another space, the ending date, the string <quote><literal>n
	  =</literal></quote> and the integer number of observations.
      This format must be respected exactly.</para>

    <para>Optionally, the first line of the index file may contain a
      short comment (64 characters) on the source and nature of the
      data, following a hash mark.  For example:</para>

    <programlisting>
    # Federal Reserve Board (interest rates)
    </programlisting>

    <para>The corresponding binary database file holds the data
	values, represented as <quote>floats</quote>, that is,
	single-precision floating-point numbers, typically taking four
	bytes apiece.  The numbers are packed <quote>by
	  variable</quote>, so that the first <emphasis>n</emphasis>
	numbers are the observations of variable 1, the next
	<emphasis>m</emphasis> the observations on variable 2, and so
	on.</para>

    <sect2 id="online-data"><title>Online access to databases</title>

      <para>As of version 0.40, <application>gretl</application> is
	able to access databases via the internet.  Several databases
	are available from Wake Forest University.  Your computer must
	be connected to the internet for this option to work.  Please
	see the item on <quote>Online databases</quote> under
	<application>gretl</application>'s Help menu.  Expect to see
	this facility developed further in future releases.</para>

    </sect2>

    <sect2 id="RATS"><title>RATS 4 databases</title>

      <para>Thanks to Thomas Doan of <citetitle>Estima</citetitle>,
	who provided me with the specification of the database format
	used by RATS 4 (Regression Analysis of Time Series),
	<application>gretl</application> can also handle such
	databases.  Well, actually, a subset of same: I have only
	worked on time-series databases containing monthly and
	quarterly series.  My university has the RATS G7 database
	containing data for the seven largest OECD economies and
	<application>gretl</application> will read that OK.</para>

      <tip><para>Visit the <application>gretl</application> <ulink
	    url="http://ricardo.ecn.wfu.edu/gretl/gretl_data.html">data 
	    page</ulink> for details and updates on available
	  data.</para></tip>

      </sect2>
    </sect1>

  <sect1 id="missing-data"><title>Missing data values</title>

    <para>These are represented internally as &minus;999.  In a
      native-format data file they should be represented the same way.
      When importing CSV data <application>gretl</application> accepts
      any of three representations of missing values: &minus;999, the
      string <literal>NA</literal>, or simply a blank cell.  Blank
      cells should, of course, be properly delimited, e.g.
      <literal>120.6,,5.38</literal>, in which the middle value is
      presumed missing.</para>

    <para>As for handling of missing values in the course of
      statistical analysis, <application>gretl</application> does the
      following:</para>

    <itemizedlist>
      <listitem><para>In calculating descriptive statistics (mean,
	  standard deviation, etc.) under the <command>summary</command>
	  command, missing values are simply skipped and the sample
	  size adjusted appropriately.</para></listitem>
      <listitem><para>In running regressions
	  <application>gretl</application> first adjusts the beginning
	  and end of the sample range, truncating the sample if need
	  be.  Missing values at the beginning of the sample are
	  common in time series work due to the inclusion of lags,
	  first differences and so on; missing values at the end of
	  the range are not uncommon due to differential updating of
	  series and possibly the inclusion of
	  leads.</para></listitem>  
      <listitem><para>If <application>gretl</application> detects any
	  missing values <quote>inside</quote> the (possibly
	  truncated) sample range for a regression it gives an error
	  message and refuses to produce estimates.</para></listitem>
    </itemizedlist>

    <para>Missing values in the middle of a data set are a problem. In
      a cross-sectional data set it may be possible to move the
      offending observations to the beginning or the end of the file,
      but obviously this won't do with time series data.  For those
      who know what they are doing (!), the <command>misszero</command> function
      is provided under the <command>genr</command> command.  By doing
    </para> 

    <para><command>genr foo = misszero(bar)</command>
    </para>

    <para>you can produce a series <command>foo</command> which is
      identical to <command>bar</command> except that any &minus;-999
      values become zeros.  Then you can use carefully constructed
      dummy variables to, in effect, drop the missing observations
      from the regression while retaining the surrounding sample
      range.<footnote><para><command>genr</command> also offers the inverse
      function to <command>misszero</command>, namely
      <command>zeromiss</command>, which replaces zeros in a given
      series with the missing observation code.</para>
      </footnote></para>

    </sect1>

  <sect1 id="scratch"><title>Creating a data file from scratch</title>

    <para>There are four ways to do this: (1) Use your favorite text
      editor to create the data file and header file independently.
      (2) Use your favorite spreadsheet to establish the data file,
      save it in Comma Separated Values format, then use
      <application>gretl</application>'s <quote>Import CSV</quote>
      option. (3) Use <application>gretl</application>'s built-in
      spreadsheet. (4) Select data series from a suitable
      database.</para>

    <para>Here are a few comments and details on these methods.</para>

    <sect2><title>Using a text editor</title>
      <para>This may be the method of choice for those who have a
	strong preference in editors, and who don't mind taking a few
	minutes to study the specifications for valid data and header
	files, as set out in <xref linkend="datafiles"/>
	above.</para>

      <para>Note that this method can be problematic under MS Windows
	due to the propensity of Microsoft tools
	<quote>helpfully</quote> to add the suffix
	<filename>.txt</filename> to the filename you specify when you
	save a plain text file&mdash;even if it already has a suffix.
	This will render the files unusable with
	<application>gretl</application>: you'll have to rename them
	manually, outside of the editor.</para>
    </sect2>

    <sect2><title>Using a separate spreadsheet</title>
      <para>This may be a good choice if you're comfortable with a
	particular spreadsheet. Of course if you use a spreadsheet
	you're able to carry out various transformations of the
	<quote>raw</quote> data with ease (adding things up, taking
	percentages or whatever): note, however, that you can also do
	this sort of thing easily&mdash;perhaps more
	easily&mdash;within <application>gretl</application>, by using
	the tools under the <quote>Data, Add variables</quote> menu
	and/or <quote>Variable, define new variable</quote>.</para>

	<para>If you take this option, please pay attention to the
	  specification of what your spreadsheet should look like
	  before you save it in CSV format (<xref linkend="other-formats"/>).
	  </para>
  
	<para>You may wish to establish a
	  <application>gretl</application> data set piece by piece,
	  via incremental importation of CSV data.  This is supported
	  as follows. When you have a datafile open already, and then
	  select the menu item <quote>File, Open data, import
	    CSV&hellip;</quote> the program checks for conformability
	  between the existing data set and the new import.  If the
	  data frequency, starting observation and ending observation
	  all seem to match, the new data are merged into the data
	  set.  If not, an error message is printed and the import is
	  refused.  If you want to import the new data <emphasis>in
	  place of</emphasis> the existing data set, you can achieve this by
	  first selecting <quote>File, Clear data
	    set</quote>.</para>
    </sect2>

    <sect2><title>Built-in spreadsheet</title>
      <para>Under <application>gretl</application>'s <quote>File,
	  Create data set</quote> menu you can choose the sort of data
	set you want to establish (e.g. quarterly time series,
	cross-sectional).  You will then be prompted for starting and
	ending dates (or observation numbers) and the name of the
	first variable to add to the data set. After supplying this
	information you will be faced with a simple spreadsheet into
	which you can type data values.  In the spreadsheet window,
	clicking the right mouse button will invoke a popup menu which
	enables you to add a new variable (column), to add an
	observation (append a row at the foot of the sheet), or to
	insert an observation at the selected point (move the data
	down and insert a blank row.)</para>
  
	<para>Once you have entered data into the spreadsheet you
	  import these into <application>gretl</application>'s
	  workspace using the spreadsheet's <quote>File, Apply
	    changes</quote> menu item.</para>

	<para>Please note that <application>gretl</application>'s
	  spreadsheet is quite basic and has no support for functions
	  or formulas.  Data transformations are done via the
	  <quote>Data</quote> or <quote>Variable</quote> menus in the
	  main <application>gretl</application> window.</para>
    </sect2>

    <sect2><title>Selecting from a database</title>
      <para>The remaining alternative is to establish your data set by
	selecting variables from a database.
	<application>gretl</application> comes with a database of US
	macroeconomic time series and, as mentioned above, the program
	will reads RATS 4 databases.</para>   
  
      <para>Begin with <application>gretl</application>'s <quote>File,
	  Browse databases</quote> menu item. This has three forks:
	<quote>gretl native</quote>, <quote>RATS 4</quote> and
	<quote>on database server</quote>.  You should be able to find
	the file <filename>bcih.bin</filename> in the file selector
	that opens if you choose the <quote>gretl native</quote>
	option&mdash;this file is supplied with the
	distribution.</para>
	
      <para>You won't find anything under <quote>RATS 4</quote> unless
	  you have purchased RATS data<footnote><para>See <ulink
		url="http://www.estima.com/">www.estima.com</ulink></para>
	  </footnote>.  If you do possess RATS data you should go into
	  <application>gretl</application>'s <quote>File, Preferences,
	    General</quote> dialog, select the Databases tab, and fill
	  in the correct path to your RATS files.</para> 
  
      <para>If your computer is connected to the internet you should
	find several databases (at Wake Forest University) under
	<quote>on database server</quote>.  You can browse these
	remotely; you also have the option of installing them onto
	your own computer.  The initial remote databases window has an
	item showing, for each file, whether it is already installed
	locally (and if so, if the local version is up to date with
	the version at Wake Forest).</para> 

      <para>Assuming you have managed to open a database you can
	import selected series into <application>gretl</application>'s
	workspace by using the <quote>Import</quote> menu item in the
	database window.</para>   

    </sect2>

    <sect2><title>Further notes</title>
  
      <para>
	<application>gretl</application> has no problem compacting
	data series of relatively high frequency (e.g. monthly) to a
	lower frequency (e.g. quarterly): this is done by averaging.
	But it has no way of converting lower frequency data to
	higher.  Therefore if you want to import series of various
	different frequencies from a database into
	<application>gretl</application> <emphasis>you must start by
	  importing a series of the lowest frequency you intend to
	  use.</emphasis> This will initialize your
	<application>gretl</application> data set to the low
	frequency, and higher frequency data can be imported
	subsequently (they will be compacted automatically).  If you
	start with a high frequency series you will not be able to
	import any series of lower frequency.</para>

    <para>If you establish a data set by any means other than creating
      a data file and header file with a text editor (that is, if you
      import CSV, use <application>gretl</application>'s spreadsheet,
      or select from a database) you would be well advised to save the
      data in <application>gretl</application>'s native format before
      quitting the program.  You can do this via the <quote>File, Save
	data&hellip;</quote> menu item.</para>
    </sect2>

    </sect1>

  </chapter>

  <chapter id="panel"><title>Panel data</title>

    <para>Panel data (pooled cross-section and time-series) require
      special care.  Here are some pointers.</para>

    <para>Consider a data set composed of observations on each of
      <emphasis>n</emphasis> cross-sectional units (countries, states,
      persons or whatever) in each of <emphasis>T</emphasis> periods.
      Let each observation comprise the values of
      <emphasis>m</emphasis> variables of interest.  The data set then
      contains <emphasis>mnT</emphasis> values.</para>

    <para>The data should be arranged <quote>by observation</quote>:
      each row represents an observation; each column contains the
      values of a particular variable. The data matrix then has
      <emphasis>nT</emphasis> rows and <emphasis>m</emphasis> columns.
      That leaves open the matter of how the rows should be arranged.
      There are two possibilities.<footnote><para>If you don't intend
	  to make any
	  conceptual or statistical distinction between
	  cross-sectional and temporal variation in the data you can
	  arrange the rows arbitrarily, but this is probably wasteful
	  of information.</para>
      </footnote></para>

    <itemizedlist>
      <listitem><para>Rows grouped by <emphasis>unit</emphasis>.
	  Think of the data matrix as composed of
	  <emphasis>n</emphasis> blocks, each having
	  <emphasis>T</emphasis> rows. The first block of
	  <emphasis>T</emphasis> rows contains the observations on
	  cross-sectional unit 1 for each of the periods; the next
	  block contains the observations on unit 2 for all periods;
	  and so on.  In effect, the data matrix is a set of
	  time-series data sets, stacked vertically.</para></listitem>
      <listitem><para>Rows grouped by <emphasis>period</emphasis>.
	  Think of the data matrix as composed of
	  <emphasis>T</emphasis> blocks, each having
	  <emphasis>n</emphasis> rows. The first
	  <emphasis>n</emphasis> rows contain the observations for
	  each of the cross-sectional units in period 1; the next
	  block contains the observations for all units in period 2;
	  and so on.  The data matrix is a set of cross-sectional data
	  sets, stacked vertically.</para></listitem>
    </itemizedlist>

    <para>You may use whichever arrangement is more convenient.  The
      first is perhaps easier to keep straight.  If you use the second
      then of course you must ensure that the cross-sectional units
      appear in the same order in each of the period data
      blocks.</para>

    <para>In either case you can use the frequency field in the
      <emphasis>observations</emphasis> line of the data header file (see
      <xref linkend="datafiles"/>) to make life a little easier.</para>

    <itemizedlist>
      <listitem><para><emphasis>Grouped by unit</emphasis>: Set the
	  frequency equal to <emphasis>T</emphasis>. Suppose you have
	  observations on 20 units in each of 5 time periods. Then
	  this observations line is appropriate: <literal>5 1.1
	    20.5</literal> (read: frequency 5, starting with the
	  observation for unit 1, period 1, and ending with the
	  observation for unit 20, period 5). Then, for instance, you
	  can refer to the observation for unit 2 in period 5 as
	  <literal>2.5</literal>, and that for unit 13 in period 1 as
	  <literal>13.1</literal>.</para></listitem>
      <listitem><para><emphasis>Grouped by period</emphasis>: Set the
	  frequency equal to <emphasis>n</emphasis>.  In this case if
	  you have observations on 20 units in each of 5 periods, the
	  observations line should be: <literal>20 1.01 5.20</literal>
	  (read: frequency 20, starting with the observation for
	  period 1, unit 01, and ending with the observation for
	  period 5, unit 20).  One refers to the observation for unit
	  2, period 5 as <literal>5.02</literal>.</para></listitem>
    </itemizedlist>

    <para>If you decide to construct a panel data set using a
      spreadsheet program first, then bring the data into
      <application>gretl</application> as a CSV import, the program
      will (probably) not at first recognize the special nature of the
      data.  You can fix this by using the command
      <command>setobs</command> (see <xref linkend="cmdref"/>) or the
      GUI menu item
      <quote>Sample, Set frequency, startobs&hellip;</quote>.</para>

    <sect1 id="dummies"><title>Dummy variables</title>

      <para>In a panel study you may wish to construct dummy variables
	of one or both of the following sorts: (a) dummies as unique
	identifiers for the cross-sectional units, and (b) dummies as
	unique identifiers of the time periods.  The former can be
	used, for instance, to allow the intercept of the regression
	to differ across the units, the latter to allow the intercept
	to differ across periods. (You will not want to include all of
	these dummies in a given regression!)</para>

      <para>You can use two special functions to create such dummies.
	These are found under the <quote>Data, Add variables</quote>
	menu in the GUI, or under the <command>genr</command> command
	in script mode or <application>gretlcli</application>.</para>

      <orderedlist>
	<listitem><para><quote>periodic dummies</quote> (script
	    command <command>genr dummy</command>).  The common use
	    for this command is to create a set of periodic dummy
	    variables up to the data frequency in a time-series study
	    (for instance a set of quarterly dummies for use in
	    seasonal adjustment). But it also works with panel data.
	    Note that the interpretation of the dummies created by
	    this command differs depending on whether the data rows
	    are grouped by unit or by period. If the grouping is by
	    <emphasis>unit</emphasis> (frequency
	    <emphasis>T</emphasis>) the resulting variables are
	    <emphasis>period dummies</emphasis> and there will be
	    <emphasis>T</emphasis> of them. For instance
	    <varname>dummy_2</varname> will have value 1 in each data
	    row corresponding to a period 2 observation, 0 otherwise.
	    If the grouping is by <emphasis>period</emphasis>
	    (frequency <emphasis>n</emphasis>) then
	    <emphasis>n</emphasis> <emphasis>unit dummies</emphasis>
	    will be generated: <varname>dummy_2</varname> will have
	    value 1 in each data row associated with cross-sectional
	    unit 2, 0 otherwise.</para></listitem>
	<listitem><para><quote>panel dummies</quote> (script command
	    <command>genr paneldum</command>).  This creates all the
	    dummies, unit and period, at a stroke.  The default
	    presumption is that the data rows are grouped by unit. The
	    unit dummies are named <varname>du_1</varname>,
	    <varname>du_2</varname> and so on, while the period
	    dummies are named <varname>dt_1</varname>,
	    <varname>dt_2</varname>, etc. The <varname>u</varname>
	    (for unit) and <varname>t</varname> (for time) in these
	    names will be wrong if the data rows are grouped by
	    period: to get them right in that setting use
	    <command>genr paneldum -o</command> (script mode
	    only).</para>
	</listitem>
      </orderedlist>

      <para>If a panel data set has the <literal>YEAR</literal> of the
	observation entered as one of the variables you can create a
	periodic dummy to pick out a particular year, e.g.
	<command>genr dum = (YEAR=1960)</command>.  You can also
	create periodic dummy variables using the modulus operator,
	<literal>%</literal>.  For instance, to create a dummy with
	value 1 for the first observation and every thirtieth
	observation thereafter, 0 otherwise, do</para>

      <programlisting>
	genr index genr dum = ((index-1)%30) = 0
      </programlisting>

    </sect1>

    <sect1 id="panel-lagged"><title>Using lagged values with panel data</title>

      <para>If the time periods are evenly spaced you may want to use
	lagged values of variables in a panel regression.  In this
	case arranging the data rows by <emphasis>unit</emphasis>
	(stacked time-series) is definitely preferable.</para>

      <para>Suppose you create a lag of variable<varname>x1</varname>, using
	<command>genr x1_1 = x1(-1)</command>.  The values of this
	variable will be mostly correct, but at the boundaries of the
	unit data blocks they will be spurious and unusable.  E.g. the
	value assigned to <varname>x1_1</varname> for observation 
	<literal>2.1</literal>
	is not the first lag of <varname>x1</varname> at all, but rather the
	last observation of <varname>x1</varname> for unit 1.</para>

      <para>If a lag of this sort is to be included in a regression
	you must ensure that the first observation from each unit
	block is dropped. One way to achieve this is to use Weighted
	Least Squares (<command>wls</command>) using an appropriate
	dummy variable as weight.  This dummy (call it
	<command>lagdum</command>) should have value 0 for the
	observations to be dropped, 1 otherwise.  In other words, it
	is complementary to a dummy variable for period 1.  Thus if
	you have already issued the command <command>genr
	  dummy</command> you can now do <command>genr lagdum = 1 -
	  dummy_1</command>.  If you have used <command>genr
	  paneldum</command> you would now say <command>genr lagdum =
	  1 - dt_1</command>. Either way, you can now do</para>

      <para>
	<command>wls lagdum y const x1_1 ...</command>
      </para>

      <para>to get a pooled regression using the first lag
	of <varname>x1</varname>, dropping all observations from period
	1.</para>

      <para>Another option is to use the <command>smpl</command> with the
	<command>-o</command> flag and a suitable dummy variable.  Here are
	illustrative commands, assuming the unit data blocks each
	contain 30 observations and we want to drop the first row of
	each:</para>

      <programlisting>
	(* create index variable *) 
	genr index 
	(* create dum = 0 for every 30th obs *) 
	genr dum = ((index-1)%30) > 0 
	(* sample based on this dummy *) 
	smpl -o dum 
	(* recreate the obs. structure, for 56 units *) 
	setobs 29 1.01 56.29
      </programlisting>

      <para>You can now run regressions on the restricted data set
	without having to use the <command>wls</command> command.  If
	you plan to reuse the restricted data set you may wish to save
	it using the <command>store</command> command (see <xref
	  linkend="cmdref"/> below).</para>
    </sect1>

    <sect1 id="PWT"><title>Illustration: the Penn World Table</title>

      <para>The Penn World Table (homepage <ulink
	  url="http://pwt.econ.upenn.edu/">here</ulink>) is a rich
	macroeconomic panel dataset, spanning 152 countries over the
	years 1950&ndash;1992.  The data are available in
	<application>gretl</application> format; please see the
	<application>gretl</application> <ulink
	  url="http://ricardo.ecn.wfu.edu/gretl/gretl_data.html">data
	  site</ulink> (this is a free download, although it is not
	included in the main <application>gretl</application>
	package).</para>  

      <para><xref linkend="examp-pwt"/> below opens
	<filename>pwt56_60_89.dat</filename>, a subset of the pwt
	containing data on 120 countries, 1960&ndash;89, for 20 variables,
	with no missing observations (the full data set, which is also
	supplied in the pwt package for
	<application>gretl</application>, has many missing
	observations). Total growth of real GDP, 1960&ndash;89, is
	calculated for each country and regressed against the 1960
	level of real GDP, to see if there is evidence for
	convergence.</para>

      <example id="examp-pwt">
	<title>Use of the Penn World Table</title>
	<programlisting>
	  open pwt56_60_89.dat 
	  (* for 1989 (last obs), lag 29 gives 1960, the first obs *) 
	  genr gdp60 = RGDPL(-29) 
	  (* find total growth of real GDP over 30 years *) 
	  genr gdpgro = (RGDPL - gdp60)/gdp60
	  (* restrict the sample to a 1989 cross-section *) 
	  smpl -r YEAR=1989 
	  (* Convergence?  Have countries with a lower base grown
	  faster? *) 
	  ols gdpgro const gdp60 
	  (* result: No! Try inverse relationship *) 
	  genr gdp60inv = 1/gdp60 ols gdpgro const gdp60inv 
	  (* No again.  Try dropping Africa? *) 
	  genr afdum = (CCODE = 1)
	  genr afslope = afdum * gdp60 
	  ols gdpgro const afdum gdp60 afslope2 
	</programlisting>
      </example>
    </sect1>

  </chapter>

  <chapter id="getting-more-data"><title>Getting more data</title>

    <para>Besides the data files included in the
      <application>gretl</application> distribution and the Penn World
      Table mentioned above, a large collection of data of various
      sorts is available from the <application>gretl</application>
      <ulink url="http://www.ecn.wfu.edu/gretl/gretl_data.html">data
	site</ulink>. Data sources include the Board of Governors of
      the Federal Reserve System (U.S. interest rates), the Federal
      Reserve Bank of St. Louis (numerous U.S. macroeconomic time
      series, up to the present), The National Bureau of Economic
      Research (their <quote>macro history</quote> data collection,
      plus some international and industry-level data sets), and the
      Bank of Japan.</para>

    <para>In addition <application>gretl</application> comes with some
      scripts that can be used to create databases using data
      available via the Internet. These can be found in the
      <filename>utils</filename> subdirectory of the source package
      (see
      <xref linkend="intro"/> above). To run the scripts you need to
      have <application>perl</application> installed on your computer,
      and you need to be connected to the Internet.</para>

  </chapter>

  <chapter id="graphs-plots"><title>Graphs and plots</title>

    <para>A separate program, namely gnuplot, is called to generate
      graphs. Gnuplot is a very full-featured graphing program with
      myriad options. It is available from <ulink
	url="http://www.gnuplot.org/">gnuplot.org</ulink> (but note
      that a copy of gnuplot is bundled with the MS Windows version of
      <application>gretl</application>).
      <application>gretl</application> gives you direct access, via a
      graphical interface, to only a small subset of gnuplot's options
      but it tries to choose sensible values for you; it also allows
      you to take complete control over graph details if you
      wish.</para>

    <para>Under MS Windows you can click at the top left corner of a
      graph window for a pull-down gnuplot menu that lets you choose
      various things (including copying the graph to the Windows
      clipboard and sending it to a printer).</para>

    <para>For full control over a graph, follow this procedure:</para>

    <orderedlist>
      <listitem><para>Close the graph window.</para></listitem>
      <listitem><para>From the Session menu, choose <quote>Add last
	    graph</quote>.</para></listitem>
      <listitem><para>In the session icon window, right-click on the
	  new graph icon and choose either <quote>Edit using
	    GUI</quote> or <quote>Edit plot
	    commands</quote>.</para></listitem>
    </orderedlist>

    <para>The <quote>Edit using GUI</quote> item pops up a graphical
      controller for gnuplot which lets you fine-tune various aspects
      of the graph.  The <quote>Edit plot commands</quote> item opens
      an editor window containing the actual gnuplot command file for
      generating the graph: this gives you full control over graph
      details&mdash;if you know something about gnuplot. To find out
      more, see the gnuplot <ulink
	url="http://ricardo.ecn.wfu.edu/gnuplot.html">online
	manual</ulink> or <ulink
	url="http://www.gnuplot.org">gnuplot.org</ulink>.</para>

    <para>See also the entry for <command>gnuplot</command> in 
      <xref linkend="cmdref"/> below&mdash;and the
      <command>graph</command> and <command>plot</command> commands
      for <quote>quick and dirty</quote> ASCII graphs.</para>

    <figure id="fig-plot" float="1" pgwide="1">
      <title>gretl's gnuplot controller</title>
      <screenshot>
	<screeninfo>gnuplot controller</screeninfo>
	<graphic fileref="figures/plot_control.png" align="center"/>
      </screenshot>
    </figure>

  </chapter>

  <chapter id="looping"><title>Loop constructs</title>

    <sect1 id="monte-carlo"><title>Monte Carlo simulations</title>

    <para><application>gretl</application> offers (limited) support
      for Monte Carlo simulations. To do such work you should either
      use the GUI client program in <quote>script mode</quote> (<xref
	linkend="scripts"/> above), or use the command-line
      client. The command <command>loop</command> opens a special mode
      in which the program accepts commands to be repeated a specified
      number of times. Within such a loop, only four commands can be
      used: <command>genr</command>, <command>ols</command>,
      <command>print</command> and <command>store</command>. With
      <command>genr</command> and <command>ols</command> it is
      possible to do quite a lot. You exit the mode of entering loop
      commands with <command>endloop</command>: at this point the
      stacked commands are executed.  Loops cannot be nested.</para>

    <para>The <command>ols</command> command gives special output in a loop
      context: the results from each individual regression are not
      printed, but rather you get a printout of (a) the mean value of
      each estimated coefficient across all the repetitions, (b) the
      standard deviation of those coefficient estimates, (c) the mean
      value of the estimated standard error for each coefficient, and
      (d) the standard deviation of the estimated standard errors.
      This makes sense only if there is some random input at each
      step.</para>  

    <para>The <command>print</command> command also behaves differently in the
      context of a loop.  It prints the mean and standard deviation of
      the variable, across the repetitions of the loop.  It is
      intended for use with variables that have a single value at each
      iteration, for example the error sum of squares from a
      regression.</para>

    <para>The<command>store</command> command (use only one of these
      per loop) writes out the values of the specified variables, from
      each time round the loop, to the specified file.  Thus it keeps
      a complete record of the variables.  This data file can then be
      read into the program and analysed.</para>

    <para>A simple example of loop code is shown in
      <xref linkend="script-loop"/>.</para>

    <example id="script-loop"><title>Simple loop code</title>
    <programlisting>
	(* create a blank data set with series length 50 *) 
	nulldata 50 
	genr x = uniform() 
	(* open a loop, to be repeated 100 times *) 
	loop 100 
	genr u = normal()
	(* construct the dependent variable *) 
	genr y = 10*x + 20*u 
	(* run OLS regression *) 
	ols y const x 
	(* grab the R-squared value from the regression *) 
	genr r2 = $rsq 
	(* arrange for statistics on R-squared to be printed *) 
	print r2 
	(* save the individual coefficient estimates *) 
	genr a = coeff(const) 
	genr b = coeff(x)
	(* and print them to file *) 
	store foo.dat a b endloop
    </programlisting>
    </example>

    <para>This loop will print out summary statistics for the `a' and
      `b' estimates across the 100 repetitions, and also for the &rsqu;
      values for the 100 regressions.  After running the loop,
      <filename>foo.dat</filename>, which contains the individual coefficient
      estimates from all the runs, can be opened in
      <application>gretl</application> to examine the frequency
      distribution of the estimates in detail. Please note that while
      comment lines are permitted in a loop (as shown in the example),
      they cannot run over more than one line.</para>

    <para>The command <command>nulldata</command> is useful for Monte
	Carlo work.  Instead of opening a <quote>real</quote> data
	set, <command>nulldata 50</command> (for instance) opens an
	empty data set, with only a constant, with a series length of
	50.  Constructed variables can then be added using the
	<command>genr</command> command.</para>  

    <para>See the <command>seed</command> command in 
      <xref linkend="cmdref"/> for information on generating
      repeatable pseudo-random series.</para>

  </sect1>

  <sect1 id="ils"><title>Iterated least squares</title>

    <para>A further form of loop structure is provided, designed
      primarily for carrying out iterated least squares.  Greene
      (2000, ch. 11) shows how this method can be used to estimate
      nonlinear models.</para>

    <para>To open this second sort of loop you need to specify a
	<emphasis>condition</emphasis> rather than an unconditional
	number of times to iterate.  This should take the form of the
	keyword <command>while</command> followed by an inequality:
	the left-hand term should be the name of a variable that is
	already defined; the right-hand side may be either a numerical
	constant or the name of another predefined variable.  For
	example,</para> 

    <para>
      <command> loop while essdiff > .00001 
      </command>
    </para>

    <para>Execution of the commands within the loop (i.e. until
      <command>endloop</command> is encountered) will continue so long
      as the specified condition evaluates as true.</para>

    <para>I assume that if you specify a <quote>number of
	times</quote> loop you are probably doing a Monte Carlo
      analysis, and hence you're not interested in the results from
      each individual iteration but rather the moments of certain
      variables over the ensemble of iterations.  On the other hand,
      if you specify a <quote>while</quote> loop you're probably doing
      something like iterated least squares, and so you'd like to see
      the final result&mdash;as well, perhaps, as the value of some
      variable(s) (e.g. the error sum of squares from a regression)
      from each time round the loop. The behavior of the
      <command>print</command> and <command>ols</command> commands are
      tailored to this assumption.  In a <quote>while</quote> loop
      <command>print</command> behaves as usual; thus you get a
      printout of the specified variable(s) from each iteration.  The
      <command>ols</command> command prints out the results from the
      final estimation.</para>  

    <para><xref linkend="greene-consump"/> uses a <quote>while</quote>
      loop to replicate the estimation of a nonlinear consumption
      function of the form 
	<inlineequation>
	  <inlinegraphic fileref="figures/greeneC.png"/>
	  <texmath>
	    $C = \alpha + \beta Y^{\gamma} + \epsilon$
	  </texmath>
	</inlineequation>
	as presented in Greene (2000, Example 11.3).  This
      script is included in the <application>gretl</application>
      distribution under the name <filename>greene11_3.inp</filename>;
      you can find it in <application>gretl</application> under the
      menu item <quote>File, Open command file, practice file,
	Greene...</quote>.</para>

    <example id="greene-consump"><title>Nonlinear consumption function</title>
    <programlisting>
      open greene11_3.dat 
      (* run initial OLS *) 
      ols C 0 Y 
      genr essbak = $ess 
      genr essdiff = 1
      genr b0 = coeff(Y) 
      genr gamma0 = 1 
      (* form the linearized variables *) 
      genr C0 = C + gamma0 * b0 * Y^gamma0 * log(Y) 
      genr x1 = Y^gamma0 
      genr x2 = b0 * Y^gamma0 * log(Y) 
      (* iterate OLS till the error sum of squares converges *) 
      loop while essdiff > .00001 
      ols C0 0 x1 x2 -o 
      genr b0 = coeff(x1) 
      genr gamma0 = coeff(x2) 
      genr C0 = C + gamma0 * b0 * Y^gamma0 * log(Y) 
      genr x1 = Y^gamma0 genr x2 = b0 * Y^gamma0 * log(Y) 
      genr ess = $ess genr
      essdiff = abs(ess - essbak)/essbak 
      genr essbak = ess 
      endloop 
      (* print parameter estimates using their "proper names" *) 
      genr alpha = coeff(0) 
      genr beta = coeff(x1) 
      genr gamma = coeff(x2)
      print alpha beta gamma
    </programlisting>
      </example>

<!-- 
% Need section here on <quote>for i=</quote> loops
% Also need to change the following to reflect opening of databases,
% automatic attempt to distinguish datafiles and scripts. -->


  </sect1>
  </chapter>

  <chapter id="optarg"><title>Otions, arguments and path-searching</title>
  

  <sect1 id="optarg1"><title><application>gretl</application></title>

    <para>
      <command>gretl</command> (under MS Windows,
      <command>gretlw32</command>)
    </para>

    <para>
      &mdash; Opens the program and waits for user input.
    </para>

    <para>
      <command>gretl</command> <replaceable>datafile</replaceable>
    </para>

    <para>
      &mdash; Starts the program with the specified datafile in its
      workspace. The data file may be in native
      <application>gretl</application> format, CSV format, or BOX1
      format (see <xref linkend="datafiles"/> above).  The
      program will try to detect the format of the file and treat it
      appropriately. See also <xref linkend="path-search"/> below for
      path-searching behavior.
    </para>

    <para>
      <command>gretl --help</command> (or <command>gretl -h</command>)
    </para>

    <para>
      &mdash; Print a brief summary of usage and exit.
    </para>

    <para>
      <command>gretl --version</command> (or <command>gretl
	-v</command>)
    </para>

    <para>
      &mdash; Print version identification for the program and exit
    </para>

    <para>
      <command>gretl --run</command> <replaceable>scriptfile</replaceable> (or
      <command>gretl -r</command>
      <replaceable>scriptfile</replaceable>)</para>

    <para>&mdash; Start the program and open a window displaying the
      specified script file, ready to run.  See <xref linkend="path-search"/>
      below for path-searching behavior.</para>

    <para>Some things in <application>gretl</application> are
      configurable under the <quote>File, Preferences</quote>
      menu.</para>  

    <itemizedlist>
      <listitem><para>The user's base directory for
	  <application>gretl</application>-related
	  files.</para></listitem>
      <listitem><para>The base directory for
	  <application>gretl</application>'s shared
	  files.</para></listitem>
      <listitem><para>The command to launch GNU R (see 
	  <xref linkend="app-c"/>).</para></listitem>
      <listitem><para>The directory in which to start looking for
	  native <application>gretl</application>
	  databases.</para></listitem>
      <listitem><para>The directory in which to start looking for RATS
	  4 databases.</para></listitem>
      <listitem><para>The IP number of the
	  <application>gretl</application> database server to
	  access.</para></listitem>
      <listitem><para>The calculator and editor programs to launch
	  from the toolbar.</para></listitem>
      <listitem><para>The monospaced font to be used in
	  <application>gretl</application> screen
	  output.</para></listitem>
    </itemizedlist>

    <para>There are also some check boxes.  Checking the
      <quote>expert</quote> box quells some warnings that are
      otherwise issued.  Unchecking <quote>Tell me about gretl
	updates</quote> stops <application>gretl</application> from
      attempting to query the update server at start-up. Unchecking
      <quote>Show gretl toolbar</quote> turns the icon toolbar
      off.</para>

    <para>Settings chosen in this way are stored in a file named
      <filename>.gretlrc</filename> in the user's home directory on
      unix-like systems, or in a file named
      <filename>gretl.rc</filename> in the user's gretl directory
      (default <filename>c:\userdata\gretl\user</filename>) under MS
      Windows.</para>

  </sect1>

  <sect1 id="optarg2"><title><application>gretlcli</application></title>

    <para><command>gretlcli</command></para>

    <para>&mdash; Opens the program and waits for user input.
    </para>

    <para><command>gretlcli</command>
      <replaceable>datafile</replaceable></para>  

    <para>&mdash; Starts the program with the specified datafile in
      its workspace. The data file may be in native
      <application>gretl</application> format, CSV format, or BOX1
      format (see <xref linkend="datafiles"/>).  The
      program will try to detect the format of the file and treat it
      appropriately. See also <xref linkend="path-search"/> for
      path-searching behavior.</para>

    <para><command>gretlcli --help</command> (or <command>gretlcli
	-h</command>)</para>

    <para>&mdash; Prints a brief summary of usage.</para>

    <para><command>gretlcli --version</command> (or <command>gretlcli
	-v</command>)</para>

    <para>&mdash; Prints version identification for the
      program.</para>

    <para><command>gretlcli --run</command>
      <replaceable>scriptfile</replaceable> (or <command>gretlcli
	-r</command> <replaceable>scriptfile</replaceable>)</para>

    <para>&mdash; Execute the commands in
      <replaceable>scriptfile</replaceable> then hand over input to
      the command line.  See <xref linkend="path-search"/> for
      path-searching behavior.</para>  

    <para><command>gretlcli --batch</command>
      <replaceable>scriptfile</replaceable> (or <command>gretlcli
	-b</command> <replaceable>scriptfile</replaceable>)</para>

    <para>&mdash; Execute the commands in
      <replaceable>scriptfile</replaceable> then exit.  When using
      this option you will probably want to redirect output to a file.
      See <xref linkend="path-search"/> for path-searching
      behavior.</para>

    <para>When using the <command>--run</command> and
      <command>--batch</command> options, the script file in question
      must call for a data file to be opened. This can be done using
      the <command>open</command> command within the script. For
      backward compatibility with Ramanathan's original
      <application>ESL</application> program another mechanism is
      offered (<application>ESL</application> doesn't have the
      <command>open</command> command).  A line of the form:
      
      <programlisting> 
	(* ! myfile.dat *) 
      </programlisting> 

      will (a) cause <application>gretlcli</application> to load
      <filename>myfile.dat</filename>, but will (b) be ignored as a
      comment by the original <application>ESL</application>.  Note
      the specification carefully: There is exactly one space between
      the begin comment marker, <literal>(*</literal>, and the
      <literal>!</literal>; there is exactly one space between the
      <literal>!</literal> and the name of the data file.</para>

    <para>One further kludge enables <application>gretl</application>
      and <application>gretlcli</application> to get datafile
      information from the <application>ESL</application>
      <quote>practice files</quote> included with the
      <application>gretl</application> package.  A typical practice
      file begins like this:</para>

    <para>
      <literal>
      (* PS4.1, using data file DATA4-1, for reproducing Table 4.2 *)
	</literal>
    </para>

    <para>This algorithm is used: if an input line begins with the
      comment marker, search it for the string
      <literal>DATA</literal> (upper case).  If this is found,
      extract the string from the <literal>D</literal> up to the
      next space or comma, put it into lower case, and treat it as the
      name of a data file to be opened.</para>

    </sect1>

    <sect1 id="path-search"><title>Path searching</title>

      <para>When the name of a data file or script file is supplied to
	<application>gretl</application> or
	<application>gretlcli</application> on the command line (see <xref
	linkend="optarg1"/> and <xref linkend="optarg2"/>), the file is
	looked for as follows:</para>

      <orderedlist>
	<listitem><para><quote>As is</quote>.  That is, in the current
	    working directory or, if a full path is specified, at the
	    specified location.</para>
	</listitem>
	<listitem><para>In the user's gretl directory (see <xref
	    linkend="tab-path"/> for the default values).</para>
	</listitem>
	<listitem><para>In any immediate sub-directory of the user's
	    gretl directory.</para></listitem>
	<listitem><para>In the case of a data file, search continues
	    with the main <application>gretl</application> data
	    directory. In the case of a script file, the search
	    proceeds to the system script directory.  See <xref
	    linkend="tab-path"/> for the default settings.</para>
	</listitem>
	<listitem><para>In the case of data files the search then
	    proceeds to all immediate sub-directories of the main data
	    directory.</para>
	</listitem>
      </orderedlist>

      <table id="tab-path" frame="none">
	<title>Default path settings</title>
	<tgroup cols="3">
	  <thead>
	    <row>
	      <entry>&nbsp;</entry>
	      <entry>Linux/unix</entry>
	      <entry>MS Windows</entry>
	    </row>
	  </thead>
	  <tbody>
	    <row>
	      <entry>User directory</entry>
	      <entry><filename>$HOME/.gretl</filename></entry>
	      <entry><filename>PREFIX\gretl\user</filename></entry>
	    </row>
	    <row>
	      <entry>System data directory</entry>
	      <entry><filename>PREFIX/share/gretl/data</filename></entry>
	      <entry><filename>PREFIX\gretl\data</filename></entry>
	    </row>
	    <row>
	      <entry>System script directory</entry>
	      <entry><filename>PREFIX/share/gretl/scripts</filename></entry>
	      <entry><filename>PREFIX\gretl\scripts</filename></entry>
	    </row>
	  </tbody>
	</tgroup>
	<tgroup cols="1">
	  <tbody>
	    <row>
	      <entry><emphasis>Note:</emphasis>
		<filename>PREFIX</filename> denotes the base
		directory chosen at the time
		<application>gretl</application> is installed.
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </table>
		

      <para>Thus it is not necessary to specify the full path for a
	data or script file unless you wish to override the automatic
	searching mechanism. (This also applies within
	<application>gretlcli</application>, when you supply a
	filename as an argument to the <command>open</command> or
	<command>run</command> commands.)</para>

      <para>When a command script contains an instruction to open a
	data file, the search order for the data file is as stated
	above, except that the directory containing the script is also
	searched, immediately after trying to find the data file
	<quote>as is</quote>.
      </para>

      <sect2 id="MS-behave"><title>MS Windows</title>

	<para>
	  Under MS Windows the default behavior of
	  <application>gretl</application> and
	  <application>gretlcli</application> is controlled by the
	  configuration file <filename>libgretl.cfg</filename>. This
	  is first searched for in the directory containing the
	  <application>gretlcli</application> executable; if it is not
	  found there it is looked for in the root directory of the
	  current drive (i.e. as <filename>libgretl.cfg</filename>).
	  This file specifies, in order, the system gretl directory,
	  the user's home gretl directory and the path to the gnuplot
	  executable. For example, it might read as follows:

	  <programlisting> 
	    c:\userdata\gretl 
	    c:\userdata\gretl\user
	    c:\userdata\gp371w32\wgnupl32.exe 
	  </programlisting> 

	  When <application>gretl</application> for win32 is
	  installed, a version of this file, appropriate to the user's
	  choice of where to install the package, is written out
	  automatically.  You should not have to bother with this file
	  unless you should happen to want to move a
	  <application>gretl</application> installation.  In that case
	  you'll have to edit <filename>libgretl.cfg</filename>
	  appropriately, maintaining its plain ASCII character and
	  exact filename (N.B. no stupid
	  <quote><filename>.txt</filename></quote> extension as kindly
	  supplied by MS Notepad!).
	</para>

      </sect2>
    </sect1>

  </chapter>



  <chapter id="cmdref"><title>Command Reference</title>

    <sect1 id="cmd-intro"><title>Introduction</title>
    <para>
	The commands defined below may be executed in the command-line
	client program, <application>gretlcli</application>.  They may
	also be placed in a <quote>script</quote> file for execution
	in the GUI, <application>gretl</application>, or entered using
	the latter's <quote>console mode</quote>.  In most cases the
	syntax given below also applies when you are presented with a
	line to type in a dialog box in the GUI (but see also
	<application>gretl</application>'s online help), except that
	you should <emphasis>not</emphasis> type the initial command
	word&mdash;it is implicit from the context.  One other
	difference is that you should not type the
	<command>-o</command> flag for regression commands in GUI
	dialog boxes: there is a menu item for displaying the
	coefficient variance&ndash;covariance matrix (which is the
	effect of <command>-o</command> in regression commands).
      </para>

    <para>
      The following conventions are used below:</para>

    <itemizedlist>

      <listitem><para>
	  A <literal>typewriter font</literal> is used for material that you
	  would type directly, and also for internal names of
	  variables.
	</para></listitem>

      <listitem><para>Terms in <replaceable>italics</replaceable> are
	    place-holders: you should substitute something specific,
	    e.g. you might type <varname>income</varname> in place of
	    the generic <replaceable>xvar</replaceable>.
	</para></listitem>

      <listitem><para><literal>[ -o ]</literal> means that the flag
	  <command>-o</command> is optional: you may type it or not
	  (but in any case don't type the brackets).
	</para></listitem>

      <listitem><para>The phrase <quote>estimation command</quote>
	  means any one of <command>ols</command>,
	  <command>hilu</command>, <command>corc</command>,
	  <command>ar</command>, <command>arch</command>,
	  <command>hsk</command>, <command>tsls</command>,
	  <command>wls</command>, <command>hccm</command>,
	  <command>add</command>, <command>omit</command>.
	</para></listitem>

    </itemizedlist>

    <para>
      Section and Chapter references below are to Ramu Ramanathan
      (1998).
    </para>

    </sect1>

    <sect1 id="cmd-cmd">
      <title><application>gretl</application> commands</title>

    <sect2 id="add"><title>add</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>add</command>
		<replaceable>varlist</replaceable> 
		&opto;
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry>
		<command>add 5 7 9</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>add xx yy zz -o</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Must be invoked after an estimation command.  The variables in
	<replaceable>varlist</replaceable> will be added to the
	previous model and the new model estimated.  If more than one
	variable is added, then the <emphasis>F</emphasis> statistic
	for the added variables will be printed (for the OLS procedure
	only) along with the p-value for it.  A p-value below 0.05
	means that the coefficients are jointly significant at the 5
	percent level. A number of internal variables may be retrieved
	using the <command>genr</command> command, provided
	<command>genr</command> is invoked directly after this
	command. The <literal>-o</literal> flag causes the coefficient
	variance&ndash;covariance matrix to be printed.
      </para>

    </sect2>

    <sect2 id="addto"><title>addto</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>addto</command> 
		<replaceable>modelID varlist</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Example:</entry>
	      <entry>
		<command>addto 2 5 7 9</command>
	      </entry> 
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Works like the <command>add</command> command, except that you
	specify a previous model (using its ID number, which is
	printed at the start of the model output) to take as the base
	for adding variables.  The example above adds variables number
	5, 7 and 9 to Model 2.
      </para>

    </sect2>

    <sect2 id="adf"><title>adf</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry><command>adf</command> 
		<replaceable>order varname</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Example:</entry>
	      <entry>
		<command>adf 2 x1</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>


      <para>
	  Computes statistics for two Dickey-Fuller tests.  In each
	  case the null hypothesis is that the variable in question
	  exhibits a unit root. The first is a
	  <emphasis>t</emphasis>-test based on the model 
	  <informalequation>
	    <graphic fileref="figures/adf1.png"/>
	    <texmath>
	      $$(1-L)x_t=m+gx_{t-1}+\epsilon_t$$
	    </texmath>
	  </informalequation>
	  The null hypothesis is that
	  <emphasis>g</emphasis> = 0. The second (augmented) test
	  proceeds by estimating an unrestricted regression (with
	  regressors a constant, a time trend, the first lag of the
	  variable, and <replaceable>order</replaceable> lags of the
	  first difference) and a restricted version (dropping the
	  time trend and the first lag).  The test statistic is
	  <informalequation>
	    <graphic fileref="figures/adf2.png"/>
	    <texmath>
	      $$F_{2,T-k}=\frac{(ESS_r-ESS_u)/2}{ESS_u/(T-k)}$$
	    </texmath>
	  </informalequation>
	  where <emphasis>T</emphasis> is the sample size,
	  <emphasis>k</emphasis> the number of parameters in the
	  unrestricted model, and the subscripts
	  <emphasis>u</emphasis> and <emphasis>r</emphasis> denote the
	  unrestricted and restricted models respectively. Note that
	  the critical values for these statistics are not the usual
	  ones; a p-value range is printed, when it can be determined.
      </para>
    </sect2>	

    <sect2 id="ar"><title>ar</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>ar</command>
		<replaceable>lags</replaceable>
		<literal>;</literal>
		<replaceable>depvar indepvars</replaceable> 
		&opto; 
	      </entry>
	    </row>
	    <row>
	      <entry>Example:</entry>
	      <entry><command>ar 1 3 4 ; y 0 x1 x2 x3</command>
	      </entry> 
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Computes the estimates of a model using the generalized
	Cochrane&ndash;Orcutt iterative procedure (see Section 9.5 of
	Ramanathan). Iteration is terminated when successive error sum
	of squares do not vary by more than 0.005 percent or when 20
	iterations have been done. <replaceable>lags</replaceable> is
	a list of lags in the residuals, terminated by a semicolon. In
	the above example, the error term is specified as 
	<inlineequation>
	  <inlinegraphic fileref="figures/corc.png"/>
	  <texmath>
	    $u_t = \rho_1u_{t-1} + \rho_3 u_{t-3} + \rho_4 u_{t-4} +
	    e_t$
	  </texmath>
	</inlineequation></para>
	<para>
	<replaceable>depvar</replaceable> is the dependent variable
	and <replaceable>indepvars</replaceable> is the list of
	independent variables separated by spaces.  Use the number
	zero for a constant term.  If the <literal>-o</literal> flag
	is present, the covariance matrix of regression coefficients
	will be printed. Residuals of the transformed regression are
	stored under the name <varname>uhat</varname>, which can be
	retrieved by <command>genr</command>.  A number of other
	internal variables may be retrieved using the
	<command>genr</command> command, provided
	<command>genr</command> is invoked after this command.
      </para>

    </sect2>

    <sect2 id="arch"><title>arch</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>arch</command> 
		<replaceable>order depvar indepvars</replaceable> 
		&opto;
	      </entry>
	    </row>
	    <row>
	      <entry>Example:</entry>
	      <entry>
		<command>arch 4 y 0 x1 x2 x3</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	This command tests the model for ARCH (Autoregressive
	Conditional Heteroskedasticity) of the lag order specified in
	<replaceable>order</replaceable>, which must be an integer.
	If the LM test statistic has p-value below 0.10, then ARCH
	estimation is also carried out.  If the predicted variance of
	any observation in the auxiliary regression is not positive,
	then the corresponding 
	<inlineequation>
	  <inlinegraphic fileref="figures/uhatsq.png"/>
	  <texmath>
	    $\hat{u}^2$
	  </texmath>
        </inlineequation> is used instead. Weighted
	least square estimation is then performed on the original
	model. The flag <literal>-o</literal> calls for the coefficient
	covariance matrix.
	
      </para>

    </sect2>

    <sect2 id="chow"><title>chow</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>chow</command>
		<replaceable>obs</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:  </entry>
	      <entry>
		<command>chow 25</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>chow 1988.1</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Must follow an OLS regression.  Creates a dummy variable which
	equals 1 from the split point specified by
	<replaceable>obs</replaceable> to the end of the sample, 0
	otherwise, and also creates interaction terms between this
	dummy and the original independent variables.  An augmented
	regression is run including these terms and an
	<emphasis>F</emphasis> statistic is calculated, taking the
	augmented regression as the unrestricted and the original as
	restricted.  This statistic is appropriate for testing the
	null hypothesis of no structural break at the given split
	point.
      </para>

    </sect2>

    <sect2 id="coint"><title>coint</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>   
		<command>coint</command> 
		<replaceable>order depvar indepvar</replaceable>
	      </entry>
	    </row>
	    <row><entry>Examples:</entry>
	      <entry>
		<command>coint 2 y x</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>coint 4 y x1 x2</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Carries out Augmented Dickey&ndash;Fuller tests on the null
	hypothesis that each of the variables listed has a unit root,
	using the given lag order.  The cointegrating regression is
	estimated, and an ADF test is run on the residuals from this
	regression.  The Durbin&ndash;Watson statistic for the
	cointegrating regression is also given.  Note that none of
	these test statistics can be referred to the usual statistical
	tables.
      </para>

    </sect2>

    <sect2 id="corc"><title>corc</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>corc</command>
		<replaceable>depvar indepvars</replaceable>
		&opto;
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry> 
		<command>corc 1 0 2 4 6 7</command> 
	      </entry>
	    </row>
	    <row><entry></entry>
	      <entry>  
		<command>corc -o 1 0 2 4 6 7</command>
	      </entry>
	    </row>
	    <row><entry></entry>
	      <entry> 
		<command>corc y 0 x1 x2 x3</command>
	      </entry>
	    </row>
	    <row><entry></entry>
	      <entry> 
		<command>corc -o y 0 x1 x2 x3</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Computes the estimates of a model using the
	Cochrane&ndash;Orcutt iterative procedure (see Section 9.4 of
	Ramanathan) with <replaceable>depvar</replaceable> as the
	dependent variable and <replaceable>indepvars</replaceable> as
	the list of independent variables separated by spaces.  Use
	the number zero for a constant term. Iteration is terminated
	when successive &rho; values do not differ by more than 0.001
	or when 20 iterations have been done.  If the
	<literal>-o</literal> flag is present, the covariance matrix
	of regression coefficients will be printed.  Residuals of this
	transformed regression are stored under the name
	<varname>uhat</varname>.  A number of other internal variables
	may be retrieved using the <command>genr</command> command,
	provided <command>genr</command> is invoked immediately after
	this command.
      </para>

    </sect2>

    <sect2 id="corr"><title>corr</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>corr</command>
		[ <replaceable>varlist</replaceable> ]
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry>
		<command>corr 1 3 5</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry> 
		<command>corr y x1 x2 x3</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	<command>corr</command> prints correlation coefficients for
	all pairs of variables in the data set (missing values denoted
	by &minus;999 are skipped).  <command>corr</command>
	<replaceable>varlist</replaceable> prints the correlation
	coefficients for the variables in the list.
      </para>

    </sect2>


    <sect2 id="corrgm"><title>corrgm</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>  
		<command>corrgm</command>
		<replaceable>variable</replaceable> 
		[ <replaceable>maxlag</replaceable> ]
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Prints the values of the autocorrelation function for the
	<replaceable>variable</replaceable> specified (either by name
	or number).  See Ramanathan, Section 11.7.  It is thus
	<inlineequation>
	  <inlinegraphic fileref="figures/autocorr.png"/>
	  <texmath>
	      $\rho(u_t, u_{t-s})$
	    </texmath>
	</inlineequation> where 
	<emphasis>u</emphasis><subscript><emphasis>t</emphasis></subscript> is the
	<emphasis>t</emphasis>th observation of the variable 
	<emphasis>u</emphasis>u and <emphasis>s</emphasis> is the number of lags.
      </para>

      <para>
	  The partial autocorrelations are also shown: these are net
	  of the effects of intervening lags.  The command also graphs
	  the correlogram and prints the Box-Pierce
	  <emphasis>Q</emphasis> statistic for testing the null
	  hypothesis that the series is <quote>white noise</quote>.
	  This is asymptotically distributed as
	  &khgr;<superscript>2</superscript> with degrees of freedom
	  equal to the number of lags used.
      </para>

      <para>
	If an (optional) integer <replaceable>maxlag</replaceable>
	value is supplied the length of the correlogram is limited to
	at most that number of lags, otherwise the length is
	determined automatically.
      </para>

    </sect2>

    <sect2 id="criteria"><title>criteria</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>criteria</command> 
		<replaceable>ess T k</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Example: </entry>
	      <entry> 
		<command>criteria 23.45 45 8</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Computes the model selection statistics (see Ramanathan,
	Section 4.3), given <replaceable>ess</replaceable> (error sum
	of squares), the number of observations
	(<emphasis>T</emphasis>), and the number of coefficients
	(<emphasis>k</emphasis>). <emphasis>T</emphasis>,
	<emphasis>k</emphasis>, and <replaceable>ess</replaceable> may
	be numerical values or names of previously defined variables.
      </para>

    </sect2>

    <sect2 id="cusum"><title>cusum</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>cusum</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Must follow the estimation of a model via OLS.  Performs the
	CUSUM test for parameter stability.  A series of (scaled)
	one-step ahead forecast errors is obtained by running a series
	of regressions: the first regression uses the first
	<emphasis>k</emphasis> observations and is used to generate a
	prediction of the dependent variable at observation at
	observation <emphasis>k</emphasis> + 1; the second uses the
	first <emphasis>k</emphasis> + 1 observations and generates a
	prediction for observation <emphasis>k</emphasis> + 2, and so
	on (where <emphasis>k</emphasis> is the number of parameters
	in the original model).  The cumulated sum of the scaled
	forecast errors is printed and graphed.  The null hypothesis
	of parameter stability is rejected at the 5 percent
	significance level if the cumulated sum strays outside of the
	95 percent confidence band.
      </para>

      <para>
	The Harvey&ndash;Collier <emphasis>t</emphasis> statistic for
	testing the null hypothesis of parameter stability is also
	quoted.  See Chapter 7 of Greene's <citetitle>Econometric
	  Analysis</citetitle> for details.
      </para>

    </sect2>

    <sect2 id="delete"><title>delete</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>delete</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Removes the last (highest numbered) variable from the current
	data set.  <emphasis>Use with caution</emphasis>: no
	confirmation is asked.  Can be useful for getting rid of
	temporary dummy variables.  There is no provision for deleting
	any but the last variable.
      </para>

    </sect2>

    <sect2 id="diff"><title>diff</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>      
		<command>diff</command>
		<replaceable>varlist</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	The first difference of each variable in
	<replaceable>varlist</replaceable> is obtained and the result
	stored in a new variable with the prefix
	<varname>d_</varname>.  Thus <command>diff x y</command>
	creates the new variables <varname>d_x = x(t) -
	  x(t-1)</varname> and <varname>d_y = y(t) - y(t-1)</varname>.
      </para>

    </sect2>

    <sect2 id="endloop"><title>endloop</title>

      <para>
	Terminates a simulation loop.  See <command>loop</command>.
      </para>

    </sect2>

    <sect2 id="eqnprint"><title>eqnprint</title>

      <para>
	Must follow the estimation of a model via OLS.  Prints the
	estimated model in the form of a &latex; equation, to a file
	with a name of the form <filename>equation_N.tex</filename>,
	where <varname>N</varname> is the number of models estimated
	to date in the current session.  This can be incorporated in a
	&latex; document.  See also <command>tabprint</command>.
      </para>

    </sect2>

    <sect2 id="fcast"><title>fcast</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>fcast</command>
		[ <replaceable>startobs endobs</replaceable> ]
		<replaceable>newvarname</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry> 
		<command>fcast 1997.1 1999.4 f1</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry> 
		<command>fcast f2</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Must follow an estimation command.  Forecasts are generated
	for the specified range (or the largest possible range if no
	<replaceable>startobs</replaceable> and
	<replaceable>endobs</replaceable> are given) and the values
	saved as <replaceable>newvarname</replaceable>, which can be
	printed, graphed, or plotted. The right-hand side variables
	are those in the original model.  There is no provision to
	substitute other variables.  If an autoregressive error
	process is specified (for <command>hilu</command>,
	<command>corc</command>, and <command>ar</command>) the
	forecast is conditional one step ahead and incorporates the
	error process.
      </para>

    </sect2>

    <sect2 id="fcasterr"><title>fcasterr</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>fcasterr</command> 
		<replaceable>startobs endobs</replaceable> 
		&opto;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	After estimating an OLS model which includes a constant and at
	least one independent variable (these restrictions may be
	relaxed at some point) you can use this command to print out
	fitted values over the specified observation range, along with
	the estimated standard errors of those predictions and 95
	percent confidence intervals.  If the <literal>-o</literal> flag is
	given the results will also be displayed using gnuplot.  The
	augmented regression method of Salkever (1976) is used to
	generate the forecast standard errors.
      </para>

    </sect2>

    <sect2 id="fit"><title>fit</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>  
		<command>fit</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	The <command>fit</command> command (must follow an estimation
	command) is a shortcut for the <command>fcast</command>
	command.  It generates fitted values, in a series called
	<command>autofit</command>, for the current sample, based on
	the last regression.  In the case of time-series models,
	<command>fit</command> also pops up a gnuplot graph of fitted
	and actual values of the dependent variable against time.
      </para>

    </sect2>

    <sect2 id="freq"><title>freq</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>  
		<command>freq</command>
		<replaceable>var</replaceable> 
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	  Prints the frequency distribution for
	  <replaceable>var</replaceable> (given by name or number);
	  the results of a &khgr;<superscript>2</superscript> test for
	  normality are also reported. In interactive mode a gnuplot
	  graph of the distribution is generated.  
      </para>

      </sect2>

    <sect2 id="genr"><title>genr</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>genr</command> 
		<replaceable>newvar</replaceable>
		  = <replaceable>formula</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	  Creates new variables, usually through transformations of
	  existing variables. See also <command>diff</command>,
	  <command>logs</command>, <command>lags</command>,
	  <command>ldiff</command>, <command>multiply</command> and
	  <command>square</command> for shortcuts.
      </para>

      <para>
	  Supported <emphasis>arithmetical operators</emphasis> are,
	  in order of precedence: <literal>^</literal>
	  (exponentiation); <literal>*</literal>, <literal>/</literal>
	  and <literal>%</literal> (modulus or remainder);
	  <literal>+</literal> and <literal>-</literal>. 
      </para>

      <para>
	  The available <emphasis>Boolean operators</emphasis> are
	  (again, in order of precedence): <literal>!</literal>
	  (negation), <literal>&amp;</literal> (logical AND),
	  <literal>|</literal> (logical OR), <literal>&gt;</literal>,
	  <literal>&lt;</literal>, <literal>=</literal> and
	  <literal>!=</literal> (not equal to).  The Boolean operators
	  can be used in constructing dummy variables: for instance
	  <literal>(x > 10)</literal> returns 1 if
	  <literal>x</literal> &gt; 10, 0 otherwise. Supported
	  <emphasis>functions</emphasis> fall into these groups:
      </para>

      <itemizedlist>
	<listitem><para>Standard math functions:
	    <function>abs</function>, <function>cos</function>,
	    <function>exp</function>, <function>int</function>
	    (integer part), <function>ln</function> (natural log:
	    <function>log</function> is a synonym),
	    <function>sin</function>,
	    <function>sqrt</function>.</para>
	</listitem>
	<listitem><para>Statistical functions:
	    <function>mean</function> (arithmetic mean),
	    <function>median</function>, <function>var</function>
	    (variance) <function>sd</function> (standard deviation),
	    <function>sum</function>, <function>cov</function>
	    (covariance), <function>corr</function> (correlation
	    coefficient).</para>
	</listitem>
	<listitem><para>Time-series functions:
	    <function>lag</function>, <function>lead</function>,
	    <function>diff</function> (first difference),
	    <function>ldiff</function> (log-difference, or first
	    difference of natural logs).</para>
	</listitem>
	<listitem><para>Miscellaneous: <function>cum</function>
	    (cumulate), <function>sort</function>,
	    <function>uniform</function>, <function>normal</function>,
	    <function>misszero</function> (replace the missing
	    observation code in a given series with zeros),
	    <command>zeromiss</command> (the inverse operation to
	    <command>misszero</command>).</para>
	</listitem>
      </itemizedlist> 


      <para>
	All of the above functions with the exception of
	<function>cov</function>, <function>corr</function>,
	<function>uniform</function> and <function>normal</function>
	take as their single argument either the name of a variable
	(note that you can't refer to variables by their ID numbers in
	a <command>genr</command> command) or a composite expression
	that evaluates to a variable (e.g.
	<literal>ln((x1+x2)/2)</literal>). <function>cov</function>
	and <function>corr</function> both require two arguments, and
	return respectively the covariance and the correlation
	coefficient between two named variables.
	<function>uniform()</function> and
	<function>normal()</function>, which do not take arguments,
	return pseudo-random series drawn from the uniform
	(0&ndash;100) and standard normal distributions respectively
	(see also the <command>seed</command> command).  Uniform
	series are generated using the C library function
	<function>rand()</function>; for normal series the method of
	Box and Muller (1958) is used. Besides the operators and
	functions just noted there are some special uses of
	<command>genr</command>:
      </para>

      <itemizedlist>
	<listitem><para><command>genr time</command> creates a time trend
	    variable (1,2,3,&hellip;) called <command>time</command>.
	    <command>genr index</command> does the same thing except that the
	    variable is called <varname>index</varname>.</para>
	  </listitem>
	<listitem><para><command>genr dummy</command> creates dummy
	      variables up to the periodicity of the data.  E.g. in
	      the case of quarterly data (periodicity 4), the program
	      creates <varname>dummy_1</varname> = 1 for first quarter
	      and 0 in other quarters, <varname>dummy_2</varname> = 1
	      for the second quarter and 0 in other quarters, and so
	      on.</para>
	  </listitem>
	<listitem><para><command>genr paneldum</command> creates a set
	    of special dummy variables for use with a panel data
	    set&mdash;see <xref linkend="panel"/>
	    above.</para>
	  </listitem>
	<listitem><para>Various internal variables defined in the
	    course of running a regression can be retrieved using
	    <command>genr</command>, as follows:</para>

	  <informaltable frame="none">
	    <tgroup cols="2">
	      <tbody>
		<row>
		  <entry><varname>$ess</varname></entry>
		  <entry>error sum of squares
		  </entry>
		</row>
		<row>
		  <entry><varname>$rsq</varname></entry>
		  <entry>unadjusted &rsqu;
		  </entry>
		</row>
		<row>
		  <entry><varname>$nobs</varname></entry>
		  <entry>number of observations
		  </entry>
		</row>
		<row>
		  <entry><varname>$df</varname></entry>
		  <entry>degrees of freedom
		  </entry>
		</row>
		<row>
		  <entry><varname>$trsq</varname></entry>
		  <entry>$TR^2$ (sample size times &rsqu;)
		  </entry>
		</row>
		<row>
		  <entry><varname>$sigma</varname></entry>
		  <entry>standard error of residuals
		  </entry>
		</row>
		<row>
		  <entry><varname>$lnl</varname></entry>
		  <entry>log-likelihood (logit and probit models)
		  </entry>
		</row>
		<row>
		  <entry><varname>$sigma</varname></entry>
		  <entry>standard error of residuals
		  </entry>
		</row>
		<row>
		  <entry>coeff(<replaceable>var</replaceable>)</entry>
		  <entry>estimated coefficient for variable
		    <replaceable>var</replaceable>
		  </entry>
		</row>
		<row>
		  <entry>stderr(<replaceable>var</replaceable>)</entry>
		  <entry>estimated standard error for variable 
		    <replaceable>var</replaceable>
		  </entry>
		</row>
		<row>
		  <entry>rho(<replaceable>i</replaceable>)</entry>
		  <entry><replaceable>i</replaceable>th order
		    autoregressive coefficient for residuals
		  </entry>
		</row>
		<row>
		  <entry>vcv(<replaceable>var1</replaceable>,<replaceable>var2</replaceable>)</entry>
		  <entry>covariance between coefficients for named
		    variables <replaceable>var1</replaceable> and
		    <replaceable>var2</replaceable>
		  </entry>
		</row>
	      </tbody>
	    </tgroup>
	  </informaltable>
	</listitem>
      </itemizedlist> 

      <para>
	<emphasis>Note</emphasis>: In the command-line program,
	<command>genr</command> commands that retrieve model-related
	data always reference the model that was estimated most
	recently.  This is also true in the GUI program, if one uses
	<command>genr</command> in the <quote>gretl console</quote> or
	enters a formula using the <quote>Define new variable</quote>
	option under the Variable menu in the main window.  With the
	GUI, however, you have the option of retrieving data from any
	model currently displayed in a window (whether or not it's the
	most recent model).  You do this under the <quote>Model
	  data</quote> menu in the model's window.
	Table~\ref{tab:genr} gives several examples of uses of
	<command>genr</command> with explanatory notes; here are a
	couple of tips on dummy variables:
      </para>

      <itemizedlist>
	<listitem><para>Suppose <varname>x</varname> is coded with values 1, 2,
	    or 3 and you want three dummy variables, <varname>d1</varname> = 1
	    if <varname>x</varname> = 1, 0 otherwise, <varname>d2</varname> = 1 if
	    <varname>x</varname> = 2, and so on.  To create these, use the
	    commands: 

	    <programlisting>
	      genr d1 = (x=1)
	      genr d2 = (x=2)
	      genr d3 = (x=3)
	    </programlisting>
	  </para>
	</listitem>

	<listitem><para>To create <varname>z</varname> =
	    <literal>max(x,y)</literal> do

	   <programlisting>
	      genr d = x&gt;>y
	      genr z = (x*d)+(y*(1-d))
	    </programlisting>
	  </para>
	</listitem>
      </itemizedlist>

      <para>
	TABLE goes here.
      </para>

    </sect2>

    <sect2 id="gnuplot"><title>gnuplot</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>gnuplot</command>
		<replaceable>yvars xvar</replaceable>
		<literal>[ -o | -m ]</literal>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>  
		<command>gnuplot -z</command> 
		<replaceable>yvar xvar dummy</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	In the first case the <replaceable>yvars</replaceable> are
	graphed against <replaceable>xvar</replaceable>.  If the flag
	<literal>-o</literal> is supplied the plot will use lines; if
	the flag <literal>-m</literal> is given the plot uses impulses
	(vertical lines); otherwise points will be used.
      </para>

      <para>
	In the second case <replaceable>yvar</replaceable> is graphed
	against <replaceable>xvar</replaceable> with the points shown
	in different colors depending on whether the value of
	<replaceable>dummy</replaceable> is 1 or 0.
      </para>

      <para>
	To make a time-series graph, do <command>gnuplot</command>
	<replaceable>yvars</replaceable> <varname>time</varname>.  If
	no variable named <varname>time</varname> already exists, then
	it will be generated automatically.  Special dummy variables
	will be created for plotting quarterly and monthly data.
      </para>

      <para>
	In interactive mode the result is piped to gnuplot for
	display.  In batch mode a pair of files are written,
	<filename>gpttmp01.dat</filename> and
	<filename>gpttmp01.plt</filename>.  (With subsequent uses of
	<command>gnuplot</command> similar pairs of files are created,
	with the number in the file name incremented.) The plots can
	be generated later using the command <command>gnuplot
	  gpttmp.plt</command>.  (Under MS Windows, start wgnuplot and
	open the file <filename>gpttmp01.plt</filename>.)  To gain
	control over the details of the plot, edit the
	<filename>.plt</filename> file.
      </para>

    </sect2>

    <sect2 id="graph"><title>graph</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>graph</command>
		<replaceable>var1 var2</replaceable>
		&opto;
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>graph</command> 
		<replaceable>var1 var2 var3</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	ASCII graphics.  In the first example, variable
	<replaceable>var1</replaceable> (which may be a name or a
	number) is graphed (y-axis) against
	<replaceable>var2</replaceable> (x-axis) using ASCII
	symbols. <literal>-o</literal> flag will graph with 40 rows and 60
	columns. Without it, the graph will be 20 by 60 (for screen
	output). In the second example, both
	<replaceable>var1</replaceable> and
	<replaceable>var2</replaceable> will be graphed (on y-axis)
	against <replaceable>var3</replaceable>. This is useful to
	graph observed and predicted values against time. See also the
	<command>gnuplot</command> command.
      </para>

    </sect2>

    <sect2 id="hccm"><title>hccm</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage</entry>
	      <entry>  
		<command>hccm</command>
		<replaceable>depvar indepvars</replaceable>
		&opto;
	      </entry>  
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Presents OLS estimates with the heteroskedasticity consistent
	covariance matrix estimates for the standard errors of
	regression coefficients using MacKinnon and White (1985)
	<quote>jackknife</quote> estimates (see Ramanathan, Section
	8.3).  The coefficient covariance matrix is printed if the
	<literal>-o</literal> flag is given.
      </para>

    </sect2>

    <sect2 id="help"><title>help</title>

      <para>
	<command>help</command> gives a list of available commands.
	<command>help</command> <replaceable>command</replaceable>
	describes <replaceable>command</replaceable> (e.g.\
	<command>help smpl</command>).  You can type
	<command>man</command> instead of <command>help</command> if
	you like. 
      </para> 

    </sect2>

    <sect2 id="hilu"><title>hilu</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>hilu</command>
		<replaceable>depvar indepvars</replaceable>
		&opto;
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry> 
		<command>hilu 1 0 2 4 6 7</command>  
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>  
		<command>hilu -o y 0 x1 x2 x3</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para> <command>hilu</command> computes the estimates of a model
	  using the Hildreth&ndash;Lu search procedure (fine tuned by
	  the CORC procedure) with <replaceable>depvar</replaceable>
	  as the dependent variable and
	  <replaceable>indepvars</replaceable> as the list of
	  independent variables separated by spaces.  Use the number
	  zero for a constant term. The error sum of squares of the
	  transformed model is graphed against the value of rho from
	  &minus;0.99 to 0.99.  If the <literal>-o</literal> flag is
	  present, the covariance matrix of regression coefficients
	  will be printed. Residuals of this transformed regression
	  are stored under the name <varname>uhat</varname>.
      </para>

    </sect2>

    <sect2 id="hsk"><title>hsk</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>hsk</command> 
		<replaceable>depvar indepvars</replaceable>
		&opto;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable> 

      <para>
	Prints heteroskedasticity corrected estimates (see Ramanathan,
	ch. 8) and associated statistics.  The auxiliary regression
	predicts the log of the square of residuals (using squares of
	independent variables but not their cross products) from which
	weighted least squares estimates are obtained.  If the
	<literal>-o</literal> flag is present, the covariance matrix of
	regression coefficients will be printed.  A number of internal
	variables may be retrieved using the <command>genr</command>
	command, provided <command>genr</command> is invoked
	immediately after this command.
      </para>

    </sect2>

    <sect2 id="import"><title>import</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>import</command>
		<replaceable>csvfile</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>import -o</command>
		<replaceable>boxfile</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable> 

      <para>
	Without the <literal>-o</literal> flag, brings in data from a
	comma-separated values (CSV) format file, such as can easily
	be written from a spreadsheet program.  The file should have
	variable names on the first line and a rectangular data matrix
	on the remaining lines.  Variables should be arranged <quote>by
	observation</quote> (one column per variable; each row represents an
	observation).  See section~\ref{csvetc} of this manual for
	details.
      </para>

      <para>
	With the <literal>-o</literal> flag, reads a data file in BOX1
	format, as can be obtained using the Data Extraction Service
	of the US Bureau of the Census.
      </para>

    </sect2>

    <sect2 id="info"><title>info</title>

      <para>
	<command>info</command> prints out any information contained
	in the header file corresponding to the current datafile.
	(This information must be enclosed between
	<literal>(*</literal> and <literal>*)</literal>, these markers
	being placed on separate lines.)
      </para>

    </sect2>

    <sect2 id="labels"><title>labels</title>

      <para>
	<command>labels</command> prints out the informative labels
	for any variables that have been generated using
	<command>genr</command>, and any labels added to the data set
	via the GUI.
      </para>

    </sect2>

    <sect2 id="lags"><title>lags</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>lags</command>
		<replaceable>varlist</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	  Creates new variables which are lagged values of each of the
	  variables in varlist.  The number of lagged variables equals
	  the periodicity. For example, if the periodicity is 4
	  (quarterly), the command <command>lags x y</command> creates
	  <varname>x_1</varname> = x_{t-1}, <varname>x_2</varname> $=
	  x_{t-2}$, <varname>x_3</varname> $= x_{t-3}$ and
	  <varname>x_4</varname> $= x_{t-4}$. Similarly for
	  <varname>y</varname>. These variables must be referred to in
	  the exact form, that is, with the underscore.
      </para>

    </sect2>

    <sect2 id="ldiff"><title>ldiff</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>ldiff</command>
		<replaceable>varlist</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	The first difference of the natural log of each variable in
	varlist is obtained and the result stored in a new variable
	with the prefix <varname>ld_</varname>.  Thus <command>ldiff x
	  y</command> creates the new variables
	<varname>ld_x</varname> =
	  <inlineequation>
	    <inlinegraphic fileref="figures/ldx.png"/>
	    <texmath>
	    $\ln(x_t) - \ln(x_{t-1})$ 
	    </texmath>
	  </inlineequation> and
	<varname>ld_y</varname> =
	  <inlineequation>
	    <inlinegraphic fileref="figures/ldy.png"/>
	    <texmath>
	    $\ln(y_t) - \ln(y_{t-1})$ 
	    </texmath>
	  </inlineequation>.
      </para>

    </sect2>

    <sect2 id="list"><title>list</title>

      <para>
	Prints a listing of variables currently available.
	<command>ls</command> is a synonym.  
      </para>

    </sect2>

    <sect2 id="lmtest"><title>lmtest</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>lmtest</command> 
		&opto;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	This command must immediately follow an <command>ols</command>
	command. It prints the Lagrange multiplier test statistics
	(and associated p-values) for nonlinearity and
	heteroskedasticity (White's test) or, if the
	<literal>-o</literal> flag is present, for serial correlation
	up to the periodicity.  The corresponding auxiliary regression
	coefficients are also printed out.  See Ramanathan, Chapters
	7, 8, and 9 for details. Only the squared independent
	variables are used and not their cross products.
      </para>

      <para>
	If the internal creation of squares causes exact
	multicollinearity, LM test statistics cannot be obtained.
      </para>

    </sect2>

    <sect2 id="logit"><title>logit</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>    
		<command>logit</command>
		<replaceable>depvar indepvars</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	  Binomial logit regression. The dependent variable should be
	  a binary variable.  Maximum likelihood estimates of the
	  coefficients on <replaceable>indepvars</replaceable> are
	  obtained via the EM or Expectation&ndash;Maximization method
	  (see Ruud, 2000, ch. 27).  As the model is nonlinear the
	  slopes depend on the values of the independent variables:
	  the reported slopes are evaluated at the means of those
	  variables. The &khgr;<superscript>2</superscript> statistic
	  tests the null hypothesis that all coefficients are zero
	  apart from the constant.
      </para>

      <para>
	If you want to use logit for analysis of proportions (where
	the dependent variable is the proportion of cases having a
	certain characteristic, at each observation, rather than a 1
	or 0 variable indicating whether the characteristic is present
	or not) you should not use the <command>logit</command>
	command, but rather construct the logit variable (e.g.
	<command>genr lgt_p = log(p/(1 - p))</command>) and use this
	as the dependent variable in an OLS regression.  See
	Ramanathan, ch. 12.
      </para>

    </sect2>

    <sect2 id="logs"><title>logs</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>    
		<command>logs</command>
		<replaceable>varlist</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	The natural log of each of the variables in varlist is
	obtained and the result stored in a new variable with the
	prefix <literal>l_</literal> which is <quote>el</quote>
	underscore.  <command>logs x y</command> creates the new
	variables <varname>l_x</varname> = ln(<varname>x</varname>)
	and <varname>l_y</varname> = ln(<varname>y</varname>).
      </para>

    </sect2>

    <sect2 id="loop"><title>loop</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>loop</command>
		<replaceable>number_of_times</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>loop while</command>
		<replaceable>condition</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry>
		<command>loop 1000</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>loop while essdiff > .00001</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Opens a special mode in which the program accepts commands to
	be repeated either a specified number of times, or so long as
	a specified condition holds true.  Within a loop, only four
	commands can be used: <command>genr</command>,
	<command>ols</command>, <command>print</command> and
	<command>store</command> (and store can't be used in a
	<command>while</command> loop).  With <command>genr</command>
	and <command>ols</command> it is possible to do quite a lot.
	You exit the mode of entering loop commands with
	<command>endloop</command>: at this point the stacked commands
	are executed.  Loops cannot be nested.
      </para>

      <para>
	See sections~\ref{monte} and \ref{iterate} of this manual for
	details.
      </para>

    </sect2>

    <sect2 id="meantest"><title>meantest</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>meantest</command>
		<replaceable>var1 var2</replaceable>
		&opto;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Calculates the <emphasis>t</emphasis> statistic for the null
	hypothesis that the population means are equal for the
	variables <replaceable>var1</replaceable> and
	<replaceable>var2</replaceable>, and shows its p-value.
	Without the <literal>-o</literal> flag, the statistic is
	computed on the assumption that the variances are equal for
	the two variables; with the <literal>-o</literal> flag the
	variances are assumed to be unequal. (The flag will make a
	difference only if there are different numbers of non-missing
	observations for the two variables.)
      </para>

    </sect2>

    <sect2 id="multiply"><title>multiply</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>     
		<command>multiply</command> 
		<replaceable>x suffix varlist</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry>
		<command>multiply invpop pc 3 4 5 6</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		  <command>multiply 1000 big x1 x2 x3</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	The variables in <replaceable>varlist</replaceable>
	(referenced by name or number) are multiplied by
	<replaceable>x</replaceable>, which may be either a numerical
	value or the name of a variable already defined. The products
	are named with the specified <replaceable>suffix</replaceable>
	(maximum 3 characters).  The original variable names are
	truncated first if need be.  For instance, suppose you want to
	create per capita versions of certain variables, and you have
	the variable <varname>pop</varname> (population).  A suitable
	set of commands is then: <command>genr invpop =
	  1/pop</command> <command>multiply invpop pc income
	  expend</command> which will create
	<varname>incomepc</varname> as the product of
	<varname>income</varname> and <varname>invpop</varname>, and
	<varname>expendpc</varname> as <varname>expend</varname> times
	<varname>invpop</varname>.
      </para>

    </sect2>

    <sect2 id="nulldata"><title>nulldata</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>nulldata</command>
		<replaceable>series_length</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Example:</entry>
	      <entry>    
		<command>nulldata 100</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Establishes a <quote>blank</quote> data set, containing only a
	constant, with periodicity 1 and the specified number of
	observations. This may be used for simulation purposes: some
	of the <command>genr</command> commands (e.g. <command>genr
	  uniform()</command>, <command>genr normal()</command>,
	<command>genr time</command>) will generate dummy data from
	scratch to fill out the data set. This command may be useful
	in conjunction with <command>loop</command>.  See also the
	<command>seed</command> command.
      </para>

    </sect2>

    <sect2 id="ols"><title>ols</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>  
		<command>ols</command>
		<replaceable>depvar indepvars</replaceable>
		&opto;
	      </entry>
	    </row>
	    <row>
	      <entry>Examples: </entry>
	      <entry>
		<command>ols 1 0 2 4 6 7</command>  
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>      
		<command>ols -o 1 0 2 4 6 7</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>      
		<command>ols y 0 x1 x2 x3</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>      
		<command>ols -o y 0 x1 x2 x3</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>                

      <para>
	Computes ordinary least squares estimates with
	<replaceable>depvar</replaceable> as the dependent variable
	and <replaceable>indepvars</replaceable> as the list of
	independent variables. The <literal>-o</literal> flag will
	print the covariance matrix of regression coefficients.  The
	variables can be specified either by names or by their number.
	Use the number zero for a constant term. The program also
	prints the p-values for <emphasis>t</emphasis> (two-tailed)
	and <emphasis>F</emphasis>-statistics.  A p-value below 0.01
	indicates significance at the 1 percent level and is denoted
	by <literal>***</literal>.  <literal>**</literal> indicates
	significance between 1 and 5 percent and <literal>*</literal>
	indicates significance between 5 and 10 percent levels. Model
	selection statistics (described in Ramanathan, Section 4.3)
	are also printed.  A number of internal variables may be
	retrieved using the <command>genr</command> command, provided
	<command>genr</command> is invoked immediately after this
	command.
      </para>

    </sect2>

    <sect2 id="omit"><title>omit</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>        
		<command>omit</command>
		<replaceable>varlist</replaceable> 
		&opto;
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry>
		<command>omit 5 7 9</command> 
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>     
		<command>omit xx yy zz</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	This command must be invoked after an estimation command. The
	variables in <replaceable>varlist</replaceable> will be
	omitted from the previous model and the new model estimated.
	If more than one variable is omitted, the Wald
	<emphasis>F</emphasis>-statistic for the omitted variables
	will be printed along with the p-value for it (for the OLS
	procedure only).  A p-value below 0.05 means that the
	coefficients are jointly significant at the 5 percent level.
	A number of internal variables may be retrieved using the
	<command>genr</command> command, provided
	<command>genr</command> is invoked immediately after this
	command. The coefficient covariance matrix is printed if the
	<literal>-o</literal> flag is given.
      </para>

    </sect2>

    <sect2 id="omitfrom"><title>omitfrom</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>omitfrom</command> 
		<replaceable>modelID varlist</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Example: </entry>
	      <entry> 
		<command>omitfrom 2 5 7 9</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para> Works like the <command>omit</command> command, except
	that you specify a previous model (using its ID number, which
	is printed at the start of the model output) to take as the
	base for omitting variables.  The example above omits
	variables number 5, 7 and 9 from Model 2.
      </para>

    </sect2>

    <sect2 id="open"><title>open</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>open</command>
		<replaceable>datafile</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Opens a data file.  If a data file is already open, it is
	replaced by the newly opened one.  The program will try to
	detect the format of the data file (native, CSV or BOX1) and
	treat it accordingly.
      </para>

    </sect2>

    <sect2 id="pergm"><title>pergm</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry><command>pergm</command>
		<replaceable>varname</replaceable>
		&opto;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	  Computes and displays (and if not in batch mode, graphs) the
	  spectrum of the specified variable.  Without the
	  <literal>-o</literal> flag the sample periodogram is given;
	  with the flag a Bartlett lag window of length 
	  <inlineequation>
	  <inlinegraphic fileref="figures/bartlett.png"/>
	  <texmath>
	    $2\sqrt{T}$
	  </texmath>
	  </inlineequation>
	  (where <emphasis>T</emphasis> is the sample size).  is used
	  in estimating the spectrum (see Chapter 18 of Greene's
	  <citetitle>Econometric Analysis</citetitle>). When the
	  sample periodogram is printed, a <emphasis>t</emphasis>-test
	  for fractional integration of the series (<quote>long
	    memory</quote>) is also given: the null hypothesis is that
	  the integration order is zero.
      </para>

    </sect2>

    <sect2 id="plot"><title>plot</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Examples:</entry>
	      <entry>  <command>plot x1</command></entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>  <command>plot x1 x2</command></entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>  <command>plot 3 7</command>  </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>  <command>plot -o x1 x2</command></entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Plots data values for specified variables, for the range of
	observations currently in effect, using ASCII symbols.  Each
	line stands for an observation and the values are plotted
	horizontally.  If the flag <literal>-o</literal> is present,
	<varname>x1</varname> and <varname>x2</varname> are plotted in
	the same scale, otherwise <varname>x1</varname> and
	<varname>x2</varname> are scaled appropriately.  The
	<literal>-o</literal> flag should be used only if the variables have
	approximately the same range of values (e.g. observed and
	predicted dependent variable).  See also
	<command>gnuplot</command>.
      </para>

    </sect2>

    <sect2 id="print"><title>print</title>

      <para>
	Prints the values of the specified variables for the current
	data range (see <command>smpl</command>).
      </para>

      <informaltable frame="none">
	<tgroup cols="2">
	  <tbody>
	    <row>
	      <entry><command>print</command></entry>
	      <entry> prints the entire file by variables </entry>
	    </row>
	    <row>
	      <entry><command>print -o</command> </entry>
	      <entry> prints the entire file by observations in a
		tabular form </entry>
	    </row>
	    <row>
	      <entry><command>print 3 6</command> </entry>
	      <entry> prints variables number 3 and 6 by variables
	      </entry>
	    </row>
	    <row>
	      <entry><command>print x y z</command> </entry>
	      <entry> prints <varname>x</varname>,
		<varname>y</varname> and <varname>z</varname> by
		variables </entry>
	    </row>
	    <row>
	      <entry><command>print -o x y</command> </entry>
	      <entry> prints <varname>x</varname> and
		<varname>y</varname> by observations</entry>
	    </row> 
	  </tbody> 
	</tgroup>
      </informaltable>

    </sect2>

    <sect2 id="probit"><title>probit</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>probit</command> 
		<replaceable>depvar indepvars</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	  Probit regression. The dependent variable should be a binary
	  variable. Maximum likelihood estimates of the coefficients
	  on <replaceable>indepvars</replaceable> are obtained via
	  iterated least squares (the EM or
	  Expectation&ndash;Maximization method).  As the model is
	  nonlinear the slopes depend on the values of the independent
	  variables: the reported slopes are evaluated at the means of
	  those variables.  The &khgr;<superscript>2</superscript>
	  statistic tests the null hypothesis that all coefficients
	  are zero apart from the constant.
      </para>

      <para>
	Probit for analysis of proportions is not implemented in
	<application>gretl</application> at this point.
      </para>

    </sect2>

    <sect2 id="pvalue"><title>pvalue</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry></entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		  <command>pvalue 1</command>
		  <replaceable>xvalue</replaceable>
		  (normal distribution)
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry> 
		  <command>pvalue  2</command>
		  <replaceable>df xvalue</replaceable>
		  (<emphasis>t</emphasis> distribution)
		</entry>
		
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>pvalue 3</command> 
		<replaceable>df xvalue</replaceable>
		  (&khgr;<superscript>2</superscript> distribution)
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry> 
		<command>pvalue 4</command> 
		<replaceable>dfn dfd xvalue</replaceable>
		  (<emphasis>F</emphasis> distribution)
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>pvalue 5</command> 
		<replaceable>mean variance xvalue</replaceable>
		  (Gamma distribution)
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>     

      <para>
	  Computes the area to the right of
	  <replaceable>xvalue</replaceable> in the specified
	  distribution.  <replaceable>df</replaceable> is the degrees
	  of freedom, <replaceable>dfn</replaceable> is the d.f. for
	  the numerator, <replaceable>dfd</replaceable> is the d.f.
	  for the denominator.  Instead of the code numbers you can
	  use <literal>z</literal>, <literal>t</literal>,
	  <literal>X</literal>, <literal>F</literal> and
	  <literal>G</literal> for the normal, <emphasis>t</emphasis>,
	  &khgr;<superscript>2</superscript>, <emphasis>F</emphasis>,
	  and gamma distributions respectively.
      </para>

    </sect2>

    <sect2 id="quit"><title>quit</title>

      <para>
	Exits from the program, giving you the option of saving the
	output from the session on the way out.  
      </para>

    </sect2>

    <sect2 id="rhodiff"><title>rhodiff</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>     
		<command>rhodiff</command>
		<replaceable>rholist</replaceable> 
		<literal>;</literal>
		<replaceable>varlist</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Examples: </entry>
	      <entry>
		<command>rhodiff .65 ; 2 3 4</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry> 
		<command>rhodiff r1 r2 ; x1 x2 x3</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Creates rho-differenced counterparts of the variables
	(given by number or by name) in
	<replaceable>varlist</replaceable> and adds them to the data
	set, using the suffix <literal>#</literal> for the new variables.
	Given variable <varname>v1</varname> in
	<replaceable>varlist</replaceable>, and entries
	<varname>r1</varname> and <varname>r2</varname> in
	<replaceable>rholist</replaceable>, <varname>v1# = v1(t) -
	r1*v1(t-1) - r2*v1(t-2)</varname> is created. The
	<replaceable>rholist</replaceable> entries can be given as
	numerical values or as the names of variables previously
	defined.
      </para>

    </sect2>

    <sect2 id="run"><title>run</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>run</command>
		<replaceable>inputfile</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	If the file <replaceable>inputfile</replaceable> contains
	script commands, this command will execute them one by one.
	This is a useful way of executing batch commands within an
	interactive session.
      </para>

    </sect2>

    <sect2 id="runs"><title>runs</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>runs</command>
		<replaceable>varname</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Carries out the nonparametric <quote>runs</quote> test for
	randomness of the specified variable.  If you want to test for
	randomness of deviations from the median, for a variable named
	<varname>x1</varname> with a non-zero median, you can do the
	following: 
	  <programlisting>
	    genr signx1 = x1 - median(x1)
	    runs signx1
	  </programlisting>
      </para>

    </sect2>

    <sect2 id="scatters"><title>scatters</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>scatters</command>
		<replaceable>yvar</replaceable> 
		<literal>;</literal>
		<replaceable>xvarlist</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>scatters</command>
		<replaceable>yvarlist</replaceable> 
		<literal>;</literal>
		<replaceable>xvar</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry>
		<command>scatters 1 ; 2 3 4 5</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry> 
		<command>scatters 1 2 3 4 5 6 ; time</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Plots pairwise scatters of <replaceable>yvar</replaceable>
	against all the variables in
	<replaceable>xvarlist</replaceable>, or of all the variables
	in <replaceable>yvarlist</replaceable> against
	<replaceable>xvar</replaceable>.  The first example above puts
	variable 1 on the <emphasis>y</emphasis>-axis and draws four
	graphs, the first having variable 2 on the
	<emphasis>x</emphasis>-axis, the second variable 3 on the
	<emphasis>x</emphasis>-axis, and so on.  The second example
	plots each of variables 1 through 6 against time. Scanning a
	set of such plots can be a useful step in exploratory data
	analysis.  The maximum number of plots is six; any extra
	variable in the list will be ignored.
      </para>

    </sect2>

    <sect2 id="seed"><title>seed</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>  
		<command>seed</command>
		<replaceable>integer</replaceable> 
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Sets the seed for the pseudo-random number generator for the
	<function>uniform()</function> and
	<function>normal()</function> functions (see the
	<command>genr</command> command).  By default the seed is set
	when the program is started, using the system time.  If you
	want to obtain repeatable sequences of pseudo-random numbers
	you will need to set the seed manually.
      </para>

    </sect2>

    <sect2 id="setobs"><title>setobs</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>setobs</command>
		<replaceable>periodicity startobs</replaceable> 
	      </entry>
	    </row>
	    <row>
	      <entry>Examples:</entry>
	      <entry>
		<command>setobs 4 1990.1</command> 
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry> 
		<command>setobs 12 1978.03</command> 
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry> 
		<command>setobs 20 1.01</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Use this command to force the program to interpret the current
	data set as time series or panel, when the data have been read
	in as simple undated series.
	<replaceable>periodicity</replaceable> must be an integer;
	<replaceable>startobs</replaceable> is a string representing
	the date or panel ID of the first observation. See also
	sections~\ref{dfiles} and \ref{panel} of this manual.
      </para>

    </sect2>

    <sect2 id="shell"><title>shell</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>!</command>
		<replaceable>shellcommand</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	A <command>!</command> at the beginning of a command
	line is interpreted as an escape to the user's shell.  Thus
	arbitrary shell commands can be executed from within the
	program (not available under MS Windows).
      </para>

    </sect2>

    <sect2 id="sim"><title>sim</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>     
		<command>sim</command> 
		<replaceable>startobs
		endobs y a0 a1 a2</replaceable> 
		&hellip;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
	<tgroup cols="3"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Examples:</entry>
	      <entry>
		<command>sim 1979.2 1983.1 y 0 0.9</command>
	      </entry>
	      <entry>
		creates 
		  <varname>y(t) =  0.9*y(t-1)</varname>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>sim 15 25 y 10 0.8 x</command>
	      </entry>
	      <entry>
		creates 
		  <varname>y(t) = 10 + 0.8*y(t-1) + x(t)*y(t-2)</varname>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Simulates values for <replaceable>y</replaceable> for the
	periods <replaceable>startobs</replaceable> through
	<replaceable>endobs</replaceable>.  The variable
	<replaceable>y</replaceable> must have been defined earlier
	with appropriate initial values. The formula used is
	<varname>y(t) = a0(t) + a1(t)*y(t-1) + a2(t)*y(t-2) +
	  &hellip;</varname> The <varname>ai(t)</varname> may either be
	numerical constants or variable names previously defined.
      </para>

    </sect2>

    <sect2 id="smpl"><title>smpl</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>smpl</command> 
		<replaceable>startobs endobs</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		  <command>smpl -o</command>
		  <replaceable>dummyvar</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		  <command>smpl -o</command>
		</entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		  <command>smpl -r</command> 
		<replaceable>condition</replaceable>
	      </entry>
	    </row>	
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Resets the sample range.  In the first form
	<replaceable>startobs</replaceable> and
	<replaceable>endobs</replaceable> must be consistent with the
	periodicity of the data. In the second form
	<replaceable>dummyvar</replaceable> must be an indicator
	variable with values 0 or 1 at each observation; the sample
	will be restricted to observations where the value is 1.  The
	third form, <command>smpl -o</command>, drops all observations
	for which values of one or more variables are missing.  The
	fourth form (<literal>-r</literal>) restricts the sample to
	observations that satisfy the given (Boolean) condition.
      </para>

      <informaltable frame="none">
	<tgroup cols="2">
	  <tbody>
	    <row>
	      <entry>
		<command>smpl 3 10</command>
	      </entry>
	      <entry>
		data with periodicity 1</entry>
	    </row>
	    <row>
	      <entry>
		  <command>smpl 1950 1990</command>
	      </entry>
	      <entry>
		annual data, periodicity 1</entry> 
	    </row>
	    <row>
	      <entry>
		  <command>smpl 1960.2 1982.4</command>
	      </entry>
	      <entry>
		quarterly data</entry>
	    </row>
	    <row>
	      <entry>
		  <command>smpl 1960.04 1985.10</command>
	      </entry>
	      <entry>
		monthly data</entry>
	    </row>
	    <row>
	      <entry><command>smpl 1960.2 ;</command>
	      </entry>
	      <entry>
		keep <replaceable>endobs</replaceable>
		unchanged
		</entry>
	    </row>
	    <row>
	      <entry>
		  <command>smpl ; 1984.3</command>
	      </entry>
	      <entry>
		keep <replaceable>startobs</replaceable>
		unchanged
		</entry>
	    </row>
	    <row>
	      <entry>
		  <command>smpl -o dum1</command>
	      </entry>
	      <entry>
		draw sample of observations where
		<literal>dum1=1</literal>
		</entry>
	    </row>
	    <row>
	      <entry>
		  <command>smpl -r income > 30000</command>
	      </entry>
	      <entry>
		sample cases where <varname>income</varname> has a
		value greater than 30000.
		</entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	One point should be noted about the <literal>-o</literal> and
	<literal>-r</literal> forms of <command>smpl</command>:  Any
	<quote>structural</quote> information in the data header file
	(regarding the time series or panel nature of the data) is
	lost when this command is issued.  You may reimpose structure
	with the <command>setobs</command> command.
      </para>

    </sect2>

    <sect2 id="spearman"><title>spearman</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>spearman</command>
		<replaceable>x y</replaceable> 
		&opto;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Prints Spearman's rank correlation coefficient for the two
	variables <emphasis>x</emphasis> and <emphasis>y</emphasis>.
	The variables do not have to be ranked manually in advance;
	the function takes care of this. If the <literal>-o</literal>
	flag is supplied, the original data and the ranked data are
	printed out side by side.
      </para>

      <para>
	The automatic ranking is from largest to smallest (i.e. the
	largest data value gets rank 1).  If you need to invert this
	ranking, create a new variable which is the negative of the
	original first.  For example:
      </para>

      <programlisting>
	  genr altx = -x
	  spearman altx y
      </programlisting>

    </sect2>

    <sect2 id="square"><title>square</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>square</command>
		<replaceable>x y</replaceable>
		&opto;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Generates new variables which are squares and cross products
	of selected variables (<literal>-o</literal> will create the
	cross products).  For the above example, new variables created
	will be <varname>sq_x</varname> = $x^2$,
	<varname>sq_y</varname> = $y^2$ and <varname>x_y</varname> =
	$xy$.  If a particular variable is a dummy variable it is not
	squared because we will get the same variable.  
      </para>

    </sect2>

    <sect2 id="store"><title>store</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry> 
		<command>store</command>
		<replaceable>datafile</replaceable>
		[ <replaceable>varlist</replaceable> ]
		[ <replaceable>flag</replaceable> ] 
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	<replaceable>datafile</replaceable> is the name of the file in
	which the values should be stored. A header file
	(<replaceable>datafile</replaceable>.<filename>hdr</filename>)
	is also created, and if one or more of the variables has an
	explanatory <quote>label</quote> defined, a labels file
	(<replaceable>datafile</replaceable>.<filename>lbl</filename>)
	is generated.
      </para>

      <para>
	If <replaceable>varlist</replaceable> is absent, the values of
	all the variables in the current data set will be stored.
      </para>

      <para>
	By default storage is by observations, in native
	<application>gretl</application> ASCII (plain text) format.
	There are four valid (mutually exclusive)
	<replaceable>flags</replaceable>:
      </para>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry><literal>-z</literal>
	      </entry>
	      <entry>The default format, but gzip compressed.  The
		suffix <filename>.gz</filename> is automatically added
		to the name of the data file.
	      </entry>
	    </row>
	    <row>
	      <entry><literal>-o</literal>
	      </entry>
	      <entry>Store the data by variables, in binary format
		using double precision.
	      </entry>
	    </row>
	    <row>
	      <entry><literal>-s</literal>
	      </entry>
	      <entry>Store the data by variables, in binary format
		using single precision.
	      </entry>
	    </row>
	    <row>
	      <entry><literal>-c</literal>
	      </entry>
	      <entry>Store the data in CSV (comma-separated values)
		format.  Such data can be read directly by spreadsheet
		programs.
	      </entry>
	    </row>
	    <row>
	      <entry><literal>-r</literal>
	      </entry>
	      <entry>Store the data in GNU R format.
	      </entry>
	    </row>
	    <row>
	      <entry><literal>-m</literal>
	      </entry>
	      <entry>Store the data in GNU Octave format.
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

    </sect2>

    <sect2 id="summary"><title>summary</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>
		<command>summary</command>
	      </entry>
	      <entry>print summary statistics for all variables in the
		file
	      </entry>
	    </row>
	    <row>
	      <entry>
		<command>summary 3 7 9</command>
	      </entry>
	      <entry>summary statistics for variables number 3, 7, and
		9
	      </entry>
	    </row>
	    <row>
	      <entry>
		<command>summary x y z</command>
	      </entry>
	      <entry>summary statistics for the variables
		<varname>x</varname>, <varname>y</varname>, and
		<varname>z</varname>
	      </entry>
	    </row>	
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Output consists of the mean, standard deviation (sd),
	coefficient of variation (= sd/mean), median, minimum,
	maximum, skewness coefficient, and excess kurtosis.
      </para>

    </sect2>

    <sect2 id="tabprint"><title>tabprint</title>

      <para>
	Must follow the estimation of a model via OLS.  Prints the
	estimated model in the form of a &latex; tabular environment,
	to a file with a name of the form
	<filename>model_N.tex</filename>, where <varname>N</varname>
	is the number of models estimated to date in the current
	session.  This can be incorporated in a &latex; document.  See
	also <command>eqnprint</command>.
      </para>

    </sect2>

    <sect2 id="testuhat"><title>testuhat</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>testuhat</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Must follow a model estimation command.  Gives the frequency
	distribution for the residual from the model along with a
	&khgr;<superscript>2</superscript> test for normality.
      </para>

    </sect2>

    <sect2 id="tsls"><title>tsls</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage: </entry>
	      <entry> 
		<command>tsls</command> 
		<replaceable>depvar varlist1</replaceable> 
		<literal>;</literal>
		<replaceable>varlist2</replaceable> 
		&opto;
	      </entry>
	    </row>
	    <row>
	      <entry>Example: </entry>
	      <entry>
		<command>tsls y1 0 y2 y3 x1 x2 ; 0 x1 x2 x3 x4 x5
		x6</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	This command computes two-stage least squares (TSLS) estimates
	of parameters.  <replaceable>depvar</replaceable> is the
	dependent variable, <replaceable>varlist1</replaceable> is the
	list of independent variables (including right-hand side
	endogenous variables) in the structural equation for which
	TSLS estimates are needed. <replaceable>varlist2</replaceable>
	is the combined list of exogenous and predetermined variables
	in all the equations. If <replaceable>varlist2</replaceable>
	is not at least as long as
	<replaceable>varlist1</replaceable>, the model is not
	identified.  The <literal>-o</literal> flag will print the
	covariance matrix of the coefficients.  In the above example,
	the <varname>y</varname>s are the endogenous variables and the
	<varname>x</varname>s are the exogenous and predetermined
	variables.  A number of internal variables may be retrieved
	using the <command>genr</command> command, provided
	<command>genr</command> is invoked immediately after this
	command.
      </para>

    </sect2>

    <sect2 id="var"><title>var</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>
		<command>var</command> 
		<replaceable>order depvar indepvar</replaceable>
	      </entry>
	    </row>
	    <row>
	      <entry>Examples: </entry>
	      <entry>
		<command>var 4 x1 const time x2 x3</command>
	      </entry>
	    </row>
	    <row>
	      <entry></entry>
	      <entry>
		<command>var 3 1 0 2 3 4</command>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Sets up and estimates (via OLS) a vector autoregression. The
	first argument specifies the lag order, then follows the setup
	for the first equation, as in the <command>ols</command>
	command.  Don't include lags among the elements of the
	<replaceable>indepvar</replaceable> list&mdash;they will be
	added automatically. A regression will be run for each
	variable in the list, excluding the constant, the time trend
	and any dummy variables. Output for each equation includes
	<emphasis>F</emphasis>-tests for zero restrictions on all lags
	of each of the variables, and an <emphasis>F</emphasis>-test
	for the maximum lag.
      </para>

    </sect2>

    <sect2 id="vartest"><title>vartest</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage:</entry>
	      <entry>      
		<command>vartest</command>
		<replaceable>var1 var2</replaceable>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Calculates the <emphasis>F</emphasis> statistic for the null
	hypothesis that the population variances for the variables
	<replaceable>var1</replaceable> and
	<replaceable>var2</replaceable> are equal, and shows its
	p-value.
      </para>

    </sect2>

    <sect2 id="wls"><title>wls</title>

      <informaltable frame="none">
	<tgroup cols="2"><colspec colnum="1" colwidth="&cmdcol;"/>
	  <tbody>
	    <row>
	      <entry>Usage: </entry>
	      <entry> 
		<command>wls</command>
		<replaceable>weightvar depvar indepvars</replaceable>
		&opto;
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Weighted least squares estimates are obtained using
	<replaceable>weightvar</replaceable> as the weight,
	<replaceable>depvar</replaceable> as the dependent variable
	and <replaceable>indepvars</replaceable> as the list of
	independent variables.  More specifically, an OLS regression
	is run on <replaceable>weightvar</replaceable>
	<literal>*</literal> <replaceable>depvar</replaceable> against
	<replaceable>weight</replaceable> <literal>*</literal>
	<replaceable>indepvars</replaceable>. If the
	<replaceable>weightvar</replaceable> is a dummy variable, this
	is equivalent to eliminating all observations with the number
	zero for <replaceable>weightvar</replaceable>. The flag
	<literal>-o</literal> will print the covariance matrix of
	coefficients.  A number of internal variables may be retrieved
	using the <command>genr</command> command, provided
	<command>genr</command> is invoked immediately after this
	command.
      </para>

    </sect2>

    </sect1>

  <sect1 id="estimators"><title>Estimators and tests: summary</title>

    <para><xref linkend="tab-estim"/> shows the estimators available under the
      Model menu in <application>gretl</application>'s main window.
      The corresponding script command is shown in parentheses.  For
      details consult the command's entry in
      <xref linkend="cmdref"/>.</para>

    <table id="tab-estim" frame="none">
      <title>Estimators</title>
      <tgroup cols="2">
	<thead>
	  <row>
	    <entry>Estimator</entry>
	    <entry>Comment</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry>Ordinary Least Squares
	      (<command>ols</command>)</entry>
	    <entry>The workhorse estimator</entry>
	  </row>
	  <row>
	    <entry>Weighted Least Squares
	      (<command>wls</command>)</entry>
	    <entry>Heteroskedasticity, exclusion of selected
	      observations</entry>
	  </row>
	  <row>
	    <entry>HCCM (<command>hccm</command>)</entry>
	    <entry>Heteroskedasticity corrected covariance
	      matrix</entry>
	  </row>
	  <row>
	    <entry>HCCM (<command>hccm</command>)</entry>
	    <entry>Heteroskedasticity corrected covariance
	      matrix</entry>
	  </row>
	  <row>
	    <entry>Heteroskedasticity corrected
	      (<command>hsk</command>)</entry>
	    <entry>Weighted Least Squares based on predicted error
	      variance</entry>
	  </row>
	  <row>
	    <entry>Cochrane&ndash;Orcutt (<command>corc</command>)</entry>
	    <entry>First-order autocorrelation</entry>
	  </row>
	  <row>
	    <entry>Hildreth&ndash;Lu (<command>hilu</command>)</entry>
	    <entry>First-order autocorrelation</entry>
	  </row>
	  <row>
	    <entry>Autoregressive Estimation
	      (<command>ar</command>)</entry>
	    <entry>Higher-order autocorrelation
	      (generalized Cochrane&ndash;Orcutt)</entry>
	  </row>
	  <row>
	    <entry>Vector Autoregression
	      (<command>var</command>)</entry>
	    <entry>Systems of time-series equations</entry>
	  </row>
	  <row>
	    <entry>Cointegration test
	      (<command>coint</command>)</entry>
	    <entry>Long-run relationships between series</entry>
	  </row>
	  <row>
	    <entry>Two-Stage Least Squares
	      (<command>tsls</command>)</entry>
	    <entry>Simultaneous equations</entry>
	  </row>
	  <row>
	    <entry>Logit
	      (<command>logit</command>)</entry>
	    <entry>Binary dependent variable (logistic
	      distribution)</entry>
	  </row>
	  <row>
	    <entry>Probit
	      (<command>probit</command>)</entry>
	    <entry>Binary dependent variable (normal
	      distribution)</entry>
	  </row>
	  <row>
	    <entry>Rank Correlation
	      (<command>spearman</command>)</entry>
	    <entry>Correlation with ordinal data</entry>
	  </row>
	</tbody>
      </tgroup>
    </table>

    <para><xref linkend="tab-tests"/> shows the tests that are available
      under the Tests menu in a model window, after estimation.</para>

    <table id="tab-tests" frame="none">
      <title>Tests for models</title>
      <tgroup cols="2">
	<thead>
	  <row>
	    <entry>Test</entry>
	    <entry>Corresponding command</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry>Omit variables (<emphasis>F</emphasis>-test if OLS)</entry>
	    <entry><command>omit</command></entry>
	  </row>
	  <row>
	    <entry>Add variables (<emphasis>F</emphasis>-test if OLS)</entry>
	    <entry><command>add</command></entry>
	  </row>
	  <row>
	    <entry>Nonlinearity (squares)</entry>
	    <entry><command>lmtest</command></entry>
	  </row>
	  <row>
	    <entry>Heteroskedasticity (White's test)</entry>
	    <entry><command>lmtest</command></entry>
	  </row>
	  <row>
	    <entry>Autocorrelation up to the data frequency</entry>
	    <entry><command>lmtest -o</command></entry>
	  </row>
	  <row>
	    <entry>Chow (structural break)</entry>
	    <entry><command>chow</command></entry>
	  </row>
	  <row>
	    <entry>CUSUM (parameter stability)</entry>
	    <entry><command>cusum</command></entry>
	  </row>
	  <row>
	    <entry>ARCH (conditional heteroskedasticity)</entry>
	    <entry><command>arch</command></entry>
	  </row>
	  <row>
	    <entry>Normality of residual</entry>
	    <entry><command>testuhat</command></entry>
	  </row>
	</tbody>
      </tgroup>
    </table>

    <para>
      Some additional tests are available under the Variable menu in
      the main window: the augmented Dickey&ndash;Fuller test
      (command: <command>adf</command>) and the runs test of
      randomness (command: <command>runs</command>).
    </para>

  </sect1>	

  </chapter>

  <chapter id="trouble"><title>Troubleshooting gretl</title>

    <para>
      As I steer <application>gretl</application> towards a
      <quote>stable</quote> release (version 1.0) I welcome any
      reports of bugs in the program.  I think you are unlikely to
      find bugs in the actual calculations done by
      <application>gretl</application> (although this statement does
      not constitute any sort of warranty). You may, however, come
      across bugs or oddities in the behavior of the graphical
      interface.  Please remember that the usefulness of bug reports
      is greatly enhanced if you can be as specific as possible: what
      <emphasis>exactly</emphasis> went wrong, under what conditions,
      and on what operating system?  If you saw an error message, what
      precisely did it say?  (You needn't bother, though, to quote the
      memory address numbers given in any crash reports from MS
      Windows&mdash;these won't mean anything to me.)</para>

    <para>
      As mentioned above, <application>gretl</application> calls some
      other programs to accomplish certain tasks (gnuplot for
      graphing, &latex; for high-quality typesetting of regression
      output, GNU R).  If something goes wrong with such external
      links, it is not always easy to produce an informative error
      message window.  If such a link fails when accessed from the
      <application>gretl</application> graphical interface, you may be
      able to get more information by starting
      <application>gretl</application> from the command prompt (e.g.
      from an xterm under the X window system, or from a <quote>DOS
	box</quote> under MS Windows, in which case type
      <command>gretlw32.exe</command>), rather than via a desktop menu
      entry or icon.  Additional error messages may be displayed on
      the terminal window.</para>

    <para>
      Also please note that for most external calls,
      <application>gretl</application> assumes that the programs in
      question are available in your <quote>path</quote>&mdash;that
      is, that they can be invoked simply via the name of the program,
      without supplying the program's full location.<footnote><para>The
      exception to this rule is the invocation of gnuplot under MS
      Windows, where a full path to the program is given.</para>
      </footnote>  Thus if a
      given program fails, try the experiment of typing the program
      name at the command prompt, as shown below.</para>

    <informaltable frame="none">
      <tgroup cols="4">
	<thead>
	  <row>
	    <entry>System</entry>
	    <entry>Graphing</entry>
	    <entry>Typsetting</entry>
	    <entry>GNU R</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry>X window system</entry>
	    <entry>gnuplot</entry>
	    <entry>latex, xdvi</entry>
	    <entry>R</entry>
	  </row>
	  <row>
	    <entry>MS Windows</entry>
	    <entry>wgnupl32.exe</entry>
	    <entry>latex, xdvi</entry>
	    <entry>RGui.exe</entry>
	  </row>
	</tbody>
      </tgroup>
    </informaltable>


    <para>
      If the program fails to start from the prompt, it's not a
      <application>gretl</application> issue but rather that the
      program's home directory is not in your path, or the program is
      not installed (properly).  For details on modifying your path
      please see the documentation or online help for your operating
      system or shell.
    </para>

  </chapter>

  <chapter id="cli"><title>The command line interface</title>

    <para>
      The <application>gretl</application> package includes the
      command-line program <application>gretlcli</application>.  This
      is essentially an updated version of Ramu Ramanathan's
      <application>ESL</application>.  On unix-like systems it can be
      run from the console, or in an xterm (or similar).  Under MS
      Windows it can be run in a <quote>DOS box</quote>.
      <application>gretlcli</application> has its own help file, which
      may be accessed by typing <quote>help</quote> at the prompt. It
      can be run in batch mode, sending outout directly to a file (see
      <xref linkend="optarg2"/> above).
    </para>

    <para>
      If <application>gretlcli</application> is linked to the
      <application>readline</application> library (this is
      automatically the case in the MS Windows version; also see
      <xref linkend="app-b"/>), the command line is recallable and
      editable, and offers command completion.  You can use the Up and
      Down arrow keys to cycle through previously typed commands.  On
      a given command line, you can use the arrow keys to move around,
      in conjunction with Emacs editing
      keystokes.<footnote><para>Actually, the key bindings shown below
	  are only the defaults; they can be customized.  See the
	  <ulink
	    url="http://cnswww.cns.cwru.edu/~chet/readline/readline.html">readline 
	    homepage</ulink>.</para>
      </footnote> The most common of these are:
    </para>

      <informaltable frame="none">
      <tgroup cols="2">
	<thead>
	  <row>
	    <entry>Keystroke</entry>
	    <entry>Effect</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry><literal>Ctrl-a</literal></entry>
	    <entry>go to start of line</entry>
	  </row>
	  <row>
	    <entry><literal>Ctrl-e</literal></entry>
	    <entry>go to end of line</entry>
	  </row>
	  <row>
	    <entry><literal>Ctrl-d</literal></entry>
	    <entry>delete character to right</entry>
	  </row>
	</tbody>
      </tgroup>
      </informaltable>

    <para>
      where <quote><literal>Ctrl-a</literal></quote> means press the
      <quote><literal>a</literal></quote> key while the
      <quote><literal>Ctrl</literal></quote> key is also depressed.
      Thus if you want to change something at the beginning of a
      command, you <emphasis>don't</emphasis> have to backspace over
      the whole line, erasing as you go.  Just hop to the start and
      add or delete characters.
    </para>

    <para>If you type the first letters of a command name then press
      the Tab key, readline will attempt to complete the command name
      for you.  If there's a unique completion it will be put in place
      automatically.  If there's more than one completion, pressing
      Tab a second time brings up a list.</para>

    <para>The rest of this section is given over to the changes in
      <application>gretlcli</application> relative to Ramu
      Ramanathan's original <application>ESL</application>.  Command
      scripts developed for <application>ESL</application> should be
      usable with <application>gretlcli</application> with few or no
      changes: the only things to watch for are multi-line commands
      and the <command>freq</command> command, both discussed
      below.</para>

    <sect1 id="cli-syntax"><title>Change of syntax</title>

      <para>
	There is only one significant change.  In
	<application>ESL</application>, a semicolon is used as a
	terminator for many commands.  I decided to remove this in
	<application>gretlcli</application>. Semicolons are simply
	ignored, apart from a few special cases where they have a
	definite meaning: as a separator for two lists in the
	<command>ar</command> and <command>tsls</command> commands,
	and as a marker for an unchanged starting or ending
	observation in the <command>smpl</command> command. In
	<application>ESL</application> semicolon termination gives the
	possibility of breaking long commands over more than one line;
	in <application>gretlcli</application> this is done by putting
	a trailing backslash <literal>\</literal> at the end of a line
	that is to be continued.
      </para>
    </sect1>

    <sect1 id="cli-changes"><title>Change in command-line
	    arguments</title>

      <para>
	The command-line syntax for running a batch job is simplified.
	For <application>ESL</application> you type, e.g. 

	<programlisting>
	  esl -b datafile &lt; inputfile &gt; outputfile
	</programlisting>

	For <application>gretlcli</application> you type: 

	<programlisting>
	  gretlcli -b inputfile &gt; outputfile
	</programlisting>

	The inputfile is treated as a program argument; it should
	specify a datafile to use internally, using the syntax
	<command>open datafile</command>  or the special comment 
	<literal>(* !</literal> <replaceable>datafile</replaceable>
	<literal>*)</literal>
      </para>

    </sect1>

    <sect1 id="cli-missing"><title>Commands missing from gretlcli</title>

      <para>I have not implemented the commands designed to make
	working interactively at the DOS command prompt a bit easier
	(<command>scroll</command> and <command>edit</command>).  I
	presume that with the new GUI these will not be needed, and
	that people who choose to use the command-line interface
	interactively will probably be running it in a proper
	scrollable terminal window (e.g. xterm).
      </para>
    </sect1>

    <sect1 id="cli-redefined"><title>Commands redefined in gretlcli</title>

      <para>A few commands have been simplified, or augmented, or
	their output has been changed somewhat. 
      </para>

      <itemizedlist>
	<listitem><para><command>freq</command>: At present you can't
	    specify particular ranges as in
	    <application>esl</application>.  A chi-square test for
	    normality has been added.</para>
	</listitem>
	<listitem><para><command>genr</command>: The functions
	    <function>cov</function>, <function>corr</function>,
	    <function>median</function>, <function>var</function> and
	    <function>vcv</function> have been added.</para>
	</listitem>
	<listitem><para><command>smpl</command>: The
	    <command>-o</command> switch sets the sample using a dummy
	    variable.</para>
	</listitem>
	<listitem><para><command>store</command>: The
	    <command>-o</command> switch now saves the data by
	    variable in binary format.  There are three new switches:
	    <command>-z</command> invokes gzip compression;
	    <command>-c</command> saves in CSV format;
	    <command>-r</command> saves in GNU R format;
	    <command>-m</command> saves in GNU Octave format.</para>
	</listitem>
      </itemizedlist> 

      <para>
	The output from many commands is formatted a little
	differently.
      </para>
    </sect1>

    <sect1 id="cli-added"><title>New commands added to gretlcli</title>

      <para>
	These are described in detail in <xref linkend="cmdref"/>. They
	are briefly summarized in Table~\ref{cmdtab}.
      </para> 

      <para>TABLE MISSING HERE</para>
    </sect1>

    <sect1 id="cli-newfeatures"><title>Some of the new features added to
	gretlcli</title>
      
      <itemizedlist>
	<listitem><para>Specifying lags: You don't have to create
	    lagged variables in advance of a regression.  The syntax
	    <varname>foo(-1)</varname> in a regression list will cause
	    the first lag of<varname>foo</varname> (if
	    <varname>foo</varname> exists) to be generated and added
	    to the data set.</para>
	</listitem>
	<listitem><para>You can switch data files without quitting
	    <application>gretlcli</application>. If a data file has
	    already been read, using the <command>open</command>
	    command will replace it with a new one (after a prompt for
	    whether this is really wanted).</para>
	</listitem>
	<listitem><para>You can assign <quote>labels</quote> to
	    variables: explanatory or descriptive tags of up to 128
	    characters.  These go in a file with the same basename as
	    the datafile plus the suffix <filename>.lbl</filename>.
	    One variable per line, the name of the variable first,
	    followed by white space, followed by the label.  These
	    labels will be read if they're present and can be
	    retrieved with the <command>labels</command>
	    command.</para>
	</listitem>
      </itemizedlist>
    </sect1>
			

  </chapter>

  <chapter id="nist"><title>Assessing program accuracy: the NIST
      datasets</title>

    <para>
      The U.S. National Institute of Standards and Technology (NIST)
      publishes a set of statistical reference datasets.  The object
      of this project is to <quote>improve the accuracy of statistical
	software by providing reference datasets with certified
	computational results that enable the objective evaluation of
	statistical software</quote>.</para>

    <para>As of May 2000 the website for the project can be found
      at:</para>

    <para><ulink
	url="http://www.nist.gov/itl/div898/strd/general/main.html">
	<literal>http://www.nist.gov/itl/div898/strd/general/main.html</literal></ulink></para>

    <para>while the datasets are at</para>

    <para><ulink
	url="http://www.nist.gov/itl/div898/strd/general/dataarchive.html"> 
	<literal>http://www.nist.gov/itl/div898/strd/general/dataarchive.html</literal></ulink></para>

    <para>For testing <application>gretl</application> I have made use
      of the datasets pertaining to Linear Regression and Univariate
      Summary Statistics (the others deal with ANOVA and nonlinear
      regression).</para>

    <para>I quote from the NIST text <quote>Certification Method &amp;
	Definitions</quote> regarding their certified computational
      results (emphasis added):</para>

    <blockquote>
      <para>For all datasets, multiple precision calculations
	(accurate to 500 digits) were made using the preprocessor and
	FORTRAN subroutine package of Bailey (1995, available from
	NETLIB). Data were read in exactly as multiple precision
	numbers and all calculations were made with this very high
	precision. The results were output in multiple precision, and
	only then rounded to fifteen significant digits.
	<emphasis>These multiple precision results are an
	  idealization. They represent what would be achieved if
	  calculations were made without roundoff or other
	  errors.</emphasis> Any typical numerical algorithm (i.e.
	not implemented in multiple precision) will introduce
	computational inaccuracies, and will produce results which
	differ slightly from these certified values.</para>
    </blockquote>

    <para>It is not to be expected that results obtained from ordinary
      statistical packages will agree exactly with NIST's multiple
      precision benchmark figures.  But the benchmark provides a very
      useful test for egregious errors and imprecision.</para>  

    <para>In <xref linkend="tab-linreg"/> below, <quote>OK</quote>
      means that <application>gretl</application>'s output
      agrees&mdash;to the precision given by the program, which is
      less than the 15 significant digits given by NIST&mdash;with the
      certified values for all the NIST statistics, which include
      regression coefficients and standard errors, sum of squared
      residuals or error sum of squares (ESS), standard error of
      residuals, <emphasis>F</emphasis> statistic and &rsqu;.</para>

    <table id="tab-linreg" frame="none">
      <title>NIST linear regression tests</title>
      <tgroup cols="3">
	<thead>
	  <row>
	    <entry>Dataset</entry>
	    <entry>Model</entry>
	    <entry>Performance</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry>Norris</entry>
	    <entry>Simple linear regression</entry>
	    <entry>OK</entry></row>
	  <row><entry>Pontius</entry>
	    <entry>Quadratic</entry>
	    <entry>OK</entry>
	  </row>
	  <row><entry>NoInt1</entry>
	    <entry>Simple regression, no intercept </entry>
	    <entry>OK (but see text)</entry>
	  </row>
	  <row><entry>NoInt2</entry>
	    <entry>Simple regression,
	      no intercept </entry>
	    <entry>OK (but see text)</entry>
	  </row>
	  <row><entry>Filip</entry>
	    <entry>10th degree polynomial </entry>
	    <entry>Complains of excessive multicollinearity, no
	      estimates produced</entry></row>
	  <row>
	    <entry>Longley</entry>
	    <entry>Multiple regression,
	      six independent variables 
	    </entry><entry>OK</entry></row>
	  <row>
	    <entry>Wampler1</entry>
	    <entry>5th degree polynomial
	    </entry><entry>OK</entry></row>
	  <row>
	    <entry>Wampler2</entry>
	    <entry>5th degree polynomial
	    </entry><entry>OK</entry></row>
	  <row>
	    <entry>Wampler3</entry>
	    <entry>5th degree polynomial
	    </entry><entry>OK</entry></row>
	  <row>
	    <entry>Wampler4</entry>
	    <entry>5th degree polynomial
	    </entry><entry>OK</entry></row>
	  <row>
	    <entry>Wampler5</entry>
	    <entry>5th degree polynomial
	     </entry><entry>OK</entry></row>
	</tbody>
      </tgroup>
    </table>


    <para>As can be seen from the table,
      <application>gretl</application> does a good job of tracking the
      certified results. (Total run time for the tests was 0.195
      seconds on a 333MHz i686 machine running GNU/Linux.)  With the
      <filename>Filip</filename> data set, where the model is 
      <informalequation>
	<graphic fileref="figures/poly.png"/>
	<texmath>
	  $$y_t=\beta_0+\beta_1 x_t+\beta_2 x^2_t+\beta_3 x^3_t+\cdots
	  +\beta_{10}x^{10}_t+\epsilon$$
	</texmath>
      </informalequation>
      <application>gretl</application> refuses to produce estimates
      due to a high degree of multicollinearity (the popular
      commercial econometrics program <citetitle>Eviews
	3.1</citetitle> also baulks at this regression).  Other than
      that, the program produces accurate coefficient estimates in all
      cases.</para>

    <para>In the <filename>NoInt1</filename> and
      <filename>NoInt2</filename> datasets there is a methodological
      disagreement over the calculation of the coefficient of
      determination, &rsqu;, where the regression does not have an
      intercept. <application>gretl</application> reports the square
      of the correlation coefficient between the fitted and actual
      values of the dependent variable in this case, while the NIST
      figure is 
      <informalequation>
	<graphic fileref="figures/nistr2.png"/> 
      <texmath> 
	$$R^2 = 1 - \frac{\mathrm{ESS}}{\sum y^2}$$ 
      </texmath> 
      </informalequation>
      There is no universal agreement among statisticians on the
      <quote>correct</quote> formula (see for instance the discussion
      in Ramanathan, 1998, pp. 163&ndash;4). <citetitle>Eviews
	3.1</citetitle> produces a different figure again (which has a
      negative value for the <filename>NoInt</filename> test files).
      The figure chosen by NIST was obtained for these regressions
      using the command</para>

    <para><command> genr r2alt = 1 - $ess/sum(y * y)
    </command></para>

    <para>and the numbers thus obtained were in agreement with the
      certified values, up to <application>gretl</application>'s
      precision.</para>

    <para>
      As for the univariate summary statistics, the certified
      values given by NIST are for the sample mean, sample standard
      deviation and sample lag-1 autocorrelation coefficient.  NIST
      note that the latter statistic <quote>may have several
	definitions</quote>. The certified value is computed as 

      <informalequation>
	<graphic fileref="figures/nistr1.png"/>
	<texmath>
	  $$r_1=\frac{\sum^T_2(y_t-\bar{y})(y_{t-1}-\bar{y})} {\sum^T_1
	  (y_t - \bar{y})^2}$$
	</texmath>
      </informalequation>
      
      while <application>gretl</application>
      gives the correlation coefficient between 
      <emphasis>y</emphasis><subscript><emphasis>t</emphasis></subscript>
      and
      <emphasis>y</emphasis><subscript><emphasis>t</emphasis>&minus; 1</subscript>.
      For the purposes of comparison, the NIST figure was computed
      within <application>gretl</application> as follows:</para>

    <programlisting>
      genr y1 = y(-1) 
      genr ybar = mean(y) 
      genr devy = y - ybar genr
      devy1 = y1 - ybar 
      genr ssy = sum(devy * devy) 
      smpl 2 ; 
      genr ssyy1 = sum(devy * devy1) 
      genr rnist = ssyy1 / ssy
    </programlisting>

    <para>The figure <varname>rnist</varname> was then compared with the
      certified value.
    </para>

    <para>With this modification, all the summary statistics were in
      agreement (to the precision given by
      <application>gretl</application>) for all datasets
      (<filename>PiDigits</filename>, <filename>Lottery</filename>,
      <filename>Lew</filename>, <filename>Mavro</filename>,
      <filename>Michelso</filename>, <filename>NumAcc1</filename>,
      <filename>NumAcc2</filename>, <filename>NumAcc3</filename> and
      <filename>NumAcc4</filename>).</para>

  </chapter>

  <appendix id="app-a"><title>Crash course in econometrics</title>

    <section id="metrics-intro"><title>Introduction</title>

      <para> This highly condensed discussion is no substitute for a
	proper training in econometrics, but hopefully it may serve to
	orient people without an econometrics background who
	nonetheless have some interest in experimenting with
	<application>gretl</application>, or even hacking on it (dream
	on!).
      </para>

      <para>
	The substance of econometrics is the quantification of
	relationships between economic variables using statistical
	methods.  The larger purposes served by this work include
	forecasting, policy analysis, and the assessment or refinement
	of economic theories.
      </para>

      <para>
	Much of econometrics is based on the classical statistical
	paradigm of sampling theory. Econometric relationships are
	generally represented as stochastic equations, the simplest of
	which is the simple linear regression model</para> 

	<informalequation>
        <graphic fileref="figures/linreg.png"/>
	<texmath>
	  $$y_t = \alpha + \beta x_t + \epsilon_t$$
	</texmath>
        </informalequation>
	      
	<para>This model represents the <emphasis>dependent
	  variable</emphasis>, <emphasis>y</emphasis>, at observation
	<emphasis>t</emphasis>, as a linear function (with intercept
	&alpha; and slope &beta;) of a single <emphasis>independent
	  variable</emphasis>,
	<emphasis>x</emphasis><subscript><emphasis>t</emphasis></subscript>, 
	plus a random <quote>error</quote> or
	<quote>disturbance</quote> term
	&epsiv;<subscript><emphasis>t</emphasis></subscript>. The random
	term may be thought of as summing up various influences on
	<emphasis>y</emphasis><subscript><emphasis>t</emphasis></subscript> 
	not specified in the equation, or as reflecting inherently
	stochastic behavior in <emphasis>y</emphasis>, or in various
	other ways. The task of econometric estimation is to provide
	estimates of the parameters &alpha; and &beta; (and the
	variance, &sigma;<superscript>2</superscript>, of the error
	term), given some actual data on <emphasis>x</emphasis> and
	<emphasis>y</emphasis>.</para> 

    </section>

    <section id="metrics-ols"><title>Least Squares</title>

      <para>Provided that the distribution of &epsiv; satisfies
	certain conditions (it has a mean or expected value of zero;
	it has a constant variance; it is uncorrelated across
	observations; it is uncorrelated with the independent
	variable, <emphasis>x</emphasis>), the Gauss&ndash;Markov
	Theorem tells us that optimal estimates of the regression
	parameters are delivered by the method of <emphasis>least
	  squares</emphasis>.
      </para>

      <para>
	Let the least-squares estimates of &alpha; and &beta; be
	denoted by <emphasis>a</emphasis> and <emphasis>b</emphasis>:
	we then represent the equation <quote>fitted</quote> via least
	squares as 

	<informalequation>
	  <graphic fileref="figures/yhat.png"/>
	  <texmath>
	    $$\hat{y}_t = a + b x_t$$
	  </texmath>
	</informalequation> 

	We define the regression <quote>residual</quote> as 

	<informalequation>
	  <graphic fileref="figures/resid.png"/>
	  <texmath>
	    $$\hat{\epsilon}_t = y_t - \hat{y}_t$$
	  </texmath>
	</informalequation> 

	the difference between actual
	<emphasis>y</emphasis> at observation <emphasis>t</emphasis>
	and the <quote>fitted</quote> or predicted value (which lies
	on the least-squares regression line). The least squares
	method consists in finding the specific coefficient values
	<emphasis>a</emphasis> and <emphasis>b</emphasis> which
	produce the smallest possible sum of squared residuals.
	Provided the equation in view is indeed linear, this is just
	an exercise in the differential calculus. The sum of squared
	residuals (or estimated errors), ESS, is a function of
	<emphasis>a</emphasis> and <emphasis>b</emphasis> (and the
	data).  One takes the partial derivatives of ESS with respect
	to both <emphasis>a</emphasis> and <emphasis>b</emphasis> and
	sets them to zero, then solves the resulting two equations for
	the implied <emphasis>a</emphasis> and <emphasis>b</emphasis>
	values. The same principle extends to higher-dimensional
	systems.</para>

    </section>

    <section id="pop-and-sample"><title>Population and sample</title>

      <para>On the classical sampling paradigm, the actual observed
	data
	(<emphasis>x</emphasis><subscript><emphasis>t</emphasis></subscript>, 
	<emphasis>y</emphasis><subscript><emphasis>t</emphasis></subscript>) 
	from any given period are conceived as a particular sample
	<emphasis>realization</emphasis> of the (potentially infinite)
	<emphasis>population</emphasis> of
	<emphasis>x</emphasis><subscript><emphasis>t</emphasis></subscript>, 
	<emphasis>y</emphasis><subscript><emphasis>t</emphasis></subscript> 
	pairs that could have been observed, given different possible
	<quote>drawings</quote> from the distribution of the error
	term, &epsiv;<subscript><emphasis>t</emphasis></subscript>, in
	each sub-period.  Application of the least-squares method
	guarantees the <quote>best fit</quote> (in a well-defined
	sense) to any given set of sample data, but data from any
	finite sample may be more or less unrepresentative of the
	larger population from which they are drawn.
      </para>

      <para>
	One sort of question of interest in econometrics is: Given
	that the conditions of optimality of the least-squares
	estimates are satisfied, how much confidence can one have that
	the coefficients derived via least squares lie within a
	specified distance of the <quote>true</quote> underlying
	parameters that characterize the data-generating process (DGP)
	itself?  This is the issue of <quote>confidence
	  intervals.</quote>  As a rough rule of thumb, a 95 percent
	confidence interval for a parameter can be constructed as the
	point estimate plus-or-minus two standard errors: that is
	(again, roughly) one can have 95 percent confidence that a
	given coefficient estimate is within 2 standard errors of the
	corresponding unknown parameter. Standard errors for
	coefficient estimates are reported routinely within
	<application>gretl</application>.
      </para>

      <para>
	One is also interested in hypothesis tests: For instance,
	given a certain non-zero value for a least-squares regression
	coefficient, how confident can we be that the corresponding
	unknown parameter is non-zero? It's always possible that even
	if <emphasis>x</emphasis> and <emphasis>y</emphasis> are
	<quote>truly</quote> statistically independent, one derives a
	non-zero correlation between observations of these variables
	in a finite sample by the <quote>luck of the draw.</quote> The
	larger the sample, and the larger the (absolute value of the)
	sample correlation, presumably the smaller the probability
	that this correlation could be a <quote>luck of the
	draw</quote> phenomenon.
      </para>

      <para>
	So-called <quote>p-values</quote> for hypothesis tests
	(reported in various contexts within
	<application>gretl</application>) address this issue: the
	p-value is the probability of observing a sample effect of the
	given, observed magnitude or greater, conditional on there
	being no real effect at the population level.  Thus a small
	p-value counts against the Null Hypothesis (no real effect).
	If the p-value for a given coefficient estimate is less than
	&alpha; one says that the coefficient is <quote>statistically
	  significant</quote> at the &alpha; level (e.g. a
	coefficient with a p-value &lt; .05 is significant at the 5
	percent level).</para> 

    </section>

    <section id="metrics-pathol"><title>Regression pathologies</title> 

      <para>Two other questions of interest in econometrics are:  How
	can we tell if the conditions for optimality of the least
	squares estimates are <emphasis>not</emphasis> satisfied?  And if it
	appears these conditions are violated, what better
	alternatives to least squares are available?
	<application>gretl</application> offers a battery of tests and
	alternative estimators. The tests are available under the
	menus in the model window after running a regression; the
	alternative estimators are under the Model menu in the main
	window, while details on their use are in the online help
	file. I can't hope to teach much about these topics here.
	Please consult, for instance, Ramanathan (1998) or, for a
	comprehensive treatment, Greene (2000).  Ruud (2000) is also a
	rather comprehensive resource.</para>

    </section>

    <section id="linearity"><title>Linearity: how restrictive?</title>


      <para>As mentioned above, the least squares regression routines
	in <application>gretl</application> presuppose a linear model.
	This is not quite as restrictive as it may seem.  We require
	an equation that is linear in its
	<emphasis>parameters</emphasis>, but this does not necessarily
	mean that it is linear in the variables of interest.  For
	example, all of the following equations represent nonlinear
	relationships between <emphasis>y</emphasis> and
	<emphasis>x</emphasis> that can readily be estimated using OLS
	or similar:

	<informalequation>
	<graphic fileref="figures/nonlin1.png"/>
	<texmath>
	  $y_t = \alpha + \beta(1/x_t) + \epsilon_t$
	</texmath>
	</informalequation>

	<informalequation>
	<graphic fileref="figures/nonlin2.png"/>
	  <texmath>
	  $y_t = \alpha + \beta x_t + \gamma x^2_t + \epsilon_t$
	  </texmath>
	</informalequation>

	<informalequation>
	<graphic fileref="figures/nonlin3.png"/>
	<texmath>
	  $y_t = \alpha + \beta \log x_t + \epsilon_t$
	</texmath>
	</informalequation>

	<informalequation>
	<graphic fileref="figures/nonlin4.png"/>
	<texmath>
	  $\log y_t = \alpha + \beta \log x_t + \epsilon_t$
	</texmath>
	</informalequation>

	Of course there are nonlinear relationships that cannot
	be reduced to linearity by this sort of change of variables:
	OLS cannot deal with these; more complex estimators are
	required. Of these additional estimators,
	<application>gretl</application> offers only the logit and
	probit models for a binomial dependent variable (but see also
	<xref linkend="ils"/> above for the use of iterated least
	squares in estimating nonlinear models).  As mentioned above,
	<application>gretl</application> can be complemented by GNU R
	or GNU Octave for further analysis of nonlinear relationships.
      </para>
    </section>

  </appendix>

  <appendix id="app-b"><title>Technical notes</title>

    <para>
      <application>Gretl</application> is written in the C programming
      language.  I have abided as far as possible by the ISO/ANSI C
      Standard (C89), although the graphical user interface and some
      other components necessarily make use of platform-specific
      extensions.
    </para>

    <para>
      <application>gretl</application> is being developed under Linux.
      The shared library and command-line client should compile and
      run on any platform that supports ISO/ANSI C and has the zlib
      compression library installed. The homepage for zlib can be
      found at <ulink
	url="http://www.info-zip.org/pub/infozip/zlib/">info-zip.org</ulink>. 
      If the GNU readline library is found on the host system this
      will be used for <application>gretcli</application>, providing a
      much enhanced editable command line. See the <ulink
	url="http://cnswww.cns.cwru.edu/~chet/readline/rltop.html">readline 
	homepage</ulink>.
    </para>

    <para> The graphical client program should compile and run on any
      system that, in addition to the above requirements, offers GTK
      version 1.2.3 or higher (see <ulink
	url="http://www.gtk.org/">gtk.org</ulink>).
    </para>

    <para>
      <application>gretl</application> calls gnuplot for graphing. You
      can find gnuplot at <ulink
	url="http://www.gnuplot.org/">gnuplot.org</ulink>.  As of this
      writing the curent version is 3.7.1.
    </para>

    <para>
      Some features of <application>gretl</application> (the built-in
      spreadsheet, the <quote>session</quote> icon window, some file
      selection dialogs) make use of Adrian Feguin's
      <application>gtkextra</application> library. The
      <application>gretl</application> source package includes a copy
      of the relevant gtkextra code, but if a sufficiently recent
      version of the gtkextra shared library is detected on the host
      system this will be used instead, reducing the size of the GUI
      executable.  You can find gtkextra at <ulink
	url="gtkextra.sourceforge.net">sourceforge</ulink>. 
    </para>

    <para>
      A binary version of the program is available for the Microsoft
      Windows platform (32-bit version, i.e. Windows 95 or higher).
      This version was cross-compiled under Linux using mingw (the GNU
      C compiler, <application>gcc</application>, ported for use with
      win32) and linked against the Microsoft C library,
      <filename>msvcrt.dll</filename>.  It uses Tor Lillqvist's port
      of GTK to win32. Mingw lives at <ulink
	url="http://www.mingw.org/">mingw.org</ulink> and Tor's pages
      can be found <ulink
	url="http://user.sgic.fi/~tml/gimp/win32/">here</ulink>. The
      (free, open-source) Windows installer program is courtesy of
      Jordan Russell (<ulink
	url="http://www.jrsoftware.org/">jrsoftware.org</ulink>).
    </para>

    <para>I'm hopeful that some users with coding skills may consider
      <application>gretl</application> sufficiently interesting to be
      worth improving and extending.  To date I have not attempted to
      document the libgretl API (other than via the header files
      you'll find in the <filename>lib/src</filename> subdirectory of
      the source package).  But I welcome email on this subject and if
      there's sufficient interest I'll put some time into
      documentation. 
    </para>

  </appendix>

    <appendix id="app-c"><title>Advanced econometric analysis
	with free software</title>

      <para>As mentioned in the main text,
	<application>gretl</application> offers a reasonably full
	selection of least-squares based estimators, plus a few
	additional estimators sych as (binomial) logit and probit.
	Advanced users may, however, find
	<application>gretl</application>'s menu of statistical
	routines restrictive.</para>

      <para>No doubt some advanced users will prefer to write their
	own statistical code in a fundamental computer language such
	as C, C++ or Fortran.  Another option is to use a relatively
	high-level language that offers easy matrix manipulation and
	that already has numerous statistical routines built in, or
	available as add-on packages. If the latter option sounds
	attractive, and you are interested in using free, open source
	software, I would recommend taking a look at either GNU R
	(<ulink url="http://www.r-project.org/">r-project.org</ulink>)
	or (<ulink url="http://www.octave.org/">GNU Octave</ulink>).
	These programs are very close to the commercial programs S and
	Matlab respectively. 
      </para>

      <para>
	Also as mentioned above, <application>gretl</application>
	offers the facility of exporting data in the formats of both
	Octave and R.  In the case of Octave, the
	<application>gretl</application> data set is saved thus: the
	first variable listed for export is treated as the dependent
	variable and is saved as a vector, <varname>y</varname>, while
	the remaining variables are saved jointly as a matrix,
	<varname>X</varname>. You can pull the <varname>X</varname>
	matrix apart if you wish, once the data are loaded in Octave.
	See the Octave manual for details. As for R, the exported data
	file preserves any time series structure that is apparent to
	<application>gretl</application>. The series are saved as
	individual structures. The data should be brought into R using
	the <command>source()</command> command.
      </para>

      <para>
	Of these two programs, R is perhaps more likely to be of
	immediate interest to econometricians since it offers more in
	the way of statistical routines (e.g. generalized linear
	models, maximum likelihood estimation, time series methods).
	I have therefore supplied <application>gretl</application>
	with a convenience function for moving data quickly into R.
	Under <application>gretl</application>'s Session menu, you
	will find the entry <quote>Start GNU R</quote>.  This writes
	out an R version of the current
	<application>gretl</application> data set
	(<filename>Rdata.tmp</filename>, in the user's gretl
	directory), and sources it into a new R session. A few details
	on this follow.
      </para>

      <para>
	First, the data are brought into R by writing a temporary
	version of <filename>.Rprofile</filename> in the current
	working directory.  (If such a file exists it is referenced by
	R at startup.)  In case you already have a personal
	<filename>.Rprofile</filename> in place, the original file is
	temporarily moved to <filename>.Rprofile.gretltmp</filename>,
	and on exit from <application>gretl</application> it is
	restored.  (If anyone can suggest a cleaner way of doing this
	I'd be happy to hear of it.)
      </para>

      <para>
	Second, the particular way R is invoked depends on the
	internal <application>gretl</application> variable
	<varname>Rcommand</varname>, whose value may be set under the
	File, Preferences menu.  The default command is
	<command>RGui.exe</command> under MS Windows. Under X it is
	either <command>R --gui=gnome</command> if an installation of
	the Gnome desktop (<ulink
	  url="http://www.gnome.org/">gnome.org</ulink>) was detected
	at compile time, or <command>xterm -e R</command> if Gnome was
	not found. Please note that (at present) at most three
	space-separated elements in this command string will be
	processed; any extra elements are ignored.
      </para>

  </appendix>

    <bibliography>

      <bibliomixed>
	<bibliomset relation='article'>Box, G. E. P. and Muller, M. E.
	  (1958) <title>A Note on the Generation of Random Normal
	    Deviates</title>,</bibliomset> <bibliomset
	  relation='journal'> <title>Annals of Mathematical
	    Statistics</title>, 29, pp. 610&ndash;11. </bibliomset>
      </bibliomixed>

      <bibliomixed>
	Greene, William H. (2000) <title>Econometric Analysis</title>,
	4th edition, Upper Saddle River, NJ: Prentice-Hall. 
      </bibliomixed>

      <bibliomixed>
	<bibliomset relation='article'>MacKinnon, J. G. and White, H.
	  (1985) <title>Some Heteroskedasticity-Consistent Covariance
	    Matrix Estimators with Improved Finite Sample
	    Properties</title>, </bibliomset> <bibliomset
	  relation='journal'> <title>Journal of Econometrics</title>,
	  29, pp. 305&ndash;25. </bibliomset>
      </bibliomixed>

      <bibliomixed>
	R Core Development Team (2000) <title>An Introduction to
	  R</title>, version 1.1.1, <!-- <ulink
	url="http://cran.r-project.org/doc/manuals/R-intro.pdf"></ulink>. 
	-->
      </bibliomixed>

      <bibliomixed>
	Ramanathan, Ramu (1998) <title>Introductory Econometrics with
	  Applications</title>, 4th edition, Fort Worth: Dryden.
      </bibliomixed>

      <bibliomixed>Ruud, Paul A. (2000) <title>An Introduction to
	  Classical Econometric Theory</title>, New York and Oxford:
	Oxford University Press.
      </bibliomixed>

      <bibliomixed>
	<bibliomset relation='article'> Salkever, D. (1976) <title>The
	    Use of Dummy Variables to Compute Predictions, Prediction
	    Errors, and Confidence Intervals</title>, </bibliomset>
	<bibliomset relation='journal'> <title>Journal of
	    Econometrics</title>, 4, <artpagenums>pp.
	    393&ndash;7</artpagenums>. </bibliomset>
      </bibliomixed> 

    </bibliography>

</book>







