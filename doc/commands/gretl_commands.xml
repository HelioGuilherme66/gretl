<?xml version="1.0"?>
<!DOCTYPE commandlist SYSTEM "gretl_commands.dtd">
<commandlist language="english">

<?PSGML NOFILL label code altforms altform menu-path other-access?>

  <command name="add" section="Tests" label="Add variables">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print estimates for augmented model</effect>
	</option>
      </options>
      <examples>
        <example>add 5 7 9</example>
        <example>add xx yy zz</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Must be invoked after an estimation command.  The variables in
	<repl>varlist</repl> are added to the previous model and the
	new model is estimated.  If more than one variable is added,
	the <mathvar>F</mathvar> statistic for the added variables
	will be printed (for the OLS procedure only) along with its
	p-value. A p-value below 0.05 means that the coefficients are
	jointly significant at the 5 percent level.
      </para>
      <para context="cli">
	If the <lit>--quiet</lit> option is given the printed results
	are confined to the test for the joint significance of the
	added variables, otherwise the estimates for the augmented
	model are also printed.  In the latter case, the
	<lit>--vcv</lit> flag causes the covariance matrix for the
	coefficients to be printed also.
      </para>
      <para context="gui">
	The selected variables are added to the previous model and the
	new model estimated.  If more than one variable is added, the
	<mathvar>F</mathvar> statistic for the added variables will be
	printed (for the OLS procedure only) along with its p-value. A
	p-value below 0.05 means that the coefficients are jointly
	significant at the 5 percent level. 
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/add variables</menu-path>
    </gui-access>

  </command>

  <command name="addto" section="Tests" context="cli">

    <usage>
      <arguments>
        <argument>modelID</argument>
	<argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print estimates for augmented model</effect>
	</option>
      </options>
      <examples>
        <example>addto 2 5 7 9</example>
      </examples>
    </usage>

    <description>
      <para>
	Works like the <cmd>add</cmd> command, except that you specify
	a previous model (using its ID number, which is printed at the
	start of the model output) to take as the base for adding
	variables.  The example above adds variables number 5, 7 and 9
	to Model 2.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/add variables</menu-path>
    </gui-access>

  </command>

  <command name="adf" section="Tests" label="Augmented Dickey-Fuller test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>varname</argument>
      </arguments>
      <options>
	<option>
	  <flag>--nc</flag>
	  <effect>test without a constant</effect>
	</option>
	<option>
	  <flag>--c</flag>
	  <effect>with constant only</effect>
	</option>
	<option>
	  <flag>--ct</flag>
	  <effect>with constant and trend</effect>
	</option>
	<option>
	  <flag>--ctt</flag>
	  <effect>with constant, trend and trend squared</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print regression results</effect>
	</option>
      </options>
      <examples>
	<example>adf 0 y</example>
        <example>adf 2 y --nc --c --ct</example>
      </examples>
    </usage>

    <description>
      <para context="gui">This command needs an integer lag order; if
      the order is zero a standard (not augmented) Dickey&ndash;Fuller
      test is run.</para>

      <para>
	Computes statistics for a set of Dickey&ndash;Fuller tests on
	  the specified variable, the null hypothesis being that the
	  variable has a unit root.</para>

      <para>By default, three variants of the test are shown: one
	  based on a regression containing a constant, one using a
	  constant and linear trend, and one using a constant and a
	  quadratic trend.  You can control the variants that are
	  presented by specifying one or more of the option flags.
      </para>

      <para>In all cases the dependent variable is the first
	difference of the specified variable, <mathvar>y</mathvar>,
	and the key independent variable is the first lag of
	<mathvar>y</mathvar>.  The model is constructed so that the
	coefficient on lagged <mathvar>y</mathvar> equals 1 minus the
	root in question.  For example, the model with a constant may
	be written as <equation status="display"
	tex="\[(1-L)y_t=\beta_0+(1-\alpha)y_{t-1}+\epsilon_t\]"
	ascii="(1 - L)y(t) = b0 + (1-a)y(t-1) + e(t)"
	graphic="adf1"/></para>

      <para>
	If the lag order, <mathvar>p</mathvar>, is greater than 0,
	then <mathvar>p</mathvar> lags of the dependent variable are
	included on the right-hand side of each test
	regression.</para>

      <para><emphasis>P-</emphasis>values for this test are based on
	MacKinnon (1996).  The relevant code is included by kind
	permission of the author.</para>

    </description>

    <gui-access>
      <menu-path>/Variable/Augmented Dickey-Fuller test</menu-path>
    </gui-access>

  </command>

  <command name="append" section="Dataset" context="cli">

    <usage>
      <arguments>
        <argument>datafile</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Opens a data file and appends the content to the current
	dataset, if the new data are compatible.  The program will try
	to detect the format of the data file (native, plain text, CSV
	or BOX1).
      </para>

    </description>

    <gui-access>
      <menu-path>/File/Append data</menu-path>
    </gui-access>

  </command>

  <command name="ar" section="Estimation" label="Autoregressive estimation">

    <usage>
      <arguments>
        <argument>lags</argument>
	<argument separated="true">depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
      <examples>
        <example>ar 1 3 4 ; y 0 x1 x2 x3</example>
      </examples>
    </usage>

    <description>
      <para>
	Computes parameter estimates using the generalized
	Cochrane&ndash;Orcutt iterative procedure (see Section 9.5 of
	Ramanathan). Iteration is terminated when successive error
	sums of squares do not differ by more than 0.005 percent or
	after 20 iterations.</para>

      <para context="gui">
	The <quote>list of AR lags</quote> specifies the structure of
	the error process.  For example, the entry <quote>1 3
	  4</quote> corresponds to the process: 
	<equation status="display" 
	  tex="\[u_t = \rho_1u_{t-1} + \rho_3 u_{t-3} +
	    \rho_4 u_{t-4} + e_t\]"
	  ascii="u(t) = rho1*u(t-1) + rho3+u(t-3) + rho4*u(t-4)"
	  graphic="arlags"/>
      </para>

      <para context="cli">
	<repl quote="true">lags</repl> is a list of lags in the
	residuals, terminated by a semicolon. In the above example,
	the error term is specified as 
	<equation status="display" 
	  tex="\[u_t = \rho_1u_{t-1} + \rho_3 u_{t-3} +
	    \rho_4 u_{t-4} + e_t\]"
	  ascii="u(t) = rho(1)*u(t-1) + rho(3)+u(t-3) + rho(4)*u(t-4)"
	  graphic="arlags"/>
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Time series/Autoregressive estimation</menu-path>
    </gui-access>

  </command>

  <command name="arch" section="Tests" label="ARCH test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>depvar</argument>
	<argument>indepvars</argument>
      </arguments>
      <examples>
        <example>arch 4 y 0 x1 x2 x3</example>
      </examples>
    </usage>

    <description>
      <para context="gui">
	This command needs an integer lag order.
      </para>

      <para>
	Tests the model for ARCH (Autoregressive Conditional
	Heteroskedasticity) of the specified lag order. If the LM test
	statistic has a p-value below 0.10, then ARCH estimation is
	also carried out.  If the predicted variance of any
	observation in the auxiliary regression is not positive, then
	the corresponding squared residual is used instead. Weighted
	least squares estimation is then performed on the original
	model.
      </para>
      <para>
	See also <cmdref targ="garch"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/ARCH</menu-path>
    </gui-access>

  </command>

  <command name="arma" section="Estimation" label="ARMA model">

    <usage>
      <arguments>
        <argument>p</argument>
	<argument>q</argument>
	<argument separated="true">depvar</argument>
	<argument optional="true">indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--native</flag>
	  <effect>Use native plugin (default)</effect>
        </option>
        <option>
	  <flag>--x-12-arima</flag>
	  <effect>use X-12-ARIMA for estimation</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
      </options>
      <examples>
        <example>arma 1 2 ; y</example>
	<example>arma 2 2 ; y 0 x1 x2 --verbose</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	If no <repl>indepvars</repl> list is given, estimates a
	univariate ARMA (Autoregressive, Moving Average) model.  The
	integer values <repl>p</repl> and <repl>q</repl> represent the
	AR and MA orders respectively.  If <repl>indepvars</repl> are
	added, the model becomes ARMAX.
      </para>

      <para context="gui">
	When chosen from the Variable menu, estimates a univariate
	ARMA (Autoregressive, Moving Average) model.  When chosen from
	the Model menu (<quote>ARMAX</quote>), estimates an ARMAX
	model, with additional exogenous regressors.
      </para>

      <para>The default is to use the <quote>native</quote> gretl ARMA
	function; in the case of a univariate ARMA model
	<program>X-12-ARIMA</program> may be used instead (if the
	<program>X-12-ARIMA</program> package for gretl is installed).
      </para>

      <para context="cli">The options given above may be combined,
	except that the covariance matrix is not available when
	estimation is by <program>X-12-ARIMA</program>.</para>

      <para>The native gretl ARMA algorithm is largely due to Riccardo
	<quote>Jack</quote> Lucchetti.  It uses a conditional maximum
	likelihood procedure, implemented via iterated least squares
	estimation of the outer product of the gradient (OPG)
	regression.  See <manref targ="jack-arma"/> for the logic of
	the procedure.  The AR coefficients (and those for any
	additional regressors) are initialized using an OLS
	auto-regression, and the MA coefficients are initialized at
	zero.</para>

      <para>The AIC value given in connection with ARMA models is
	calculated according to the definition used in
	<program>X-12-ARIMA</program>, namely
	  <equation status="display" 
	  tex="\[\mbox{AIC}=-2L+2k\]"
	  ascii="AIC = -2L + 2k"
	  graphic="aic"/> where <mathvar>L</mathvar> is the
	log-likelihood and <mathvar>k</mathvar> is the total number of
	parameters estimated. The <quote>frequency</quote> figure
	printed in connection with AR and MA roots is the &lgr; value
	that solves
	  <equation status="display" 
	  tex="\[z=re^{i2\pi\lambda}\]"
	  ascii="z = r * exp(i*2*pi*lambda)"
	  graphic="lambda"/> where <mathvar>z</mathvar> is the root in
	question and <mathvar>r</mathvar> is its modulus.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/ARMA model, /Model/Time series/ARMAX</menu-path>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="boxplot" section="Graphs" label="Boxplots">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--notches</flag>
	  <effect>show 90 percent interval for median</effect>
	</option>
      </options>
    </usage>

    <description>

      <para>These plots (after Tukey and Chambers) display the
	distribution of a variable. The central box encloses the
	middle 50 percent of the data, i.e. it is bounded by the first
	and third quartiles.  The <quote>whiskers</quote> extend to
	the minimum and maximum values.  A line is drawn across the
	box at the median.</para> 

      <para>In the case of notched boxes, the notch shows the limits
	of an approximate 90 percent confidence interval for the
	median.  This is obtained by the bootstrap method.</para>

      <para context="gui">After each variable specified in the boxplot
	command, a parenthesized boolean expression may be added, to
	limit the sample for the variable in question.  A space must
	be inserted between the variable name or number and the
	expression. Suppose you have salary figures for men and women,
	and you have a dummy variable <lit>GENDER</lit> with value 1
	for men and 0 for women.  In that case you could draw
	comparative boxplots with the following line in the boxplots
	dialog:</para>

      <para context="cli">After each variable specified in the boxplot
	command, a parenthesized boolean expression may be added, to
	limit the sample for the variable in question.  A space must
	be inserted between the variable name or number and the
	expression. Suppose you have salary figures for men and women,
	and you have a dummy variable <lit>GENDER</lit> with value 1
	for men and 0 for women.  In that case you could draw
	comparative boxplots with the following <repl
	  quote="true">varlist</repl>:</para>

      <code>
	salary (GENDER=1) salary (GENDER=0)
      </code>

      <para>Some details of gretl's boxplots can be controlled via a
	(plain text) file named <filename>.boxplotrc</filename>.  For
	details on this see <manref targ="sect-boxplots"/>.</para>

    </description>

    <gui-access>
      <menu-path>/Data/Graph specified vars/Boxplots</menu-path>
    </gui-access>

  </command>

  <command name="chow" section="Tests" label="Chow test">

    <usage>
      <arguments>
        <argument>obs</argument>
      </arguments>
      <examples>
        <example>chow 25</example>
        <example>chow 1988:1</example>
      </examples>
    </usage>

    <description>
      <para context="gui">This command needs an observation number (or
      date, with dated data).</para>

      <para>
	Must follow an OLS regression.  Creates a dummy variable which
	equals 1 from the split point specified by <repl>obs</repl> to
	the end of the sample, 0 otherwise, and also creates
	interaction terms between this dummy and the original
	independent variables.  An augmented regression is run
	including these terms and an <mathvar>F</mathvar> statistic is
	calculated, taking the augmented regression as the
	unrestricted and the original as restricted.  This statistic
	is appropriate for testing the null hypothesis of no
	structural break at the given split point.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Chow test</menu-path>
    </gui-access>

  </command>

  <command name="coeffsum" section="Tests" label="Sum of coefficients">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <examples>
        <example>coeffsum xt xt_1 xr_2</example>
      </examples>
    </usage>

    <description>
      <para context="gui">This command needs a list of variables,
	selected from the set of independent variables in a given
	model.</para>
      <para context="gui">
	Calculates the sum of the coefficients on the variables in the
	specified list.  Prints this sum along with its standard error
	and the p-value for the null hypothesis that the sum is zero.
      </para>
      <para context="cli">
	Must follow a regression.  Calculates the sum of the
	coefficients on the variables in <repl>varlist</repl>.  Prints
	this sum along with its standard error and the p-value for the
	null hypothesis that the sum is zero.  
      </para>
      <para>Note the difference between this and <cmdref
	  targ="omit"/>, which tests the null
	hypothesis that the coefficients on a specified subset of
	independent variables are <emphasis>all</emphasis> equal to
	zero.</para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/sum of coefficients</menu-path>
    </gui-access>

  </command>

  <command name="coint" section="Tests" 
    label="Engle-Granger cointegration test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>depvar</argument>
	<argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--nc</flag>
	  <effect>do not include a constant</effect>
	</option>
      </options>
      <examples>
	<example>coint 4 y x1 x2</example>
      </examples>
    </usage>

    <description>
      <para>
	The Engle&ndash;Granger cointegration test.  Carries out
	Augmented Dickey&ndash;Fuller tests on the null hypothesis
	that each of the variables listed has a unit root, using the
	given lag order.  The cointegrating regression is estimated,
	and an ADF test is run on the residuals from this regression.
	The Durbin&ndash;Watson statistic for the cointegrating
	regression is also given.
      </para>
      <para><emphasis>P-</emphasis>values for this test are based on
	MacKinnon (1996).  The relevant code is included by kind
	permission of the author.
      </para>
      <para context="cli">
	By default, the cointegrating regression contains a constant.
	If you wish to suppress the constant, add the <lit>--nc</lit>
	flag. 
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Time series/Cointegration test/Engle-Granger</menu-path>
    </gui-access>

  </command>

  <command name="coint2" section="Tests" label="Johansen cointegration test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>depvar</argument>
	<argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--verbose</flag>
	  <effect>print details of auxiliary regressions</effect>
        </option>
      </options>
      <examples>
        <example>coint2 2 y x</example>
	<example>coint2 4 y x1 x2 --verbose</example>
      </examples>
    </usage>

    <description>
      <para>
	Carries out the Johansen trace test for cointegration among
	the listed variables for the given order. Critical values are
	computed via J. Doornik's gamma approximation (Doornik, 1998).
	For details of this test see Hamilton, <book>Time Series
	Analysis</book> (1994), Chapter 20.
      </para>

      <para>
	The following table is offered as a guide to the
	interpretation of the results shown for the test, for the
	3-variable case.  <lit>H0</lit> denotes the null hypothesis,
	<lit>H1</lit> the alternative hypothesis, and <lit>c</lit> the
	number of cointegrating relations.</para>

      <code>
                 Rank     Trace test         Lmax test
                          H0     H1          H0     H1
                 ---------------------------------------
                  0      c = 0  c = 3       c = 0  c = 1
                  1      c = 1  c = 3       c = 1  c = 2
                  2      c = 2  c = 3       c = 2  c = 3
                 ---------------------------------------
      </code>

    </description>

    <gui-access>
      <menu-path>/Model/Time series/Cointegration test/Johansen</menu-path>
    </gui-access>

  </command>

  <command name="compact" section="Dataset" context="gui"
    label="Compact data">

    <description>

      <para>When you add to a dataset a series that is of higher
	frequency, it is necessary to <quote>compact</quote> the new
	series.  For instance, a monthly series will have to be
	compacted to fit into a quarterly dataset.</para>  

      <para>In addition, you may sometimes want to compact an entire
	dataset to a lower frequency (perhaps, prior to adding a
	lower-frequency variable to the dataset).</para>

      <para>Gretl offers four options for compacting:</para>

      <ilist>
	<li><para>Averaging: The value written to the dataset will be
	    the arithmetic mean of the relevant series values.  For
	    instance the value written for the first quarter of 1990
	    will be the average of the values for January, February
	    and March of 1990.</para>
	</li>

	<li><para>Summing: The value written to the dataset will be
	    the sum of the relevant higher-frequency values.  For
	    example, the first-quarter value will be the sum of the
	    January, February and March values.</para>
	</li>

	<li><para>End-of-period values: The value written to the
	    dataset is the last relevant value from the
	    higher-frequency data.  For example, the first quarter of
	    1990 will get the March 1990 value.</para>
	</li>

	<li><para>Start-of-period values: The value written to the
	    dataset is the first relevant value from the
	    higher-frequency data.  For example, the first quarter of
	    1990 will get the January 1990 value.</para>
	</li>
      </ilist>

      <para>In the case of compacting an entire dataset, the choice
	you make in this dialog box sets the default method.  But if
	you have set a compaction method for an individual variable
	(menu item <quote>Variable/Edit attributes</quote>) that
	method is used rather than the default.  If the compaction
	method is already set for all variables, the choice of a
	default compaction method is not presented.</para>

    </description>
  </command>

  <command name="corc" section="Estimation" label="Cochrane-Orcutt estimation">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
      <examples>
        <example>corc 1 0 2 4 6 7</example>
      </examples>
    </usage>

    <description>
      <para>
	Computes parameter estimates using the Cochrane&ndash;Orcutt
	iterative procedure (see Section 9.4 of Ramanathan). Iteration
	is terminated when successive estimates of the autocorrelation
	coefficient do not differ by more than 0.001 or after 20
	iterations.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Time series/Cochrane-Orcutt</menu-path>
    </gui-access>

  </command>

  <command name="corr" section="Statistics" context="cli">

    <usage>
      <arguments>
        <argument optional="true">varlist</argument>
      </arguments>
      <examples>
        <example>corr y x1 x2 x3</example>
      </examples>
    </usage>

    <description>
      <para>
	Prints the pairwise correlation coefficients for the variables
	in <repl>varlist</repl>, or for all variables in the data set
	if <repl>varlist</repl> is not given.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Correlation matrix</menu-path>
      <other-access>Main window pop-up menu (multiple selection)</other-access>
    </gui-access>

  </command>

  <command name="corrgm" section="Statistics" label="Correlogram">

    <usage>
      <arguments>
        <argument>variable</argument>
        <argument optional="true">maxlag</argument>
      </arguments>
      <examples>
        <example>corrgm x 12</example>
      </examples>
    </usage>

    <description>
      <para>
	Prints the values of the autocorrelation function for the
	<repl>variable</repl> specified (either by name or number).
	See Ramanathan, Section 11.7.  It is thus
	<equation status="inline" 
	  tex="$\rho(u_t, u_{t-s})$"
	  ascii="rho(u(t), u(t-s))"
	  graphic="autocorr"/> where <mathvar>u<sub>t</sub></mathvar>
	is the <mathvar>t</mathvar>th observation of the variable
	<mathvar>u</mathvar> and <mathvar>s</mathvar> is the number of
	lags.
      </para>

      <para>
	The partial autocorrelations are also shown: these are net of
	the effects of intervening lags.  The command also graphs the
	correlogram and prints the Box&ndash;Pierce
	<mathvar>Q</mathvar> statistic for testing the null hypothesis
	that the series is <quote>white noise</quote>. This is
	asymptotically distributed as chi-square with degrees of
	freedom equal to the number of lags used.
      </para>

      <para>
	If a <repl>maxlag</repl> value is specified the length of the
	correlogram is limited to at most that number of lags,
	otherwise the length is determined automatically.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Correlogram</menu-path>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="criteria" section="Utilities" context="cli">

    <usage>
      <arguments>
        <argument>ess</argument>
        <argument>T</argument>
        <argument>k</argument>
      </arguments>
      <examples>
        <example>criteria 23.45 45 8</example>
      </examples>
    </usage>

    <description>
      <para>
	Computes the Akaike Information Criterion (AIC) and Schwarz's
	Bayesian Information Criterion (BIC), given <repl>ess</repl>
	(error sum of squares), the number of observations
	(<mathvar>T</mathvar>), and the number of coefficients
	(<mathvar>k</mathvar>). <mathvar>T</mathvar>,
	<mathvar>k</mathvar>, and <repl>ess</repl> may be numerical
	values or names of previously defined variables.
      </para>
    </description>

  </command>

  <command name="critical" section="Utilities" context="cli">

    <usage>
      <arguments>
        <argument>dist</argument>
        <argument>param1</argument>
	<argument optional="true">param2</argument>
      </arguments>
      <examples>
        <example>critical t 20</example>
        <example>critical X 5</example>
        <example>critical F 3 37</example>
      </examples>
    </usage>

    <description>
      <para>If <repl>dist</repl> is <lit>t</lit>, <lit>X</lit> or
	<lit>F</lit>, prints out the critical values for the student's
	<mathvar>t</mathvar>, chi-square or <mathvar>F</mathvar>
	distribution respectively, for the common significance levels
	and using the specified degrees of freedom, given as
	<repl>param1</repl> for t and chi-square, or
	<repl>param1</repl> and <repl>param2</repl> for
	<mathvar>F</mathvar>. If <repl>dist</repl> is <lit>d</lit>,
	prints the upper and lower values of the Durbin-Watson
	statistic at 5 percent significance, for the given number of
	observations, <repl>param1</repl>, and for the range of 1 to 5
	explanatory variables.
      </para>
    </description>

    <gui-access>
      <menu-path>/Utilities/Statistical tables</menu-path>
    </gui-access>

  </command>

  <command name="cusum" section="Tests" label="CUSUM test">

    <description>
      <para>
	Must follow the estimation of a model via OLS.  Performs the
	CUSUM test for parameter stability.  A series of (scaled)
	one-step ahead forecast errors is obtained by running a series
	of regressions: the first regression uses the first
	<mathvar>k</mathvar> observations and is used to generate a
	prediction of the dependent variable at observation
	<mathvar>k</mathvar> + 1; the second uses the first
	<mathvar>k</mathvar> + 1 observations and generates a
	prediction for observation <mathvar>k</mathvar> + 2, and so on
	(where <mathvar>k</mathvar> is the number of parameters in the
	original model).  The cumulated sum of the scaled forecast
	errors is printed and graphed.  The null hypothesis of
	parameter stability is rejected at the 5 percent significance
	level if the cumulated sum strays outside of the 95 percent
	confidence band.
      </para>

      <para>
	The Harvey&ndash;Collier <mathvar>t</mathvar>-statistic for
	testing the null hypothesis of parameter stability is also
	printed.  See Chapter 7 of Greene's <book>Econometric
	  Analysis</book> for details.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/CUSUM</menu-path>
    </gui-access>

  </command>

  <command name="data" section="Dataset" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>Reads the variables in <repl>varlist</repl> from a
	database (gretl or RATS 4.0), which must have been opened
	previously using the <cmdref targ="open"/> command. In
	addition, a data frequency and sample range must be
	established using the <cmdref targ="setobs"/> and <cmdref
	  targ="smpl"/> commands
	prior to using this command. Here is a full example:</para>

      <code>
	open macrodat.rat
	setobs 4 1959:1
	smpl ; 1999:4
	data GDP_JP GDP_UK</code>

      <para>These commands open a database named
	<filename>macrodat.rat</filename>, establish a quarterly data
	set starting in the first quarter of 1959 and ending in the
	fourth quarter of 1999, and then import the series named
	<lit>GDP_JP</lit> and <lit>GDP_UK</lit>.</para>

      <para>If the series to be read are of higher frequency than the
	working data set, you must specify a compaction method as
	below:</para>

      <code>
	data (compact=average) 
	LHUR PUNEW</code>

      <para>The four available compaction methods are
	<quote>average</quote> (takes the mean of the high frequency
	observations), <quote>last</quote> (uses the last
	observation), <quote>first</quote> and <quote>sum</quote>.
      </para> 

    </description>

    <gui-access>
      <menu-path>/File/Browse databases</menu-path>
    </gui-access>

  </command>

  <command name="delete" section="Dataset" context="cli">

    <usage>
      <arguments>
        <argument optional="true">varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>Removes the listed variables (given by name or number)
	from the dataset. <emphasis>Use with caution</emphasis>: no
	confirmation is asked, and any variables with higher ID
	numbers will be re-numbered.</para>

      <para>If no <repl>varlist</repl> is given with this command, it
	deletes the last (highest numbered) variable from the
	dataset.</para>
    </description>

    <gui-access>
      <menu-path>Main window pop-up (single selection)</menu-path>
    </gui-access>

  </command>

  <command name="density" section="Statistics" context="gui"
    label="Kernel density estimation">

    <description>

      <para>Kernel density estimation proceeds by defining a set of
	evenly spaced reference points, over a suitable range in
	relation to the range of the data, and attributing a density
	to each reference point based on the actual observations in
	the vicinity.</para>
      
      <para>The formula used to compute the estimated density at each
	reference point, x, is
      </para>

      <code>
	f(x) = (1/nh) sum(t=1 to n) k((x - x(t)) / h)
      </code>

      <para>where n denotes the number of data points, h is a
	"bandwidth" parameter, and k() is the kernel function.  The
	larger the value of the bandwidth parameter, the smoother the
	estimated density.</para>

      <para>You are given the choice of using a Gaussian kernel (the
	standard normal density) or the Epanechnikov kernel.  By
	default, the bandwidth is that suggested as a rule of thumb by
	Silverman (1986), namely
      </para>

      <code>
	h = 0.9 min(s, IQR/1.349) n^{1/5}
      </code>

      <para>where s denotes the standard deviation of the data and IQR
	denotes the inter-quartile range.  You can widen or shrink the
	bandwidth via the <quote>bandwidth adjustment factor</quote>:
	the actual bandwidth used is obtained by multiplying the
	Silverman value by the adjustment factor.
      </para>

      <para>For a good introductory discussion of kernel density
	estimation see Chapter 15 of Davidson and MacKinnon's
	Econometric Theory and Methods.
      </para>

    </description>

  </command>  

  <command name="dialog" section="Estimation" context="gui"
    label="Model dialog box">

    <description>
      <para>To select the dependent variable, highlight a variable in
	the list on the left and press the <quote>Choose</quote>
	button pointing to the Dependent variable slot.  If you check
	the <quote>Set as default</quote> box, the selected variable
	will be pre-selected as dependent when the model dialog is
	next opened.  Short-cut: double-click on a variable on the
	left to select it as the dependent variable and also set it as
	the default.</para>

      <para>To select independent variables, highlight them on the
	left and press the <quote>Add</quote> button (or click the
	right mouse button).  You can highlight several contiguous
	variables by dragging with the mouse.  You can highlight a
	group of non-contiguous variables by clicking on them with the
	<lit>Ctrl</lit> key pressed.</para>

    </description>

  </command>

  <command name="diff" section="Transformations" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The first difference of each variable in <repl>varlist</repl>
	is obtained and the result stored in a new variable with the
	prefix <lit>d_</lit>.  Thus <cmd>diff x y</cmd> creates the
	new variables <lit>d_x = x(t) - x(t-1)</lit> and <lit>d_y =
	  y(t) - y(t-1)</lit>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Add variables/first differences</menu-path>
    </gui-access>

  </command>

  <command name="else" section="Programming" context="cli">

    <description><para>See <cmdref targ="if"/>.</para>
    </description>

  </command>

  <command name="end" section="Programming" context="cli">

    <description>
      <para>
	Ends a block of commands of some sort.  For example, <cmd>end
	  system</cmd> terminates an equation <cmdref targ="system"/>.
      </para>
    </description>

  </command>

  <command name="endif" section="Programming" context="cli">

    <description><para>See <cmdref targ="if"/>.</para>
    </description>

  </command>

  <command name="endloop" section="Programming" context="cli">

    <description>
      <para>
	Marks the end of a command loop.  See <cmdref targ="loop"/>.
      </para>
    </description>

  </command>

  <command name="eqnprint" section="Printing" context="cli">

    <usage>
      <arguments>
        <argument optional="true">-f filename</argument>
      </arguments>
      <options>
        <option>
	  <flag>--complete</flag>
	  <effect>Create a complete document</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Must follow the estimation of a model.  Prints the estimated
	model in the form of a &latex; equation.  If a filename is
	specified using the <lit>-f</lit> flag output goes to that
	file, otherwise it goes to a file with a name of the form
	<filename>equation_N.tex</filename>, where <lit>N</lit> is the
	number of models estimated to date in the current session.
	See also <cmdref targ="tabprint"/>.
      </para>

      <para>
	If the <lit>--complete</lit> flag is given, the &latex; file is
	a complete document, ready for processing; otherwise it must
	be included in a document.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /LaTeX</menu-path>
    </gui-access>

  </command>

  <command name="equation" section="Estimation" context="cli">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <examples>
        <example>equation y x1 x2 x3 const</example>
      </examples>
    </usage>

    <description>
      <para>
	Specifies an equation within a system of equations (see 
	<cmdref targ="system"/>).  The syntax for specifying an
	equation within an SUR system is the same as that for, e.g., 
	<cmdref targ="ols"/>.  For an equation within a Three-Stage Least
	Squares system you may either (a) give an OLS-type equation
	specification and provide a common list of instruments using the
	<cmd>instr</cmd> keyword (again, see <cmdref targ="system"/>),
	or (b) use the same equation syntax as for <cmdref targ="tsls"/>.
      </para>
    </description>

  </command>

  <command name="estimate" section="Estimation" context="cli">

    <usage>
      <arguments>
        <argument>systemname</argument>
        <argument>estimator</argument>
      </arguments>
      <options>
	<option>
	  <flag>--iterate</flag>
	  <effect>iterate to convergence</effect>
	</option>
	<option>
	  <flag>--no-df-corr</flag>
	  <effect>no degrees of freedom correction</effect>
	</option>
	<option>
	  <flag>--geomean</flag>
	  <effect>see below</effect>
	</option>
      </options>
      <examples>
        <example>estimate "Klein Model 1" method=fiml</example>
	<example>estimate Sys1 method=sur</example>
	<example>estimate Sys1 method=sur --iterate</example>
      </examples>
    </usage>

    <description>
      <para>
	Calls for estimation of a system of equations, which must have
	been previously defined using the <cmdref targ="system"/>
	command.  The name of the system should be given first,
	surrounded by double quotes if the name contains spaces.  The
	estimator, which must be one of <cmd>ols</cmd>,
	<cmd>tsls</cmd>, <cmd>sur</cmd>, <cmd>3sls</cmd>,
	<cmd>fiml</cmd> or <cmd>liml</cmd>, is preceded by the string
	<lit>method=</lit>.
      </para>
      <para>If the system in question has had a set of restrictions
	applied (see the <cmdref targ="restrict"/> command),
	estimation will be subject to the specified restrictions.
      </para>
      <para>
	If the estimation method is <cmd>sur</cmd> or <cmd>3sls</cmd>
	and the <lit>--iterate</lit> flag is given, the estimator will
	be iterated.  In the case of SUR, if the procedure converges
	the results are maximum likelihood estimates.  Iteration of
	three-stage least squares, however, does not in general
	converge on the full-information maximum likelihood results.
	The <lit>--iterate</lit> flag is ignored for other methods of
	estimation.  
      </para>
      <para>If the equation-by-equation estimators <cmd>ols</cmd> or
	<cmd>tsls</cmd> are chosen, the default is to apply a degrees
	of freedom correction when calculating standard errors. This
	can be suppressed using the <lit>--no-df-corr</lit> flag. This
	flag has no effect with the other estimators; no degrees of
	freedom correction is applied in any case.
      </para>
      <para>By default, the formula used in calculating the
	elements of the cross-equation covariance matrix is
	<equation status="display"
	tex="\[\hat{\sigma}_{i,j}=\frac{\hat{u}_i' \hat{u}_j}{T}\]"
	ascii="sigma(i,j) = u(i)' * u(j) / T"
	graphic="syssigma1"/>
	If the <lit>--geomean</lit> flag is
	given, a degrees of freedom correction is applied: the
	formula is
	<equation status="display"
	tex="\[\hat{\sigma}_{i,j}=\frac{\hat{u}_i' \hat{u}_j}{\sqrt{(T-k_i)(T-k_j)}}\]"
	ascii="sigma(i,j) = u(i)' * u(j) / sqrt((T - ki) * (T - kj))"
	graphic="syssigma2"/>
	where the <mathvar>k</mathvar>s denote the number of
	independent parameters in each equation.
      </para>
    </description>

  </command>

  <command name="export" section="Dataset" context="gui"
    label="Export data">

    <description>
      <para>You may export data in Comma-Separated Values (CSV)
	format: such data may be opened in spreadsheets and many other
	application programs.</para>

      <para>You may also export data in the native formats of GNU R or
	GNU octave.  For further information on these programs (both
	of which support advanced statistical analysis) please see
	their respective websites, http://www.r-project.org/ and
	http://www.octave.org/</para>
    </description>
  </command>

  <command name="factorized" section="Graphs" context="gui"
    label="Factorized plot">

    <description>
      <para>This command requires the selection of three variables,
	the last of which must be a dummy variable (values 1 or 0).
	The Y variable is plotted against the X variable, with the
	data points colored differently depending on the value of the
	third.</para>

      <para>Example: You have data on wages and educational attainment
	for a sample of people; you also have a dummy variable with
	value 1 for men and 0 for women (as in Ramanathan's
	<filename>data7-2</filename>).  A <quote>factorized
	  plot</quote> of <lit>WAGE</lit> against <lit>EDUC</lit>
	using the <lit>GENDER</lit> dummy as factor will show the data
	points for men in one color and those for women in another
	(with a legend to identify them).</para>

    </description>

  </command>
  

  <command name="fcast" section="Prediction" context="cli">

    <usage>
      <arguments>
        <argument optional="true" >startobs endobs</argument>
	<argument>fitvar</argument>
      </arguments>
      <examples>
        <example>fcast 1997:1 2001:4 f1</example>
	<example>fcast fit2</example>
      </examples>
    </usage>

    <description>
      <para>
	Must follow an estimation command.  Forecasts are generated
	for the specified range (or the largest possible range if no
	<repl>startobs</repl> and <repl>endobs</repl> are given) and
	the values saved as <repl>fitvar</repl>, which can be
	printed, graphed, or plotted. The right-hand side variables
	are those in the original model.  There is no provision to
	substitute other variables.  If an autoregressive error
	process is specified (for <cmd>hilu</cmd>, <cmd>corc</cmd>,
	and <cmd>ar</cmd>) the forecast is conditional one step ahead
	and incorporates the error process.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Model data/Forecasts</menu-path>
    </gui-access>

  </command>

  <command name="fcasterr" section="Prediction"
    label="Forecasts with confidence intervals">

    <usage>
      <arguments>
        <argument>startobs</argument>
        <argument>endobs</argument>
      </arguments>
      <options>
        <option>
	  <flag>--plot</flag>
	  <effect>display graph</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	After estimating a model via OLS you can use this command to
	print out fitted values over the specified observation range,
	along with the estimated standard errors of those predictions
	and 95 percent confidence intervals.</para>  

      <para>The standard errors are calculated in the manner described
	by Wooldridge in chapter 6 of his <book>Introductory
	  Econometrics</book>. They incorporate two sources of
	variation: the variance associated with the expected value of
	the dependent variable, conditional on the given values of the
	independent variables, and the variance of the regression
	residuals.</para>

    </description>

    <gui-access>
      <menu-path>Model window, /Model data/Forecasts</menu-path>
    </gui-access>

  </command>

  <command name="fit" section="Prediction" context="cli">

    <description>
      <para>
	A shortcut to <cmd>fcast</cmd>. Must follow an estimation
	command.  Generates fitted values, in a series called
	<lit>autofit</lit>, for the current sample, based on the last
	regression.  In the case of time-series models, also pops up a
	graph of fitted and actual values of the dependent variable
	against time.
      </para>
    </description>

  </command>

  <command name="freq" section="Statistics" context="cli">

    <usage>
      <arguments>
        <argument>var</argument>
      </arguments>
      <options>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of histogram</effect>
        </option>
        <option>
	  <flag>--gamma</flag>
	  <effect>test for gamma distribution</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	With no options given, displays the frequency distribution for
	<repl>var</repl> (given by name or number) and shows the
	results of the Doornik&ndash;Hansen chi-square test for
	normality.
      </para>
      <para>
	If the <literal>--quiet</literal> option is given, the
	histogram is not shown.  If the <literal>--gamma</literal>
	option is given, the test for normality is replaced by Locke's
	nonparametric test for the null hypothesis that the variable
	follows the gamma distribution; see Locke (1976), Shapiro and
	Chen (2001).
      </para>
      <para>
	In interactive mode a graph of the distribution is displayed.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Frequency distribution</menu-path>
    </gui-access>

  </command>

  <command name="function" section="Programming" context="cli">

    <description>
      <para>
	Define a function.  This entry still to be written.
      </para>
    </description>

  </command>  

  <command name="garch" section="Estimation" label="GARCH model">

    <usage>
      <arguments>
        <argument>p</argument>
	<argument>q</argument>
	<argument separated="true">depvar</argument>
	<argument optional="true">indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--arma-init</flag>
	  <effect>initial variance parameters from ARMA</effect>
        </option>
      </options>
      <examples>
        <example>garch 1 1 ; y</example>
	<example>garch 1 1 ; y 0 x1 x2 --robust</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Estimates a GARCH model (GARCH = Generalized Autoregressive
	Conditional Heteroskedasticity), either a univariate model or,
	if <repl>indepvars</repl> are specified, including the given
	exogenous variables.  The integer values <repl>p</repl> and
	<repl>q</repl> represent the lag orders in the conditional
	variance equation.
	<equation status="display"
	  tex="\[h_t = \alpha_0 + \sum_{i=1}^q \alpha_i \varepsilon^2_{t-i} +
	  \sum_{j=1}^p \beta_i h_{t-j}\]"
	  ascii="h(t) = a(0) + sum(i=1 to q) a(i)*u(t-i) + sum(j=1 to p) b(j)*h(t-j)"
	  graphic="garch_h"/>
      </para>

      <para context="gui">
	Estimates a GARCH model (GARCH = Generalized Autoregressive
	Conditional Heteroskedasticity), either a univariate model or,
	if independent variables are selected, including the given
	exogenous variables.  The conditional variance equation is
	shown below.
	<equation status="display" tex="\[h_t = \alpha_0 + 
	\sum_{i=1}^q \alpha_i \varepsilon^2_{t-i} + \sum_{j=1}^p
	\beta_i h_{t-j}\]" ascii="h(t) = a(0) + sum(i=1 to q) a(i)*u(t-i) +
	sum(j=1 to p) b(j)*h(t-j)" graphic="garch_h"/>
      </para>

      <para>The gretl GARCH algorithm is basically that of Fiorentini,
	Calzolari and Panattoni (1996), used by kind permission of
	Professor Fiorentini.</para>

      <para context="cli">Several variant estimates of the coefficient
	covariance matrix are available with this command.  By
	default, the Hessian is used unless the <lit>--robust</lit>
	option is given, in which case the QML (White) covariance
	matrix is used.  Other possibilities (e.g. the information
	matrix, or the Bollerslev&ndash;Wooldridge estimator) can be
	specified using the <cmdref targ="set"/> command.
      </para>

      <para context="gui">Several variant estimates of the coefficient
	covariance matrix are available with this command.  By
	default, the Hessian is used unless the <quote>Robust standard
	  errors</quote> box is checked, in which case the QML (White)
	covariance matrix is used.  Other possibilities (e.g. the
	information matrix, or the Bollerslev&ndash;Wooldridge
	estimator) can be specified using the <cmdref targ="set"/>
	command.
      </para>

      <para context="cli">
	By default, the estimates of the variance parameters are
	initialized using the unconditional error variance from
	initial OLS estimation for the constant, and small positive
	values for the coefficients on the past values of the squared
	error and the error variance.  The flag <lit>--arma-init</lit>
	calls for the starting values of these parameters to be
	set using an initial ARMA model, exploiting the relationship
	between GARCH and ARMA set out in Chapter 21 of Hamilton's
	Time Series Analysis.  In some cases this may improve the
	chances of convergence.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Time series/GARCH model</menu-path>
    </gui-access>

  </command>

  <command name="genr" section="Dataset"
    label="Generate a new variable">

    <usage>
      <arguments>
        <argument>newvar</argument>
        <argument>= formula</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Creates new variables, usually through transformations of
	existing variables. See also <cmdref targ="diff"/>, <cmdref
	  targ="logs"/>,
	<cmdref targ="lags"/>, <cmdref targ="ldiff"/>, <cmdref
	  targ="multiply"/> and
	<cmdref targ="square"/> for shortcuts.
      </para>

      <para>
	Supported <emphasis>arithmetical operators</emphasis> are, in
	order of precedence: <lit>^</lit> (exponentiation);
	<lit>*</lit>, <lit>/</lit> and <lit>%</lit> (modulus or
	remainder); <lit>+</lit> and <lit>-</lit>. 
      </para>

      <para>
	The available <emphasis>Boolean operators</emphasis> are
	(again, in order of precedence): <lit>!</lit> (negation),
	<lit>&amp;</lit> (logical AND), <lit>|</lit> (logical OR),
	<lit>&gt;</lit>, <lit>&lt;</lit>, <lit>=</lit>,
	<lit>&gt;=</lit> (greater than or equal), <lit>&lt;=</lit>
	(less than or equal) and <lit>!=</lit> (not equal).  The
	Boolean operators can be used in constructing dummy variables:
	for instance <lit>(x > 10)</lit> returns 1 if <lit>x</lit>
	&gt; 10, 0 otherwise.</para>

      <para>Supported <emphasis>functions</emphasis> fall into these
	groups:
      </para>

      <ilist>
	<li><para>Standard math functions: <func>abs</func>,
	    <func>cos</func>, <func>exp</func>, <func>int</func>
	    (integer part), <func>ln</func> (natural log:
	    <func>log</func> is a synonym), <func>sin</func>,
	    <func>sqrt</func>.</para>
	</li>
	<li><para>Statistical functions: <func>max</func> (maximum
	    value in a series), <func>min</func> (minimum),
	    <func>mean</func> (arithmetic mean), <func>median</func>,
	    <func>var</func> (variance) <func>sd</func> (standard
	    deviation), <func>sst</func> (sum of squared deviations
	    from the mean), <func>sum</func>, <func>cov</func>
	    (covariance), <func>corr</func> (correlation coefficient),
	    <func>pvalue</func>, <func>sort</func>, <func>cum</func>
	    (cumulate, or running sum), <func>resample</func>
	    (resample a series with replacement, for bootstrap
	    purposes), <func>hpfilt</func> (Hodrick&ndash;Prescott
	    filter: this function returns the <quote>cycle</quote>
	    component of the series), <func>bkfilt</func>
	    (Baxter&ndash;King bandpass filter).</para>
	</li>
	<li><para>Time-series functions: <func>diff</func> (first
	    difference), <func>ldiff</func> (log-difference, or first
	    difference of natural logs).  To generate lags of a
	    variable <lit>x</lit>, use the syntax <cmd>x(-N)</cmd>,
	    where <lit>N</lit> represents the desired lag length; to
	    generate leads, use <cmd>x(+N)</cmd>.</para>
	</li>
	<li><para>Dataset functions: <func>misszero</func> (replace
	    the missing observation code in a given series with
	    zeros); <func>zeromiss</func> (the inverse operation to
	    <func>misszero</func>); <func>nobs</func> (return the
	    number of valid observations in a given data series);
	    <func>missing</func> (at each observation, 1 if the
	    argument has a missing value, 0 otherwise);
	    <func>ok</func> (the opposite of <func>missing</func>).
	  </para>
	</li>
	<li><para>Pseudo-random numbers: <func>uniform</func>,
	    <func>normal</func>.</para>
	</li>
      </ilist> 

      <para>
	All of the above functions with the exception of
	<func>cov</func>, <func>corr</func>, <func>pvalue</func>,
	<func>uniform</func> and <func>normal</func> take as their
	single argument either the name of a variable (note that you
	can't refer to variables by their ID numbers in a
	<cmd>genr</cmd> formula) or an expression that evaluates to a
	variable (e.g. <lit>ln((x1+x2)/2)</lit>). <func>cov</func> and
	<func>corr</func> both require two arguments, and return
	respectively the covariance and the correlation coefficient
	between the arguments. The <func>pvalue</func> function takes
	the same arguments as the <cmdref targ="pvalue"/> command, but
	in this context commas should be placed between the arguments.
	This function returns a one-tailed p-value, and in the case of
	the normal and <emphasis>t</emphasis> distributions, it is for
	the <quote>short tail</quote>.  With the normal, for example,
	both 1.96 and -1.96 will give a result of around 0.025.
	</para>

      <para>
	<func>uniform()</func> and <func>normal()</func>, which do not
	take arguments, return pseudo-random series drawn from the
	uniform (0&ndash;1) and standard normal distributions
	respectively (see also the
	<cmdref targ="set"/> command, <lit>seed</lit> option).
	Uniform series are generated using the Mersenne
	Twister;<footnote><para>See Matsumoto and
	    Nishimura (1998).  The implementation is provided by
	    <program>glib</program>, if available, or by the C code
	    written by Nishimura and Matsumoto.</para></footnote> for
	normal series the method of Box and Muller (1958) is used,
	taking input from the Mersenne Twister.</para>

      <para>Besides the operators and functions just noted there are
	some special uses of <cmd>genr</cmd>:
      </para>

      <ilist>
	<li><para><cmd>genr time</cmd> creates a time trend variable
	    (1,2,3,&hellip;) called <cmd>time</cmd>. <cmd>genr
	      index</cmd> does the same thing except that the variable
	    is called <lit>index</lit>.</para>
	</li>
	<li><para><cmd>genr dummy</cmd> creates dummy variables up to
	    the periodicity of the data.  E.g. in the case of
	    quarterly data (periodicity 4), the program creates
	    <lit>dummy_1</lit> = 1 for first quarter and 0 in other
	    quarters, <lit>dummy_2</lit> = 1 for the second quarter
	    and 0 in other quarters, and so on.</para>
	</li>
	<li><para><cmd>genr paneldum</cmd> creates a set of special
	    dummy variables for use with a panel data set &mdash; see
	    <cmdref targ="panel"/>.</para>
	</li>
	<li><para>Various internal variables defined in the course of
	    running a regression can be retrieved using
	    <cmd>genr</cmd>, as follows:</para>

	  <table lwidth="100pt" rwidth="300pt">
	    <row> 
	      <cell><lit>$ess</lit></cell>
	      <cell>error sum of squares</cell> 
	    </row> 
	    <row>
	      <cell><lit>$rsq</lit></cell> 
	      <cell>unadjusted <emphasis>R</emphasis>-squared</cell> 
	    </row> 
	    <row>
	      <cell><lit>$T</lit></cell> 
	      <cell>number of observations used</cell> 
	    </row> 
	    <row> 
	      <cell><lit>$df</lit></cell>
	      <cell>degrees of freedom</cell> 
	    </row> 
	    <row>
	      <cell><lit>$trsq</lit></cell>
	      <cell><emphasis>TR</emphasis>-squared (sample size times
		<emphasis>R</emphasis>-squared)</cell> 
	    </row> 
	    <row>
	      <cell><lit>$sigma</lit></cell> 
	      <cell>standard error of residuals</cell> 
	    </row> 
	    <row>
	      <cell><lit>$aic</lit></cell> 
	      <cell>Akaike Information Criterion</cell> 
	    </row> 
	    <row>
	      <cell><lit>$bic</lit></cell> 
	      <cell>Schwarz's Bayesian Information Criterion</cell> 
	    </row> 
	    <row>
	      <cell><lit>$lnl</lit></cell> 
	      <cell>log-likelihood (where applicable)</cell> 
	    </row> 
	    <row>
	      <cell>coeff(<repl>var</repl>)</cell> 
	      <cell>estimated coefficient for variable
		<repl>var</repl></cell> 
	    </row> 
	    <row>
	      <cell>stderr(<repl>var</repl>)</cell> 
	      <cell>estimated standard error for variable
		<repl>var</repl> 
	      </cell> 
	    </row> 
	    <row>
	      <cell>rho(<repl>i</repl>)</cell>
	      <cell><repl>i</repl>th order autoregressive coefficient
		for residuals</cell> 
	    </row> 
	    <row>
	      <cell>vcv(<repl>x1</repl>,<repl>x2</repl>)</cell>
	      <cell>covariance between coefficients for named
		variables <repl>x1</repl> and <repl>x2</repl> 
	      </cell> 
	    </row>
	  </table>

	</li>
      </ilist> 

      <para>
	<emphasis>Note</emphasis>: In the command-line program,
	<cmd>genr</cmd> commands that retrieve model-related data
	always reference the model that was estimated most recently.
	This is also true in the GUI program, if one uses
	<cmd>genr</cmd> in the <quote>gretl console</quote> or enters
	a formula using the <quote>Define new variable</quote> option
	under the Variable menu in the main window.  With the GUI,
	however, you have the option of retrieving data from any model
	currently displayed in a window (whether or not it's the most
	recent model).  You do this under the <quote>Model
	  data</quote> menu in the model's window.</para>

      <para>The internal series <lit>uhat</lit> and <lit>yhat</lit>
	hold, respectively, the residuals and fitted values from the
	last regression.</para>

      <para>
	Two <quote>internal</quote> dataset variables are available:
	<lit>$nobs</lit> holds the number of observations in the
	current sample range (which may or may not equal the value of
	<lit>$T</lit>, the number of observations used in estimating
	the last model), and <lit>$pd</lit> holds the frequency or
	periodicity of the data (e.g. 4 for quarterly data).</para>

      <para>The variable <lit>t</lit> serves as an index of the
	observations. For instance <lit>genr dum = (t=15)</lit>
	will generate a dummy variable that has value 1 for
	observation 15, 0 otherwise.  The variable <lit>obs</lit> is
	similar but more flexible: you can use this to pick out
	particular observations by date or name.  For example,
	<lit>genr d = (obs&gt;1986:4)</lit> or <lit>genr d =
	  (obs="CA")</lit>. The last form presumes that the
	observations are labeled; the label must be put in double
	quotes.</para>

      <para>Scalar values can be pulled from a series in the context
	of a <lit>genr</lit> formula, using the syntax
	<repl>varname</repl><lit>[</lit><repl>obs</repl><lit>]</lit>.
	The <repl>obs</repl> value can be given by number or date.
	Examples: <lit>x[5]</lit>, <lit>CPI[1996:01]</lit>.  For daily
	data, the form <repl>YYYY/MM/DD</repl> should be used, e.g.
	<lit>ibm[1970/01/23]</lit>.
      </para>

      <para>An individual observation in a series can be modified via
	<lit>genr</lit>.  To do this, a valid observation number or
	date, in square brackets, must be appended to the name of the
	variable on the left-hand side of the formula.  For example,
	<lit>genr x[3] = 30</lit> or <lit>genr x[1950:04] =
	  303.7</lit>.
      </para>

      <para>
	<tabref targ="tab-genr"/> gives several examples of uses of
	<cmd>genr</cmd> with explanatory notes; here are a
	couple of tips on dummy variables:
      </para>

      <ilist>
	<li><para>Suppose <lit>x</lit> is coded with values 1, 2, or 3
	    and you want three dummy variables, <lit>d1</lit> = 1 if
	    <lit>x</lit> = 1, 0 otherwise, <lit>d2</lit> = 1 if
	    <lit>x</lit> = 2, and so on.  To create these, use the
	    commands:</para>

	  <code>
	    genr d1 = (x=1)
	    genr d2 = (x=2)
	    genr d3 = (x=3)</code>
	</li>

	<li><para>To create <lit>z</lit> = <lit>max(x,y)</lit>
	    do</para>

	  <code>
	    genr d = x&gt;y
	    genr z = (x*d)+(y*(1-d))</code>
	</li>
      </ilist>

      <table id="tab-genr" title="Examples of use of genr command"
	lhead="Formula" rhead="Comment">
	<row>
	  <cell><lit>y = x1^3</lit></cell>
	  <cell><lit>x1</lit> cubed</cell>
	</row>          
	<row>
	  <cell><lit>y = ln((x1+x2)/x3)</lit></cell>
	  <cell></cell>
	</row>
	<row>
	  <cell><lit>z = x&gt;y</lit></cell>
	  <cell><lit>z(t)</lit> = 1 if <lit>x(t) &gt; y(t)</lit>,
	    otherwise 0</cell>
	</row> 
	<row>
	  <cell><lit>y = x(-2)</lit></cell>
	  <cell><lit>x</lit> lagged 2 periods</cell>
	</row>     
	<row>
	  <cell><lit>y = x(2)</lit></cell>
	  <cell><lit>x</lit> led 2 periods</cell>
	</row>
	<row>
	  <cell><lit>y = diff(x)</lit></cell>
	  <cell><lit>y(t) = x(t) - x(t-1)</lit></cell>
	</row>
	<row>
	  <cell><lit>y = ldiff(x)</lit></cell>
	  <cell><lit>y(t) = log x(t) - log x(t-1)</lit>, the
	    instantaneous rate of growth of <lit>x</lit></cell>
	</row>
	<row>
	  <cell><lit>y = sort(x)</lit></cell>
	  <cell>sorts <lit>x</lit> in increasing order and stores in
	    <lit>y</lit></cell>
	</row>
	<row>
	  <cell><lit>y = -sort(-x)</lit></cell>
	  <cell>sort <lit>x</lit> in decreasing order</cell>
	</row>
	<row>
	  <cell><lit>y = int(x)</lit></cell>
	  <cell>truncate <lit>x</lit> and store its integer value as
	    <lit>y</lit></cell>
	</row>
	<row>
	  <cell><lit>y = abs(x)</lit></cell>
	  <cell>store the absolute values of <lit>x</lit></cell>
	</row>
	<row>
	  <cell><lit>y = sum(x)</lit></cell>
	  <cell>sum <lit>x</lit> values excluding missing &minus;999
	    entries</cell>
	</row>
	<row>
	  <cell><lit>y = cum(x)</lit></cell>
	  <cell>cumulation: 
		<equation status="inline"
		  tex="$y_t = \sum_{\tau=1}^t x_{\tau}$"
		  ascii="y(t) = the sum from s=1 to s=t of x(s)"
		  graphic="cumulate"/>
	  </cell>
	</row>
	<row>
	  <cell><lit>aa = $ess</lit></cell>
	  <cell>set <lit>aa</lit> equal to the Error Sum of Squares
	    from last regression</cell>
	</row>
	<row>
	  <cell><lit>x = coeff(sqft)</lit></cell>
	  <cell>grab the estimated coefficient on the variable
	    <lit>sqft</lit> from the last regression</cell>
	</row>
	<row>
	  <cell><lit>rho4 = rho(4)</lit></cell>
	  <cell>grab the 4th-order autoregressive coefficient from the
	    last model (presumes an <lit>ar</lit> model)</cell>
	</row>
	<row>
	  <cell><lit>cvx1x2 = vcv(x1, x2)</lit></cell>
	  <cell>grab the estimated coefficient covariance of vars
	    <lit>x1</lit> and <lit>x2</lit> from the last model</cell>
	</row>
	<row>
	  <cell><lit>foo = uniform()</lit></cell>
	  <cell>uniform pseudo-random variable in range
	    0&ndash;1</cell>
	</row>
	<row>
	  <cell><lit>bar = 3 * normal()</lit></cell>
	  <cell>normal pseudo-random variable, &mu; = 0, &sigma; =
	    3</cell>
	</row>
	<row>
	  <cell><lit>samp = !missing(x)</lit></cell>
	  <cell>= 1 for observations where <lit>x</lit> is not
	    missing.</cell>
	</row>
      </table>

    </description>

    <gui-access>
      <menu-path>/Variable/Define new variable</menu-path>
      <other-access>Main window pop-up menu</other-access>
    </gui-access>

  </command>

  <command name="gnuplot" section="Graphs" context="cli">

    <usage>
      <arguments>
        <argument>yvars</argument>
        <argument>xvar</argument>
	<argument optional="true">dumvar</argument>
      </arguments>
      <options>
        <option>
	  <flag>--with-lines</flag>
	  <effect>use lines, not points</effect>
        </option>
        <option>
	  <flag>--with-impulses</flag>
	  <effect>use vertical lines</effect>
        </option>
        <option>
	  <flag>--suppress-fitted</flag>
	  <effect>don't show least squares fit</effect>
        </option>
        <option>
	  <flag>--dummy</flag>
	  <effect>see below</effect>
        </option>
      </options>
      <examples>
        <example>gnuplot y1 y2 x</example>
        <example>gnuplot x time --with-lines</example>
	<example>gnuplot wages educ gender --dummy</example>
      </examples>
    </usage>

    <description>
      <para>Without the <lit>--dummy</lit> option, the
	<repl>yvars</repl> are graphed against <repl>xvar</repl>. With
	<lit>--dummy</lit>, <repl>yvar</repl> is graphed against
	<repl>xvar</repl> with the points shown in different colors
	depending on whether the value of <repl>dumvar</repl> is 1 or
	0.</para>

      <para>
	The <lit>time</lit> variable behaves specially: if it does not
	already exist then it will be generated automatically.
      </para>

      <para>
	In interactive mode the result is displayed immediately. In
	batch mode a gnuplot command file is written, with a name on
	the pattern <filename>gpttmpN.plt</filename>, starting with N
	= <lit>01</lit>. The actual plots may be generated later using
	<program>gnuplot</program> (under MS Windows,
	<program>wgnuplot</program>).
      </para>

      <para>A further option to this command is available: following
	the specification of the variables to be plotted and the
	option flag (if any), you may add literal gnuplot commands to
	control the appearance of the plot (for example, setting the
	plot title and/or the axis ranges).  These commands should be
	enclosed in braces, and each gnuplot command must be
	terminated with a semi-colon.  A backslash may be used to
	continue a set of gnuplot commands over more than one line.
	Here is an example of the syntax:
      </para>

      <para>
	<lit>{ set title 'My Title'; set yrange [0:1000]; }</lit>
      </para>

    </description>

    <gui-access>
      <menu-path>/Data/Graph specified vars</menu-path>
      <other-access>Main window pop-up menu, graph button on toolbar</other-access>
    </gui-access>

  </command>

  <command name="graph" section="Graphs" context="cli">

    <usage>
      <arguments>
        <argument>yvars</argument>
        <argument>xvar</argument>
      </arguments>
      <options>
        <option>
	  <flag>--tall</flag>
	  <effect>use 40 rows</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	ASCII graphics.  The <repl>yvars</repl> (which may be given by
	name or number) are  graphed against <repl>xvar</repl> using
	ASCII symbols. The <lit>--tall</lit> flag will produce a graph
	with 40 rows and 60 columns. Without it, the graph will be 20
	by 60 (for screen output).  See also <cmdref targ="gnuplot"/>.
      </para>
    </description>

  </command>

  <command name="graphing" section="Graphs" context="gui"
    label="Graphing">

    <description>

      <para>Gretl calls a separate program, namely gnuplot, to
	generate graphs.  Gnuplot is a very full-featured graphing
	program with myriad options.  Gretl gives you direct access,
	via a graphical interface, to a subset of these options and it
	tries to choose sensible values for you; it also allows you to
	take complete control over graph details if you wish.</para>

      <para>With a graph displayed, you can click on the graph window
	for a pop-up menu with the following options:</para>

      <ilist>
	<li><para>Save as postscript: save the graph in encapsulated
	    postscript (EPS) format</para>
	</li>
	<li><para>Save as PNG: save in Portable Network Graphics
	    format</para>
	</li>
	<li><para>Save to session as icon: the graph will appear in
	    iconic form when you select <quote>Icon view</quote> from
	    the Session menu</para>
	</li>
	<li><para>Zoom: lets you select an area within the graph for
	    closer inspection</para>
	</li>
	<li><para>Print: (on the Gnome desktop and MS Windows only)
	    lets you print the graph directly</para>
	</li>
	<li><para>Copy to clipboard: (MS Windows only) lets you paste
	    the graph into Windows applications such as MS
	    Word</para>
	</li>
	<li><para>Edit: opens a controller for the plot which lets you
	    adjust various aspects of its appearance</para>
	</li>
	<li><para>Close: closes the graph window</para>
	</li>
      </ilist>

      <para>
	If you know something about gnuplot and wish to get finer
	control over the appearance of a graph than is available via
	the graphical controller (<quote>Edit</quote> option), you
	have two further options:</para>

      <ilist>
	<li><para>Once the graph is saved as a session icon, you can
	    right-click on its icon for a further pop-up menu.  One of
	    the options here is <quote>Edit plot commands</quote>,
	    which opens an editing window with the actual gnuplot
	    commands displayed. You can edit these commands and either
	    save them for future processing or send them to gnuplot
	    (with the <quote>File/Send to gnuplot</quote> menu item in
	    the plot commands editing window).</para>
	</li>
	<li><para>Another way to save the plot commands (or to save
	    the displayed plot in formats other than EPS or PNG) is to
	    use <quote>Edit</quote> item on a graph's pop-up menu to
	    invoke the graphical controller, then click on the
	    <quote>Output to file</quote> tab in the controller.  You
	    are then presented with a drop-down menu of formats in
	    which to save the graph.</para>
	</li>
      </ilist>

      <para>
	To find out more about gnuplot, see
	http://ricardo.ecn.wfu.edu/gnuplot.html or
	http://www.gnuplot.info</para>

    </description>

  </command>

  <command name="graphpag" section="Graphs" context="gui"
    label="Gretl graph page">

    <description>

      <para>The session <quote>graph page</quote> will work only if
	you have the &latex; typesetting system installed, and are able
	to generate and view postscript output.</para>

      <para>In the session icon window, you can drag up to eight
	graphs onto the graph page icon.  When you double-click on the
	graph page (or right-click and select <quote>Display</quote>),
	a page containing the selected graphs will be composed and
	opened in your postscript viewer.   From there you should be
	able to print the page.</para>

      <para>To clear the graph page, right-click on its icon and
	select <quote>Clear</quote>.</para> 

      <para>On systems other than MS Windows, you may have to adjust
	the setting for the program used to view postscript.  Find
	that under the <quote>Programs</quote> tab in the gretl
	Preferences dialog box (under the File menu in the main
	window).</para>

    </description>

  </command>

  <command name="3-D" section="Graphs" context="gui"
    label="3-dimensional plots">

    <description>
      <para>This feature works best if you have gnuplot 3.8 or higher
	installed.  In that case you can manipulate the 3-D plot with
	the mouse (rotate it, and expand or shrink the axes).</para>

      <para>In composing a 3-D plot, note that the Z-axis will be
	shown as the vertical axis.  Thus if you have some dependent
	variable that you think may be influenced by two independent
	variables, you should put the dependent variable on the
	Z-axis, and the independent variables on the X and Y
	axes.</para>  

      <para>Unlike most other gretl graphs, 3-D plots are controlled
	by gnuplot rather than gretl itself.  The gretl graph-editing
	menu is not available.</para>

    </description>
  </command>
    
  <command name="hausman" section="Tests"
    label="Hausman test (panel diagnostics)">

    <description>
      <para>
	This test is available only after estimating a model using the
	<cmdref targ="pooled"/> command (see also <cmd>panel</cmd> and
	<cmd>setobs</cmd>).  It tests the simple pooled model against
	the principal alternatives, the fixed effects and random
	effects models.</para>

      <para>
	The fixed effects model adds a dummy variable for all but one
	of the cross-sectional units, allowing the intercept of the
	regression to vary across the units.  An
	<mathvar>F</mathvar>-test for the joint significance of these
	dummies is presented.  The random effects model decomposes the
	residual variance into two parts, one part specific to the
	cross-sectional unit and the other specific to the particular
	observation.  (This estimator can be computed only if the
	number of cross-sectional units in the data set exceeds the
	number of parameters to be estimated.) The Breusch&ndash;Pagan
	LM statistic tests the null hypothesis (that the pooled OLS
	estimator is adequate) against the random effects
	alternative.</para>

      <para>
	The pooled OLS model may be rejected against both of the
	alternatives, fixed effects and random effects. Provided the
	unit- or group-specific error is uncorrelated with the
	independent variables, the random effects estimator is more
	efficient than the fixed effects estimator; otherwise the
	random effects estimator is inconsistent and the fixed effects
	estimator is to be preferred. The null hypothesis for the
	Hausman test is that the group-specific error is not so
	correlated (and therefore the random effects model is
	preferable).  A low p-value for this test counts against the
	random effects model and in favor of fixed effects.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/panel diagnostics</menu-path>
    </gui-access>

  </command>

  <command name="hccm" section="Estimation" context="cli"
    label="HCCM estimation">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Heteroskedasticity-Consistent Covariance Matrix: this command
	runs a regression where the coefficients are estimated via the
	standard OLS procedure, but the standard errors of the
	coefficient estimates are computed in a manner that is robust
	in the face of heteroskedasticity, namely using the
	MacKinnon&ndash;White <quote>jackknife</quote> procedure.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/HCCM</menu-path>
    </gui-access>

  </command>

  <command name="hccme" section="Estimation" context="gui"
    label="Robust standard errors">

    <description>
      <para>
	You are offered several variant calculations for standard
	errors that are robust in the presence of heteroskedasticity
	(and, in the case of the HAC estimator, autocorrelation).
      </para>

      <para>HC0 produces the original <quote>White's standard
	errors</quote>; HC1, HC2, HC3 and HC3a are subsequent
	variations that are generally reckoned to produce superior
	(more reliable) results.  For details of the estimators, see
	MacKinnon and White (Journal of Econometrics, 1985) or
	Davidson and MacKinnon, Econometric Theory and Methods
	(Oxford, 2004).  The labels given here are those used by
	Davidson and MacKinnon.  Variant <quote>HC3a</quote> is the
	jackknife, as described in MacKinnon and White (1985); HC3 is
	a close approximation to the jackknife.  
      </para>

      <para>If you use the HAC estimator for time-series data, you are
	able to fine-tune the lag-length using the <cmd>set</cmd>
	command.  Please see the gretl manual or the script commands
	help file for details.</para>

      <para>Two robust estimators of the covariance matrix are
	offered for GARCH models: QML is the Quasi-Maximum Likelihood
	Estimator, and BW is the Bollerslev-Wooldridge estimator.
      </para>

    </description>

  </command>

  <command name="help" section="Utilities" context="cli">

    <description>
      <para>
	Gives a list of available commands. <cmd>help</cmd>
	<repl>command</repl> describes <repl>command</repl> (e.g.
	<cmd>help smpl</cmd>).  You can type <cmd>man</cmd> instead of
	<cmd>help</cmd> if you like. 
      </para> 
    </description>

    <gui-access>
      <menu-path>/Help</menu-path>
    </gui-access>

  </command>

  <command name="hilu" section="Estimation" label="Hildreth-Lu estimation">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>Computes parameter estimates for the specified model using
	the Hildreth&ndash;Lu search procedure (fine-tuned by the CORC
	procedure). This procedure is designed to correct for serial
	correlation of the error term.  The error sum of squares of
	the transformed model is graphed against the value of rho from
	&minus;0.99 to 0.99.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Time Series/Hildreth-Lu</menu-path>
    </gui-access>

  </command>

  <command name="hsk" section="Estimation"
    label="Heteroskedasticity-corrected estimates">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	An OLS regression is run and the residuals are saved.  The
	logs of the squares of these residuals then become the
	dependent variable in an auxiliary regression, on the
	right-hand side of which are the original independent
	variables plus their squares.  The fitted values from the
	auxiliary regression are then used to construct a weight
	series, and the original model is re-estimated using weighted
	least squares.  This final result is reported.</para>

      <para>The weight series is formed as
	<equation status="inline"
	  tex="$1/\sqrt{e^{y^*}}$"
	  ascii="1/sqrt(exp(y*))"
	  graphic="hsk"/>, where <mathvar>y<super>*</super></mathvar>
	denotes the fitted values from the auxiliary
	regression.</para>
    </description>

    <gui-access>
      <menu-path>/Model/Heteroskedasticity corrected</menu-path>
    </gui-access>

  </command>

  <command name="if" section="Programming" context="cli">

    <description>
      <para>Flow control for command execution.  The syntax is:</para>

      <table lwidth="25pt" rwidth="100pt">
	<row><cell><lit>if</lit></cell>
	  <cell><repl>condition</repl></cell>
	</row>
	<row><cell></cell><cell><repl>commands1</repl></cell>
	</row>
	<row><cell><lit>else</lit></cell><cell></cell>
	</row>
	<row><cell></cell><cell><repl>commands2</repl></cell>
	</row>
	<row><cell><lit>endif</lit></cell><cell></cell>
	</row>
      </table>

      <para><repl>condition</repl> must be a Boolean expression, for
	the syntax of which see <cmdref targ="genr"/>.  The
	<cmd>else</cmd> block is optional; <lit>if</lit> &hellip;
	<lit>endif</lit> blocks may be nested.</para>
    </description>

  </command>

  <command name="import" section="Dataset" context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
      <options>
        <option>
	  <flag>--box1</flag>
	  <effect>BOX1 data</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Brings in data from a comma-separated values (CSV) format
	file, such as can easily be written from a spreadsheet
	program.  The file should have variable names on the first
	line and a rectangular data matrix on the remaining lines.
	Variables should be arranged <quote>by observation</quote>
	(one column per variable; each row represents an observation).
	See <manref targ="datafiles"/> for details.
      </para>

      <para>
	With the <lit>--box1</lit> flag, reads a data file in BOX1
	format, as can be obtained using the Data Extraction Service
	of the US Bureau of the Census.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Open data/import</menu-path>
    </gui-access>

  </command>

  <command name="info" section="Dataset" context="cli">

    <description>
      <para>
	Prints out any supplementary information stored with the
	current datafile.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Read info</menu-path>
      <other-access>Data browser windows</other-access>
    </gui-access>

  </command>

  <command name="label" section="Dataset" label="Edit attributes of variable">

    <usage>
      <arguments>
        <argument>varname</argument>
        <argument flag="-d ">description</argument>
        <argument flag="-n ">displayname</argument>
      </arguments>
      <examples>
        <example>label x1 -d "Description of x1" -n "Graph name"</example>
      </examples>
    </usage>

    <description context="cli">
      <para>Sets the descriptive label for the given variable (if the
	<lit>-d</lit> flag is given, followed by a string in double
	quotes) and/or the <quote>display name</quote> for the
	variable (if the <lit>-n</lit> flag is given, followed by a
	quoted string).  If a variable has a display name, this is
	used when generating graphs.
      </para>
    </description>

    <description context="gui">

      <para>
	In this dialog box you can:</para>

      <para>* Rename a variable.</para>

      <para>* Add or edit a description of the variable: this appears
	next to the variable name in the gretl main window.</para>

      <para>* Add or edit the "display name" for the variable.  This
	string (maximum 19 characters) is shown in place of the
	variable name when the variable is displayed in a graph.  This
	for instance you can associate a more comprehensible string
	such as "T-bill rate" with a cryptically named variable such
	as "tb3".</para>

      <para>* (In case of a time-series dataset) set the compaction
	method for the variable.  This method will be used if you
	decide to reduce the frequency of the dataset, or if you
	update the variable by importing from a database where the
	variable is at a higher frequency than in the working dataset.
      </para>

    </description>

    <gui-access>
      <menu-path>/Variable/Edit attributes</menu-path>
      <other-access>Main window pop-up menu</other-access>
    </gui-access>

  </command>

  <command name="kpss" section="Tests" label="KPSS test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>varname</argument>
      </arguments>
      <options>
	<option>
	  <flag>--trend</flag>
	  <effect>include a trend</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print regression results</effect>
	</option>
      </options>
      <examples>
	<example>kpss 8 y</example>
        <example>kpss 4 x1 --trend</example>
      </examples>
    </usage>

    <description>

      <para context="gui">
	Computes the KPSS test (Kwiatkowski, Phillips, Schmidt and
	Shin, 1992) for stationarity of a variable.  The null
	hypothesis is that the variable in question is stationary,
	either around a level or, if the <quote>include a
	  trend</quote> box is checked, around a deterministic linear
	trend.  
      </para>

      <para context="cli">
	Computes the KPSS test (Kwiatkowski, Phillips, Schmidt and
	Shin, 1992) for stationarity of a variable.  The null
	hypothesis is that the variable in question is stationary,
	either around a level or, if the <lit>--trend</lit> option is
	given, around a deterministic linear trend.  
      </para>

      <para context="gui">
	The selected lag order determines the size of the window used
	for Bartlett smoothing.  If the <quote>show regression
	  results</quote> box is checked the results of the auxiliary
	regression are printed, along with the estimated variance of
	the random walk component of the variable.
      </para>

      <para context="cli">
	The order argument determines the size of the window used for
	Bartlett smoothing.  If the <lit>--verbose</lit> option is
	chosen the results of the auxiliary regression are printed,
	along with the estimated variance of the random walk component
	of the variable.
      </para>

    </description>

    <gui-access>
      <menu-path>/Variable/KPSS test</menu-path>
    </gui-access>

  </command>

  <command name="labels" section="Dataset" context="cli">

    <description>
      <para>
	Prints out the informative labels for any variables that have
	been generated using <cmd>genr</cmd>, and any labels added to
	the data set via the GUI.
      </para>
    </description>

  </command>

  <command name="lad" section="Estimation"
    label="Least Absolute Deviation estimation">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Calculates a regression that minimizes the sum of the absolute
	deviations of the observed from the fitted values of the
	dependent variable.  Coefficient estimates are derived using
	the Barrodale&ndash;Roberts simplex algorithm; a warning is
	printed if the solution is not unique. Standard errors are
	derived using the bootstrap procedure with 500 drawings.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Least Absolute Deviation</menu-path>
    </gui-access>

  </command>

  <command name="lags" section="Transformations" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Creates new variables which are lagged values of each of the
	variables in <repl>varlist</repl>.  The number of lagged
	variables equals the periodicity. For example, if the
	periodicity is 4 (quarterly), the command <cmd>lags x y</cmd>
	creates <lit>x_1</lit> = <lit>x(t-1)</lit>, <lit>x_2</lit> =
	<lit>x(t-2)</lit>, <lit>x_3</lit> = <lit>x(t-3)</lit> and
	<lit>x_4</lit> <lit>x(t-4)</lit>. Similarly for <lit>y</lit>.
	These variables must be referred to in the exact form, that
	is, with the underscore.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Add variables/lags of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="ldiff" section="Transformations" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The first difference of the natural log of each variable in
	<repl>varlist</repl> is obtained and the result stored in a
	new variable with the prefix <lit>ld_</lit>.  Thus <cmd>ldiff
	  x y</cmd> creates the new variables <lit>ld_x</lit> =
	  <equation status="inline" 
	  tex="$\ln(x_t) - \ln(x_{t-1})$"
	  ascii="ln[x(t)] - ln[x(t-1)]"
	  graphic="ldx"/> and <lit>ld_y</lit> =
	  <equation status="inline" 
	  tex="$\ln(y_t) - \ln(y_{t-1})$"
	  ascii="ln[y(t)] - ln[y(t-1)]"
	  graphic="ldy"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Add variables/log differences</menu-path>
    </gui-access>

  </command>

  <command name="leverage" section="Tests"
    label="Influential observations">

    <usage>
      <options>
        <option>
	  <flag>--save</flag>
	  <effect>save variables</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Must immediately follow an <cmd>ols</cmd> command. Calculates
	the leverage (<mathvar>h</mathvar>, which must lie in the
	range 0 to 1) for each data point in the sample on which the
	previous model was estimated.  Displays the residual
	(<mathvar>u</mathvar>) for each observation along with its
	leverage and a measure of its influence on the estimates, 
	  <equation status="inline" 
	  tex="$uh/(1 - h)$"
	  ascii="u*h/(1-h)"
	  graphic="influence"/>. <quote>Leverage points</quote> for
	which the value of <mathvar>h</mathvar> exceeds
	2<mathvar>k</mathvar>/<mathvar>n</mathvar> (where
	<mathvar>k</mathvar> is the number of parameters being
	estimated and <mathvar>n</mathvar> is the sample size) are
	flagged with an asterisk.  For details on the concepts of
	leverage and influence see Davidson and MacKinnon (1993,
	Chapter 2).
      </para>

      <para>
	DFFITS values are also shown: these are <quote>studentized
	  residuals</quote> (predicted residuals divided by their
	standard errors) multiplied by 
	  <equation status="inline" 
	  tex="$\sqrt{h/(1 - h)}$"
	  ascii="sqrt[h/(1 - h)]"
	  graphic="dffit"/>. For a discussion of studentized residuals
	and DFFITS see G. S. Maddala, <book>Introduction to
	  Econometrics</book>, chapter 12; also Belsley, Kuh and
	Welsch (1980).  Briefly, a <quote>predicted residual</quote>
	is the difference between the observed value of the dependent
	variable at observation <mathvar>t</mathvar>, and the fitted
	value for observation <mathvar>t</mathvar> obtained from a
	regression in which that observation is omitted (or a dummy
	variable with value 1 for observation <mathvar>t</mathvar>
	alone has been added); the studentized residual is obtained by
	dividing the predicted residual by its standard error.</para>

      <para context="cli">If the <lit>--save</lit> flag is given with
	this command, then the leverage, influence and DFFITS values
	are added to the current data set.</para>

      <para context="gui">
	The "+" icon at the top of the leverage test window brings up
	a dialog box that allows you to save one or more of the test
	variables to the current data set.</para>

    </description>

    <gui-access>
      <menu-path>Model window, /Tests/influential observations</menu-path>
    </gui-access>

  </command>

  <command name="lmtest" section="Tests" label="LM tests">

    <usage>
      <options>
        <option>
	  <flag>--logs</flag>
	  <effect>non-linearity, logs</effect>
        </option>
        <option>
	  <flag>--autocorr</flag>
	  <effect>serial correlation</effect>
        </option>
        <option>
	  <flag>--squares</flag>
	  <effect>non-linearity, squares</effect>
        </option>
        <option>
	  <flag>--white</flag>
	  <effect>heteroskedasticity (White's test)</effect>
        </option>
      </options>
    </usage>

    <description context="gui">
      <para>
	Under this heading fall several hypothesis tests.  What they
	have in common is that the test involves the estimation of an
	auxiliary regression, where the dependent variable is the
	residual from some <quote>original</quote> regression.  The
	right-hand side variables include those from the original
	regression, along with some additional terms.  The test
	statistic is calculated as (sample size * Rsquared) from the
	auxiliary regression: this is distributed as chi-square with
	degrees of freedom equal to the number of additional terms,
	under the null hypothesis that the additional terms have no
	explanatory power over the residual.  A <quote>large</quote>
	chi-square value (small p-value) suggests that this null
	hypothesis should be rejected.
      </para>
    </description>

    <description context="cli">
      <para>
	Must immediately follow an <cmd>ols</cmd> command. Carries out
	some combination of the following: Lagrange Multiplier tests
	for nonlinearity (logs and squares), White's test for
	heteroskedasticity, and the LMF test for serial correlation up
	to the periodicity (see Kiviet, 1986).  The corresponding
	auxiliary regression coefficients are also printed out.  See
	Ramanathan, Chapters 7, 8, and 9 for details. In the case of
	White's test, only the squared independent variables are used
	and not their cross products.  In the case of the
	autocorrelation test, if the p-value of the LMF statistic is
	less than 0.05 (and the model was not originally estimated
	with robust standard errors) then serial correlation-robust
	standard errors are calculated and displayed.  For details on
	the calculation of these standard errors see Wooldridge (2002,
	Chapter 12).
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests</menu-path>
    </gui-access>

  </command>

  <command name="logistic" section="Estimation"
    label="Logistic regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
	<argument optional="true" flag="ymax=">value</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
      <examples>
        <example>logistic y const x</example>
        <example>logistic y const x ymax=50</example>
      </examples>
    </usage>

    <description>
      <para>
	Logistic regression: carries out an OLS regression using the
	logistic transformation of the dependent variable,
	<equation status="display" 
	  tex="\[\log\left(\frac{y}{y^*-y}\right)\]"
	  ascii="log(y/(ymax - y))"
	  graphic="logistic1"/>
      </para>

      <para context="cli">The dependent variable must be strictly
	positive.  If it is a decimal fraction, between 0 and 1, the
	default is to use a <mathvar>y<super>*</super></mathvar> value
	(the asymptotic maximum of the dependent variable) of 1. If
	the dependent variable is a percentage, between 0 and 100, the
	default <mathvar>y<super>*</super></mathvar> is 100.  If you
	wish to set a different maximum, use the optional
	<lit>ymax=</lit><repl>value</repl> syntax following the list
	of regressors.  The supplied value must be greater than all of
	the observed values of the dependent variable.</para>

      <para context="gui">The dependent variable must be strictly
	positive.  If it is a decimal fraction, between 0 and 1, the
	default is to use a ymax value (the asymptotic maximum of the
	dependent variable) of 1.  If the dependent variable is a
	percentage, between 0 and 100, the default ymax value is 100.
	You are presented with a dialog box that allows you to specify
	a different maximum if you wish.  The supplied ymax value must
	be greater than all of the observed values of the dependent
	variable.</para>

      <para>The fitted values and residuals from the regression are
	automatically transformed using 	  
	<equation status="display" 
	  tex="\[y=\frac{y^*}{1+e^{-x}}\]"
	  ascii="y = ymax / (1 + exp(-x))"
	  graphic="logistic2"/> where <mathvar>x</mathvar> represents
	either a fitted value or a residual from the OLS regression
	using the transformed dependent variable.  The reported values
	are therefore comparable with the original dependent
	variable.</para>

      <para>Note that if the dependent variable is binary, you should
	use the <cmd>logit</cmd> command instead.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Logistic</menu-path>
    </gui-access>

  </command>

  <command name="logit" section="Estimation"
    label="Logit regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Binomial logit regression. The dependent variable should be a
	binary variable.  Maximum likelihood estimates of the
	coefficients on <repl>indepvars</repl> are obtained via the EM
	or Expectation&ndash;Maximization method (see Ruud, 2000,
	Chapter 27).  As the model is nonlinear the slopes depend on
	the values of the independent variables: the reported slopes
	are evaluated at the means of those variables. The chi-square
	statistic tests the null hypothesis that all coefficients are
	zero apart from the constant.
      </para>

      <para>
	If you want to use logit for analysis of proportions (where
	the dependent variable is the proportion of cases having a
	certain characteristic, at each observation, rather than a 1
	or 0 variable indicating whether the characteristic is present
	or not) you should not use the <cmd>logit</cmd> command, but
	rather construct the logit variable (e.g. <cmd>genr lgt_p =
	  log(p/(1 - p))</cmd>) and use this as the dependent variable
	in an OLS regression.  See Ramanathan, Chapter 12.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Logit</menu-path>
    </gui-access>

  </command>

  <command name="logs" section="Transformations" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The natural log of each of the variables in
	<repl>varlist</repl> is obtained and the result stored in a
	new variable with the prefix <lit>l_</lit> which is
	<quote>el</quote> underscore.  <cmd>logs x y</cmd> creates the
	new variables <lit>l_x</lit> = ln(<lit>x</lit>) and
	<lit>l_y</lit> = ln(<lit>y</lit>).
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Add variables/logs of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="loop" section="Programming" context="cli">

    <usage>
      <arguments>
        <argument>control</argument>
      </arguments>
      <options>
	<option>
	  <flag>--progressive</flag>
	  <effect>enable special forms of certain commands</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>report details of genr commands</effect>
	</option>
      </options>
      <examples>
        <example>loop 1000</example>
	<example>loop 1000 --progressive</example>
        <example>loop while essdiff > .00001</example>
        <example>loop for i=1991..2000</example>
        <example>loop for (r=-.99; r&lt;=.99; r+=.01)</example>
      </examples>
    </usage>

    <description>
      <para>The parameter <repl quote="true">control</repl> must take
	one of four forms, as shown in the examples: an integer number
	of times to repeat the commands within the loop;
	<quote><lit>while</lit></quote> plus a numerical condition;
	<quote><lit>for</lit></quote> plus a range of values for the
	internal index variable <lit>i</lit>; or
	<quote><lit>for</lit></quote> plus three expressions in
	parentheses, separated by semicolons.  In the last form the
	left-hand expression initializes a variable, the middle
	expression sets a condition for iteration to continue, and the
	right-hand expression sets an increment or decrement to be
	applied at the start of the second and subsequent iterations.
	(This is a restricted form of the <lit>for</lit> statement in
	the C programming language.)
      </para>

      <para>This command opens a special mode in which the program
	accepts commands to be executed repeatedly.  Within a loop,
	only certain commands can be used: <cmd>genr</cmd>,
	<cmd>ols</cmd>, <cmd>print</cmd>, <cmd>printf</cmd>,
	<cmd>pvalue</cmd>, <cmd>sim</cmd>, <cmd>smpl</cmd>,
	<cmd>store</cmd>, <cmd>summary</cmd>, <cmd>if</cmd>,
	<cmd>else</cmd> and <cmd>endif</cmd>. You exit the mode of
	entering loop commands with <cmd>endloop</cmd>: at this point
	the stacked commands are executed.
      </para>

      <para>See <manref targ="looping"/> for further details and
	examples.  The effect of the <lit>--progressive</lit> option
	(which is designed for use in Monte Carlo simulations) is
	explained there.</para>
    </description>

  </command>

  <command name="markers" section="Dataset" context="gui"
    label="Add case markers">

    <description>
      <para>
	This command needs the name of a file containing "case
	markers", that is, short identifying strings for the
	individual observations in the data set (for example, country
	or city names or codes).  These marker strings should be no
	more than 8 characters long.  The file should contain one
	marker per line, and there should be just as many markers as
	observations in the current dataset. If these conditions are
	met and the specified file is found, the case markers will be
	added; they will be visible when you choose "Display values"
	under gretl's Data menu.</para>
    </description>
  </command>

  <command name="meantest" section="Tests"
    label="Difference of means">

    <usage>
      <arguments>
        <argument>var1</argument>
        <argument>var2</argument>
      </arguments>
      <options>
        <option>
	  <flag>--unequal-vars</flag>
	  <effect>assume variances are unequal</effect>
        </option>
      </options>
    </usage>

    <description>
      <para context="cli">
	Calculates the <mathvar>t</mathvar> statistic for the null
	hypothesis that the population means are equal for the
	variables <repl>var1</repl> and <repl>var2</repl>, and shows
	its p-value. By default the test statistic is calculated on
	the assumption that the variances are equal for the two
	variables; with the <lit>--unequal-vars</lit>option the
	variances are assumed to be different.  This will make a
	difference to the test statistic only if there are different
	numbers of non-missing observations for the two variables.
      </para>
      <para context="gui">
	Calculates the t statistic for the null hypothesis that the
	population means are equal for two selected variables, and
	shows its p-value.  The command may be called with or without
	the assumption that the variances are equal for the two
	variables (although this will make a difference to the test
	statistic only if there are different numbers of non-missing
	observations for the two variables.)</para>
    </description>

    <gui-access>
      <menu-path>/Data/Difference of means</menu-path>
    </gui-access>

  </command>

  <command name="missing" section="Dataset" context="gui"
    label="Missing data values">

    <description>
      <para>Set a numerical value that will be interpreted as
	<quote>missing</quote> or <quote>not available</quote>, either
	for a particular data series (under the Variable menu) or
	globally for the entire data set (under the Sample
	menu).</para> 

      <para>Gretl has its own internal coding for missing values, but
	sometimes imported data may employ a different code.  For
	example, if a particular series is coded such that a value of
	-1 indicates <quote>not applicable</quote>, you can select
	<quote>Set missing value code</quote> under the Variable menu
	and type in the value <quote>-1</quote> (without the quotes).
	Gretl will then read the -1s as missing observations.</para>
    </description>
  </command>

  <command name="modeltab" section="Utilities"
    label="The model table">

    <usage>
      <arguments>
        <argument>add</argument>
        <argument alternate="true">show</argument>
        <argument alternate="true">free</argument>
      </arguments>
    </usage>

    <description context="gui"> 
      <para>
	In econometric research it is common to estimate several
	models with a common dependent variable&mdash;the models
	differing in respect of which independent variables are
	included, or perhaps in respect of the estimator used.  In
	this situation it is convenient to present the regression
	results in the form of a table, where each column contains the
	results (coefficient estimates and standard errors) for a
	given model, and each row contains the estimates for a given
	variable across the models.</para>

      <para>Gretl provides a means of constructing such a table (and
	copying it in plain text, &latex; or Rich Text Format).  Here is
	how to do it:</para>

      <nlist>
	<li><para>1. Estimate a model which you wish to include in the
	    table, and in the model display window, under the File
	    menu, select <quote>Save to session as icon</quote> or
	    <quote>Save as icon and close</quote>.</para>
	</li>

	<li><para>2. Repeat step 1 for the other models to be included in
	    the table (up to a total of six models).</para>
	</li>

	<li><para>3. When you are done estimating the models, open the
	    icon view of your gretl session (by selecting <quote>icon
	      view</quote> under the Session menu in the main gretl
	    window, or by clicking the <quote>session icon
	      view</quote> icon on the gretl toolbar).</para>
	</li>

	<li><para>4. In session icon view, there is an icon labeled
	    <quote>Model table</quote>. Decide which model you wish to
	    appear in the left-most column of the model table and add
	    it to the table, either by dragging its icon onto the
	    Model table icon, or by right-clicking on the model icon
	    and selecting <quote>Add to model table</quote> from the
	    pop-up menu.</para>
	</li>

	<li><para>5. Repeat step 4 for the other models you wish to
	    include in the table.  The second model selected will
	    appear in the second column from the left, and so
	    on.</para>
	</li>

	<li><para>6. When you are finished composing the model table,
	    display it by double-clicking on its icon.  Under the Edit
	    menu in the window which appears, you have the option of
	    copying the table to the clipboard in various
	    formats.</para>
	</li>

	<li><para>7. If the ordering of the models in the table is not
	    what you wanted, right-click on the model table icon and
	    select <quote>Clear table</quote>.  Then go back to step 4
	    above and try again.</para>
	</li>
      </nlist>
    </description>

    <description context="cli">
      <para>Manipulates the gretl <quote>model table</quote>. See
	<manref targ="modes"/> for details. The sub-commands have
	the following effects: <cmd>add</cmd> adds the last model
	estimated to the model table, if possible; <cmd>show</cmd>
	displays the model table in a window; and <cmd>free</cmd>
	clears the table.</para>
    </description>

    <gui-access>
      <menu-path>Session window, Model table icon</menu-path>
    </gui-access>

  </command>

  <command name="mpols" section="Estimation"
    label="Multiple-precision OLS">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Computes OLS estimates for the specified model using multiple
	precision floating-point arithmetic.  This command is
	available only if <program>gretl</program> is compiled with
	support for the Gnu Multiple Precision library (GMP).
      </para>

      <para context="cli">
	To estimate a polynomial fit, using multiple precision
	arithmetic to generate the required powers of the independent
	variable, use the form, e.g. <cmd>mpols y 0 x ; 2 3 4</cmd>
	This does a regression of <lit>y</lit> on <lit>x</lit>,
	<lit>x</lit> squared, <lit>x</lit> cubed and <lit>x</lit> to
	the fourth power.  That is, the numbers (which must be
	positive integers) to the right of the semicolon specify the
	powers of <lit>x</lit> to be used.  If more than one
	independent variable is specified, the last variable before
	the semicolon is taken to be the one that should be raised to
	various powers.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/High precision OLS</menu-path>
    </gui-access>

  </command>

  <command name="multiply" section="Transformations" context="cli">

    <usage>
      <arguments>
        <argument>x</argument>
        <argument>suffix</argument>
        <argument>varlist</argument>
      </arguments>
      <examples>
        <example>multiply invpop pc 3 4 5 6</example>
        <example>multiply 1000 big x1 x2 x3</example>
      </examples>
    </usage>

    <description>
      <para>
	The variables in <repl>varlist</repl> (referenced by name or
	number) are multiplied by <repl>x</repl>, which may be either
	a numerical value or the name of a variable already defined.
	The products are named with the specified <repl>suffix</repl>
	(maximum 3 characters). The original variable names are
	truncated first if need be. For instance, suppose you want to
	create per capita versions of certain variables, and you have
	the variable <lit>pop</lit> (population).  A suitable set of
	commands is then:</para>
      
      <code>
	genr invpop = 1/pop
	multiply invpop pc income</code>

      <para>which will create <lit>incomepc</lit> as the product of
	<lit>income</lit> and <lit>invpop</lit>, and
	<lit>expendpc</lit> as <lit>expend</lit> times
	<lit>invpop</lit>.
      </para>
    </description>

  </command>

  <command name="nls" section="Estimation"
    label="Nonlinear Least Squares">

    <usage>
      <arguments>
        <argument>function</argument>
        <argument>derivatives</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description context="gui">

      <para>Performs Nonlinear Least Squares (NLS) estimation using a
	modified version of the Levenberg&ndash;Marquandt algorithm.
	You must supply a function specification; it is recommended
	that you also supply expressions for the derivatives of this
	function with respect to each of the parameters if
	possible.</para>

      <para>Example: Suppose we have a data set with
	variables C and Y (e.g. greene11_3.gdt) and we wish to
	estimate a nonlinear consumption function of the form
	<equation status="display"
	  tex="\[C = \alpha + \beta Y^{\gamma}\]"
	  ascii="C = alpha + beta * Y^gamma"
	  graphic="greene_Cfunc"/></para>

      <para>The parameters alpha, beta and gamma must
	first be added to the dataset and given initial values.  This
	can be done using the genr command or via menu choices.
	Appropriate <quote>genr</quote> lines may be typed into the
	NLS specification window prior to the function
	specification.</para>

      <para>In the NLS window we type the following
	lines:</para>

      <code>
	C = alpha + beta * Y^gamma
	deriv alpha = 1
	deriv beta = Y^gamma
	deriv gamma = beta * Y^gamma * log(Y)
      </code>

      <para>The first line specifies the regression
	function, and the next three lines supply the derivatives of
	that function with respect to each of the parameters in turn.
	If the "deriv" lines are not given, a numerical approximation
	to the Jacobian is computed.</para>

      <para>If the parameters alpha, beta and gamma were
	not previously declared we could preface the above lines with
	something like the following:</para>

      <code>
	genr alpha = 1
	genr beta = 1
	genr gamma = 1
      </code>

      <para>For further details on NLS estimation please see
	<manref targ="chap-nls"/>.</para>

    </description>

    <description context="cli">

      <para>
	Performs Nonlinear Least Squares (NLS) estimation using a
	modified version of the Levenberg&ndash;Marquandt algorithm.
	The user must supply a function specification.  The parameters
	of this function must be declared and given starting values
	(using the <cmd>genr</cmd> command) prior to estimation.
	Optionally, the user may specify the derivatives of the
	regression function with respect to each of the parameters; if
	analytical derivatives are not supplied, a numerical
	approximation to the Jacobian is computed.</para>

      <para>
	It is easiest to show what is required by example.  The
	following is a complete script to estimate the nonlinear
	consumption function set out in William Greene's
	<book>Econometric Analysis</book> (Chapter 11 of the 4th
	edition, or Chapter 9 of the 5th).  The numbers to the left of
	the lines are for reference and are not part of the commands.
	Note that the <lit>--vcv</lit> option, for printing the
	covariance matrix of the parameter estimates, attaches to the
	final command, <lit>end nls</lit>.</para>

      <code>
	1   open greene11_3.gdt
	2   ols C 0 Y
	3   genr alpha = coeff(0)
	4   genr beta = coeff(Y)
	5   genr gamma = 1.0
	6   nls C = alpha + beta * Y^gamma
	7   deriv alpha = 1
	8   deriv beta = Y^gamma
	9   deriv gamma = beta * Y^gamma * log(Y)
	10  end nls --vcv
      </code>

      <para>
	It is often convenient to initialize the parameters by
	reference to a related linear model; that is accomplished here
	on lines 2 to 5.  The parameters alpha, beta and gamma could
	be set to any initial values (not necessarily based on a model
	estimated with OLS), although convergence of the NLS procedure
	is not guaranteed for an arbitrary starting point.</para>

      <para>
	The actual NLS commands occupy lines 6 to 10. On line 6 the
	<cmd>nls</cmd> command is given: a dependent variable is
	specified, followed by an equals sign, followed by a function
	specification.  The syntax for the expression on the right is
	the same as that for the <cmd>genr</cmd> command.  The next
	three lines specify the derivatives of the regression function
	with respect to each of the parameters in turn.  Each line
	begins with the keyword <cmd>deriv</cmd>, gives the name of a
	parameter, an equals sign, and an expression whereby the
	derivative can be calculated (again, the syntax here is the
	same as for <cmd>genr</cmd>). These <cmd>deriv</cmd> lines are
	optional, but it is recommended that you supply them if
	possible. Line 10, <cmd>end nls</cmd>, completes the command
	and calls for estimation.</para>

      <para>For further details on NLS estimation please see
	<manref targ="chap-nls"/>.</para>

    </description>

    <gui-access>
      <menu-path>/Model/Nonlinear Least Squares</menu-path>
    </gui-access>

  </command>

  <command name="noecho" section="Obsolete" context="cli">
    <description>
      <para>Obsolete command.  See <cmdref targ="set"/>.
      </para>
    </description>
  </command>

  <command name="nulldata" section="Dataset"
    label="Creating a blank dataset">

    <usage>
      <arguments>
        <argument>series_length</argument>
      </arguments>
      <examples>
        <example>nulldata 500</example>
      </examples>
    </usage>

    <description>
      <para>
	Establishes a <quote>blank</quote> data set, containing only a
	constant and an index variable, with periodicity 1 and the
	specified number of observations. This may be used for
	simulation purposes: some of the <cmd>genr</cmd> commands
	(e.g. <cmd>genr uniform()</cmd>, <cmd>genr normal()</cmd>)
	will generate dummy data from scratch to fill out the data
	set. This command may be useful in conjunction with
	<cmd>loop</cmd>.  See also the <quote>seed</quote> option to
	the <cmdref targ="set"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Create data set</menu-path>
    </gui-access>

  </command>

  <command name="ols" section="Estimation"
    label="Ordinary Least Squares">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
        <option>
	  <flag>--no-df-corr</flag>
	  <effect>suppress degrees of freedom correction</effect>
        </option>
        <option>
	  <flag>--print-final</flag>
	  <effect>see below</effect>
        </option>
      </options>
      <examples>
        <example>ols 1 0 2 4 6 7</example>
	<example>ols y 0 x1 x2 x3 --vcv</example>
	<example>ols y 0 x1 x2 x3 --quiet</example>
      </examples>
    </usage>

    <description>
      <para context="gui">
        Computes ordinary least squares (OLS) estimates for the
	specified model.
      </para>

      <para context="cli">
        Computes ordinary least squares (OLS) estimates with
	<repl>depvar</repl> as the dependent variable and
	<repl>indepvars</repl> as the list of independent variables.
	Variables may be specified by name or number; use the number
	zero for a constant term. 
      </para>

      <para>Besides coefficient estimates and standard errors, the
	program also prints p-values for <mathvar>t</mathvar>
	(two-tailed) and <mathvar>F</mathvar>-statistics.  A p-value
	below 0.01 indicates statistical significance at the 1 percent
	level and is marked with <lit>***</lit>. <lit>**</lit>
	indicates significance between 1 and 5 percent and
	<lit>*</lit> indicates significance between the 5 and 10
	percent levels. Model selection statistics (the Akaike
	Information Criterion or AIC and Schwarz's Bayesian Information
	Criterion) are also printed.  The formula used for the AIC is
	that given by Akaike (1974), namely minus two times the
	maximized log-likelihood plus two times the number of
	parameters estimated.</para>

      <para context="cli">If the option <lit>--no-df-corr</lit> is
	given, the usual degrees of freedom correction is not applied
	when calculating the estimated error variance (and hence also
	the standard errors of the parameter estimates).</para>

      <para context="cli">
	The option <lit>--print-final</lit> is applicable only in the
	context of a <cmdref targ="loop"/>.  It arranges for the
	regression to be run silently on all but the final iteration
	of the loop. See <manref targ="loop-examples"/> for details.
      </para>

      <para context="cli">Various internal variables may be retrieved
	using the <cmdref targ="genr"/> command, provided
	<cmd>genr</cmd> is invoked immediately after this command.
      </para>

      <para context="cli">The specific formula used for generating
	robust standard errors (when the <lit>--robust</lit> option is
	given) can be adjusted via the <cmdref targ="set"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Ordinary Least Squares</menu-path>
      <other-access>Beta-hat button on toolbar</other-access>
    </gui-access>

  </command>

  <command name="omit" section="Tests" label="Omit variables">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print estimates for reduced model</effect>
	</option>
      </options>
      <examples>
        <example>omit 5 7 9</example>
      </examples>
    </usage>

    <description>
      <para>
	This command must follow an estimation command. The selected
	variables are omitted from the previous model and the new
	model estimated. If more than one variable is omitted, the
	Wald <mathvar>F</mathvar>-statistic for the omitted variables
	will be printed along with its p-value (for the OLS procedure
	only).  A p-value below 0.05 means that the coefficients are
	jointly significant at the 5 percent level.
      </para>
      <para context="cli">
	If the <lit>--quiet</lit> option is given the printed results
	are confined to the test for the joint significance of the
	omitted variables, otherwise the estimates for the reduced
	model are also printed.  In the latter case, the
	<lit>--vcv</lit> flag causes the covariance matrix for the
	coefficients to be printed also.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/omit variables</menu-path>
    </gui-access>

  </command>

  <command name="omitfrom" section="Tests" context="cli">

    <usage>
      <arguments>
        <argument>modelID</argument>
        <argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print estimates for reduced model</effect>
	</option>
      </options>
      <examples>
        <example>omitfrom 2 5 7 9</example>
      </examples>
    </usage>

    <description>
      <para>Works like <cmdref targ="omit"/>, except that you specify
	a previous model (using its ID number, which is printed at the
	start of the model output) to take as the base for omitting
	variables.  The example above omits variables number 5, 7 and
	9 from Model 2.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/omit variables</menu-path>
    </gui-access>

  </command>

  <command name="online" section="Dataset" context="gui"
    label="Access online databases">

    <description>
      <para>
	Gretl is able to access databases at Wake Forest University
	(your computer must be connected to the internet for this to
	work).</para>

      <para>Under the <quote>File, Browse databases</quote> menu,
	select the item <quote>on database server</quote>. A window
	should appear, showing a listing of the gretl databases
	available at Wake Forest. (Depending on your location and the
	speed of your internet connection, this may take a few
	seconds.)  Along with the name of the database and a short
	description, there will appear a <quote>Local status</quote>
	entry: this shows whether you have the database installed
	locally (on the hard drive of your computer) and if so,
	whether or not it is up to date with the version on the
	server.</para>

      <para>If you have a given database installed locally, and it is
	up to date, there is no advantage in accessing it via the
	server.  But for a database that is not already installed and
	up to date, you may wish to get a listing of the data series:
	click on <quote>Get series listing</quote>.  This brings up a
	further window, from which you can display the values of a
	chosen data series, graph those values, or import them into
	gretl's workspace.  These tasks can be accomplished using the
	<quote>Series</quote> menu, or via the popup menu that appears
	when you click the right mouse button on a given series.  You
	can also search the listing for a variable of interest (the
	<quote>Find</quote> menu item).</para>

      <para>If you want faster access to the data, or wish to access
	the database offline, then select the line showing the
	database you want, in the initial database window, and press
	the <quote>Install</quote> button.  This will download the
	database in compressed format, then uncompress it and install
	it on your hard drive. Thereafter you should be able to find
	it under the <quote>File, Browse databases, gretl
	  native</quote> menu.</para>

    </description>
  </command>

  <command name="open" section="Dataset" context="cli">

    <usage>
      <arguments>
        <argument>datafile</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Opens a data file.  If a data file is already open, it is
	replaced by the newly opened one.  The program will try to
	detect the format of the data file (native, plain text, CSV or
	BOX1).
      </para>

      <para>
	This command can also be used to open a database (gretl or
	RATS 4.0) for reading.  In that case it should be followed by
	the <cmdref targ="data"/> command to extract particular series
	from the database.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Open data</menu-path>
      <other-access>Drag a data file into gretl (MS Windows or Gnome)</other-access>
    </gui-access>

  </command>

  <command name="outfile" section="Printing" context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
        <argument>option</argument>
      </arguments>
      <options>
        <option>
	  <flag>--append</flag>
	  <effect>append to file</effect>
        </option>
        <option>
	  <flag>--close</flag>
	  <effect>close file</effect>
        </option>
        <option>
	  <flag>--write</flag>
	  <effect>overwrite file</effect>
        </option>
      </options>
      <examples>
        <example>outfile --write regress.txt</example>
        <example>outfile --close</example>
      </examples>
    </usage>

    <description>
      <para>Diverts output to <repl>filename</repl>, until further
	notice.  Use the flag <lit>--append</lit> to append output to
	an existing file or <lit>--write</lit> to start a new file
	(or overwrite an existing one).  Only one file can be opened
	in this way at any given time.</para>

      <para>The <lit>--close</lit> flag is used to close an output
	file that was previously opened as above.  Output will then
	revert to the default stream.</para>

      <para>In the first example command above, the file
	<filename>regress.txt</filename> is opened for writing, and in
	the second it is closed.  This would make sense as a sequence
	only if some commands were issued before the
	<lit>--close</lit>.  For example if an <cmd>ols</cmd> command
	intervened, its output would go to
	<filename>regress.txt</filename> rather than the
	screen.</para>
    </description>

  </command>

  <command name="panel" section="Dataset" context="cli"
    label="Panel data structure">

    <usage>
      <options>
        <option>
	  <flag>--cross-section</flag>
	  <effect>stacked cross sections</effect>
        </option>
        <option>
	  <flag>--time-series</flag>
	  <effect>stacked time series</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>Request that the current data set be
	interpreted as a panel (pooled cross section and time series).
	By default, or with the <lit>--time-series</lit> flag, the
	data set is taken to be in the form of stacked time series
	(successive blocks of data contain time series for each
	cross-sectional unit).  With the <lit>--cross-section</lit>
	flag, the data set is read as stacked cross-sections
	(successive blocks contain cross sections for each time
	period).  See also
	<cmdref targ="setobs"/>.
      </para>
    </description>

  </command>

  <command name="pca" section="Statistics"
    label="Principal Components Analysis">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--save-all</flag>
	  <effect>Save all components</effect>
        </option>
        <option>
	  <flag>--save</flag>
	  <effect>Save major components</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>Principal Components Analysis.  Prints the eigenvalues of
	the correlation matrix for the variables in
	<repl>varlist</repl> along with the proportion of the joint
	variance accounted for by each component.  Also prints the
	corresponding eigenvectors (or <quote>component
	  loadings</quote>).</para>  

      <para>If the <lit>--save</lit> flag is given, components with
	eigenvalues greater than 1.0 are saved to the dataset as
	variables, with names <lit>PC1</lit>, <lit>PC2</lit> and so
	on.  These artificial variables are formed as the sum of
	(component loading) times (standardized <lit>Xi</lit>), where
	<lit>Xi</lit> denotes the <mathvar>i</mathvar>th variable in
	<repl>varlist</repl>.</para>

      <para>If the <lit>--save-all</lit> flag is given, all of the
	components are saved as described above.</para> 
    </description>

    <gui-access>
      <menu-path>Main window pop-up (multiple selection)</menu-path>
    </gui-access>

  </command>

  <command name="pergm" section="Statistics"
    label="Periodogram">

    <usage>
      <arguments>
        <argument>varname</argument>
      </arguments>
      <options>
        <option>
	  <flag>--bartlett</flag>
	  <effect>use Bartlett lag window</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Computes and displays (and if not in batch mode, graphs) the
	spectrum of the specified variable.  Without the
	<lit>--bartlett</lit> flag the sample periodogram is given;
	with the flag a Bartlett lag window of length
	<equation status="inline"
	  tex="$2\sqrt{T}$"
	  ascii="2*sqrt(T)"
	  graphic="tworootT"/> (where <mathvar>T</mathvar> is the
	sample size) is used in estimating the spectrum (see Chapter
	18 of Greene's <book>Econometric Analysis</book>). When the
	sample periodogram is printed, a <mathvar>t</mathvar>-test for
	fractional integration of the series (<quote>long
	  memory</quote>) is also given: the null hypothesis is that
	the integration order is zero.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/spectrum</menu-path>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="plot" section="Graphs" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--one-scale</flag>
	  <effect>force a single scale</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Plots the values for specified variables, for the range of
	observations currently in effect, using ASCII symbols.  Each
	line stands for an observation and the values are plotted
	horizontally.  By default the variables are scaled
	appropriately.  See also <cmdref targ="gnuplot"/>.
      </para>
    </description>

  </command>

  <command name="pooled" section="Estimation"
    label="Pooled OLS">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--unit-weights</flag>
	  <effect>feasible GLS</effect>
	</option>
	<option>
	  <flag>--iterate</flag>
	  <effect>iterate to Maximum Likelihood solution</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>By default, estimates a model via OLS (see <cmdref
	  targ="ols"/> for details on syntax), and flags it as a
	pooled or panel model, so that the <cmdref targ="hausman"/>
	test item becomes available.
      </para>
      <para>If the <lit>--unit-weights</lit> flag is given, estimation
	is by feasible GLS, using weights constructed from the
	specific error variances per cross-sectional unit.  This
	offers a gain in efficiency over OLS if the error variance
	differs across units.
      </para>
      <para>If, in addition, the <lit>--iterate</lit> flag is given,
	the GLS estimator is iterated: if this procedure converges it
	yields Maximum Likelihood estimates.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Pooled OLS</menu-path>
    </gui-access>

  </command>

  <command name="print" section="Printing" context="cli">

    <usage>
      <arguments>
	<argument>varlist</argument>
	<argument alternate="true">string_literal</argument>
      </arguments>
      <options>
	<option>
	  <flag>--byobs</flag>
	  <effect>by observations</effect>
	</option>
	<option>
	  <flag>--ten</flag>
	  <effect>use 10 significant digits</effect>
	</option>
      </options>
      <examples>
	<example>print x1 x2 --byobs</example>
	<example>print "This is a string"</example>
      </examples>
    </usage>

    <description>
      <para>
	If <repl>varlist</repl> is given, prints the values of the
	specified variables; if no list is given, prints the values of
	all variables in the current data file. If the
	<lit>--byobs</lit> flag is given the data are printed by
	observation, otherwise they are printed by variable.  If the
	<lit>--ten</lit> flag is given the data are printed by
	variable to 10 significant digits.
      </para>

      <para>
	If the argument to <cmd>print</cmd> is a literal string (which
	must start with a double-quote, <lit>"</lit>), the string is
	printed as is.  See also <cmdref
	  targ="printf"/>.</para>
    </description>

    <gui-access>
      <menu-path>/Data/Display values</menu-path>
    </gui-access>

  </command>

  <command name="printf" section="Printing" context="cli">

    <usage>
      <arguments>
        <argument>format</argument>
        <argument>args</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Prints scalar values under the control of a format string
	(providing a small subset of the <lit>printf()</lit> statement
	in the C programming language). Recognized formats are
	<lit>%g</lit> and <lit>%f</lit>, in each case with the various
	modifiers available in C.  Examples: the format
	<lit>%.10g</lit> prints a value to 10 significant figures;
	<lit>%12.6f</lit> prints a value to 6 decimal places, with a
	width of 12 characters.</para>  

      <para>The format string itself must be enclosed in double
	quotes.  The values to be printed must follow the format
	string, separated by commas.  These values should take the
	form of either (a) the names of variables in the dataset, or
	(b) expressions that are valid for the <cmd>genr</cmd>
	command.  The following example prints the values of two
	variables plus that of a calculated expression:</para>

      <code>
	ols 1 0 2 3
	genr b = coeff(2)
	genr se_b = stderr(2)
	printf "b = %.8g, standard error %.8g, t = %.4f\n", b, se_b, b/se_b
      </code>

      <para>
	The maximum length of a format string is 127 characters.  The
	escape sequences <lit>\n</lit> (newline), <lit>\t</lit> (tab),
	<lit>\v</lit> (vertical tab) and <lit>\\</lit> (literal
	backslash) are recognized.  To print a literal percent sign,
	use <lit>%%</lit>.</para>
    </description>

  </command>

  <command name="probit" section="Estimation"
    label="Probit model">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	The dependent variable should be a binary variable. Maximum
	likelihood estimates of the coefficients on
	<repl>indepvars</repl> are obtained via iterated least squares
	(the EM or Expectation&ndash;Maximization method).  As the
	model is nonlinear the slopes depend on the values of the
	independent variables: the reported slopes are evaluated at
	the means of those variables.  The chi-square statistic tests
	the null hypothesis that all coefficients are zero apart from
	the constant.
      </para>

      <para>
	Probit for analysis of proportions is not implemented in
	<program>gretl</program> at this point.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Probit</menu-path>
    </gui-access>

  </command>

  <command name="pvalue" section="Utilities" context="cli">

    <usage>
      <arguments>
        <argument>dist</argument>
        <argument optional="true">params</argument>
	<argument>xval</argument>
      </arguments>
      <examples>
        <example>pvalue z zscore</example>
	<example>pvalue t 25 3.0</example>
	<example>pvalue X 3 5.6</example>
	<example>pvalue F 4 58 fval</example>
	<example>pvalue G xbar varx x</example>
      </examples>
    </usage>

    <description>
      <para>
	Computes the area to the right of <repl>xval</repl> in the
	specified distribution (<lit>z</lit> for Gaussian,
	<lit>t</lit> for Student's <mathvar>t</mathvar>, <lit>X</lit>
	for chi-square, <lit>F</lit> for <mathvar>F</mathvar> and
	<lit>G</lit> for gamma).  For the <mathvar>t</mathvar> and
	chi-square distributions the degrees of freedom must be given;
	for <mathvar>F</mathvar> numerator and denominator degrees of
	freedom are required; and for gamma the mean and variance are
	needed.
      </para>
    </description>

    <gui-access>
      <menu-path>/Utilities/p-value finder</menu-path>
    </gui-access>

  </command>

  <command name="pwe" section="Estimation" label="Prais-Winsten estimator">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
      <examples>
        <example>pwe 1 0 2 4 6 7</example>
      </examples>
    </usage>

    <description>
      <para>
	Computes parameter estimates using the Prais&ndash;Winsten
	procedure, an implementation of feasible GLS which is designed
	to handle first-order autocorrelation of the error term.  The
	procedure is iterated, as with <cmdref targ="corc"/>; the
	difference is that while Cochrane&ndash;Orcutt discards the
	first observation, Prais&ndash;Winsten makes use of it. See,
	for example, Chapter 13 of Greene's <book>Econometric
	  Analysis</book> (2000) for details.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Time series/Prais-Winsten</menu-path>
    </gui-access>

  </command>

  <command name="quit" section="Utilities" context="cli">

    <description>
      <para>
	Exits from the program, giving you the option of saving the
	output from the session on the way out.  
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Exit</menu-path>
    </gui-access>

  </command>

  <command name="rename" section="Dataset" context="cli">

    <usage>
      <arguments>
        <argument>varnumber</argument>
        <argument>newname</argument>
      </arguments>
    </usage>

    <description>
      <para>Changes the name of the variable with identification
	number <repl>varnumber</repl> to <repl>newname</repl>.  The
	<repl>varnumber</repl> must be between 1 and the number of
	variables in the dataset.  The new name must be of 8
	characters maximum, must start with a letter, and must be
	composed of only letters, digits, and the underscore
	character.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Edit attributes</menu-path>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="reset" section="Tests" label="Ramsey's RESET">

    <description>
      <para>
	Must follow the estimation of a model via OLS. Carries out
	Ramsey's RESET test for model specification (non-linearity) by
	adding the square and the cube of the fitted values to the
	regression and calculating the <mathvar>F</mathvar> statistic
	for the null hypothesis that the parameters on the two added
	terms are zero.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Ramsey's RESET</menu-path>
    </gui-access>

  </command>

  <command name="restrict" section="Tests" label="Linear restrictions">

    <description>
      <para>
	Imposes a set of linear restrictions on either (a) the model
	last estimated or (b) a system of equations previously defined
	and named.  The syntax and effects of the command differ
	slightly in the two cases.
      </para>

      <para>
	In both cases the set of restrictions should be started with
	the keyword <quote>restrict</quote> and terminated with
	<quote>end restrict</quote>.  In the single equation case the
	restrictions are implicitly to be applied to the last model,
	and they are evaluated as soon as the <lit>restrict</lit>
	command is terminated.  In the system case the initial
	<quote>restrict</quote> must be followed by the name of a
	previously defined system of equations (see <cmdref
	  targ="system"/>).  The restrictions are evaluated when
	the system is next estimated, using the <cmdref
	targ="estimate"/> command.
      </para>

      <para>Each restriction in the set should be expressed as an
	equation, with a linear combination of parameters on the left
	and a numeric value to the right of the equals sign. In the
	single-equation case, parameters are referenced in the form
	b<repl>N</repl>, where <repl>N</repl> represents the position
	in the list of regressors, starting at zero.  For example,
	<lit>b1</lit> denotes the second regression parameter.  In the
	system case, parameters are referenced using <lit>b</lit> plus
	two numbers in square brackets.  The leading number represents
	the position of the equation within the system, starting from
	1, and the second number indicates position in the list of
	regressors, starting at zero.  For example <lit>b[2,0]</lit>
	denotes the first parameter in the second equation, and
	<lit>b[3,1]</lit> the second parameter in the third equation.
      </para>

      <para>The <lit>b</lit> terms in the equation representing a
	restriction equation may be prefixed with a numeric
	multiplier, using <lit>*</lit> to represent multiplication,
	for example <lit>3.5*b4</lit>.
      </para>

      <para>Here is an example of a set of restrictions for a
	previously estimated model:</para>

      <code>
	restrict
	 b1 = 0
	 b2 - b3 = 0
	 b4 + 2*b5 = 1
	end restrict</code>

      <para>And here is an example of a set of restrictions to be
	applied to a named system.  (If the name of the system does
	not contain spaces, the surrounding quotes are not required.)
      </para>

      <code>
	restrict "System 1"
	 b[1,1] = 0
	 b[1,2] - b[2,2] = 0
	 b[3,4] + 2*b[3,5] = 1
	end restrict</code>

      <para>In the single-equation case the restrictions are evaluated
	via a Wald F-test, using the coefficient covariance matrix of
	the model in question.  In the system case, the full results
	of estimating the system subject to the restrictions are
	shown; the test statistic depends on the estimator chosen (a
	Likelihood Ratio test if the system is estimated using a
	Maximum Likelihood method, or an asymptotic F-test otherwise.)
      </para>

    </description>

    <gui-access>
      <menu-path>Model window, /Tests/linear restrictions</menu-path>
    </gui-access>

  </command>

  <command name="rhodiff" section="Transformations" context="cli">

    <usage>
      <arguments>
        <argument>rholist</argument>
        <argument separated="true">varlist</argument>
      </arguments>
      <examples>
        <example>rhodiff .65 ; 2 3 4</example>
        <example>rhodiff r1 r2 ; x1 x2 x3</example>	
      </examples>
    </usage>

    <description>
      <para>
	Creates rho-differenced counterparts of the variables (given
	by number or by name) in <repl>varlist</repl> and adds them to
	the data set, using the suffix <lit>#</lit> for the new
	variables. Given variable <lit>v1</lit> in
	<repl>varlist</repl>, and entries <lit>r1</lit> and
	<lit>r2</lit> in <repl>rholist</repl>, <lit>v1# = v1(t) -
	  r1*v1(t-1) - r2*v1(t-2)</lit> is created. The
	<repl>rholist</repl> entries can be given as numerical values
	or as the names of variables previously defined.
      </para>
    </description>

  </command>

  <command name="rmplot" section="Graphs"
    label="Range-mean plot">

    <usage>
      <arguments>
        <argument>varname</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Range&ndash;mean plot: this command creates a simple graph to
	help in deciding whether a time series,
	<mathvar>y</mathvar>(t), has constant variance or not.  We
	take the full sample t=1,...,T and divide it into small
	subsamples of arbitrary size <mathvar>k</mathvar>. The first
	subsample is formed by
	<mathvar>y</mathvar>(1),...,<mathvar>y</mathvar>(k), the
	second is <mathvar>y</mathvar>(k+1), ...,
	<mathvar>y</mathvar>(2k), and so on.  For each subsample we
	calculate the sample mean and range (= maximum minus minimum),
	and we construct a graph with the means on the horizontal axis
	and the ranges on the vertical. So each subsample is
	represented by a point in this plane.  If the variance of the
	series is constant we would expect the subsample range to be
	independent of the subsample mean; if we see the points
	approximate an upward-sloping line this suggests the variance
	of the series is increasing in its mean; and if the points
	approximate a downward sloping line this suggests the variance
	is decreasing in the mean.</para>

      <para>Besides the graph, gretl displays the means and ranges for
	each subsample, along with the slope coefficient for an OLS
	regression of the range on the mean and the p-value for the
	null hypothesis that this slope is zero.  If the slope
	coefficient is significant at the 10 percent significance
	level then the fitted line from the regression of range on
	mean is shown on the graph.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Range-mean graph</menu-path>
    </gui-access>

  </command>

  <command name="run" section="Programming" context="cli">

    <usage>
      <arguments>
        <argument>inputfile</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Execute the commands in <repl>inputfile</repl> then return
	control to the interactive prompt.
      </para>
    </description>

    <gui-access>
      <menu-path>Run icon in script window</menu-path>
    </gui-access>

  </command>

  <command name="runs" section="Tests" label="Runs test">

    <usage>
      <arguments>
        <argument>varname</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Carries out the nonparametric <quote>runs</quote> test for
	randomness of the specified variable.  If you want to test for
	randomness of deviations from the median, for a variable named
	<lit>x1</lit> with a non-zero median, you can do the
	following:</para>

      <code>
	genr signx1 = x1 - median(x1)
	runs signx1
      </code>
    </description>

    <gui-access>
      <menu-path>/Variable/Runs test</menu-path>
    </gui-access>

  </command>

  <command name="sampling" section="Dataset" context="gui"
    label="Setting the sample">

    <description>
      <para>The Sample menu offers several ways of selecting a
      sub-sample from the current dataset.</para>

      <para>
	If you choose <quote>Sample/Define based on dummy...</quote>
	you are prompted to select a dummy (indicator) variable, which
	must have the values 0 or 1 at each observation.  The sample
	will be restricted to observations for which the dummy's value
	is 1.</para>

      <para>If you choose <quote>Sample/Restrict based on
	  criterion...</quote> you need to supply a Boolean (logical)
	expression, of the same sort that you would use to define a
	dummy variable.  For example the expression <quote>sqft >
	  1400</quote> will select only cases for which the variable
	sqft has a value greater than 1400. Conditions may be
	concatenated using the logical operators <quote>&amp;</quote>
	(AND) and <quote>|</quote> (OR), and may be negated using
	<quote>!</quote> (NOT).</para>

      <para>The menu item <quote>Sample/Drop all obs with missing
	  values</quote> redefines the sample to exclude all
	observations for which values of one or more variables are
	missing (leaving only complete cases).</para>  

      <para>To select observations for which a particular variable has
	no missing values, use <quote>Sample/Restrict based on
	  criterion...</quote> and supply the Boolean condition
	<quote>!missing(varname)</quote> (replace
	<quote>varname</quote> with the name of the variable you want
	to use).</para>  

      <para>If the observations are named, you can re-sample to
	exclude a particular observation using, say, obs!="France" as
	the Boolean criterion.  The observation name must be enclosed
	in double quotes.</para>

      <para>One point should be noted about defining a sample based on
	a dummy variable, a Boolean expression, or on the missing
	values criterion: Any <quote>structural</quote> information in
	the data header file (regarding the time series or panel
	nature of the data) is lost.  You may reimpose structure with
	<quote>Sample/Set frequency, startobs...</quote>.</para>

      <para>Please see <manref targ="sampling"/> for further details.</para>

    </description>
  </command>


  <command name="scatters" section="Graphs"
    label="Multiple scatterplots">

    <usage>
      <arguments>
        <argument>yvar</argument>
        <argument separated="true">xvarlist</argument>
	<argument alternate="true">yvarlist ; xvar</argument>
      </arguments>
      <examples>
        <example>scatters 1 ; 2 3 4 5</example>
        <example>scatters 1 2 3 4 5 6 ; 7</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Plots pairwise scatters of <repl>yvar</repl> against all the
	variables in <repl>xvarlist</repl>, or of all the variables in
	<repl>yvarlist</repl> against <repl>xvar</repl>.  The first
	example above puts variable 1 on the <mathvar>y</mathvar>-axis
	and draws four graphs, the first having variable 2 on the
	<mathvar>x</mathvar>-axis, the second variable 3 on the
	<mathvar>x</mathvar>-axis, and so on.  The second example
	plots each of variables 1 through 6 against variable 7 on the
	<mathvar>x</mathvar>-axis. Scanning a set of such plots can be
	a useful step in exploratory data analysis.  The maximum
	number of plots is six; any extra variable in the list will be
	ignored.
      </para>
      <para context="gui">
	Draws a set of pairwise scatters of the selected <quote>Y-axis
	  variable</quote> against each of the selected <quote>X-axis
	  variables</quote> in turn.  Scanning a set of such plots can
	be a useful step in exploratory data analysis.  The maximum
	number of plots is six; any extra variables in the X-axis list
	will be ignored.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Multiple scatterplots</menu-path>
    </gui-access>

  </command>

  <command name="seed" section="Obsolete" context="cli">
    <description>
      <para>Obsolete command.  See <cmdref targ="set"/>.
      </para>
    </description>
  </command>

  <command name="seed" section="Programming" context="gui"
    label="Random seed">

    <description>
      <para>Requires an integer as input.  Sets the seed for the
	pseudo-random number generator used by the random uniform and
	random normal options under the Data, Add variables menu.  By
	default the seed is set when the program is started, using the
	system time.  If you want to obtain repeatable sequences of
	pseudo-random numbers you need to set the seed manually.
      </para>
    </description>
  </command>

  <command name="set" section="Programming" context="cli">

    <usage>
      <arguments>
        <argument>variable</argument>
        <argument>value</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Set the values of various program parameters.  The given value
	remains in force for the duration of the gretl session unless
	it is changed by a further call to <cmd>set</cmd>.  The
	parameters that can be set in this way are enumerated below.
	Note that the settings of <lit>hac_lag</lit> and
	<lit>hc_version</lit> are used when the <lit>--robust</lit>
	option is given to the <cmd>ols</cmd> command. 
      </para>

      <table lwidth="80pt" rwidth="320pt">
	<row>
	  <cell><lit>echo</lit></cell>
	  <cell>values: <lit>off</lit> or <lit>on</lit> (the default).
	    Suppress or resume the echoing of commands in gretl's
	    output.</cell>
	</row>
	<row>
	  <cell><lit>qr</lit></cell>
	  <cell>values: <lit>on</lit> or <lit>off</lit> (the default).
	    Use QR rather than Cholesky decomposition in calculating
	    OLS estimates.</cell>
	</row>
	<row>
	  <cell><lit>hac_lag</lit></cell>
	  <cell>values: <lit>nw1</lit> (the default) or
	    <lit>nw2</lit>, or an integer.  Sets the maximum lag
	    value, <mathvar>p</mathvar>, used when calculating HAC
	    (Heteroskedasticity and Autocorrelation Consistent)
	    standard errors using the Newey-West approach, for time
	    series data. <lit>nw1</lit> and <lit>nw2</lit> represent
	    two variant automatic calculations based on the sample
	    size, <mathvar>T</mathvar>: for nw1, 
	  <equation status="inline"
	    tex="$p = 0.75 \times T^{1/3}$"
	    ascii="p = 0.75 * T^(1/3)"
	    graphic="nw1"/>,
	    and for nw2, 
	  <equation status="inline"
	    tex="$p = 4 \times (T/100)^{2/9}$"
	    ascii="p = 4 * (T/100)^(2/9)"
	    graphic="nw2"/>.	  
	  </cell>
	</row>
	<row>
	  <cell><lit>hc_version</lit></cell>
	  <cell>values: 0 (the default), 1, 2 or 3. Sets the variant
	    used when calculating Heteroskedasticity Consistent
	    standard errors with cross-sectional data.  The options
	    correspond to the HC0, HC1, HC2 and HC3 discussed by
	    Davidson and MacKinnon in <book>Econometric Theory and
	      Methods</book>, chapter 5.  HC0 produces what are
	    usually called <quote>White's standard
	      errors</quote>.</cell>
	</row>
	<row>
	  <cell><lit>force_hc</lit></cell> <cell>values: off (the
	    default) or on.  By default, with time-series data and
	    when the <lit>--robust</lit> option is given with
	    <cmd>ols</cmd>, the HAC estimator is used.  If you set
	    <lit>force_hc</lit> to <quote>on</quote>, this forces
	    calculation of the regular Heteroskedasticity Consistent
	    Covariance Matrix (which does not take autocorrelation
	    into account).
	  </cell>
	</row>
	<row>
	  <cell><lit>garch_vcv</lit></cell>
	  <cell>values: <lit>unset</lit>, <lit>hessian</lit>,
	    <lit>im</lit> (information matrix) , <lit>op</lit> (outer
	    product matrix), <lit>qml</lit> (QML estimator),
	    <lit>bw</lit> (Bollerslev&ndash;Wooldridge). Specifies the
	    variant that will be used for estimating the coefficient
	    covariance matrix, for GARCH models.  If <lit>unset</lit>
	    is given (the default) then the Hessian is used unless the
	    <quote>robust</quote> option is given for the garch
	    command, in which case QML is used.</cell>
	</row>
	<row>
	  <cell><lit>hp_lambda</lit></cell>
	  <cell>values: <lit>auto</lit> (the default), or a numerical
	  value.  Sets the smoothing parameter for the
	  Hodrick&ndash;Prescott filter (see the <lit>hpfilt</lit>
	  function under the <cmd>genr</cmd> command).  The default is
	  to use 100 times the square of the periodicity, which gives
	  100 for annual data, 1600 for quarterly data, and so
	  on.</cell>
	</row>
      </table>

    </description>
  </command>

  <command name="setobs" section="Dataset" context="cli"
    label="Frequency and starting observation">

    <usage>
      <arguments>
        <argument>periodicity</argument>
        <argument>startobs</argument>
      </arguments>
      <options>
        <option>
	  <flag>--cross-section</flag>
	  <effect>interpret as cross section</effect>
        </option>
        <option>
	  <flag>--time-series</flag>
	  <effect>interpret as time series</effect>
        </option>
        <option>
	  <flag>--stacked-cross-section</flag>
	  <effect>interpret as panel data</effect>
        </option>
        <option>
	  <flag>--stacked-time-series</flag>
	  <effect>interpret as panel data</effect>
        </option>
      </options>
      <examples>
        <example>setobs 4 1990:1 --time-series</example>
        <example>setobs 12 1978:03</example>
	<example>setobs 1 1 --cross-section</example>
        <example>setobs 20 1:1 --stacked-time-series</example>
      </examples>
    </usage>

    <description>
      <para>
	Force the program to interpret the current data set as having
	a specified structure.  
      </para>
      <para>
	The <repl>periodicity</repl>, which must be an integer,
	represents frequency in the case of time-series data (1 =
	annual; 4 = quarterly; 12 = monthly; 52 = weekly; 5, 6, or 7 =
	daily; 24 = hourly).  In the case of panel data the
	periodicity means the number of lines per data block: this
	corresponds to the number of cross-sectional units in the case
	of stacked cross-sections, or the number of time periods in
	the case of stacked time series.  In the case of simple
	cross-sectional data the periodicity should be set to 1.
      </para>
      <para>
	The starting observation represents the starting date in the
	case of time series data.  Years may be given with two or four
	digits; subperiods (for example, quarters or months) should be
	separated from the year with a colon.  In the case of panel
	data the starting observation should be given as 1:1; and in
	the case of cross-sectional data, as 1.  Starting observations
	for daily or weekly data should be given in the form YY/MM/DD
	or YYYY/MM/DD (or simply as 1 for undated data).  
      </para>
      <para>
	If no explicit option flag is given to indicate the structure
	of the data the program will attempt to guess the structure
	from the information given.
      </para>

    </description> 

    <gui-access>
      <menu-path>/Sample/Dataset structure</menu-path>
    </gui-access>
      
  </command>

  <command name="setmiss" section="Dataset"
    label="Missing value code">

    <usage>
      <arguments>
        <argument>value</argument>
        <argument optional="true">varlist</argument>
      </arguments>
      <examples>
        <example>setmiss -1</example>
        <example>setmiss 100 x2</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Get the program to interpret some specific numerical data
	value (the first parameter to the command) as a code for
	<quote>missing</quote>, in the case of imported data.  If this
	value is the only parameter, as in the first example above,
	the interpretation will be applied to all series in the data
	set.  If <repl quote="true">value</repl> is followed by a list
	of variables, by name or number, the interpretation is
	confined to the specified variable(s). Thus in the second
	example the data value 100 is interpreted as a code for
	<quote>missing</quote>, but only for the variable
	<lit>x2</lit>.
      </para>
      
      <para context="gui">
	Set a numerical value that will be interpreted as "missing" or
	"not applicable", either for a particular data series (under
	the Variable menu) or globally for the entire data set (under
	the Sample menu).</para> 
      
      <para context="gui">
	Gretl has its own internal coding for missing values, but
	sometimes imported data may employ a different code.  For
	example, if a particular series is coded such that a value of
	-1 indicates "not applicable", you can select "Set missing
	value code" under the Variable menu and type in the value "-1"
	(without the quotes).  Gretl will then read the -1s as missing
	observations.</para>

    </description>

    <gui-access>
      <menu-path>/Sample/Set missing value code</menu-path>
    </gui-access>

  </command>

  <command name="shell" section="Utilities" context="cli">

    <usage>
      <arguments>
        <argument>shellcommand</argument>
      </arguments>
      <examples>
        <example>! ls -al</example>
	<example>! notepad</example>
      </examples>
    </usage>

    <description>
      <para>
	A <cmd>!</cmd> at the beginning of a command line is
	interpreted as an escape to the user's shell.  Thus arbitrary
	shell commands can be executed from within <program>gretl</program>.
      </para>
    </description>

  </command>

  <command name="sim" section="Dataset"
    label="Simulation" context="cli">

    <usage>
      <arguments>
        <argument optional="true">startobs endobs</argument>
	<argument>varname</argument>
	<argument>a0 a1 a2 &hellip;</argument>
      </arguments>
      <examples>
        <example>sim 1979.2 1983.1 y 0 0.9</example>
	<example>sim 15 25 y 10 0.8 x</example>
      </examples>
    </usage>

    <description>
      <para>
	Simulates values for <repl>varname</repl> for the current
	sample range, or for the range <repl>startobs</repl> through
	<repl>endobs</repl> if these optional arguments are given. The
	variable <repl>y</repl> must have been defined earlier with
	appropriate initial values. The formula used is 
	<equation status="display"
	  tex="\[y_t=a_{0t} + a_{1t}y_{t-1} + a_{2t}y_{t-2} + \dots\]"
	  ascii="y(t) = a0(t) + a1(t)*y(t-1) + a2(t)*y(t-2) + ..."
	  graphic="simformula"/> 
	The <lit>ai(t)</lit> terms may be
	either numerical constants or variable names previously
	defined; these terms may be prefixed with a minus sign.
      </para>

      <para>This command is deprecated.  You should use <cmdref
	  targ="genr"/> instead.</para>
    </description>

  </command>

  <command name="smpl" section="Dataset" context="cli">

    <!-- don't break the lines below or the text version will get messed
    up -->

    <usage>
      <altforms>
	<altform><lit>smpl</lit> <repl>startobs endobs</repl></altform>
	<altform><lit>smpl</lit> <repl>+i -j</repl></altform>
	<altform><lit>smpl</lit> <repl>dumvar</repl> <lit>--dummy</lit></altform>
	<altform><lit>smpl</lit> <repl>condition</repl> <lit>--restrict</lit></altform>
	<altform><lit>smpl</lit> <lit>--no-missing [ </lit><repl>varlist</repl> <lit>]</lit></altform>
	<altform><lit>smpl</lit> <repl>n</repl> <lit>--random</lit></altform>
	<altform><lit>smpl full</lit></altform>
      </altforms>
      <examples>
        <example>smpl 3 10</example>
	<example>smpl 1960:2 1982:4</example>
	<example>smpl +1 -1</example>
	<example>smpl x > 3000 --restrict</example>
	<example>smpl y > 3000 --restrict --replace</example>
	<example>smpl 100 --random</example>
      </examples>
    </usage>

    <description>
      <para>
	Resets the sample range.  The new range can be defined in
	several ways.  In the first alternate form (and the first two
	examples) above, <repl>startobs</repl> and <repl>endobs</repl>
	must be consistent with the periodicity of the data.  Either
	one may be replaced by a semicolon to leave the value
	unchanged.  In the second form, the integers <repl>i</repl>
	and <repl>j</repl> (which may be positive or negative, and
	should be signed) are taken as offsets relative to the
	existing sample range. In the third form <repl>dummyvar</repl>
	must be an indicator variable with values 0 or 1 at each
	observation; the sample will be restricted to observations
	where the value is 1. The fourth form, using
	<lit>--restrict</lit>, restricts the sample to observations
	that satisfy the given Boolean condition (which is specified
	according to the syntax of the <cmdref targ="genr"/>
	command).</para>

      <para>With the <lit>--no-missing</lit> form, if
	<repl>varlist</repl> is specified observations are selected on
	condition that all variables in <repl>varlist</repl> have
	valid values at that observation; otherwise, if no
	<repl>varlist</repl> is given, observations are selected on
	condition that <emphasis>all</emphasis> variables have valid
	(non-missing) values.</para>

      <para>With the <lit>--random</lit> flag, the specified number of
	cases are selected from the full dataset at random.  If you
	wish to be able to replicate this selection you should 
	set the seed for the random number generator first (see the
	<cmdref targ="set"/> command).</para>

      <para>The final form, <lit>smpl full</lit>, restores the full
	data range.
      </para>

      <para>Note that sample restrictions are, by default, cumulative:
	the baseline for any <lit>smpl</lit> command is the current
	sample. If you wish the command to act so as to replace any
	existing restriction you can add the option flag
	<lit>--replace</lit> to the end of the command.</para>

      <para>The internal variable <lit>obs</lit> may be used with the
	<lit>--restrict</lit> form of <lit>smpl</lit> to exclude
	particular observations from the sample.  For example</para>

      <code>
	smpl obs!=4 --restrict</code> 

      <para>will drop just the fourth observation. If the data points
	are identified by labels,</para>

      <code>
	smpl obs!="USA" --restrict</code>
	  
      <para>will drop the observation with label <quote>USA</quote>.
      </para>

      <para>
	One point should be noted about the <lit>--dummy</lit>,
	<lit>--restrict</lit> and <lit>--no-missing</lit> forms of
	<lit>smpl</lit>: Any <quote>structural</quote> information in
	the data file (regarding the time series or panel nature of
	the data) is lost when this command is issued.  You may
	reimpose structure with the <cmdref targ="setobs"/> command.
      </para>

      <para>Please see <manref targ="sampling"/> for further details.</para>

    </description>

    <gui-access>
      <menu-path>/Sample</menu-path>
    </gui-access>

  </command>

  <command name="spearman" section="Statistics"
    label="Spearmans's rank correlation">

    <usage>
      <arguments>
        <argument>x</argument>
        <argument>y</argument>
      </arguments>
      <options>
        <option>
	  <flag>--verbose</flag>
	  <effect>print ranked data</effect>
        </option>
      </options>
    </usage>

    <description>
      <para context="cli">
	Prints Spearman's rank correlation coefficient for the two
	variables <mathvar>x</mathvar> and <mathvar>y</mathvar>. The
	variables do not have to be ranked manually in advance; the
	function takes care of this.
      </para>

      <para context="gui">
	Prints Spearman's rank correlation coefficient for a specified
	pair of variables.  The variables do not have to be ranked
	manually in advance; the function takes care of this.</para>

      <para>
	The automatic ranking is from largest to smallest (i.e. the
	largest data value gets rank 1).  If you need to invert this
	ranking, create a new variable which is the negative of the
	original first.  For example:
      </para>

      <code>
	genr altx = -x
	spearman altx y</code>
    </description>

    <gui-access>
      <menu-path>/Model/Rank correlation</menu-path>
    </gui-access>

  </command>

  <command name="square" section="Transformations" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--cross</flag>
	  <effect>generate cross-products as well as squares</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Generates new variables which are squares of the variables in
	<repl>varlist</repl> (plus cross-products if the
	<lit>--cross</lit> option is given).  For example, <cmd>square
	  x y</cmd> will generate <lit>sq_x</lit> = <lit>x</lit>
	squared, <lit>sq_y</lit> = <lit>y</lit> squared and
	(optionally) <lit>x_y</lit> = <lit>x</lit> times <lit>y</lit>.
	If a particular variable is a dummy variable it is not squared
	because we will get the same variable.  
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Add variables/squares of variables</menu-path>
    </gui-access>

  </command>

  <command name="store" section="Dataset" label="Saving data">

    <usage>
      <arguments>
        <argument>datafile</argument>
        <argument optional="true">varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--csv</flag>
	  <effect>use CSV format</effect>
        </option>
        <option>
	  <flag>--gnu-octave</flag>
	  <effect>use GNU Octave format</effect>
        </option>
        <option>
	  <flag>--gnu-R</flag>
	  <effect>use GNU R format</effect>
        </option>
        <option>
	  <flag>--traditional</flag>
	  <effect>use traditional ESL format</effect>
        </option>
        <option>
	  <flag>--gzipped</flag>
	  <effect>apply gzip compression</effect>
        </option>
        <option>
	  <flag>--dat</flag>
	  <effect>use PcGive ASCII format</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Saves either the entire dataset or, if a <repl>varlist</repl>
	is supplied, a specified subset of the variables in the
	current dataset, to the file given by
	<repl>datafile</repl>.</para>

      <para>
	By default the data are saved in <quote>native</quote> gretl
	format, but the option flags permit saving in several
	alternative formats.  CSV (Comma-Separated Values) data may be
	read into spreadsheet programs, and can also be manipulated
	using a text editor.  The formats of
	<program>Octave</program>, <program>R</program> and
	<program>PcGive</program> are designed for use with the
	respective programs.  Gzip compression may be useful for large
	datasets.  See <manref targ="datafiles"/> for details on the
	various formats.</para>

      <para>
	Note that any scalar variables will not be saved
	automatically: if you wish to save scalars you must explicitly
	list them in <repl>varlist</repl>.
      </para>  
	
    </description>

    <gui-access>
      <menu-path>/File/Save data; /File/Export data</menu-path>
    </gui-access>

  </command>

  <command name="summary" section="Statistics" context="cli">

    <usage>
      <arguments>
        <argument optional="true">varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Print summary statistics for the variables in
	<repl>varlist</repl>, or for all the variables in the data set
	if <repl>varlist</repl> is omitted. Output consists of the
	mean, standard deviation (sd), coefficient of variation (=
	sd/mean), median, minimum, maximum, skewness coefficient, and
	excess kurtosis.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Summary statistics</menu-path>
      <other-access>Main window pop-up menu</other-access>
    </gui-access>

  </command>

  <command name="system" section="Estimation" label="Systems of equations">

    <usage>
      <altforms>
	<altform><lit>system method=</lit><repl>estimator</repl></altform>
	<altform><lit>system name=</lit><repl>sysname</repl></altform>
      </altforms>
      <arguments>
        <argument>savevars</argument>
      </arguments>
      <examples>
	<example>system name="Klein Model 1"</example>
        <example>system method=sur</example>
	<example>system method=sur save=resids</example>
	<example>system method=3sls save=resids,fitted</example>
      </examples>
    </usage>

    <description>

      <para context="gui">Gretl offers a mechanism for estimating
	systems of equations, but at present this is available only in
	script mode.  Please see the online help for script command
	syntax, or the gretl manual.
      </para>

      <para context="cli">
	Starts a system of equations.  Either of two forms of the
	command may be given, depending on whether you wish to save
	the system for estimation in more than one way or just
	estimate the system once.</para>

      <para context="cli">
	To save the system you should give it a name, as in the first
	example (if the name contains spaces it must be surrounded by
	double quotes).  In this case you estimate the system using
	the <cmdref targ="estimate"/> command.  With a saved system of
	equations, you are able to impose restrictions (including
	cross-equation restrictions) using the <cmdref
	targ="restrict"/> command.
      </para>

      <para context="cli">
	Alternatively you can specify an estimator for the system
	using <lit>method=</lit> followed by a string identifying one
	of the supported estimators: <cmd>ols</cmd> (Ordinary Least
	Squares), <cmd>tsls</cmd> (Two-Stage Least Squares)
	<cmd>sur</cmd> (Seemingly Unrelated Regressions),
	<cmd>3sls</cmd> (Three-Stage Least Squares), <cmd>fiml</cmd>
	(Full Information Maximum Likelihood) or <cmd>liml</cmd>
	(Limited Information Maximum Likelihood).  In this case the
	system is estimated once its definition is complete.  
      </para>

      <para context="cli">An equation system is terminated by the line
	<cmd>end system</cmd>.  Within the system four sorts of
	statement may be given, as follows.</para>

      <ilist context="cli">
	<li><para><cmdref targ="equation"/>: specify an equation
	    within the system.  At least two such statements must be
	    provided.</para>
	</li>
	<li><para><cmd>instr</cmd>: for a system to be estimated via
	    Three-Stage Least Squares, a list of instruments (by
	    variable name or number). Alternatively, you can put this
	    information into the <cmd>equation</cmd> line using the
	    same syntax as in the <cmdref targ="tsls"/>
	    command.</para>
	</li>
	<li><para><cmd>endog</cmd>: for a system of simultaneous
	    equations, a list of endogenous variables.  This is
	    primarily intended for use with FIML estimation, but with
	    Three-Stage Least Squares this approach may be used
	    instead of giving an <cmd>instr</cmd> list; then all the
	    variables not identified as endogenous will be used as
	    instruments.</para>
	</li>
	<li><para><cmd>identity</cmd>: for use with FIML, an identity
	    linking two or more of the variables in the system.  This
	    sort of statement is ignored when an estimator other than
	    FIML is used.
	  </para>
	</li>
      </ilist>
	
      <para context="cli">
	In the optional <cmd>save=</cmd> field of the command you can
	specify whether to save the residuals (<cmd>resids</cmd>)
	and/or the fitted values (<cmd>fitted</cmd>).
      </para>

      <para context="cli">For full examples of the specification and
	estimation of systems of equations, please see the scripts
	<filename>klein.inp</filename> and
	<filename>greene14_2.inp</filename> (supplied with the gretl
	distribution).
      </para>

    </description>

  </command>

  <command name="tabprint" section="Printing" context="cli">

    <usage>
      <arguments>
        <argument optional="true">-f filename</argument>
      </arguments>
      <options>
        <option>
	  <flag>--complete</flag>
	  <effect>Create a complete document</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Must follow the estimation of a model.  Prints the estimated
	model in the form of a &latex; table.  If a filename is
	specified using the <lit>-f</lit> flag output goes to that
	file, otherwise it goes to a file with a name of the form
	<filename>model_N.tex</filename>, where <lit>N</lit> is the
	number of models estimated to date in the current session. See
	also <cmdref targ="eqnprint"/>.
      </para>

      <para>
	If the <lit>--complete</lit> flag is given the &latex; file is
	a complete document, ready for processing; otherwise it must
	be included in a document.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /LaTeX</menu-path>
    </gui-access>

  </command>

  <command name="testuhat" section="Tests" context="cli"
    label="Normality of residual">

    <description>
      <para>
	Must follow a model estimation command.  Gives the frequency
	distribution for the residual from the model along with a
	chi-square test for normality, based on the procedure
	suggested by Doornik and Hansen (1984).
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/normality of residual</menu-path>
    </gui-access>

  </command>

  <command name="tobit" section="Estimation" label="Tobit model">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>Estimates a Tobit model.  This model may be appropriate
	when the dependent variable is <quote>truncated</quote>.  For
	example, positive and zero values of purchases of durable
	goods on the part of individual households are observed, and
	no negative values, yet decisions on such purchases may be
	thought of as outcomes of an underlying, unobserved
	disposition to purchase that may be negative in some cases.
	For details see Greene's <book>Econometric Analysis</book>,
	Chapter 20.</para>
    </description>

    <gui-access>
      <menu-path>/Model/Tobit</menu-path>
    </gui-access>

  </command>

  <command name="tsls" section="Estimation"
    label="Two-Stage Least Squares">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
	<argument separated="true">instruments</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
      </options>      
      <examples>
        <example>tsls y1 0 y2 y3 x1 x2 ; 0 x1 x2 x3 x4 x5 x6</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Computes two-stage least squares (TSLS or IV) estimates:
	<repl>depvar</repl> is the dependent variable,
	<repl>indepvars</repl> is the list of independent variables
	(including right-hand side endogenous variables) in the
	structural equation for which TSLS estimates are needed; and
	<repl>instruments</repl> is the combined list of exogenous and
	predetermined variables in all the equations. If the
	<repl>instruments</repl> list is not at least as long as
	<repl>indepvars</repl>, the model is not identified.</para>

      <para context="cli">
	In the above example, the <lit>y</lit>s are the endogenous
	variables and the <lit>x</lit>s are the exogenous and
	predetermined variables.  
      </para>

      <para context="gui">
	This command requires the selection of two lists of variables:
	the independent variables to appear in the given model and a
	set of "instruments".  The latter comprises the exogenous
	and/or predetermined variables that may be used as regressors
	to derive fitted values for the right-hand side endogenous
	variables.</para>

      <para context="gui">If some of the right-hand side variables for
	the model are exogenous, they should be referenced in both
	lists.</para>
    </description>

    <gui-access>
      <menu-path>/Model/Two-Stage least Squares</menu-path>
    </gui-access>

  </command>

  <command name="var" section="Estimation"
    label="Vector Autoregression">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>varlist</argument>
	<argument separated="true">detlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--impulse-responses</flag>
	  <effect>print impulse responses</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
      </options>
      <examples>
        <example>var 4 x1 x2 x3 ; const time</example>
      </examples>
    </usage>

    <description>

      <para context="gui">
	This command requires specification of:</para>

      <ilist context="gui">
	<li><para context="gui">- the dependent variable for the first
	    equation in the VAR system;</para>
	</li>

	<li><para context="gui">- the lag order, that is, the number of
	    lags of each variable that should be included in the
	    system;</para>
	</li>

	<li><para context="gui">- any "deterministic" terms (e.g. the
	    constant, a time trend, seasonal dummy variables and so
	    on); and</para>
	</li>

	<li><para context="gui">- a list of independent variables, lags
	    of which will be included on the right-hand side of the
	    equations (note: do not include lagged variables in this
	    list -- they will be added automatically).</para>
	</li>
      </ilist>

      <para context="gui">A separate regression will be run for
	variable in the system.  Output for each equation includes
	F-tests for zero restrictions on all lags of each of the
	variables and an F-test for the maximum lag, along with
	(optionally) forecast variance decompositions and impulse
	response functions.</para>

      <para context="cli">
	Sets up and estimates (using OLS) a vector autoregression
	(VAR).  The first argument specifies the lag order, then
	follows the setup for the first equation.  Don't include lags
	among the elements of <repl>varlist</repl> &mdash; they will
	be added automatically.  The semi-colon separates the
	stochastic variables, for which <repl>order</repl> lags will
	be included, from deterministic terms in <repl>detlist</repl>,
	such as the constant, a time trend, and dummy
	variables.</para>

      <para context="cli">
	In fact, gretl is able to recognize the more common
	deterministic variables (constant, time trend, dummy variables
	with no values other than 0 and 1) as such, so these do not
	have to placed after the semi-colon.  More complex
	deterministic variables (e.g. a time trend interacted with a
	dummy variable) must be put after the semi-colon.</para>

      <para context="cli">
	A separate regression is run for each variable in
	<repl>varlist</repl>. Output for each equation includes
	<mathvar>F</mathvar>-tests for zero restrictions on all lags
	of each of the variables, an <mathvar>F</mathvar>-test for the
	significance of the maximum lag, and, if the
	<lit>--impulse-responses</lit> flag is given, forecast
	variance decompositions and impulse responses.</para>

      <para>
	Forecast variance decompositions and impulse responses are
	based on the Cholesky decomposition of the contemporaneous
	covariance matrix, and in this context the order in which the
	(stochastic) variables are given matters.  The first variable
	in the list is assumed to be <quote>most exogenous</quote>
	within-period.</para>

      <para context="cli">
	The <lit>--quiet</lit> flag suppresses the printing of
	output.  This will be useful only in the context of a script
	which saves the VAR for future examination.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Time series/Vector autoregression</menu-path>
    </gui-access>

  </command>

  <command name="varlist" section="Dataset" context="cli">

    <description>
      <para>
	Prints a listing of variables currently available.
	<cmd>list</cmd> and <cmd>ls</cmd> are synonyms.  
      </para>
    </description>

  </command>

  <command name="vartest" section="Tests"
    label="Difference of variances">

    <usage>
      <arguments>
        <argument>var1</argument>
        <argument>var2</argument>
      </arguments>
    </usage>

    <description>
      <para context="cli">
	Calculates the <mathvar>F</mathvar> statistic for the null
	hypothesis that the population variances for the variables
	<repl>var1</repl> and <repl>var2</repl> are equal, and shows
	its p-value.
      </para>
      <para context="gui">
	Calculates the <mathvar>F</mathvar> statistic for the null
	hypothesis that the population variances are equal for the
	two selected variables, and shows its p-value.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Difference of variances</menu-path>
    </gui-access>

  </command>

  <command name="vif" section="Tests" context="cli"
    label="Variance Inflation Factors">

    <description>
      <para>
	Must follow the estimation of a model which includes at least
	two independent variables. Calculates and displays the
	Variance Inflation Factors (VIFs) for the regressors.  The VIF
	for regressor <mathvar>j</mathvar> is defined as
	<equation status="display" 
	  tex="\[\frac{1}{1-R_j^2}\]"
	  ascii="1/(1 - Rj^2)"
	  graphic="vif"/> where <mathvar>R<sub>j</sub></mathvar> is
	the coefficient of multiple correlation between regressor
	<mathvar>j</mathvar> and the other regressors. The factor has
	a minimum value of 1.0 when the variable in question is
	orthogonal to the other independent variables.  Neter,
	Wasserman, and Kutner (1990) suggest inspecting the largest
	VIF as a diagnostic for collinearity; a value greater than 10
	is sometimes taken as indicating a problematic degree of
	collinearity.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/collinearity</menu-path>
    </gui-access>

  </command>

  <command name="wls" section="Estimation"
    label="Weighted Least Squares">

    <usage>
      <arguments>
        <argument>wtvar</argument>
        <argument>depvar</argument>
	<argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
      </options> 
    </usage>

    <description>
      <para context="cli">
	Computes weighted least squares estimates using
	<repl>wtvar</repl> as the weight, <repl>depvar</repl> as the
	dependent variable, and <repl>indepvars</repl> as the list of
	independent variables.  Specifically, an OLS regression is run
	on <repl>wtvar</repl> <lit>*</lit> <repl>depvar</repl> against
	<repl>wtvar</repl> <lit>*</lit> <repl>indepvars</repl>. If the
	<repl>wtvar</repl> is a dummy variable, this is equivalent to
	eliminating all observations with value zero for
	<repl>wtvar</repl>.
      </para>
      <para context="gui">
	Let "wtvar" denote the variable selected in the "Weight
	variable" box.  An OLS regression is run, where the dependent
	variable is the product of wtvar and the selected dependent
	variable, and the independent variables are also multiplied by
	wtvar.  If wtvar is a dummy variable, this is equivalent to
	eliminating all observations with value zero for wtvar.</para>
    </description>

    <gui-access>
      <menu-path>/Model/Weighted Least Squares</menu-path>
    </gui-access>

  </command>

</commandlist>

