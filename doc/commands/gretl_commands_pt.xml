<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE commandref SYSTEM "gretl_commands.dtd">

<commandref language="portuguese">

<?PSGML NOFILL label code altforms altform menu-path equation other-access?>

  <command name="add" section="Tests" label="Acrescentar variáveis ao modelo">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--lm</flag>
	  <effect>fazer um teste LM, apenas para OLS (Ordinary Least Squares)</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>mostrar apenas o resultado básico do teste</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>não mostrar nada</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar matriz de covariância para o modelo aumentado</effect>
	</option>
	<option>
	  <flag>--both</flag>
	  <effect>apenas para estimação IV, ver abaixo</effect>
	</option>
      </options>
      <examples>
        <example>add 5 7 9</example>
        <example>add xx yy zz --quiet</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Tem que ser invocado após um comando de estimação. As variáveis na
	<repl>lista-de-variáveis</repl> são acrescentadas ao modelo anterior
    e o novo modelo é estimado. Os resultados do teste de significância
    conjunta podem ser obtidos usando os acessores  <lit>$test</lit> e
    <lit>$pvalue</lit>.
      </para>
      <para context="cli">
	Por omissão é efetuada uma estimação da versão aumentada do modelo
    original, com a inclusão das variáveis da <lit>lista-de-variáveis</lit>.
    O teste é um teste de Wald sobre o modelo aumentado, que substitui o
    original como o <quote>modelo corrente</quote> para o propósito de, por
    exemplo, obter os resíduos como <lit>$uhat</lit> ou realizar mais testes.
      </para>
      <para context="cli">
	Em alternativa, usando a opção <opt>lm</opt> (apenas no caso de 
	modelos estimados por OLS), será efetuado um teste LM. Uma regressão
    auxiliar é executada, na qual a variável dependente é o resíduo do 
    modelo anterior e as variáveis independentes são as mesmas do modelo
    anterior mais as da <lit>lista-de-variáveis</lit>. De acordo com a 
    hipótese nula de que as variáveis acrescentadas não acrescentam poder
    explicativo, a dimensão da amostra vezes o R quadrado não-ajustado desta
    regressão tem uma distribuição Qui-quadrado com um grau de liberdade igual
    ao número de regressores adicionado. Neste caso o modelo original não
    é alterado.
      </para>
      <para context="cli">
	A opção <lit>--both</lit> é específico para o método dos mínimos quadrados
    de duas fases: indica que as novas variáveis devem ser acrescentadas
    tanto à lista de regressores como à lista de instrumentos, por omissão
    apenas serão acrescentadas à lista de regressores.
      </para>
      <para context="gui">
	As variáveis seleccionadas são acrescentadas ao modelo anterior e o novo
	modelo é estimado. É apresentada a estatística de teste para a 
	significância conjunta das variáveis acrescentadas, assim como o respectivo
	p valor.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Acrescentar variáveis</menu-path>
    </gui-access>

  </command>

  <command name="addline" section="Graphs" label="Acrescentar linha a um gráfico"
    context="gui">

    <description>
      <para>
	Esta caixa de diálogo permite acrescentar uma linha a um gráfico, definida
    por uma expressão.  A expressão tem que ser aceitável por gnuplot.
    Use <lit>x</lit> para designar o valor da variável no eixo dos x.  Tenha em
    atenção que gnuplot usa <lit>**</lit> para potenciação, e que o símbolo de 
    decimais tem que ser <quote>.</quote>.  Exemplos:
      </para>
      <code>
	10+0.35*x
	100+5.3*x-0.12*x**2
	sin(x)
	exp(sqrt(pi*x))
      </code>
    </description>
  </command>


  <command name="adf" section="Tests" label="Teste de Dickey-Fuller aumentado">

    <usage>
      <arguments>
        <argument>ordem</argument>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--nc</flag>
	  <effect>teste sem constante</effect>
	</option>
	<option>
	  <flag>--c</flag>
	  <effect>apenas com constante</effect>
	</option>
	<option>
	  <flag>--ct</flag>
	  <effect>com constante e tendência</effect>
	</option>
	<option>
	  <flag>--ctt</flag>
	  <effect>com constante, tendência e quadrado da tendência</effect>
	</option>
	<option>
	  <flag>--seasonals</flag>
	  <effect>incluir variáveis sazonais auxiliares</effect>
	</option>
	<option>
	  <flag>--gls</flag>
	  <effect>atenuar média ou tendência usando GLS</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar resultados da regressão</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar resultados</effect>
	</option>
	<option>
	  <flag>--difference</flag>
	  <effect>usar a primeira diferença da variável</effect>
	</option>
	<option>
	  <flag>--test-down</flag>
	  <optparm optional="true">critério</optparm>
	  <effect>ordem de desfasamento automática</effect>
	</option>
	<option>
	  <flag>--perron-qu</flag>
	  <effect>ver abaixo</effect>
	</option>	
      </options>
      <examples>
	<example>adf 0 y</example>
        <example>adf 2 y --nc --c --ct</example>
        <example>adf 12 y --c --test-down</example>
	<demos>
	  <demo>jgm-1996.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para context="gui">Este comando requer uma ordem de desfasamento inteira; se
        a ordem for 0 é executado um teste Dickey&ndash;Fuller padrão (não aumentado).
	Determina estatísticas para um conjunto de testes de Dickey&ndash;Fuller
        sobre a variável especificada, com a hipótese nula de que a variável
        tem uma raiz unitária.  (Mas se a opção de diferenciação tiver sido 
        dada, a primeira diferença da variável é obtida, e a discussão abaixo
        deve ser interpretada como sendo referente à variável transformada.)
      </para>

      <para context="cli">
	The options shown above and the discussion which follows pertain
	to the use of the <lit>adf</lit> command with regular time series
	data. For use of this command with panel data please see below.
      </para>

      <para context="cli">
	Computes a set of Dickey&ndash;Fuller tests on each of the the
	listed variables, the null hypothesis being that the variable in
	question has a unit root.  (But if the <opt>difference</opt>
	flag is given, the first difference of the variable is taken prior
	to testing, and the discussion below must be taken as referring to
	the transformed variable.)
      </para>

      <para context="cli">
        Por omissão, são apresentadas duas variantes do teste: uma baseada
        na regressão contendo uma constante e uma usando uma constante e uma
        tendência linear.  Você pode controlar as variantes que são apresentadas ao especificar
        uma ou mais opções.
      </para>

      <para context="cli">
	The <opt>--gls</opt> option can be used in conjunction with
	one or other of the flags <opt>--c</opt> and <opt>--ct</opt>
	(the model with constant, or model with constant and trend).
	The effect of this option is that the de-meaning or
	de-trending of the variable to be tested is done using the GLS
	procedure suggested by <cite key="ERS96">Elliott,
	Rothenberg and Stock (1996)</cite>, which gives a test of
	greater power than the standard Dickey&ndash;Fuller approach.
	This option is not compatible with <opt>--nc</opt>,
	<opt>--ctt</opt> or <opt>--seasonals</opt>.
      </para>      

      <para>
        Em todos os casos a variável dependente é a primeira diferença
        da variável especificada, <math>y</math>, e a variável independente
        chave é o primeiro desfasamento de <math>y</math>.  O modelo é construido
        de modo a que o coeficiente do desfasamento de <math>y</math> iguale a
        raiz em questão menos 1.  Por exemplo, o modelo com uma constante pode
        ser escrito como <equation status="display"
	tex="\[(1-L)y_t=\beta_0+(\alpha-1)y_{t-1}+\epsilon_t\]"
	ascii="(1 - L)y(t) = b0 + (a-1)y(t-1) + e(t)"
	  graphic="adf1"/> Under the null hypothesis of a unit root the
	coefficient on lagged <math>y</math> equals zero; under the
	alternative that <math>y</math> is stationary this coefficient is
	negative.
      </para>

      <para context="cli">
	Se a <repl>ordem</repl> de desfasamento (daqui em diante, 
        <math>k</math>) é maior que 0, então <math>k</math> desfasamentos
        da variável dependente são incluidos no lado direito das regressões
        de teste.  If the order is given as &minus;1,
	<math>k</math> is set following the recommendation of <cite
	key="schwert89">Schwert (1989)</cite>, namely the integer part
	of 12(<math>T</math>/100)<sup>0.25</sup>, where <math>T</math>
	is the sample size.   Se a opção <lit>--test-down</lit> foi dada, 
        <math>k</math> é considerada como sendo o desfasamento 
        <emphasis>máximo</emphasis> e a ordem de desfasamento efectivamente
        usada é obtida testando para baixo.  O critério para testar para baixo
        pode ser selecionado usando o parâmetro opcional, que  deve ser um de 
        <lit>AIC</lit>, <lit>BIC</lit> ou <lit>tstat</lit>; por omissão é
        <lit>AIC</lit>.
      </para>


      <para context="cli">
	When testing down via AIC or BIC is called for, the final lag
	order for the ADF equation is that which optimizes the chosen
	information criterion (Akaike or Schwarz Bayesian).  The
	exact procedure depends on whether or not the <opt>--gls</opt>
	option is given: when GLS detrending is specified, AIC and BIC
	are the <quote>modified</quote> versions described in <cite
	key="ng-perron01">Ng and Perron (2001)</cite>, otherwise they
	are the standard versions. In the GLS case a refinement is
	available: if the additional option <opt>perron-qu</opt> is
	given, the modified information criteria are computed
	according to the revised method recommended by <cite
	key="perron-qu07">Perron and Qu (2007)</cite>.
      </para>

      <para context="gui">
	When testing down via AIC or BIC is called for, the final lag
	order for the ADF equation is that which optimizes the chosen
	information criterion (Akaike or Schwarz Bayesian).
      </para>      
	
      <para>
	When testing down via the <math>t</math>-statistic method
	is called for, the procedure is as follows:
      </para>

      <nlist>
	<li><para>Estimar a regressão de Dickey&ndash;Fuller com
	    <math>k</math> desfasamentos da variável dependente.
	  </para>
	</li>
	<li><para>O último desfasamento é significante?  Se sim, executar o teste
            com com a ordem de desfasamento, <math>k</math>.  Senão, fazer 
	    <math>k</math> = <math>k</math> &minus; 1; se 
	    <math>k</math> for igual a 0, executar o teste com a
            ordem de desfasamento 0, senão saltar para o passo 1.
	  </para>
	</li>
      </nlist>

      <para>No contexto do passo 2 acima, <quote>significante</quote>
	quer dizer que para o último desfasamento, a estatística-<math>t</math>,
        que segue uma distribuição normal, tem um <emphasis>p</emphasis> valor
        bilateral assimptótico menor ou igual a 0,10.</para> 

      <para>Os <emphasis>p</emphasis> valores para os testes de
	Dickey&ndash;Fuller baseiam-se em MacKinnon (1996).  O código relevante
	é incluído com a generosa permissão do autor.  In the case of the test with linear
	trend using GLS these <emphasis>P</emphasis>-values are not
	applicable; critical values from Table 1 in <cite
	key="ERS96">Elliott, Rothenberg and Stock (1996)</cite> are
	shown instead.
      </para>

      <subhead context="cli">Panel data</subhead>

      <para context="cli">
	When the <lit>adf</lit> command is used with panel data, to
	produce a panel unit root test, the applicable options and the
	results shown are somewhat different.
      </para>
      <para context="cli">
	First, while you may give a list of variables for testing in
	the regular time-series case, with panel data only one
	variable may be tested per command. Second, the options
	governing the inclusion of deterministic terms become mutually
	exclusive: you must choose between no-constant, constant only,
	and constant plus trend; the default is constant only. In
	addition, the <opt>--seasonals</opt> option is not available.
	Third, the <opt>--verbose</opt> option has a different meaning:
	it produces a brief account of the test for each individual
	time series (the default being to show only the overall
	result).
      </para>
      <para context="cli">
	The overall test (null hypothesis: the series in question has
	a unit root for all the panel units) is calculated in one or
	both of two ways: using the method of <cite key="IPS03">Im,
	Pesaran and Shin (Journal of Econometrics, 2003)</cite> or that
	of <cite key="choi01">Choi (Journal of International Money and
	Finance, 2001)</cite>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Variável/Teste de Dickey-Fuller aumentado</menu-path>
    </gui-access>

  </command>

  <command name="anova" label="ANOVA" section="Statistics">
    <usage>
      <arguments>
        <argument>response</argument>
        <argument>treatment</argument>
        <argument optional="true">block</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar resultados</effect>
	</option>
      </options>
    </usage>
    <description>
      <para>
	Análise de Variância: <repl>response</repl> é uma série que mede
	um efeito com interesse e <repl>treatment</repl> tem que ser uma
	variável discreta que codifica dois ou mais tipos de tratamento
	(ou não-tratamento).  Para ANOVA de duas-vias, a variável 
	<repl>block</repl> (que também deve ser discreta) codifica os 
	valores de uma variável de controlo.
      </para>
      <para context="cli">
	Quando não se usa a opção <opt>--quiet</opt>, este comando mostra a
	tabela das somas de quadrados e médias quadradas juntamente com um
	teste <math>F</math>.  O teste <math>F</math> e o seu valor P podem 
	ser obtidos usando os acessores <lit>$test</lit> e <lit>$pvalue</lit>
	respetivamente.
      </para>
      <para>
	A hipótese nula do teste <math>F</math> é de que a resposta média é
	invariante com o tipo de tratamento, ou por outras palavras, que o 
	tratamento não produz efeito.  Em termos formais, o teste é apenas
	válido se a variância da resposta for igual para todos os tipos de 
	tratamentos.
      </para>
      <para>
	Note que os resultados apresentados por este comando pertencem a um
	subconjunto da informação resultante do procedimento seguinte, que é
	facilmente implementável em gretl.  Crie um conjunto de variáveis
	auxiliares que codifiquem todos os tipos de tratamentos, exceto um.
	No caso da ANOVA de duas-vias, adicionalmente, crie um conjunto de
	variáveis auxiliares que codifiquem todos os <quote>blocks</quote>
	(controlos), exceto um.
	De seguida efectue uma regressão sobre <repl>response</repl> com uma
	constante e com as variáveis auxiliares usando <cmdref targ="ols"/>.
	No caso da ANOVA singular (ou uma-via) a  tabela é produzida passando
	a opção <opt>--anova</opt> para <lit>ols</lit>.  No caso da ANOVA de 
	duas-vias o teste F relevante, é obtido usando o comando <cmdref
	targ="omit"/>.  Por exemplo (assumindo que <lit>y</lit> é a resposta,
	<lit>xt</lit> codifica os tratamentos, e <lit>xb</lit> codifica os 
	controlos):
      </para>
      <code>
	# uma-via
	list dxt = dummify(xt)
	ols y 0 dxt --anova
	# duas-vias
	list dxb = dummify(xb)
	ols y 0 dxt dxb
	# teste da significância conjunta de dxt
	omit dxt --quiet
      </code>
    </description>

    <gui-access>
      <menu-path>/Modelo/Outros modelos lineares/ANOVA</menu-path>
    </gui-access>

  </command>

  <command name="append" section="Dataset" label="Acrescentar dados" context="cli">

    <usage>
      <arguments>
        <argument>ficheiro-de-dados</argument>
      </arguments>
      <options>
	<option>
	  <flag>--time-series</flag>
	  <effect>ver abaixo</effect>
	</option>
	<option>
	  <flag>--update-overlap</flag>
	  <effect>ver abaixo</effect>
	</option>
	<option>
	  <note>Ver abaixo para opções especializadas adicionais</note>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Abre um ficheiro de dados e acrescenta esse conteúdo ao conjunto 
        de dados actual, se os novos dados forem compatíveis.  O programa
        tentará determinar o formato do ficheiro de dados (nativo, texto 
        simples, CSV, Gnumeric, Excel, etc.).
      </para>
	<para>
	Os dados acrescentados podem tomar a forma de observações
        adicionais em variáveis já existentes, ou em novas variáveis.
	Caso sejam novas variáveis, estas terão que ser compatíveis de
	acordo com:
        (a) o número de observações dos novos dados seja o mesmo que nos
	dados existentes, ou
        (b) que os novos dados estejam acompanhados de informação clara
	sobre as observações de modo que gretl possa decidir onde colocar
	os valores.  
      </para>
      <para>
	Existe uma funcionalidade especial para acrescentar a um conjunto de
	dados de painel. Seja <math>n</math> o número de unidades de secção 
	cruzada no painel, <math>T</math> o número de períodos temporais, e
	<math>m</math> o número de observações dos novos dados. Se <math>m =
	  n</math> os novos dados serão tomados como invariantes-temporais,
	e serão copiados para a posição em cada período temporal. Por outro
	lado, se <math>m = T</math> os dados serão tratados como sendo 
	não-variantes a longo das unidades de painel, e serão copiados para a
	posição em cada unidade. Se o painel é <quote>quadrado</quote>, e
	<math>m</math> é igual a <math>n</math> e a <math>T</math>, acontece
	uma ambiguidade. Neste caso, por omissão, trata-se cada novos dados
	como sendo invariantes-temporais, mas você pode forçar gretl para
	tratar os novos dados como sendo série temporal ao fornecer a opção
	<opt>--time-series</opt>.  (Esta opção é ignorada nos outros casos.)
      </para>
      <para>
	When a data file is selected for appending, there may be an
	area of overlap with the existing dataset; that is, one or
	more series may have one or more observations in common across
	the two sources. If the option <opt>update-overlap</opt> is
	given, the <lit>append</lit> operation will replace any
	overlapping observations with the values from the selected
	data file, otherwise the values currently in place will be
	unaffected.
      </para>
      <para>
	The additional specialized options <opt>sheet</opt>,
	<opt>coloffset</opt>, <opt>rowoffset</opt> and
	<opt>fixed-cols</opt> work in the same way as with <cmdref
	targ="open"/>; see that command for explanations.
      </para>
      <para>
	Ver também o comando <cmdref targ="join"/> para um manuseamento
        mais sofisticado com várias fontes de dados.
      </para>
    </description>

    <gui-access>
      <menu-path>/Ficheiro/Acrescentar dados</menu-path>
    </gui-access>

  </command>

  <command name="ar" section="Estimation" label="Estimação autoregressiva">

    <usage>
      <arguments>
        <argument>desfasamentos</argument>
	<argument separated="true">variável-dependente</argument>
        <argument>variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar matriz de covariância</effect>
	</option>
      </options>
      <examples>
        <example>ar 1 3 4 ; y 0 x1 x2 x3</example>
      </examples>
    </usage>

    <description>
      <para>
	Determina estimativas para os parâmetros usando o
        procedimento iteractivo e generalizado de 
        Cochrane&ndash;Orcutt (ver a Secção 9.5 de Ramanathan, 
        2002). A iteração termina quando os erros das somas de 
        quadrados sucessivos não difiram em mais que 0,005 porcento 
        ou após 20 iterações.</para>

      <para context="gui">
	A <quote>lista de desfasamentos AR</quote> especifica a estrutura do
	processo de erro.  Por exemplo, a entrada <quote>1 3
	  4</quote> corresponde ao processo: 
	<equation status="display" 
	  tex="\[u_t = \rho_1u_{t-1} + \rho_3 u_{t-3} +
	    \rho_4 u_{t-4} + e_t\]"
	  ascii="u(t) = rho1*u(t-1) + rho3*u(t-3) + rho4*u(t-4)"
	  graphic="arlags"/>
      </para>

      <para context="cli">
	<repl quote="true">desfasamentos</repl> é uma lista de desfasamentos nos 
        resíduos, terminada por um ponto-e-vírgula. No exemplo acima
        o termo do erro é especificado como
	<equation status="display" 
	  tex="\[u_t = \rho_1u_{t-1} + \rho_3 u_{t-3} +
	    \rho_4 u_{t-4} + e_t\]"
	  ascii="u(t) = rho(1)*u(t-1) + rho(3)*u(t-3) + rho(4)*u(t-4)"
	  graphic="arlags"/>
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/Estimação autoregressiva</menu-path>
    </gui-access>

  </command>

  <command name="ar1" section="Estimation" label="Estimação AR(1)">

    <usage>
      <arguments>
	<argument>variável-dependente</argument>
        <argument>variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--hilu</flag>
	  <effect>usar o procedimento Hildreth&ndash;Lu</effect>
	</option>
	<option>
	  <flag>--pwe</flag>
	  <effect>usar o estimador Prais&ndash;Winsten</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
	</option>
	<option>
	  <flag>--no-corc</flag>
	  <effect>não aperfeiçoar os resultados com Cochrane-Orcutt</effect>
	</option>
	<option>
	  <flag>--loose</flag>
	  <effect>usar critério de convergência mais relaxado</effect>
	</option>
      </options>
      <examples>
        <example>ar1 1 0 2 4 6 7</example>
	<example>ar1 y 0 xlist --pwe</example>
	<example>ar1 y 0 xlist --hilu --no-corc</example>
      </examples>
    </usage>

    <description>
      <para>
	Determina estimativas admissíveis GLS para um modelo em que se
	assume que o termo de erro segue um processo autoregressivo de
	primeira-ordem.
      </para>
      <para>
	O método por omissão é o procedimento iterativo de Cochrane&ndash;Orcutt
	(ver, por exemplo, a Secção 9.4 de Ramanathan, 2002). A iteração termina
	quando as estimativas sucessivas do coeficiente de autocorrelação não 
	diferirem por mais de 0.001 ou após 20 iterações.
      </para>
      <para>
	Se tiver sido dada a opção <lit>--hilu</lit>, é utilizado o 
	procedimento de pesquisa de Hildreth&ndash;Lu.  Os resultados são depois
	aperfeiçoados usando o método Cochrane&ndash;Orcutt, exceto se tiver sido
	indicada a opção <lit>--no-corc</lit>.  (Esta última opção é ignorada se
	 não tiver sido usado <lit>--hilu</lit>).
      </para>
      <para>
	Se tiver sido dada a opção <lit>--pwe</lit>, é usado o estimador 
	de Prais&ndash;Winsten.  Isto involve uma iteração semelhante à de
	Cochrane&ndash;Orcutt; a diferença é que equanto 
	Cochrane&ndash;Orcutt descarta a primeira observação, a de 
	Prais&ndash;Winsten faz uso dela. Para mais detalhes ver, por 
	exemplo, o Capítulo 13 do livro de Greene, <book>Econometric Analysis</book>
	(2000).
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/AR(1)</menu-path>
    </gui-access>

  </command>

  <command name="arbond" section="Estimation" label="Arellano-Bond">

    <usage>
      <arguments>
	<argblock>
	  <argument>p</argument>
	  <argument optional="true">q</argument>
	</argblock>
	<argblock separated="true">
	  <argument>variável-dependente</argument>
	  <argument>variáveis-independentes</argument>
	</argblock>
	<argblock optional="true" separated="true">
	  <argument>instrumentos</argument>
	</argblock>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar o modelo estimado</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar matriz de covariância</effect>
	</option>
        <option>
	  <flag>--two-step</flag>
	  <effect>executa estimação pelo Método Generalizado dos Momentos (GMM) de 2-fases</effect>
        </option>
        <option>
	  <flag>--time-dummies</flag>
	  <effect>acrescenta variáveis auxiliares tempo</effect>
        </option>
        <option>
	  <flag>--asymptotic</flag>
	  <effect>erros padrão assimptóticos não corrigidos</effect>
        </option>
      </options>
      <examples>
        <example>arbond 2 ; y Dx1 Dx2</example>
        <example>arbond 2 5 ; y Dx1 Dx2 ; Dx1</example>
	<example>arbond 1 ; y Dx1 Dx2 ; Dx1 GMM(x2,2,3)</example>
	<demos>
	  <demo>arbond91.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Executa a estimação de modelos de painel dinâmico (ou seja,
        modelos de painel que contenham um ou mais desfasamentos da variável
        dependente) recorrendo ao método Método Generalizado dos Momentos
	(GMM-DIF) desenvolvido por <cite
	key="arellano-bond91">Arellano and Bond (1991)</cite>.
	Por favor ver <cmdref targ="dpanel"/> para uma versão mais flexível
	e actualizada deste comando que também usa GMM-SYS para além do 
	GMM-DIF.
      </para>
      <para>
	O parâmetro <repl>p</repl> representa a ordem da autoregressão 
        para a variável dependente.  O parâmetro opcional <repl>q</repl>
        indica o máximo desfasamento do nível da variável dependente a ser usada
        como um instrumento.  Se este argumento for omitido, ou de valor 
        0, todos os desfasamentos disponíveis são usados.
      </para>
      <para>
	A variável dependente deve ser dada na forma de níveis; ela será
        automaticamente diferenciada (pois este estimador usa 
        diferenciação para anular os efeitos individuais).  As variáveis
        independentes não são automaticamente diferenciadas; se você 
        pretende usar diferenças (o que acontece em geral para variáveis
        quantitativas, mas não será, por exemplo, para variáveis 
        auxiliares temporais), deve primeiro criar essas diferenças e 
        depois especificar estas como sendo regressoras.
      </para>
      <para>
	O último campo (opcional) do comando serve para especificar 
        instrumentos.  Se não for dado nenhum, então é assumido que todas
        as variáveis independentes são estritamente exógenas.  Se você
        especificar alguns instrumentos, então deve incluir na lista 
        quaisquer variáveis independentes estritamente exógenas.  Para
        regressores predeterminados, você pode usar a função 
        <lit>GMM</lit> para incluir uma gama de desfasamentos especificada
	no modo bloco-diagonal.  Isto é ilustrado no terceiro exemplo acima.
        O primeiro argumento de <lit>GMM</lit> é o nome da variável em 
        questão, o segundo é o desfasamento mínimo a ser usado como
	instrumento, e o terceiro é o desfasamento máximo.  Se o terceiro 
	argumento for dado como 0, todos os desfasamentos disponíveis são
	usados.
      </para>
      <para>
	Por omissão são apresentados os resultados da estimação 1-fase 
        (com erros padrão robustos).  Opcionalmente, você pode escolher 
        estimação de 2-fases.  Em ambos os casos são efectuados testes de
        autocorrelação de ordem 1 e 2, assim como o teste de 
        sobre-identificação de Sargan e o teste de Wald para a 
        significância conjunta dos regressores.  Note-se que este modelo 
        de diferenciação com autocorrelação de primeira-ordem não invalida 
        o modelo, mas que a autocorrelação de segunda-ordem não respeita 
        as assunções estatísticas presentes.
      </para>
      <para>
	No caso da estimação de 2-fases, por omissão, os erros padrão são
        determinados usando a correcção de amostra-finita sugerida por 
        <cite key="windmeijer05">Windmeijer (2005)</cite>.  Os erros padrão
	assimptóticos associados ao estimador de 2-fases são em geral 
	considerados como guias para inferência pouco fiáveis, mas se por
	alguma razão os pretender ver, você pode usar a opção 
	<lit>--asymptotic</lit> para desligar a correcção de Windmeijer.
      </para>
      <para>
	Se for dada a opção <lit>--time-dummies</lit>, são acrescentadas
        variáveis auxiliares temporais aos regressores especificados.  
        Para evitar colinearidade perfeita com a constante, o número de 
        auxiliares é uma unidade a menos que o número máximo de períodos
        usados na estimação.  As auxiliares são introduzidas por níveis;
        se você deseja usar auxiliares de tempo na forma de 
        primeiras-diferenças, você terá que definir e acrescentar essas
        variáveis manualmente.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Painel/Arellano-Bond</menu-path>
    </gui-access>

  </command>

  <command name="arch" section="Estimation" label="Modelo ARCH">

    <usage>
      <arguments>
        <argument>ordem</argument>
        <argument>variável-dependente</argument>
	<argument>variáveis-independentes</argument>
      </arguments>
      <examples>
        <example>arch 4 y 0 x1 x2 x3</example>
      </examples>
    </usage>

    <description>
	<para>
	This command is retained at present for backward
	compatibility, but you are better off using the maximum
	likelihood estimator offered by the <cmdref targ="garch"/>
	command; for a plain ARCH model, set the first GARCH
	parameter to 0.
      </para>
      <para>
	Estima a especificação do modelo fornecido aceitando em ARCH
	(Heterosquedicidade Condicional Autoregressiva).  O modelo é
	primeiramente estimado em OLS, e depois é efectuada uma 
	regressão auxiliar, na qual, o quadrado dos resíduos da primeira
	fase é regredido com os seus próprios valores desfasados.  A fase
	final é uma estimação por mínimos quadrados com pesos, usando como
	pesos os recíprocos das variâncias de erro ajustadas da regressão
	auxiliar.  (Se a variância predita de de alguma observação na 
	regressão auxiliar for não positiva, então será usada o 
	correspondente resíduo quadrado).
      </para>
      <para>
	Os valores <lit>alpha</lit> mostrados abaixo dos coeficientes
	são os parâmetros estimados do processo ARCH da regressão auxiliar.
      </para>
      <para>
	Ver também <cmdref targ="garch"/> e <cmdref targ="modtest"/> (a opção
	<opt>--arch</opt>).  
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/ARCH</menu-path>
    </gui-access>

  </command>

  <command name="arima" section="Estimation" label="Modelo ARMA">

    <usage>
      <arguments>
	<argblock>
	  <argument>p</argument>
	  <argument>d</argument>
	  <argument>q</argument>
	</argblock>
	<argblock separated="true" optional="true">
	  <argument>P</argument>
	  <argument>D</argument>
	  <argument>Q</argument>
	</argblock>
	<argument separated="true">variável-dependente</argument>
	<argument optional="true">variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das iterações</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>mostrar matriz de covariância</effect>
        </option>
	<option>
	  <flag>--hessian</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--opg</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--nc</flag>
	  <effect>não incluir uma constante</effect>
        </option>
        <option>
	  <flag>--conditional</flag>
	  <effect>usar verosimilhança máxima condicional</effect>
        </option>
        <option>
	  <flag>--x-12-arima</flag>
	  <effect>usar X-12-ARIMA para estimação</effect>
        </option>
	<option>
	  <flag>--lbfgs</flag>
	  <effect>usar maximizador L-BFGS-B</effect>
	</option>
	<option>
	  <flag>--y-diff-only</flag>
	  <effect>ARIMAX especial, ver abaixo</effect>
	</option>
	<option>
	  <flag>--save-ehat</flag>
	  <effect>ver abaixo</effect>
	</option>
      </options>
      <examples>
        <example>arima 1 0 2 ; y</example>
	<example>arima 2 0 2 ; y 0 x1 x2 --verbose</example>
	<example>arima 0 1 1 ; 0 1 1 ; y --nc</example>
      </examples>
    </usage>

    <description>

      <para context="cli">
	Se não for dada a lista de <repl>variáveis-independentes</repl>,
        é estimado um modelo ARIMA (Média Móvel, Autoregressiva, Integrada)
        univariado.  O valores inteiros <repl>p</repl>, <repl>d</repl> e 
        <repl>q</repl> representam respectivamente, a ordem autoregressiva 
        (AR), a ordem de diferenciação, e ordem da média móvel (MA). 
	  Estes valores podem ser fornecidos na forma numérica, ou como
        nome de variáveis escalares pré-existentes.  Por exemplo, um valor
        de 1 em <repl>d</repl>, significa que a primeira diferença da variável
        dependente deve ser obtida antes de estimar os parâmetros ARMA.
      </para>

      <para context="cli">
	Se você pretender apenas incluir no modelo desfasamentos específicos
	AR ou MA (e não todos os desfasamentos até uma certa ordem) você pode
	substituir <repl>p</repl> e/ou <repl>q</repl> de acordo com:
	(a) o nome de uma matriz pré-definida contendo um conjunto de valores
	inteiros, ou
	(b) uma expressão tal como <lit>{1 4}</lit>; ou seja, um conjunto de 
	desfasamentos separados por espaços dentro de chavetas.
      </para>

      <para context="cli">
	Os valores inteiros opcionais,<repl>P</repl>, <repl>D</repl> e
	<repl>Q</repl> representam respectivamente, a sazonalidade AR,
        a ordem para diferenciação de sazonalidade e a ordem de 
        sazonalidade MA.  Estes são apenas aplicáveis se os dados tiverem
        uma frequência superior a 1 (por exemplo, trimestral ou 
        mensal). Mais uma vez, estas ordens podem ser dadas na forma 
        numérica ou como variáveis.
      </para>

      <para context="cli">
	No caso univariado é incluído no modelo por omissão, um interceptor,  
        mas isto pode ser suprimido com a opção	<lit>--nc</lit>.  Se forem
        fornecidas <repl>variáveis-independentes</repl>, o modelo passa a 
        ser ARMAX; neste caso a constante deve ser explicitamente incluída
        se você pretender um interceptor (tal como no segundo exemplo acima).
      </para>

      <para context="cli">
	Existe outra forma alternativa para este comando: se você não 
        pretende aplicar diferenciação (seja sazonal ou não-sazonal), 
        você pode omitir ambos os parâmetros <repl>d</repl> e 
        <repl>D</repl>, em vez de entrar explicitamente zeros.  Além 
        disso, <lit>arma</lit> é um sinónimo ou aliás para 
        <lit>arima</lit>.  Assim, por exemplo, o comando seguinte é 
        válido para especificar o modelo ARMA(2, 1):
      </para>
      <code context="cli">
	arma 2 1 ; y
      </code>

      <para context="gui">
	Estima um modelo ARMA, com ou sem regressores exógenos.  Se a ordem
        de diferenciação for superior a zero o modelo passa a ser ARIMA.  Se
        os dados têm uma frequência superior a 1, é apresentada a opção de 
        inclusão de uma componente sazonal.
      </para>
   
      <para context="gui">
	Se você pretender apenas incluir no modelo desfasamentos específicos
	AR ou MA (e não todos os desfasamentos até uma certa ordem) ative a
	caixa de seleção à direita do seletor de números e escreva no campo de
	texto uma lista	de desfasamentos, separados por espaços.
	Alternativamente, se existir uma matriz contendo o conjunto de
	desfasamentos pretendidos, você pode introduzir o seu nome no campo de
	texto.
      </para>

      <para>
	O normal é usar a funcionalidade <quote>nativa</quote> gretl
        ARMA, com estimação de Máxima Verosimilhança (ML) exata 
        (usando o filtro de Kalman).  Outras opções são: código nativo,
         ML condicional; <program>X-12-ARIMA</program>, ML exata; e 
	<program>X-12-ARIMA</program>, ML condicional.  (As últimas
        duas opções estão disponíveis apenas se o programa
	<program>X-12-ARIMA</program> estiver instalado.)  Para detalhes
        sobre estas opções, veja por favor <guideref targ="chap:timeseries"/>.
      </para>

      <para context="cli">
	Quando o código nativo ML é usado, os erros padrão são por omissão 
	baseados numa aproximação numérica da (inversa negativa da) Hessiana,
	ou em recurso, no produto externo do gradiente (OPG) caso falhe o 
	cálculo da Hessiana numérica. Podem ser usadas duas opções 
	(mutualmente exclusivas) para forçar a situação: a opção <opt>--opg</opt> 
	força o uso do método OPG, sem tentar obter a Hessiana, enquanto a
	opção <opt>--hessian</opt> desativa o OPG em último recurso.
	Note que a falha na determinação da Hessiana numérica indica, em geral
	um modelo incorretamente especificado.
      </para>

      <para context="cli">
	A opção <opt>--lbfgs</opt> é específica para estimação usando
	código nativo ARMA e ML exata:
	significa o uso do algoritmo de <quote>memória limitada</quote>
	L-BFGS-B em vez do usual maximizador BFGS.  Isto pode ajudar em
	alguns casos onde a convergência é difícil de atingir.
      </para>

      <para context="cli">
	A opção <opt>--y-diff-only</opt> é específica na estimação de modelos
	ARIMAX (modelos com uma ordem de integração não-nula e que incluam 
	regressores exógenos), e aplica-se apenas quando se usa a ML exata e
	nativa de gretl. Para esses modelos o comportamento normal é de 
	diferenciar tanto as variáveis dependentes como as regressoras,
	mas quando esta opção é fornecida, apenas é diferenciada a variável
	dependente, mantendo-se as variáveis regressoras na forma de nível.
      </para>

      <para context="cli">
	A opção <opt>--save-ehat</opt> é aplicável apenas quando se
	usa estimação ML nativa e exata. O efeito é o de disponibilizar
	um vector contendo a estimativa óptima de período
	<math>t</math> da perturbação data-<math>t</math> ou inovação:
	isto pode ser recuperado com o uso do acessor <lit>$ehat</lit>.
	Estes valores diferem da série dos resíduos (<lit>$uhat</lit>),
	que contém os erros de predição um-passo-à-frente.
      </para>

      <para>O valor AIC retornado em ligação com os modelos ARIMA é
	calculado conforme a definição usada no programa 
	<program>X-12-ARIMA</program>, nomeadamente
	  <equation status="display" 
	  tex="\[\mbox{AIC}=-2\ell + 2k\]"
	  ascii="AIC = -2L + 2k"
	  graphic="aic"/> onde 
	<equation status="inline" 
	  tex="$\ell$" ascii="L"
	  graphic="ell"/> é o
	logaritmo da verosimilhança e <math>k</math> é o número
        total de parâmetros estimados.  Note-se que o programa 
        <program>X-12-ARIMA</program> não produz critérios de informação
        tal como o AIC quando a estimação é por ML condicional.
      </para>

      <para context="tex">
	As raízes AR e MA apresentadas em ligação com a estimação ARMA
	são baseadas na seguinte representação de um processo ARMA($p,q$):
	\[
	(1-\phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p)Y =
          c + (1 + \theta_1 L + \theta_2 L^2 + \cdots +
         \theta_q L^q)\varepsilon_t
        \]
        As raízes AR são portanto as soluções de
        \[
         1 - \phi_1 z - \phi_2 z^2 - \cdots - \phi_p L^p = 0
        \]
        e a estabilidade requer que estas raízes estejam fora do círculo
	unitário.
      </para>

      <para context="tex">
	A imagem da <quote>frequência</quote> apresentada
	em ligação com as raízes AR e MA é o valor $\lambda$ que solve 
	$z=re^{i2\pi\lambda}$, onde $z$ é a raiz em questão
	e $r$ é o seu módulo.
      </para>

      <para context="notex">
	As raízes AR e MA apresentadas em ligação com a estimação ARMA
	são baseadas na seguinte representação de um processo ARMA(p,q):
      </para>
      <mono context="notex">
	(1 - a_1*L - a_2*L^2 - ... - a_p*L^p)Y =
          c + (1 + b_1*L + b_2*L^2 + ... + b_q*L^q) e_t
      </mono>
      <para context="notex">
        As raízes AR são portanto as soluções de
      </para>
      <mono context="notex">
         1 - a_1*z - a_2*z^2 - ... - a_p*L^p = 0
      </mono>
      <para context="notex">
        e a estabilidade requer que estas raízes estejam fora do círculo
	unitário.
      </para>

      <para context="notex">
	A imagem da <quote>frequência</quote> apresentada
	em ligação com as raízes AR e MA é o valor &lgr; que resolve
	<math>z</math> = <math>r</math> * exp(i*2*&pi;*&lgr;) onde 
	<math>z</math> é a raiz em questão e <math>r</math>
	 o seu módulo.
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/ARIMA</menu-path>
      <other-access>Menu de contexto da janela principal (selecção singular)</other-access>
    </gui-access>

  </command>

  <command name="bfgs-config" section="Estimation" label="Opções do maximizador BFGS"
    context="gui">
    <description>
      <para>
	Esta janela de diálogo permite-lhe controlar alguns aspetos de operação
	do maximizador BFGS.  Se o maximizador falhar em convergir, em alguns casos, 
	pode ajudar aumentar o número máximo  de iterações e, ou, aumentar a tolerância
	de convergência (ser mais permissivo).
	No entanto, se tiver sido usada uma tolerância muito alta, esses resultados 
	devem ser encarados com desconfiança pois é possível que o modelo que se está
	a estimar não tenha sido bem especificado.
      </para>
      <para>
	Na maior parte das aplicações nós recomendamos o uso do maximizador 
	BFGS normal, mas para algums problemas a variante do algoritmo com 
	<quote>memória limitada</quote>, L-BFGS-B, pode produzir uma 
	convergência mais rápida.  Quando se seleciona L-BFGS-B, tem-se a 
	opção de definir o número de correções usadas na matriz de memória 
	limitada (entre 3 e 20, com um valor por omissão de 8).
      </para>
    </description>
  </command>

  <command name="biprobit" section="Estimation" label="Bivariate probit"
    context="cli">
    <usage>
      <arguments>
        <argument>variável-dependente1</argument>
	<argument>variável-dependente2</argument>
        <argument>variáveis-independentes1</argument>
	<argument separated="true" optional="true">variáveis-independentes2</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
        </option>
	<option>
	  <flag>--robust</flag>
	  <effect>erros padrão robustos</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>variável-agrupada</optparm>
	  <effect>ver a explicação em <cmdref targ="logit"/></effect>
        </option>
	<option>
	  <flag>--opg</flag>
	  <effect>ver abaixo</effect>
        </option>
	<option>
	  <flag>--save-xbeta</flag>
	  <effect>ver abaixo</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar informação adicional</effect>
        </option>
      </options>      
      <examples>
        <example>biprobit y1 y2 0 x1 x2</example>
	<example>biprobit y1 y2 0 x11 x12 ; 0 x21 x22</example>
	<demos>
	  <demo>biprobit.inp</demo>
	</demos>
      </examples>
    </usage>
    <description>
      <para>
	Estima um modelo probit bivariado, usando o método de
	Newton&ndash;Raphson para maximizar a verosimilhança.
      </para>
      <para>
	A lista de argumentos começa com as duas variáveis dependentes
	(binárias), seguidas pela lista de regressores.  Se a segunda 
	lista tiver sido dada, separada por um ponto-e-vírgula, ela 
	será interpretada como sendo o conjunto de regressores para a
	segunda equação, e as <repl>variáveis-independentes1</repl> são
	específicas para a primeira equação; caso contrário as 
	<repl>variáveis-independentes1</repl> são consideradas 
	representando o conjunto de regressores comum.
      </para>
      <para>
	Por omissão, os erros padrão são calculados usando uma aproximação
	númerica por convergência da Hessiana.  Mas se tiver sido dada a 
	opção <opt>--opg</opt> a matriz de covariância será baseada no
	produto externo do gradiente (OPG), ou se a opção 
	<opt>--robust</opt> tiver sido dada, o erros padrão QML serão calulados
	usando a <quote>sandwich</quote> entre a inversa da Hessiana e a OPG.
      </para>
      <para>
	Depois duma estimação com sucesso, o acessor <lit>$uhat</lit>
	obtém uma matriz de duas colunas que são os resíduos
	generalizados das duas equações; ou seja, os valores esperados
	das perturbações condicionadas pelos resultados observados e
	covariados.  Por omissão o <lit>$yhat</lit> obtém a matriz
	com quatro colunas, que são as probabilidades estimadas para os
	quatro possíveis resultados conjuntos para (<math>y</math><sub>1</sub>,
	<math>y</math><sub>2</sub>), e pela ordem (1,1), (1,0), (0,1),
	(0,0). Alternativamente, se tiver sido usada a opção
	<lit>--save-xbeta</lit>, o <lit>$yhat</lit> tem duas colunas e 
	contém os valores das funções de índice para as respectivas equações.
      </para>
      <para>
	A saída inclui o teste de razões de verosimilhança para a hipótese
	nula de que as perturbações nas duas equações são não-correlacionadas.
      </para>
    </description>
  </command>

  <command name="bootstrap" section="Tests" label="Opções do Bootstrap"
    context="gui">

    <description>

      <para>Nesta janela de diálogo você pode escolher:</para>

      <ilist>
	<li>
	  <para>
	    A variável ou coeficiente a examinar.  (Com este método, você
	    apenas pode testar um coeficiente de cada vez.)
	  </para>
	</li>
	<li>
	  <para>
	    O tipo de análise a efectuar. O intervalo de confiança 
	    (95 porcento) omissão baseia-se diretamente nos quantis
	    das estimativas dos coeficientes bootstrap.  A versão
	    <quote>studentized</quote> segue o texto de Davidson e
	    MacKinnon <book>Economic Theory and Methods</book> (ETM),
	    Capítulo 5: em cada replicação bootstrap uma razão-<math>t</math>
	    é construída como (a) a diferença entre as estimativas do
	    coeficiente corrente e da linha-base, dividida pela (b) 
	    estimativa do erro padrão da linha-base. Então o intervalo 
	    de confiança é construído segundo os quantis desta 
	    razão-<math>t</math>, tal como explicado no ETM. A opção
	    valor-P baseia-se na distribuição da razão-<math>t</math>
	    do bootstrap:
	    é a proporção de replicações onde o valor absoluto desta 
	    estatística excede o valor absoluto da razão-<math>t</math>
	    da linha-base.
	  </para>
	</li>
	<li>
	  <para>Resíduos reamostrados versus erros normalizados simulados.
	    No primeiro caso os resíduos originais (re-escalados como 
	    sugerido em ETM) são reamostrados com substituição.  No segundo
	    caso são gerados valores normalizados pseudo-aleatórios a partir
	    da variância original dos resíduos.
	  </para>
	</li>
	<li>
	  <para>O número de replicações a realizar.  Note que quando você
	    está a construir um intervalo de confiança a 95 porcento, é
	    desejável que 0.05(<math>B</math> + 1)/2 resulte num valor
	    inteiro (onde <math>B</math> é o número de replicações).
	    Portanto o gretl pode ajustar o número de replicações de modo a
	    garantir isso.
	  </para>
	</li>
	<li>
	  <para>Se se produz ou não um gráfico da distribuição bootstrap.
	    Esta opção usa o mecanismo de estimação de densidade de núcleo
	    do gretl.
	  </para>
	</li>
      </ilist>

    </description>
  </command>

  <command name="boxplot" section="Graphs" label="Gráficos caixa-com-bigodes">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--notches</flag>
	  <effect>mostrar intervalo de 90 porcento para a mediana delimitado por entalhes</effect>
	</option>
	<option>
	  <flag>--factorized</flag>
	  <effect>ver abaixo</effect>
	</option>
	<option>
	  <flag>--panel</flag>
	  <effect>ver abaixo</effect>
	</option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>nome</optparm>
	  <effect>mostra o gráfico das colunas da matriz indicada</effect>
        </option>
	<option>
	  <flag>--output</flag>
	  <optparm>nome-de-ficheiro</optparm>
	  <effect>envia a saída para o ficheiro especificado</effect>
	</option>
      </options>
    </usage>

    <description>

      <para>Estes gráficos apresentam a distribuição de uma variável.  A
	caixa central contém os 50 porcento dos dados centrais, &ie; está
	limitada pelos primeiro e terceiro quartis.  Os <quote>bigodes</quote>
        estendem-se até aos valores mínimo e máximo.  É desenhada uma linha
	que corta a caixa na mediana.  Um símbolo <quote>+</quote> indica a
	média.  Se tiver sido selecionada a opção de mostrar o intervalo de
	confiança para a média, ele é obtido pelo método 'bootstrap' e 
	apresentado na forma de linhas a tracejado acima e abaixo da mediana.
      </para> 

      <para context="gui">
	A opção <opt>factorized</opt> permite examinar a distribuição 
	de uma variável condicionada pelo valor de um fator discreto. Por
	exemplo, se um conjunto de dados contém salários e uma variável
	auxiliar para o sexo, você pode selecionar a variável salário como
	alvo e o sexo como fator, para visualizar lado a lado gráficos
	caixas-com-bigodes dos salários de homens e mulheres.
      </para>

       <para context="cli">
	A opção <opt>factorized</opt> permite examinar a distribuição 
	de uma variável condicionada pelo valor de um fator discreto. Por
	exemplo, se um conjunto de dados contém salários e uma variável
	auxiliar para o sexo, você pode selecionar a variável salário como
	alvo e o sexo como fator para visualizar lado a lado gráficos
	caixas-com-bigodes dos salários de homens e mulheres, como em
      </para>
      <code context="cli">
	boxplot salario sexo --factorized
      </code>
      <para context="cli">
	Note que neste caso você tem que especificar exatamente duas
	variáveis, com a variável fator em segundo lugar.
      </para>

      <para context="cli">
	Se o conjunto de dados atual é de painel, e apenas tiver sido 
	especificada uma variável, a opção <opt>panel</opt> produz uma
	série de gráficos caixas-com-bigodes lado a lado, para cada uma
	das <quote>unidades</quote> do painel ou grupo.
      </para>

      <para context="cli">
	Em modo interativo o resultado é mostrado imediatamente. Em
	modo lote ('batch') o comportamento normal é o de criar um
	ficheiro de comandos gnuplot na diretoria de trabalho do
	utilizador, cujo nome segue a forma <filename>gpttmpN.plt</filename>,
   	iniciando com N = <lit>01</lit>. Esses gráficos poderão ser 
	posteriormente gerados com o programa <program>gnuplot</program> 
	(em MS Windows, <program>wgnuplot</program>).  Este comportamento
	pode ser modificado usando a opção <opt>--output=</opt>
	<repl>nome-de-ficheiro</repl>.  Para mais detalhes, ver o 
	comando <cmdref targ="gnuplot"/>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Ver/Gráfico das variáveis/Caixa com bigodes</menu-path>
    </gui-access>

  </command>

  <command name="break" section="Programming" 
    label="Sair de um ciclo" context="cli">

    <description>

      <para>Sai de um ciclo.  Este comando pode apenas ser usado
        dentro de um ciclo; ele termina a execução de comandos e
        sai de dentro do ciclo (o mais interior).  
        Ver também <cmdref targ="loop"/>.
      </para> 
    </description>
  </command>

  <command name="bwfilter" section="Transformations" context="gui"
    label="Filtro de Butterworth">

    <description>
      <para>
	O filtro de Butterworth é uma aproximação a um filtro ideal
	de onda-quadrada que deixa passar frequências de uma certa
	gama com potência máxima enquanto bloqueia todas as outras.
      </para>
      <para>
	Em princípio, valores altos do parâmetro de ordem, <math>n</math>, produzem
	uma melhor aproximação ao filtro ideal, mas tem uma possível
	penalidade de instabilidade numérica.  O valor de <quote>corte</quote> 
	define a banda de passagem e a banda de bloqueio.  Ele é expresso em graus,
	e tem que ser maior que 0 e menor que 180&deg; (ou &pi;	radianos, 
	correspondendo à maior frequência nos dados).  Valores menores do corte 
	produzem uma tendência mais suave (alisamento).
      </para>
      <para>
	Observar o periodograma da série em estudo é uma análise preliminar que é
	útil quando se pretende aplicar este filtro.  Para mais detalhes ver
	<guideref targ="chap:genr"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Filtro/Butterworth</menu-path>
    </gui-access>

  </command>

  <command name="catch" section="Programming" 
    label="Capturar erros" context="cli">
    <usage>
      <syntax><lit>catch </lit><repl>command</repl></syntax>
    </usage>
    <description>
      <para>
	Isto não é um comando em sentido estrito mas pode ser usado como 
	um prefixo na maior parte dos comandos: o efeito é o de prevenir
	a eventual interrupção da execução de comandos (ou de um ficheiro
	de comandos) quando ocorra um erro num comando.  Se acontecer um 
	erro, este fica registado como sendo um erro interno que pode ser
	acedido com <lit>$error</lit> (um valor de zero indica sucesso).
	O valor de <lit>$error</lit> deve ser sempre verificado 
	imediatamente a seguir ao uso de <lit>catch</lit>, e deve ser 
	tomada a ação adequada se o comando falhou.
      </para> 
      <para>
	A palavra reservada <lit>catch</lit> não pode ser usada antes de <lit>if</lit>,
	<lit>elif</lit> ou <lit>endif</lit>.
      </para>
    </description>
  </command>

  <command name="chow" section="Tests" label="Teste de Chow">

<usage>
      <altforms>
        <altform><lit>chow</lit> <repl>observação</repl></altform>
        <altform><lit>chow</lit> <repl>variável-auxiliar</repl> <lit>--dummy</lit></altform>
      </altforms>
      <options>
	<option>
	  <flag>--dummy</flag>
	  <effect>usar uma variável auxiliar pré-existente</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar estimativas para o modelo aumentado</effect>
	</option>
	<option>
	  <flag>--limit-to</flag>
	  <optparm>list</optparm>
	  <effect>limit test to subset of regressors</effect>
	</option>	
      </options>
      <examples>
        <example>chow 25</example>
        <example>chow 1988:1</example>
	<example>chow female --dummy</example>
      </examples>
    </usage>

    <description>
      <para context="gui">
	Este comando requer um número de uma observação (ou uma data, 
        em dados cronológicos).
      </para>

      <para>
	Tem que se seguir a uma regressão de Mínimos Quadrados (OLS). Se um número de observação ou uma data tiver sido dado, produz um teste sobre a hipótese nula de não haver quebra estrutural no ponto de separação indicado. O procedimento cria uma variável auxiliar que é igual a 1 a partir do ponto
        especificado por <repl>observação</repl> até ao final da amostra,
        caso contrário é 0, e cria também termos de interação entre
        esta variável auxiliar e as variáveis regressoras originais. Se tiver sido dada uma variável auxiliar, será testada a hipótese nula de homogeneidade estrutural no que diz respeito a essa variável auxiliar. Mais uma vez, os termos de interação são acrescentados. Em quaisquer dos casos, é executada uma regressão aumentada que inclui estes termos e
        é calculada a estatística <math>F</math>, considerando 
        a regressão aumentada como não restringida e a original como
        restringida.  Mas se o modelo original usou um estimador robusto para a matriz de covariância, a estatística de teste é um valor de qui-quadrado de Wald baseada num estimador robusto da matriz de covariância da regressão aumentada.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Teste de Chow</menu-path>
    </gui-access>
  </command>

  <command name="clear" section="Programming" context="cli">
    <usage>
      <options>
	<option>
	  <flag>--dataset</flag>
	  <effect>apagar apenas o conjunto de dados</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Sem opções, apaga da memória todos os objetos gravados, incluindo o conjunto
	de dados corrente.  Note que ao abrir um novo conjunto de dados, ou ao usar
	o comando <cmd>nulldata</cmd> para criar um conjunto de dados vazio,
	obterá o mesmo efeito, por isso o uso de <cmd>clear</cmd> normalmente não
	é necessário.
      </para>
      <para>
	Se tiver sido dada a opção <lit>--dataset</lit>, então apenas o conjunto
	de dados é apagado; os outros objetos gravados como matrizes e escalares
	serão preservados.
      </para>
    </description>
  </command>

  <command name="cluster" section="Estimation" 
	   label="Estimação de variância robusta" context="gui">
    <description>
      <para>
	Se você selecionar a segunda opção então tem que fornecer o
	nome de uma variável agrupadora. Esta variável deve ter pelo
        menos dois valores distintos mas geralmente deve ter 
        substancialmente menos valores distintos do que há observações
	no intervalo da amostra.
      </para>
      <para>
	O estimador de variância <quote>agrupação-robusta</quote> 
        divide a amostra num número de subconjuntos ou agrupamentos
        de acordo com o valor tomado na variável selecionada. Em vez da
        assunção clássica de que o erro é independente e identicamente
        distribuído, este estimador permite que a variância do erro
        seja diferente por agrupamento e também permite algum grau de
        dependência do erro dentro de cada agrupamento.
      </para>
    </description>
  </command>

  <command name="coeffsum" section="Tests" label="Soma de coeficientes">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <examples>
        <example>coeffsum xt xt_1 xr_2</example>
	<demos>
	  <demo>restrict.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para context="gui">Este comando requer uma lista de variáveis,
        escolhidas a partir do conjunto de variáveis independentes de um
        dado modelo.</para>
      <para context="gui">
	Calcula a soma dos coeficientes nas variáveis indicadas na lista.
        Apresenta esta soma juntamente com o seu erro padrão e o p-value
        para a hipótese nula de que a soma é zero.
      </para>
      <para context="cli">
	Tem que se seguir a uma regressão.  Calcula a soma dos 
        coeficientes nas variáveis indicadas na <repl>lista-de-variáveis</repl>.
        Apresenta esta soma juntamente com o seu erro padrão e o p-value
        para a hipótese nula de que a soma é zero.
      </para>
      <para>Note-se a diferença entre este teste e <cmdref
	  targ="omit"/>, que testa a hipótese nula de que os coeficientes
        num conjunto especificado de variáveis independentes são
        <emphasis>todos</emphasis> iguais a zero.</para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Soma de coeficientes</menu-path>
    </gui-access>

  </command>

  <command name="coint" section="Tests" 
    label="Teste de cointegração Engle-Granger">

    <usage>
      <arguments>
        <argument>ordem</argument>
        <argument>variável-dependente</argument>
	<argument>variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--nc</flag>
	  <effect>não incluir uma constante</effect>
	</option>
	<option>
	  <flag>--ct</flag>
	  <effect>incluir constante e tendência</effect>
	</option>
	<option>
	  <flag>--ctt</flag>
	  <effect>incluir constante e tendência quadrática</effect>
	</option>
	<option>
	  <flag>--skip-df</flag>
	  <effect>não efectuar testes DF nas variáveis individuais</effect>
	</option>
	<option>
	  <flag>--test-down</flag>
	  <optparm optional="true">criterion</optparm>
	  <effect>ordem de desfasamento automática</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes adicionais das regressões</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
	</option>
      </options>
      <examples>
	<example>coint 4 y x1 x2</example>
	<example>coint 0 y x1 x2 --ct --skip-df</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	O teste de cointegração Engle&ndash;Granger.  O procedimento 
        por omissão é: (1) efetuar testes de Dickey&ndash;Fuller (DF) 
        segundo a hipótese nula de que cada variável listada tem uma raiz 
        unitária; (2) estima a regressão de cointegração; e (3) executar um 
        teste DF sobre os resíduos da regressão de cointegração. Se for 
        dada a opção <lit>--skip-df</lit>, o passo (1) é omitido.
      </para>
      <para context="cli">
	Se a ordem de desfasamento especificada é positiva, todos os testes
	Dickey&ndash;Fuller usam essa ordem, com esta qualificação: se a 
	opção <opt>--test-down</opt> for dada, o valor indicado é tomado 
	como sendo o máximo e a ordem de desfasamento efetivamente usada em
	cada caso é obtida testando para baixo. Ver o comando 
	<cmdref targ="adf"/> para detalhes sobre este procedimento.
      </para>
      <para context="cli">
	Por omissão, a regressão de cointegração contém uma constante.  Se
        você deseja suprimir a constante, acrescente a opção <lit>--nc</lit>.
	Se você deseja aumentar a lista de termos determinísticos na regressão
        de cointegração com uma tendência linear ou quadrática, use a opção
        <lit>--ct</lit> ou <lit>--ctt</lit>.  Estas opções são mutualmente
        exclusivas.
      </para>

      <para context="gui">
	O teste de cointegração Engle&ndash;Granger.  O procedimento
        por omissão é: (1) efetuar testes de Dickey&ndash;Fuller (DF)
        segundo a hipótese nula de que cada variável listada tem uma
        raiz unitária; (2) estima a regressão de cointegração; e (3)  
        executar um teste DF sobre os resíduos da regressão de 
        cointegração. No entanto, se estiver selecionada a opção
        <quote>ignorar os testes iniciais DF</quote> os testes no 
        passo (1) são omitidos.
      </para>
      <para context="gui">
        Se a ordem de desfasamento <math>k</math>, é maior que zero, então
        são incluídos <math>k</math> desfasamentos no lado direito de cada
        regressão de teste, excepto se estiver selecionada a opção 
        <quote>Testar para baixo a partir do desfasamento de maior ordem</quote>:
        nesse caso ela é encarada como sendo o máximo desfasamento e a 
        efetivamente usada em cada caso é obtida testando para baixo.  
        Ver o comando <cmdref targ="adf"/> para detalhes sobre este 
        procedimento.
      </para>
      <para context="gui">
	Por omissão, as regressões de cointegração incluem uma constante.
        Se você deseja suprimir a constante, ou juntar uma tendência linear
        ou quadrática, selecione a opção apropriada a partir do conjunto de 
        botões de rádio disponíveis na janela do teste de cointegração.
      </para>

      <para>Os <emphasis>P-</emphasis>values para este teste são baseados em
	MacKinnon (1996).  O código relevante é incluído com a
        generosa permissão do autor.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/Testes de Cointegração/Engle-Granger</menu-path>
    </gui-access>

  </command>

  <command name="coint2" section="Tests" label="Teste de cointegração de Johansen">

    <usage>
      <arguments>
        <argument>ordem</argument>
        <argument>listaY</argument>
      <argblock optional="true" separated="true">
	  <argument>listaX</argument>
	</argblock>
	<argblock optional="true" separated="true">
	  <argument>listaRx</argument>
	</argblock>
      </arguments>
      <options>
        <option>
	  <flag>--nc</flag>
	  <effect>sem constante</effect>
        </option>
        <option>
	  <flag>--rc</flag>
	  <effect>constante restringida</effect>
        </option>
	<option>
	  <flag>--uc</flag>
	  <effect>constante não restringida</effect>
        </option>
        <option>
	  <flag>--crt</flag>
	  <effect>constante e tendência restringida</effect>
        </option>
        <option>
	  <flag>--ct</flag>
	  <effect>constante e tendência não restringida</effect>
        </option>
        <option>
	  <flag>--seasonals</flag>
	  <effect>incluir auxiliares sazonais centradas</effect>
        </option>
	<option>
	  <flag>--asy</flag>
	  <effect>registar valores p assimtóticos</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>apenas mostrar os testes</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>não mostrar nada</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das regressões auxiliares</effect>
        </option>
      </options>
      <examples>
        <example>coint2 2 y x</example>
	<example>coint2 4 y x1 x2 --verbose</example>
	<example>coint2 3 y x1 x2 --rc</example>
      </examples>
    </usage>

    <description>
      <para>
	Executa o teste de Johansen para a cointegração entre as
	variáveis em <repl>listaY</repl> para a dada ordem de 
	desfasamento.  Para detalhes sobre este teste ver 
	<guideref targ="chap:vecm"/> ou <cite key="hamilton94">Hamilton (1994)</cite>,
	Capítulo 20. Os valores p são calculados usando a aproximação
	gama de Doornik <cite key="doornik98" p="true">(Doornik, 1998)</cite>.
	 São mostrados dois conjuntos de valores p para o teste traço,
	valores assintóticos imediatos e valores ajustados para o 
	tamanho da amostra.  Por omissão, o acessor <lit>$pvalue</lit>
	obtém a variante ajustada, mas se opção <opt>--asy</opt> for 
	dada, pode ser usado para registar os valores assintóticos.
      </para>

      <para context="gui">
	Efetua o teste de Johansen para a cointegração entre as
	variáveis listadas para a ordem de desfasamento selecionada.
	  Para detalhes sobre este teste ver, por exemplo, o livro 
	de Hamilton, <book>Time Series Analysis</book> (1994), 
	Capítulo 20.  Os valores p são calculados usando a aproximação
	gama de  Doornik.
      </para>

      <para context="cli">
	A inclusão de termos determinísticos no modelo é controlada por
       intermédio das opções.  Por omissão, se não tiver sido indicada nenhuma
       opção, será incluída uma <quote>constante não restringida</quote>, o
       que permite a presença de um interceptor não-nulo nas relações
       cointegrantes assim como uma tendência nos níveis das variáveis
       endógenas.  Na literatura derivada do trabalho de Johansen (ver por
       exemplo o livro dele de 1995) isto é frequentemente referido como sendo
       o <quote>caso 3</quote>.  As primeiras quatro opções apresentadas
       acima, que são mutualmente exclusivas, produzem respectivamente os
       casos 1, 2, 4, e 5.  O significado destes casos e os critérios para
       seleccionar um caso estão explicados no 
       <guideref targ="chap:vecm"/>.
      </para>

      <para context="gui">
        A inclusão de termos determinísticos no modelo é controlada por
        intermédio dos botões das opções.  Por omissão, se não tiver sido
        indicada nenhuma opção, será incluída uma <quote>constante não
        restringida</quote>, o que permite a presença de um interceptor 
        não-nulo nas relações cointegrantes assim como uma tendência nos
        níveis das variáveis endógenas.  Na literatura derivada do trabalho
        de Johansen (ver por exemplo o livro dele de 1995) isto é
        frequentemente referido como sendo o <quote>caso 3</quote>.  Os 
        outros quatro botões de opções produzem respectivamente os casos 1,
        2, 4, e 5.  O significado destes casos e os critérios para 
        selecionar um caso estão explicados no
        <guideref targ="chap:vecm"/>.
      </para>

<para context="cli">
	As listas opcionais <repl>listaX</repl> e <repl>listaRx</repl>
	permite-lhe controlar as variáveis exógenas: estas entram no 
	sistema como não restringidas (<repl>listaX</repl>) ou
	restringidas ao espaço de cointegração (<repl>listaRx</repl>).
	 Estas listas são separadas da  <repl>listaY</repl> e entre 
	elas usando ponto-e-vírgulas.
      </para>

      <para context="gui">
	Você pode controlar as variáveis exógenas adicionando-as à 
	lista na caixa inferior.  Por omissão estas entram no modelo 
	na forma não restringida  (indicada por um <lit>U</lit> junto
	do nome da variável).  Se você pretender que uma certa variável
	seja restringida ao espaço de cointegração, clique com botão 
	direito sobre ela e selecione <quote>Restringida</quote> do 
	menu de contexto.  O símbolo junto à variável mudará para R.
      </para>

      <para context="cli">
	A opção <lit>--seasonals</lit>, que pode ser combinada com qualquer
        outra opção, especifica a inclusão de um conjunto de variáveis
        auxiliares sazonais.  Esta opção apenas está disponível para dados
        trimestrais ou mensais.
      </para>

      <para context="gui">
        Se os dados são trimestrais ou mensais, fica disponível a seleção
        de uma opção que lhe permite incluir um conjunto de variáveis
        auxiliares sazonais centradas.  Em todos os casos, a opção
        (<quote>Mostrar detalhes</quote>) permite apresentar as regressões
        auxiliares que formam o ponto de partida do procedimento da estimação
        de máxima verosimilhança de Johansen.
      </para>

      <para context="notex">
	A seguinte tabela serve como um guia à interpretação dos resultados
        apresentados pelo teste, num caso de 3-variáveis.  <lit>H0</lit>
        significa a hipótese nula, <lit>H1</lit> a hipótese alternativa, e
        <lit>c</lit> o número de relações cointegrantes.</para>
      <mono context="notex">
         Ordem    Teste Traço        Teste Lmax
                  H0     H1          H0     H1 
         ---------------------------------------
          0      c = 0  c = 3       c = 0  c = 1
          1      c = 1  c = 3       c = 1  c = 2
          2      c = 2  c = 3       c = 2  c = 3
         ---------------------------------------
      </mono>
      <para context="tex">
	A seguinte tabela serve como um guia
        à interpretação dos resultados
        apresentados pelo teste, num caso de
        3-variáveis.  $H_0$ significa a hipótese
        nula, $H_1$ hipótese alternativa, e $c$ o 
        número de relações cointegrantes	

	\begin{center}
	\begin{tabular}{cllll}
	&amp; \multicolumn{2}{c}{Teste Traço} &amp;
	   \multicolumn{2}{c}{Teste $\lambda$-max} \\
	Rank &amp;  \multicolumn{1}{c}{$H_0$} &amp; 
	       \multicolumn{1}{c}{$H_1$} &amp; 
	       \multicolumn{1}{c}{$H_0$} &amp; 
	       \multicolumn{1}{c}{$H_1$} \\ [4pt]
 	0 &amp; $c$ = 0 &amp; $c$ = 3 &amp; $c$ = 0 &amp; $c$ = 1 \\
	1 &amp; $c$ = 1 &amp; $c$ = 3 &amp; $c$ = 1 &amp; $c$ = 2 \\
	2 &amp; $c$ = 2 &amp; $c$ = 3 &amp; $c$ = 2 &amp; $c$ = 3 
	\end{tabular}
	\end{center}
      </para>

      <para>
	Ver também o comando <cmdref targ="vecm"/>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/Testes de Cointegração/Johansen</menu-path>
    </gui-access>

  </command>

  <command name="compact" section="Dataset" context="gui"
    label="Compactar dados">

    <description>

      <para>Quando você acrescenta a um conjunto de dados uma série temporal
        cuja frequência é superior, é necessário <quote>compactar</quote> a
        nova série.  Por exemplo, uma série mensal terá que ser compactada
        para caber num conjunto de dados trimestral.</para>  

      <para>Adicionalmente, por vezes você pode querer compactar um conjunto
        de dados para uma frequência mais baixa (eventualmente, antes de
        acrescentar uma variável de baixa-frequência ao conjunto de dados).</para>

      <para>Gretl oferece quatro opções de compactação:</para>

      <ilist>
	<li><para>Média: O valor escrito no conjunto de dados 
            será a média aritmética dos valores relevantes da série
            temporal.  Por exemplo, o valor escrito para o primeiro
            trimestre de 1990 será a média dos valores de janeiro, 
            fevereiro e março de 1990.</para>
	</li>

	<li><para>Soma: O valor escrito no conjunto de dados 
            será a soma dos valores de alta-frequência relevantes.
            Por exemplo, o valor escrito para o primeiro
            trimestre será a soma dos valores de janeiro, 
            fevereiro e março.</para>
	</li>

	<li><para>Valores fim-do-período: O valor escrito no conjunto
            de dados será o último valor relevante dos dados de 
            alta-frequência.  Por exemplo, o valor escrito para o
            primeiro trimestre de 1990 será o valor de março de 1990.</para>
	</li>

	<li><para>Valores início-do-período: O valor escrito no conjunto
            de dados será o primeiro valor relevante dos dados de 
            alta-frequência.  Por exemplo, o valor escrito para o
            primeiro trimestre de 1990 será o valor de janeiro de 1990.</para>
	</li>
      </ilist>

      <para>Caso esteja a compactar a totalidade de um conjunto de
        dados, a escolha que você fizer neste diálogo, define o método
        por omissão.  Mas se definiu um método de compactação 
        individualmente para uma variável (no menu, 
        <quote>Variável/Editar características</quote>), esse método
        será usado e não o por omissão.  Se o método de compactação
        já estiver definido para todas as variáveis, não será 
        apresentada a escolha do método.</para>

    </description>
   <gui-access>
      <menu-path>/Dados/Compactar Dados</menu-path>
    </gui-access>
  </command>

  <command name="controlled" section="Graphs" context="gui"
    label="Gráfico X-Y com controlo">

    <description>
      <para>
	Este comando requer a seleção de três variáveis, uma para o eixo 
        dos X, uma para o eixo dos Y, e outra para a que você deseja controlar
        (chamemos-lhe Z). O gráfico apresenta Y ajustado contra X ajustado, 
        onde a versão ajustada da variável são os resíduos de uma regressão
        por Mínimos Quadrados (OLS) sobre Z.  
      </para>
      <para>
	Exemplo: Você tem uma amostra com dados de salários, experiência e 
	nível de educação de um conjunto de pessoas.  Você pretende ver 
        um gráfico dos salários contra o nível de educação controlando pela
        experiência.  Nesse caso você seleciona salários para o eixo dos Y,
        nível de educação para o eixo dos X, e experiência como controlo.
	O gráfico apresenta os salários contra a educação, com ambas as 
        variáveis <quote>purgadas</quote> do efeito experiência.
      </para>
    </description>

  </command>

  <command name="corr" section="Statistics" 
    label="Coeficientes de correlação" context="cli">

    <usage>
      <arguments>
        <argument optional="true">lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--uniform</flag>
	  <effect>garante amostra uniforme</effect>
	</option>
	<option>
	  <flag>--spearman</flag>
	  <effect>Ró de Spearman</effect>
	</option>
	<option>
	  <flag>--kendall</flag>
	  <effect>Tau de Kendall</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostra classificações ('rankings')</effect>
	</option>
      </options>
      <examples>
        <example>corr y x1 x2 x3</example>
	<example>corr ylist --uniform</example>
	<example>corr x y --spearman</example>
      </examples>
    </usage>

    <description context="gui">
      <para>
	Apresenta os coeficientes de correlação emparelhados 
        (correlação momento-produto de Pearson) para as variáveis indicadas.
        O comportamento normal é usar todas as observações para calcular 
        cada coeficiente emparelhado, mas se a caixa de seleção estiver
        selecionada a amostra será limitada (caso seja necessário) de modo
        que o mesmo conjunto de observações seja usado em todos os 
        coeficientes.  Esta opção apenas tem efeito se existirem valores
        omissos em quantidades diferentes para as variáveis usadas.
      </para>
    </description>

    <description context="cli">
      <para>
	Por omissão, apresenta os coeficientes de correlação emparelhados 
        (correlação momento-produto de Pearson) para as variáveis 
        <repl>lista-de-variáveis</repl>, ou para todas as variáveis no 
        conjunto de dados se não tiver sido indicada <repl>lista-de-variáveis</repl>.
          O comportamento normal é o de usar todas as observações disponíveis para calcular 
        cada coeficiente emparelhado, mas se tiver sido dada a opção <flag>--uniform</flag> 
        a amostra será limitada (caso seja necessário) de modo
        que o mesmo conjunto de observações seja usado em todos os 
        coeficientes.  Esta opção apenas tem efeito se existirem valores
        omissos em quantidades diferentes para as variáveis usadas.
      </para>
      <para>
	As opções (mutualmente exclusivas) <opt>--spearman</opt> e 
	<opt>--kendall</opt> produzem, respetivamente, o Ró de correlação
        de Spearman e o Tau de correlação de ordem de Kendall em vez do 
        usual coeficiente de Pearson.  Quando uma destas opções é dada,
        a <repl>lista-de-variáveis</repl> deve apenas conter duas variáveis.
      </para>
      <para>
	Ao calcular uma correlação de ordem, pode ser dada a opção 
        <opt>verbose</opt> para mostrar os dados originais e ordenados
        (caso contrário esta opção será ignorada).
      </para>
    </description>

    <gui-access>
      <menu-path>/Ver/Matriz de correlação</menu-path>
      <other-access>Menu de contexto da janela principal (selecção múltipla)</other-access>
    </gui-access>

  </command>

  <command name="corrgm" section="Statistics" label="Correlograma">

    <usage>
      <arguments>
        <argument>série</argument>
        <argument optional="true">ordem</argument>
      </arguments>
      <options>
       <option>
	  <flag>--bartlett</flag>
	  <effect>use Bartlett standard errors</effect>
        </option>	
       <option>
	  <flag>--plot</flag>
	  <optparm>modo ou nome-de-ficheiro</optparm>
	  <effect>ver abaixo</effect>
        </option>
      </options>
      <examples>
        <example>corrgm x 12</example>
      </examples>
    </usage>

    <description>
      <para context="notex">
	Apresenta os valores da função de autocorrelação para a
	<repl>série</repl>, que pode ser especificada por nome ou por
        número.
	Os valores são definidos como &rgr;(<math>u</math><sub>t</sub>,
	<math>u</math><sub>t-s</sub>) onde <math>u</math><sub>t</sub>
	é a <math>t</math>&ndash;ésima observação da variável <math>u</math> e <math>s</math>
        é o número de desfasamentos.
      </para>
      <para context="tex">
	Apresenta os valores da função de autocorrelação para a
	<repl>série</repl>, que pode ser especificada por nome ou por
        número.  Os valores são definidos como $\hat{\rho}(u_t, u_{t-s})$, onde
	$u_t$ é a $t$&ndash;ésima observação da variável $u$ e
	$s$ é o número de desfasamentos.
      </para>
      <para>
	Também são apresentadas as autocorrelações parciais (obtidas segundo
        o algoritmo de Durbin&ndash;Levinson): estas constituem a rede dos
        efeitos dos desfasamentos intervenientes.  Adicionalmente, é 
        apresentada a estatística de teste <math>Q</math> de 
        Ljung&ndash;Box. Esta pode ser usada para testar a hipótese nula de que
        a série é <quote>ruído branco</quote>: terá uma distribuição
        qui-quadrado assimptótico com os graus de liberdade iguais ao número de
        desfasamentos usados.
      </para>
      <para>
	Se o valor <repl>ordem</repl> for especificado o comprimento do
        correlograma fica limitado a esse máximo número de desfasamentos, senão o 
        comprimento é determinado automaticamente, como uma função da
        frequência dos dados e do número de observações.
      </para>
      <para>
	Por omissão é apresentado um gráfico do correlograma:
        um gráfico gnuplot em modo interativo ou um gráfico ASCII em
        modo de lote de comandos. Isto pode ser ajustado por via da opção
        <opt>plot</opt>. Os parâmetros válidos para esta opção são <lit>none</lit>
	(para suprimir o gráfico); <lit>ascii</lit> (para produzir um gráfico
        de texto mesmo em modo interativo); <lit>display</lit> (para produzir
	um gráfico gnuplot mesmo em modo de lote de comandos); ou o nome de um
        ficheiro. O efeito de se fornecer um nome de ficheiro é como descrito para
	a opção <opt>output</opt> do comando <cmdref targ="gnuplot"/>.
      </para>
      <para>
	Depois de completar com sucesso, os acessores <lit>$test</lit> e
	<lit>$pvalue</lit> contêm os respectivos valores do teste de
	Ljung&ndash;Box para a maior ordem apresentada. Note que se você
	apenas quiser determinar a estatística <math>Q</math>, provavelmente
	você quererá usar a função <fncref targ="ljungbox"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Correlograma</menu-path>
      <other-access>Menu de contexto da janela principal (selecção singular)</other-access>
    </gui-access>

  </command>

  <command name="count-model" section="Estimation" context="gui"
    label="Modelos para a contagem de dados">

    <description>
      <para>
	A variável dependente é tida como representando a contagem de 
	ocorrência de eventos de alguma natureza, e tem que ter apenas
	valores inteiros não-negativos. Por omissão, usa-se a 
	distribuição de Poisson, mas o seletor de menu permite escolher
	a distribuição Binomial Negativa. (A variante 'NegBin 2' é 
	normalmente usada em econometria, mas também está disponível a 
	menos usada 'NegBin 1').
      </para>
      <para>
	Opcionalmente, você pode acrescentar na especificação uma variável
	<quote>offset</quote>.  Esta é uma variável de escala, cujo logarítmo
	é acrescentado à função de regressão linear (implicitamente com um 
	coeficiente de 1.0).  Isto faz sentido quando você espera que o número
	de ocorrências do evento em questão seja proporcional a algum factor 
	conhecido (TODO: other things equal).  Por exemplo, o número de acidentes
	de trânsito pode ser assumido como proporcional ao volume de trânsito
	(TODO: other things equal), e nesse caso o volume de trânsito pode ser
	especificado como um <quote>offset</quote> num modelo de taxa de acidentes.
	A variável 'offset' tem que ser estritamente positiva.
      </para>
      <para>
	Por omissão os erros padrão são calculados usando uma aproximação numérica
	da Hessiana por convergência.  Mas se estiver seleccionada a caixa dos
	<quote>Erros padrão robustos</quote> então serão calculados os erros padrão
	QML, usando uma <quote>sandwich</quote> da inversa da Hessiana e do produto
	externo do gradiente.
      </para>
    </description>
  </command>

  <command name="curve" section="Graphs" label="Desenha uma curva"
    context="gui">

    <description>
      <para>
	Esta caixa de diálogo permite-lhe criar um gráfico gnuplot
        a partir de uma expressão. Esta tem que ser uma expressão 
        válida para o gnuplot.  Use <lit>x</lit> como o identificador
	dos valores da variável no eixo dos X. Tenha em atenção que o
	gnuplot usa <lit>**</lit> para potenciação, e que o caratere
	dos decimais tem que ser o <quote>.</quote>.  Exemplos:
      </para>
      <code>
	10+0.35*x
	100+5.3*x-0.12*x**2
	sin(x)
	exp(sqrt(pi*x))
      </code>
      <para>
	Para acrescentar uma linha adicional a um gráfico criado por 
        este processo, clicar no gráfico e selecionar <quote>Editar</quote>, 
        selecionar o separador <quote>Linhas</quote> na janela de edição
        do gráfico, e use o botão <quote>Acrescentar linha...</quote>.
      </para>
    </description>
  </command>

  <command name="cusum" section="Tests" label="Teste CUSUM">

    <usage>
      <options>
	<option>
	  <flag>--squares</flag>
	  <effect>executa o teste CUSUMSQ</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>apenas mostra o teste Harvey&ndash;Collier</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Tem que se seguir à estimação de um modelo por via de OLS.  Executa 
        o teste CUSUM &mdash;ou se for dada a opção <lit>--squares</lit>,
        o teste CUSUMSQ &mdash;para a estabilidade dos parâmetros.  É obtida
        uma série temporal de erros de predição um passo-à-frente, pela 
        execução de séries de regressões: a primeira regressão usa as 
        primeiras <math>k</math> observações e é usada para gerar a
        predição da variável dependente na observação
        <math>k</math> + 1; a segunda usa a primeira predição para a
        observação <math>k</math> + 2, e por aí a diante 
	(onde <math>k</math> é o número de parâmetros no modelo original).
      </para>
      <para>
	A soma acumulada dos erros de predição escalados, ou os quadrados desses
        erros, é mostrada e apresentada em gráfico.  A hipótese nula para a
        estabilidade dos parâmetros é rejeitada ao nível de cinco porcento, 
        se a soma acumulada se desviar do intervalo de confiança de 95 porcento.
      </para>
      <para>
	No caso do teste  CUSUM, é também apresentada a estatística de teste
        <math>t</math> de Harvey&ndash;Collier, para a hipótese nula
        da estabilidade dos parâmetros.  Ver o livro 
        <book>Econometric Analysis</book> de Greene para mais detalhes.  Para
        o teste CUSUMSQ, o intervalo de confiança a 95 porcento é calculado de
        acordo com o algoritmo apresentado por 
	<cite key="edgerton94">Edgerton e Wells (1994)</cite>.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Teste CUSUM(SQ)</menu-path>
    </gui-access>

  </command>

  <command name="daily-purge" section="Dataset" 
	   label="Purge daily data" context="gui">
    <description>
      <para>
	If a daily dataset is nominally on a 7-day calendar but
	in fact only includes business-day data, it is recommended
	that you delete the blank weekend rows, thereby switching
	to a 5-day calendar.
      </para>
      <para>
	If a business-daily dataset contains a relatively small number
	of rows with no data entries (presumably due to trading
	holidays) you may wish to delete these rows. In effect, this
	means treating the missing values for holidays as non-existent
	rather than truly <quote>missing</quote>, and treating the
	trading days as forming a continuous time-series.
      </para>
      <para>
	Note that if you take either of these options gretl will
	nonetheless preserve the date information, and it will be
	possible to reconstruct the full calendar dataset later
	if that's required.
      </para>
    </description>
  </command>

  <command name="data" section="Dataset" 
    label="Importar de uma base de dados" context="cli">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--compact</flag>
	  <optparm>method</optparm>
	  <effect>specify compaction method</effect>
	</option>
	<option>
	  <flag>--interpolate</flag>
	  <effect>do interpolation for low-frequency data</effect>
	</option>	
	<option>
	  <flag>--quiet</flag>
	  <effect>não reportar resultados exceto quando hajam erros</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Lê as variáveis indicadas na <repl>lista-de-variáveis</repl> a
	partir de uma base de dados (gretl, RATS 4.0 ou PcGive), que
	deve ter sido préviamente aberta usando o comando 
	<cmdref targ="open"/>.  A frequência dos dados e o intervalo da
	amostra podem ser definidos pelos comandos <cmdref targ="setobs"/> 
	e <cmdref targ="smpl"/> antes de usar este comando. 
	Apresenta-se um exemplo completo:</para>
      <code>
	open macrodat.rat
	setobs 4 1959:1
	smpl ; 1999:4
	data GDP_JP GDP_UK
      </code>
      <para>
	Os comandos acima abrem a base de dados com o nome 
	<filename>macrodat.rat</filename>, definem um conjunto de dados
	trimestral iniciando no primeiro trimestre de 1959 e terminando
	no quarto trimestre de 1999, e depois importam as séries temporais
	com os nomes <lit>GDP_JP</lit> e <lit>GDP_UK</lit>.</para>
      <para>
	Se os comandos <lit>setobs</lit> e <lit>smpl</lit> não tiverem sido
	especificados deste modo, a frequência dos dados e o intervalo da 
	amostra serão definidos usando a primeira variável lida da base de
	dados.
      </para>
      <para>
	Se as séries temporais a serem lidas forem de frequência 
	superior à do conjunto de dados em uso, você pode especificar
	um método de compactação tal como abaixo:</para>
      <code>
	data (compact=average) LHUR PUNEW
      </code> 
      <para>
	Os quatro métodos de compactação disponíveis são: 
	Média; <quote>average</quote> (usa a média das observações
	de maior frequência),
	Último; <quote>last</quote> (usa a última observação),
	Primeiro; <quote>first</quote> e 
	Soma; <quote>sum</quote>.  Se não tiver sido indicado
	nenhum métod, será usado a Média.
      </para>
      <para>
	If the series to be read are of lower frequency than the
	working dataset, the default is to repeat the values of the
	added data as required, but the <opt>interpolate</opt> option
	can be used to request interpolation using the method of <cite
	key="chowlin71">Chow and Lin (1971)</cite>: the regressors are
	a constant and quadratic trend and an AR(1) disturbance
	process is assumed. Note, however, that this option is
	available only for conversion from quarterly data to monthly
	or annual data to quarterly.
      </para>
      <para>
	In the case of native gretl databases (only), the
	<quote>glob</quote> characters <lit>*</lit> and <lit>?</lit>
	can be used in <repl>varlist</repl> to import series that
	match the given pattern. For example, the following will
	import all series in the database whose names begin with
	<lit>cpi</lit>:
      </para>
      <code>
	data cpi*
      </code>

    </description>

    <gui-access>
      <menu-path>/Ficheiro/Bases de Dados</menu-path>
    </gui-access>

  </command>

  <command name="data-files" section="Programming" 
	   label="Data files" context="gui">
    <description>
      <para>
	This dialog enables you to specify additional files to be
	included with a function package. Including such material
	implies that the package takes the form of a zip file.  If
	gretl is to build the zip file for you, all files referenced
	here must be present in the same directory as the gfn file.
	Sub-directories can be listed as well as regular files; in
	that case it is implied that all of their contents should be
	included in the zip package.
      </para>
      <para>
	There are two main intended uses for this facility. First, you
	can include a data file for use with the package's sample
	script, if none of the data files supplied with the gretl
	distribution are suitable. In this case the data should be in
	gretl native format (<lit>gdt</lit> or binary
	<lit>gdtb</lit>). Second, if your package requires a big
	matrix (for example, holding critical values for a specialized
	test statistic) it may be more convenient to include this as a
	gretl matrix file (<lit>mat</lit>) than to assemble the matrix
	via multiple hansl statements.
      </para>
      <para>
	To access a packaged <lit>gdt</lit> or <lit>gdtb</lit> file
	from a sample script, use the <opt>frompkg</opt> option with
	the <lit>open</lit> command, supplying the name of the
	package as a parameter, as in
      </para>
      <code>
	open almon.gdt --frompkg=almonreg
      </code>
      <para>
	To read a packaged matrix file from within your package code,
	use the built-in string variable <lit>$pkgdir</lit>, as in
      </para>
      <code>
	string mname = sprintf("%s/A.mat", $pkgdir)
	matrix A = mread(mname)
      </code>
      <para>
	(Note that <quote><lit>/</lit></quote> will work OK as
	path separator on MS Windows.)
      </para>
    </description>
  </command>  

  <command name="dataset" section="Dataset" 
    label="Manipular o conjunto de dados" context="cli">

    <usage>
      <arguments>
        <argument>palavra-chave</argument>
	<argument>parâmetros</argument>
      </arguments>
      <examples>
        <example>dataset addobs 24</example>
	<example>dataset insobs 10</example>
        <example>dataset compact 1</example>
        <example>dataset compact 4 last</example>
        <example>dataset expand interp</example>
        <example>dataset transpose</example>
	<example>dataset sortby x1</example>
	<example>dataset resample 500</example>
	<example>dataset renumber x 4</example>
	<example>dataset clear</example>
      </examples>
    </usage>

    <description>
      <para>
	Efectua diversas operações sobre o conjunto de dados como
	um todo, dependendo da <repl>palavra-chave</repl>, que tem 
	que ser <lit>addobs</lit>, <lit>insobs</lit>, <lit>clear</lit>,
	<lit>compact</lit>, <lit>expand</lit>, <lit>transpose</lit>,
	<lit>sortby</lit>, <lit>dsortby</lit>, <lit>resample</lit> ou
	<lit>renumber</lit>. Nota: exceptuando <lit>clear</lit>, estas
	ações não estão disponíveis enquanto o conjunto de dados estiver
	subamostrado por seleção de casos com algum critério Booleano.
      </para>
      <para>
	<lit>addobs</lit>: Tem que ser seguida por um inteiro positivo.
	Acrescenta o número indicado de observações adicionais no
	final do conjunto de dados em uso.  Essencialmente, isto é 
	pretendido para efeitos de predição.  Os valores na maior parte
	das variáveis no intervalo acrescentado, serão marcados como 
	omissos, mas certas variáveis determinísticas são reconhecidas
	e extendidas, nomeadamente, uma tendência linear simples e 
	variáveis periódicas auxiliares.
      </para>
      <para>
	<lit>insobs</lit>: Tem que ser seguida por um inteiro positivo
	que não seja maior que o número de observações atual. Inserte
	uma única observação na posição indicada. Todos os dados
	subsequentes são deslocados uma posição e o conjunto de dados
	fica extendido com mais uma observação. Todas as variáveis exceto
	a constante recebem um valor omisso na nova observação. Esta ação
	não está disponível para conjuntos de dados de painel.
      </para>
      <para>
	<lit>clear</lit>: Não necessita parâmetros.  Limpa o conjunto
	de dados corrente, ficando gretl no seu estado 
	<quote>vazio</quote> inicial.
      </para>      
      <para>
	<lit>compact</lit>: Tem que ser seguida por um inteiro positivo
	representando uma nova frequência, que deve ser inferior à
	frequência atual (por exemplo, um valor 4 quando a frequência
	corrente é 12, indica a compactação de mensal para trimestral).
	Este comando apenas está disponível para séries temporais; ele
	compacta todas as séries temporais do conjunto de dados para a
	nova frequência.  Pode ser dado um segundo parâmetro, nomeadamente
	um de <lit>sum</lit>, <lit>first</lit> ou <lit>last</lit>, para
	especificar, respectivamente, compactação usando a soma dos valores
	de maior frequência, valores de ínicio e de fim de período. Por 
	omissão é feita compactação por média.
      </para>
      <para>
	<lit>expand</lit>: Este comando está apenas disponível
	para séries temporais anuais ou trimestrais: dados anuais
	podem ser expandidos para trimestrais, e dados trimestrais
	para frequência mensal.  Por omissão todas as séries temporais
	no conjunto de dados são preenchidas com repetição de valores
	existentes até atingirem a nova frequência, mas se tiver sido 
	acrescentado o modificador <lit>interp</lit> então as séries
	temporais serão expandidas usando a interpolação de Chow-Lin: 
	os regressores são a constante e a tendência quadrada e é
	assumido um processo de perturbação AR(1).
      </para>
      <para>
	<lit>transpose</lit>: Não necessita parâmetros.
	Transpõe o conjunto de dados actual.  Isto é, cada observação
	(linha) será tratada como uma variável (coluna), e cada variável
	como uma observação.  Este comando pode ser útil se quando os dados
	tenham sido lidos a partir de uma origem externa em que as linhas da
	tabela de dados representam variáveis.
      </para>
      <para>
	<lit>sortby</lit>: É necessário o nome de uma lista ou de uma única 
	série de dados. Se tiver sido dada uma série de dados, as observações
	em todas as variáveis do conjunto de dados são re-ordenadas por ordem
	crescente da série especificada.  Se tiver sido dada uma lista, a 
	ordenação é hierárquica: se as observações estiverem empatadas no que 
	diz respeito à primeira variável chave então é usada a segunda chave 
	para desempatar, e assim sucessivamente até que não haja empates ou 
	se tenham esgotado as chaves. Note que este comando apenas está
	disponível para dados sem data.
      </para>
      <para>
	<lit>dsortby</lit>: Funciona como <lit>sortby</lit> exceto que
	a re-ordenação é por ordem decrescente das séries chave.
      </para>
      <para>
	<lit>resample</lit>: Constrói um novo conjunto de dados
	por amostragem aleatória, com substituição das linhas
	do conjunto de dados corrente. Requer um argumento, 
	designadamente o número de linhas a incluir. Este pode ser
	menor, igual ou maior que o número de observações nos dados
	originais.  O conjunto de dados original pode ser obtido 
	usando o comando <lit>smpl full</lit>.  
      </para>
      <para>
	<lit>renumber</lit>: Requer o nome de uma de uma série seguido
	por um inteiro entre 1 e o número de séries no conjunto de dados
	menos 1. Move a série especificada para a posição indicada dentro
	do conjunto de dados, renumerando adequadamente as restantes séries.
	(A posição 0 está ocupada pela constante, que não pode ser movida.)
      </para>
    </description>

    <gui-access>
      <menu-path>/Dados</menu-path>
    </gui-access>

  </command>

  <command name="datasort" section="Dataset" context="gui"
    label="Ordenar dados">

    <description>
      <para>
	A variável selecionada é usada como a chave de ordenação
	para todo o conjunto de dados.  As observações em todas
	as variáveis são re-ordenadas por ordem crescente dos
	valores da variável chave, ou por valores decrescentes
	caso tenha selecionado a opção <quote>Descrescente</quote>.
      </para>
    </description>
  </command>

  <command name="debug" section="Programming" context="cli"
    label="Despiste de erros de programa">

    <usage>
      <arguments>
        <argument>função</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Comando experimental para despiste de erros de programa ('debugger')
	em funções definidas pelo utilizador, disponível em programas de 
	linha-de-comandos, gretlcli, e na consola do ambiente gráfico (GUI).
	O comando <lit>debug</lit> deve ser utilizado antes da chamada da 
	função, mas depois da definição desta. O efeito é de que a execução
	é suspensa quando a função é chamada e é activada uma linha de 
	interação especial.
      </para>
      <para>
	Na linha de interação, você introduz <lit>next</lit> para executar
	o comando seguinte dentro da função, ou <lit>continue</lit> para 
	permitir continuar a execução da função sem impedimentos. Estes
	comandos podem ser abreviados com <lit>n</lit> e <lit>c</lit> 
	respetivamente.  Você também pode interpôr uma instrução nesta
	linha de interação, por exemplo um comando <lit>print</lit> para
	mostrar o valor atual de alguma variável de interesse.
      </para>
    </description>
  </command>  

  <command name="delete" section="Dataset" 
    label="Apagar variáveis" context="cli">

    <usage>
      <altforms>
        <altform><lit>delete</lit> <repl>lista-de-variáveis</repl></altform>
        <altform><lit>delete</lit> <repl>nome-de-variável</repl></altform>
	<altform><lit>delete --type=</lit><repl>nome-de-tipo</repl></altform>
      </altforms>
      <options>
	<option>
	  <flag>--db</flag>
	  <effect>apaga séries em base de dados</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Este comando é um destrutor geral para variáveis com nome
	(quer sejam séries, escalares, matrizes, texto, ou 'bundles').
	Tem que ser usada com cuidado; não será peguntada nenhuma
	confirmação.
      </para>
      <para>
	No caso de uma série <repl>nome-de-variável</repl> pode tomar a 
	forma de uma lista com nome, o que faz com que todas as séries
	nessa lista sejam apagadas, ou pode tomar a forma de uma lista
	explícita de séries por nome ou número ID.  Note que quando
	você apaga séries, quaisquer séries com números ID superiores
	àquelas que estão na lista a apagar serão renumeradas.
      </para>
      <para>
	Se for dada a opção <opt>--db</opt>, o comando apaga as séries
	listadas não do conjunto de dados atual mas de uma base de dados
	gretl, assumindo que a base de dados está aberta e que o utilizador
	tem permissão de escrita no ficheiro em questão.  Ver também o 
	comando	<cmdref targ="open"/>.
      </para>
      <para>
	Se tiver sido dada a opção <opt>--type</opt> ela terá de ser
        acompanhada por um dos seguintes nomes-de-tipo:
	<lit>matrix</lit> (matriz), <lit>bundle</lit> ('bundle'), <lit>string</lit> (texto),
	<lit>list</lit> (lista), ou <lit>scalar</lit> (escalar). O efeito é o de apagar todas
	as variáveis do tipo dado. Neste caso (somente neste), não deve ser indicado o argumento
	<repl>nome-de-variável</repl>.
      </para>
    </description>

    <gui-access>
      <menu-path>Menu de contexto da janela principal (selecção singular)</menu-path>
    </gui-access>

  </command>

  <command name="density" section="Statistics" context="gui"
    label="Estimação de densidade de núcleo">

    <description>

      <para>O procedimento de estimação de densidade de núcleo 
	cria um conjunto de pontos de referência igualmente
	espaçados, ao longo de um intervalo adequado em relação
	ao intervalo dos dados, e atribui uma densidade a cada
	ponto de referência baseado nas observações reais na
	vizinhança.
      </para>
      <para>A função utilizada para determinar a densidade estimada
	em cada ponto de referência, <math>x</math>, é
      <equation status="display"
	tex="\[f(x)=(1/nh) \sum_{t-1}^{n} k\left((x-x_t)/h\right)\]"
	ascii="f(x) = (1/nh) sum(t=1 to n) k((x - x(t)) / h)"
	graphic="kernel1"/>
	onde <math>n</math> é o número de pontos de dados, 
	<math>h</math> é o parâmetro de <quote>largura de banda</quote>,
	e <math>k</math>() é a função núcleo.
	Quanto maior for o valor da largura de banda, mais alisada será
	a densidade estimada.
      </para>
      <para>
	Você pode escolher o núcleo Gaussiano (a densidade normal padrão)
	ou núcleo de Epanechnikov.  Por omissão, a largura de banda é
	a sugerida pela regra de ouro de 
	<cite key="silverman96">Silverman (1986)</cite>, nomeadamente
	<equation status="display"
	  tex="\[h=0.9 {\rm min}(s, {\rm IQR}/1.349) n^{1/5}\]"
	  ascii="h = 0.9 min(s, IQR/1.349) n^{1/5}"
	  graphic="kernel2"/>
	onde <math>s</math> designa o desvio padrão dos dados
	e IQR é a amplitude inter-quartil.  Você pode alargar
	ou encolher a largura de banda por meio do 
	<quote>fator de ajustamento de largura de banda</quote>: 
	a largura de banda final é a obtida pela multiplicação do
	valor de Silverman pelo fator de ajustamento.
      </para>
      <para>
	Para uma boa introdução sobre a estimação de densidade de
	núcleo ver o Capítulo 15 do livro de Davidson e MacKinnon,
	<book>Econometric Theory and Methods</book>.
      </para>

    </description>

  </command>  

  <command name="dfgls" section="Tests" context="gui"
    label="O teste ADF-GLS">

    <description>
      <para>
	O teste ADF-GLS é uma variante do teste de Dickey&ndash;Fuller
	para uma raiz unitária, para o caso onde se assume que a 
	variável a ser testada tem uma média não-nula ou manifesta
	uma tendência linear.  A diferença está no processo de 
	anulação da média ou da têndencia da variável ser feito com
	o procedimento GLS sugerido por Elliott, Rothenberg
	e Stock (1996).  Isto resulta num teste mais potente do que
	o da abordagem padrão de Dickey&ndash;Fuller.
      </para>
      <para>
	Ver também o comando <cmdref targ="adf"/> e a opção <opt>--gls</opt>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Testes de raiz unitária/Teste ADF-GLS</menu-path>
    </gui-access>

  </command>

  <command name="dialog" section="Estimation" context="gui"
    label="Janela de diálogo do modelo">

    <description>
      <para>Para selecionar a variável dependente, escolher uma 
	variável na lista à esquerda e clicar no botão <quote>Escolher</quote>
	que aponta para o espaço da variável dependente.  Se você activar
	a caixa de seleção <quote>Definir por omissão</quote>, a variável
	selecionada ficará pré-selecionada nas próximas vezes que abrir uma
	janela de diálogo do modelo.  Atalho: Fazer clique-duplo numa variável
	à esquerda para a selecionar como variável dependente e também 
	defini-la como por omissão.
      </para>
      <para>Para selecionar as variáveis independentes, escolhê-las na 
	lista à esquerda e clicar no botão <quote>Acrescentar</quote>
	(ou clicar o botão direito do rato).  Você pode escolher um grupo
	não-contínuo de variáveis clicando nelas mantendo a tecla 
	<lit>Ctrl</lit> pressionada.
      </para>
    </description>

  </command>

  <command name="diff" section="Transformations" 
    label="Primeiras diferenças" context="cli">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
    </usage>

    <description>
      <para>
	É obtida a primeira diferença de cada variável na
	<repl>lista-de-variáveis</repl> e o resultado é guardado
	numa nova variável com o prefixo <lit>d_</lit>.  Portanto
	<cmd>diff x y</cmd> cria as duas novas variáveis
      </para>
      <mono>
	d_x = x(t) - x(t-1)
	d_y = y(t) - y(t-1)
      </mono>
    </description>

    <gui-access>
      <menu-path>/Acrescentar/Primeiras diferenças das variáveis selecionadas</menu-path>
    </gui-access>

  </command>

  <command name="difftest" section="Tests" 
    label="Testes não paramétricos para as diferenças" context="cli">

    <usage>
      <arguments>
        <argument>variável1</argument>
	<argument>variável2</argument>
      </arguments>
      <options>
	<option>
	  <flag>--sign</flag>
	  <effect>Teste dos Sinais, por omissão</effect>
	</option>
	<option>
	  <flag>--rank-sum</flag>
	  <effect>Teste ordinal da soma de Wilcoxon</effect>
	</option>
	<option>
	  <flag>--signed-rank</flag>
	  <effect>Teste ordinal dos sinais de Wilcoxon</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Realiza um teste não-paramétrico para a diferença entre duas 
	populações ou grupos, o teste específico depende da opção 
	selecionada.
      </para>
      <para>
	Com a opção <opt>--sign</opt>, é executado o teste dos Sinais.
	Este teste baseia-se no facto de duas amostras <math>x</math> 
	e <math>y</math>, terem sido extraídas aleatóriamente de uma
	mesma distribuição, a probabilidade
	de
	<math>x</math><sub>i</sub> &gt;
	<math>y</math><sub>i</sub>, para cada observação
	<math>i</math>, deve ser igual a 0,5.  A estatística de teste é
	<math>w</math>, o número de observações para as quais 
	<math>x</math><sub>i</sub> &gt;
	<math>y</math><sub>i</sub>. Sob a hipótese nula de que segue uma
	distribuição Binomial com os parâmetros
	(<math>n</math>, 0,5), onde <math>n</math> é o número de observações.
      </para>
      <para>
	Com a opção <opt>--rank-sum</opt>, é executado o teste ordinal da soma
	de Wilcoxon.  Este teste consiste em ordenar as observações de ambas as
	amostras conjuntamente, da menor para a maior, e depois determinar a
	soma das ordens de uma das amostras.  As duas amostras não necessitam ser
	do mesmo tamanho, e se isso acontece então usa-se a de menor dimensão
	no cálculo da soma das ordens.  Sob a hipótese nula de que as amostras
	terem sido extraídas de populações com a mesma mediana, a distribuição de
	probabilidade da soma das ordens pode ser determinada para quaisquer
	tamanhos das amostras; e para amostras consideravelmente grandes existe
	uma forte aproximação a uma distribuição Normal.
      </para>
      <para>
	Com a opção <opt>--signed-rank</opt>, é executado o teste ordinal
	dos sinais de Wilcoxon.  Este destina-se para pares de dados
	associados assim como, por exemplo, os valores das variáveis de 
	uma amostra de indivíduos antes e depois de algum tratamento.
	O teste começa por encontrar as diferenças entre as observações
	emparelhadas,
	<math>x</math><sub>i</sub> &minus;
	<math>y</math><sub>i</sub>, ordenando estas diferenças por valor
	absoluto, e então atribuindo a cada par um posto com sinal, 
	coincidindo o sinal com o sinal da diferença.  De seguida é
	calculado o <math>W</math><sub>+</sub>, que é a soma dos postos
	com sinal positivo.  Tal como o teste ordinal da soma, esta
	estatística tem uma distribuição bem definida, sob a hipótese
	nula de que a diferença mediana é zero, que converge para a 
	distribuição Normal em amostras de tamanho razoável.
      </para>
      <para>
	Para os testes de Wilcoxon, se a opção <opt>--verbose</opt> tiver sido
	dada então será mostrado as ordens.  (Esta opção não tem efeito se
	tiver sido selecionado o teste dos Sinais.)
      </para>
    </description>

  </command>

  <command name="discrete" section="Transformations" 
    label="Marca variáveis como sendo discretas" context="cli">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--reverse</flag>
	  <effect>marca variáveis como sendo contínuas</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Marca cada variável em <repl>lista-de-variáveis</repl> como sendo discreta. Por
	omissão todas as variáveis são tratadas como sendo contínuas; ao
	marcar uma variável como sendo discreta afeta o modo como a variável
	é usada em diagramas de frequência, e também permite-lhe selecionar
	a variável para o comando <cmdref targ="dummify"/>.
      </para>
      <para>
	Se a opção <opt>--reverse</opt> tiver sido dada, é feito o contrário;
	ou seja, as variáveis em <repl>lista-de-variáveis</repl> são marcadas
	como sendo contínuas.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Editar caraterísticas</menu-path>
    </gui-access>

  </command>

  <command name="dpanel" section="Estimation" label="Modelos de painel dinâmico">

    <usage>
      <arguments>
	<argument>p</argument> 
	<argblock separated="true">
	  <argument>variável-dependente</argument>
	  <argument>variáveis-independentes</argument>
	</argblock>
	<argblock optional="true" separated="true">
	  <argument>instrumentos</argument>
	</argblock>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar o modelo estimado</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
	</option>
        <option>
	  <flag>--two-step</flag>
	  <effect>efetuar estimação GMM de dois passos</effect>
        </option>
        <option>
	  <flag>--system</flag>
	  <effect>acrescentar equações por níveis</effect>
        </option>
        <option>
	  <flag>--time-dummies</flag>
	  <effect>acrescentar variáveis auxiliares tempo</effect>
        </option>
        <option>
	  <flag>--dpdstyle</flag>
	  <effect>comportamento semelhante ao do pacote DPD do Ox</effect>
        </option>
        <option>
	  <flag>--asymptotic</flag>
	  <effect>erros padrão assimptóticos não corrigidos</effect>
        </option>
      </options>
      <examples>
        <example>dpanel 2 ; y x1 x2</example>
	<example>dpanel 2 ; y x1 x2 --system</example>
        <example>dpanel {2 3} ; y x1 x2 ; x1</example>
	<example>dpanel 1 ; y x1 x2 ; x1 GMM(x2,2,3)</example>
	<demos>
	  <demo>bbond98.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Efectua a estimação de modelos de dados de painel dinâmico
	(ou seja, modelos de painel que incluem um ou mais desfasamentos
	da variável dependente) usando os métodos GMM-DIF ou GMM-SYS.
      </para>
      <para context="cli">
	O parâmetro <repl>p</repl> representa a ordem de autoregressão
	para a variável dependente.  Na forma mais simples esta é um
	valor escalar, mas este argumento pode ser uma matriz pré-definida,
	para especificar um conjunto (eventualmente descontínuo) de 
	desfasamentos a serem usados.
      </para>
      <para>
	A variável dependente e os regressores devem ser dados na 
	forma de níveis; eles serão automaticamente diferenciados
	(pois este estimador usa diferenciação para anular os efeitos
	individuais).
      </para>
      <para context="cli">
	O último campo (opcional) do comando é para especificar 
	instrumentos.  Se não tiver sido dado instrumentos, é assumido 
	que todas as variáveis independentes são estritamente exógenas.
	Caso você especifique alguns intrumentos, deverá incluir na
	lista variáveis independentes estritamente exógenas.  Para 
	regressores pré-determinados, você pode usar a função 
	<lit>GMM</lit> para incluir a especificação de uma gama de 
	desfasamentos numa forma bloco-diagonal.  Isto está exemplificado
	no terceiro exemplo acima.  O primeiro argumento de <lit>GMM</lit> 
	é o nome da variável em questão, o segundo é o desfasamento mínimo
	a ser usado como instrumento, e o terceiro é o desfasamento máximo.
	A mesma sintaxe pode ser utilizada na função <lit>GMMlevel</lit>
	para especificar instrumentos do tipo GMM para as equações nos 
	níveis.
      </para>
      <para context="gui">
	No que diz respeito à manipulação de instrumentos,
	veja a documentação da versão de sequência de comandos deste
	comando.  Presentemente você não pode especificar no interface
	gráfico (GUI) instrumentos explicítamente: todas as variáveis
	independentes são consideras como sendo estritamente exógenas.
      </para>
      <para>
	Por omissão são apresentados os resultados da estimação a um passo
	(juntamente com erros padrão robustos). Opcionalmente você pode 
	selecionar a estimação a dois passos.  Em ambos os casos são 
	determinados os testes de autocorrelação de ordem 1 e 2, assim como
	o teste de Sargan para a sobre-identificação e o teste de Wald para
	a significância conjunta dos regressores.  Note que neste modelo
	diferenciado a autocorrelação de primeira ordem não é um risco para
	a validade do modelo, mas a autocorrelação de segunda ordem viola
	as assunções estatísticas presentes.
      </para>
      <para context="cli">
	No caso da estimação em dois passos, os erros padrão são obtidos
	por omissão usando a correção de amostra-finita sugerida por <cite
	key="windmeijer05">Windmeijer (2005)</cite>.  Os erros padrão
	assimptóticos associados ao estimador de dois passos, são em geral,
	considerados como um guia pouco fiável para inferência, mas se por
	alguma razão você desejar observá-los você pode usar a opção
	<opt>--asymptotic</opt> para desligar a correção de Windmeijer.
      </para>
      <para context="cli">
	Se tiver sido dada a opção <opt>--time-dummies</opt>, uma
	conjunto de variáveis auxiliares tempo é acrescentado aos
	regressores especificados. O número de auxiliares é menos um
	que o número máximo de períodos usados na estimação, para
	assim se evitar a colinearidade exata com a constante.
	As variáveis auxiliares entram na forma diferenciada, exceto
	se tiver sido dada a opção <lit>--dpdstyle</lit>, entrando 
	nesse caso por níveis.
      </para>
      <para>
	Para mais detalhes e exemplos, ver o <guideref
	targ="chap:dpanel"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Painel/Modelo de painel dinâmico</menu-path>
    </gui-access>

  </command>

  <command name="dummify" section="Transformations" context="gui"
    label="Create sets of dummies">

    <description>
      <para>
	The <quote>dummify</quote> operation is available only
	for discrete-valued series. Its effect is to create a
	set of dummy variables coding for the distinct values
	present in the series.
      </para>
      <para>
	For example suppose one has a series named <lit>race</lit>,
	with values 1 for <quote>white</quote>, 2 for
	<quote>black</quote>, 3 for <quote>hispanic</quote> and 4 for
	<quote>other</quote>. To dummify this series means to create 4
	dummy variables: the first has value 1 for all observations at
	which race = 1, zero otherwise; the second has value 1 for all
	observations at which race = 2, zero otherwise; and so on.
      </para>
      <para>
	In practice it's likely that for a discrete series with
	<math>k</math> categories you will want to create only
	<math>k</math> &minus; 1 dummies, to avoid falling into
	the so-called <quote>dummy variable trap</quote>. Hence you
	have the option of dropping either the lowest or the
	highest value from the coding.
      </para>
    </description>

  </command>

  <command name="dummify" section="Transformations" 
    label="Cria conjuntos de variáveis auxiliares ('dummies')" context="cli">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--drop-first</flag>
	  <effect>omitir da codificação o menor valor</effect>
	</option>
	<option>
	  <flag>--drop-last</flag>
	  <effect>omitir da codificação o maior valor</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Para cada uma das variáveis adequadas em <repl>lista-de-variáveis</repl>,
	cria um conjunto de variáveis auxiliares codificando para os diferentes 
	valores dessa variável. É adequado para as variáveis que tenham sido 
	explícitamente marcadas como sendo discretas, ou aquelas que tomem uma
	quantidade razoavelmente pequena de valores todos eles 
	<quote>quase redondos</quote> (múltiplos de 0,25).
      </para>
      <para>
	Por omissão é criada uma variável auxiliar para cada valor
	distinto na variável em questão. Por exemplo se uma variável
	discreta <lit>x</lit> tiver 5 valores distintos, serão criadas
	5 variáveis auxiliares e acrescentadas ao conjunto de dados,
	com os nomes, <lit>Dx_1</lit>, <lit>Dx_2</lit> e por aí adiante.
	A primeira variável auxiliar terá o valor 1 para observações onde
	<lit>x</lit> toma o seu valor mais pequeno, 0 caso contrário; a
	variável auxiliar seguinte terá o valor 1 quando <lit>x</lit>
	toma o seu segundo valor mais pequeno, e por aí adiante.  Se uma
	das opções <opt>--drop-first</opt> ou <opt>--drop-last</opt>
	tiver sido acrescentada, então o menor ou o maior valor de cada 
	variável será omitido da codificação (o que pode ser útil para
	evitar a <quote>armadilha das variáveis auxiliares</quote>).
      </para>
      <para>
	Este comando também pode ser introduzido no contexto da 
	especificação de uma regressão.  Por exemplo, a linha seguinte
	especifica um modelo onde <lit>y</lit> é regredido sobre o
	conjunto de variáveis auxiliares codificadas em <lit>x</lit>.
	(Neste contexto, as opções não podem ser passadas a <cmd>dummify</cmd>.)
      </para>
      <code>
	ols y dummify(x)
      </code>
    </description>

    <gui-access>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="duration" section="Estimation" label="Modelos de Durações" 
    context="cli">
    <usage>
      <arguments>
	  <argument>variável-dependente</argument>
	  <argument>variáveis-independentes</argument>
        <argument separated="true" optional="true">variável-censora</argument>
      </arguments>
      <options>
        <option>
          <flag>--exponential</flag>
          <effect>usar a distribuição exponencial</effect>
        </option>
        <option>
          <flag>--loglogistic</flag>
          <effect>usar a distribuição log-logística</effect>
        </option>
        <option>
          <flag>--lognormal</flag>
          <effect>usar a distribuição log-normal</effect>
        </option>
        <option>
          <flag>--medians</flag>
          <effect>os valores ajustados são medianas</effect>
        </option>
        <option>
          <flag>--robust</flag>
          <effect>erros padrão robustos (QML)</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>variável-agrupada</optparm>
	  <effect>ver a explicação em <cmdref targ="logit"/></effect>
        </option>
        <option>
          <flag>--vcv</flag>
          <effect>mostrar a matriz de covariância</effect>
        </option>
        <option>
          <flag>--verbose</flag>
          <effect>mostrar detalhes das iterações</effect>
        </option>
      </options>      
      <examples>
        <example>duration y 0 x1 x2</example>
	<example>duration y 0 x1 x2 ; cens</example>
      </examples>
    </usage>

    <description>
      <para>
	Estima um modelo de duração: a variável dependente (que tem que
	ser positiva) representa a duração de algum tipo de estado num 
	certo assunto, por exemplo a duração de episódios de desemprego
	para uma seção-cruzada de inquiridos.  Por omissão é utilizada a
	distribuição de Weibull, mas é possível usar as distribuições
	exponencial, log-logística e a log-normal.
      </para>
      <para>
	Se alguma das medições de durações estiver censurada ('right-censored')
	(por exemplo, para um certo índividuo um episódio de desemprego não
	chegou ao fim dentro do período de observação) então você pode acrescentar
	como último argumento a <repl>variável-censora</repl>, que é uma série
	na qual valores diferentes de zero indicam caso censurados.
      </para>
      <para>
	Por omissão os valores ajustados obtidos por intermédio do acessor
	<lit>$yhat</lit> são as médias condicionadas das durações, mas se 
	tiver sido dada a opção <lit>--medians</lit>, então <lit>$yhat</lit>
	devolve as medianas condicionadas.
      </para>
      <para>
	Para mais detalhes ver <guideref targ="chap:probit"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Modelos não-lineares/Dados de durações...</menu-path>
    </gui-access>

  </command>  

  <command name="elif" section="Programming" label="Controlo de fluxo" context="cli">
    <description>
      <para>Ver <cmdref targ="if"/>.
      </para>
    </description>
  </command>

  <command name="else" section="Programming" label="Controlo de fluxo" context="cli">
    <description>
      <para>Ver <cmdref targ="if"/>. Note que <cmd>else</cmd>
	requer uma linha para ele mesmo, antes do comando
	condicional seguinte.  Você pode juntar um comentário,
	como em
      </para>
      <code>
	else # Certo, fazer algo diferente
      </code>
      <para>
	Mas não pode juntar a um comando, como em
      </para>
      <code>
	else x = 5 # Errado!
      </code>
    </description>
  </command>

  <command name="ema-filter" section="Transformations" context="gui"
    label="Exponential Moving Average">

    <description>
      <para>
	The formula for the exponential moving average (EMA) employed
	by gretl is that of <cite key="roberts59">Roberts
	(1959)</cite>, namely
      </para>
      <para>
	<math>s</math><sub>t</sub> =
	&agr;<math>y</math><sub>t</sub> +
        (1&minus;&agr;)<math>s</math><sub>t&minus;1</sub>
      </para>
      <para>
	where <math>s</math> is the EMA, <math>y</math> is the
	original series, and &agr; is a constant between 0 and
	1. Larger values of &agr; place more weight on the current
	observation; smaller values produce greater smoothing.
      </para>
      <para>
	The <quote>initial EMA value</quote>, however specified, is
	taken to be the last pre-sample value, meaning that
	calculation of the filter starts with the first observation in
	the current sample range.
      </para>
      <para>
	For a command-line equivalent, see the <fncref targ="movavg"/>
	function.
      </para>
    </description>

  </command>  

  <command name="end" section="Programming" 
    label="Termina um bloco de comandos" context="cli">
    <description>
      <para>
	Termina diversos tipos de bloco de comandos. Por exemplo, 
	<cmd>end system</cmd> termina uma equação <cmdref targ="system"/>.
      </para>
    </description>
  </command>

  <command name="endif" section="Programming" label="Controlo de fluxo" context="cli">
    <description><para>Ver <cmdref targ="if"/>.</para>
    </description>
  </command>

  <command name="endloop" section="Programming" 
    label="Termina um ciclo de comandos" context="cli">
    <description>
      <para>
	Marca o fim de um ciclo de comandos.  Ver <cmdref targ="loop"/>.
      </para>
    </description>
  </command>

  <command name="eqnprint" section="Printing" 
    label="Mostra o modelo como equação" context="cli">

    <usage>
      <options>
        <option>
	  <flag>--complete</flag>
	  <effect>Cria um documento completo</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>nome-de-ficheiro</optparm>
	  <effect>envia a saída para o ficheiro especificado</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Tem que ser invocado a seguir à estimação de um modelo.
	Mostra o modelo estimado na forma de uma equação &latex;.  Se
	o nome-de-ficheiro tiver sido especificado usando a opção 
	<lit>-f</lit> a saída é redirecionada para esse ficheiro,
	caso contrário vai para um ficheiro com o nome no formato
	<filename>equation_N.tex</filename>, onde <lit>N</lit> é o
	número de modelos estimados até ao momento na sessão
	corrente.
	Ver também <cmdref targ="tabprint"/>.
      </para>
      <para>
	Se a opção <opt>--complete</opt> tiver sido dada, o ficheiro &latex; é
	um documento completo, pronto para ser processado; de outro modo ele
	terá que ser incluído num documento.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do Modelo, /LaTeX</menu-path>
    </gui-access>

  </command>

  <command name="equation" section="Estimation" 
    label="Define uma equação dentro de um sistema de equações" context="cli">

    <usage>
      <arguments>
        <argument>variável-dependente</argument>
	  <argument>variáveis-independentes</argument>
      </arguments>
      <examples>
        <example>equation y x1 x2 x3 const</example>
      </examples>
    </usage>

    <description>
      <para>
	Especifica uma equação dentro de um sistema de equações (ver 
	<cmdref targ="system"/>).  A sintaxe para especificar uma 
	equação dentro de um sistema SUR é o mesmo que em, por exemplo, 
	<cmdref targ="ols"/>.  Para uma equação dentro de um sistema
	de Mínimos Quadrados de Três-Fases você tanto pode (a) fornecer
	uma especificação de equação tipo OLS e dar uma lista comum de 
	instrumentos usando o comando <cmd>instr</cmd> (mais uma vez, ver
	 <cmdref targ="system"/>), ou (b) usar a mesma sintaxe de equação
	como para <cmdref targ="tsls"/>.
      </para>
    </description>

  </command>

  <command name="estimate" section="Estimation" 
    label="Estimar um sistema de equações" context="cli">

    <usage>
      <arguments>
        <argument optional="true">nome-do-sistema</argument>
        <argument optional="true">estimador</argument>
      </arguments>
      <options>
	<option>
	  <flag>--iterate</flag>
	  <effect>iterar até à convergência</effect>
	</option>
	<option>
	  <flag>--no-df-corr</flag>
	  <effect>não usar correção de graus de liberdade</effect>
	</option>
	<option>
	  <flag>--geomean</flag>
	  <effect>ver abaixo</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar resultados</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das iterações</effect>
	</option>
      </options>
      <examples>
        <example>estimate "Klein Model 1" method=fiml</example>
	<example>estimate Sys1 method=sur</example>
	<example>estimate Sys1 method=sur --iterate</example>
      </examples>
    </usage>

    <description>
      <para>
	Chama a estimação de um sistema de equações, que foi 
	previamente definido usando o comando <cmdref targ="system"/>.
	O nome do sistema deve ser dado em primeiro lugar, dentro de
	aspas caso contenha espaços.  O estimador, que tem que ser um de
	<cmd>ols</cmd>,	<cmd>tsls</cmd>, <cmd>sur</cmd>, <cmd>3sls</cmd>,
	<cmd>fiml</cmd> ou <cmd>liml</cmd>, é precedido pelo texto
	<lit>method=</lit>. Estes argumentos são opcionais se o sistema
	em questão já foi estimado e ocupa a posição do 
	<quote>último modelo</quote>; nesse caso o estimador é o mesmo
	definido anteriormente.
      </para>
      <para>
	Se o sistema em questão tinha aplicado um conjunto de restrições
	(ver o comando <cmdref targ="restrict"/>), a estimação estará
	sujeita às restrições especificadas.
      </para>
      <para>
	Se o método de estimação é <cmd>sur</cmd> ou <cmd>3sls</cmd>
	e a opção <opt>--iterate</opt> tiver sido dada, o estimador
	será iterado.  No caso do SUR, se o procedimento convergir
	os resultados são estimativas de máxima vesrosimilhança.  No
	entanto, a iteração de Mínimos Quadrados de Três-Fases, geralmente não
 	converge para resultados de informação-completa de máxima verosimilhança.
	A opção <opt>--iterate</opt> é ignorada nos outros métodos de estimação.
      </para>
      <para>
	Se tiverem sido escolhidos os estimadores equação-a-equação <cmd>ols</cmd>
	ou <cmd>tsls</cmd>, por omissão é aplicada uma correção dos graus de
	liberdade quando se calcula os erros padrão.  Isto pode ser suprimido
	usando a opção <opt>--no-df-corr</opt>. Esta opção não tem efeito nos outros
	estimadores; de qualquer modo não seria aplicada a correção de graus de 
	liberdade.
      </para>
      <para>
	Por omissão, a equação usada no cálculo dos elementos da
	matriz de covariância das equações cruzadas é
	<equation status="display"
	tex="\[\hat{\sigma}_{i,j}=\frac{\hat{u}_i' \hat{u}_j}{T}\]"
	ascii="sigma(i,j) = u(i)' * u(j) / T"
	graphic="syssigma1"/>
	Se tiver sido dada a opção <opt>--geomean</opt>, a 
	correção de graus de liberdade será aplicada: a equação é
	<equation status="display"
	tex="\[\hat{\sigma}_{i,j}=\frac{\hat{u}_i' \hat{u}_j}{\sqrt{(T-k_i)(T-k_j)}}\]"
	ascii="sigma(i,j) = u(i)' * u(j) / sqrt((T - ki) * (T - kj))"
	graphic="syssigma2"/>
	onde os <math>k</math>s são o número de parâmetros independentes em cada 
	equação.
      </para>
      <para>
	Se tiver sido dada a opção <opt>verbose</opt> e ter sido especificado um
	método iterativo, serão mostrados os detalhes das iterações.
      </para>
    </description>

  </command>

  <command name="eval" section="Utilities">
    <usage>
      <arguments>
        <argument>expression</argument>
      </arguments>
      <examples>
        <example>eval x</example>
	<example>eval inv(X'X)</example>
	<example>eval sqrt($pi)</example>
      </examples>
    </usage>    
    <description>
      <para>
	This command makes gretl act like a glorified calculator.  The
	program evaluates <repl>expression</repl> and prints its
	value. The argument may be the name of a variable, or
	something more complicated. In any case, it should be an
	expression which could stand as the right-hand side of an
	assignment statement.
      </para>
    </description>
  </command>

  <command name="expand" section="Dataset" context="gui"
    label="Expandir dados">

    <description>
      <para>
	Se você pretender acrescentar a um conjunto de dados uma série
	que tenha uma frequência inferior, será necessário 
	<quote>expandir</quote> a nova série.  Por examplo, uma série
	trimestral terá que ser expandida para caber dentro de um conjunto
	de dados mensal.  Adicionalmente, você pode desejar expandir
	a totalidade de um conjunto de dados para uma frequência superior
	(eventualmente, antes de acrescentar uma variável de 
	alta-frequência ao conjunto de dados).
      </para>
      <para>
	Expandir dados deve ser considerada uma opção para 
	<quote>especialistas</quote>; você tem que saber o que está
	a fazer.  Ao combinar séries de diferentes frequências dentro
	de um conjunto de dados, você deve preferencialmente compactar
	os dados de alta-frequência em vez de expandir as séries
	baixa-frequência.
      </para>
      <para>
	Dito isto, gretl oferece duas opções: valores de 
	alta-frequência podem ser interpolados usando o método
	de Chow e Lin (1971), ou os valores de seéries de 
	baixa-frequência podem ser repetidos o número de vezes 
	necessárias.
      </para>
      <para>
	O método de Chow-Lin baseia-se em regressão, usando uma 
	constante e uma tendência quadrática e assumindo um 
	processo autoregressivo de primeira ordem para as 
	perturbações. Este procedimento usa quatro graus de liberdade.
	Quanto à repetição de valores, suponha que temos uma série
	trimestral com o valor 35,5 em 1990:1, o primeiro trimestre de
	1990.  Ao expandir para mensal, o valor 35,5 será atribuído
	às observações de janeiro, fevereiro e março de 1990.  A
	variável expandida será assim inútil para análises apuradas
	de séries temporais, excetuando o caso especial onde você
	sabe que a variável em questão se mantém de fato constante
	ao longo dos subperíodos.
      </para>
    </description>
  </command>

  <command name="export" section="Dataset" context="gui"
    label="Exportar dados">

     <description>
      <para>
	Voce pode exportar dados no formato Valores Separados por 
	Vírgulas (CSV): esse tipo de dados pode ser aberto em
	folhas-de-cálculo e em muitos outros programas.  Se
	você escolher esta opção poderá selecionar outras
	opções sobre o formato específico do ficheiro CSV.
      </para>
      <para>
	Você também tem a possibilidade de exportar dados na forma
	<quote>nativa</quote> de ficheiro-de-dados gretl, ou (se os dados
	forem adequandos) exportar para uma base-de-dados gretl. Ver
	<url>gretl.sourceforge.net/gretl_data.html</url> para uma listagem
	de bases-de-dados gretl.
      </para>
      <para>
	Você também pode exportar os dados num formato adequado
	para usar nos seguintes programas:
      </para>
      <ilist>
	<li>
	  <para>GNU R (<url>www.r-project.org</url>)</para>
	</li>
	<li>
	  <para>GNU octave (<url>www.gnu.org/software/octave</url>)
	  </para>
	</li>
	<li>
	  <para>JMulTi (<url>www.jmulti.de</url>)</para>
	</li>
	<li>
	  <para>PcGive (<url>www.pcgive.com</url>)</para>
	</li>
      </ilist>
      <para>
	Se você quiser exportar dados copiando para a memória em vez
	de escrever para um ficheiro em disco, selecione a série que
	você quer copiar na janela principal, clique com o botão 
	direito e selecione <quote>Copiar para a memória de edição</quote>. 
	(Neste contexto apenas é suportado o formato CSV.) 
      </para>
    </description>
  </command>

  <command name="factorized" section="Graphs" context="gui"
    label="Gráfico com separação fatorizada">

    <description>
      <para>
	Este comando requer a seleção de três variáveis sendo a última
	delas uma variável auxiliar (valores 1 ou 0). A variável Y é
	representada contra a variável X, com os pontos coloridos
	diferentemente de acordo com o valor da terceira.
      </para>
      <para>
	Examplo: Você tem dados de salários e níveis de educação obtidos
	numa amostra de pessoas; você tem também uma variável auxiliar
	com valores 1 para homens e 0 para mulheres (tal como nos dados
	<filename>data7-2</filename> de Ramanathan).  Um 
	<quote>gráfico com separação fatorizada</quote> de <lit>WAGE</lit>
	contra <lit>EDUC</lit> usando a variável auxiliar <lit>GENDER</lit>
	como fator mostrará os pontos de dados para homens numa cor e os das
	mulheres noutra (com uma legenda para as identificar).
      </para>
    </description>

  </command>

  <command name="fcast" section="Prediction" 
    label="Gerar predições">

    <usage>
      <arguments>
        <argument optional="true">observações-iniciais observações-finais</argument>
	<argument optional="true">passos-à-frente</argument>
	<argument optional="true">nome-de-variável</argument>
      </arguments>
      <options>
        <option>
	  <flag>--dynamic</flag>
	  <effect>criar predição dinâmica</effect>
        </option>
        <option>
	  <flag>--static</flag>
	  <effect>criar predição estática</effect>
        </option>
        <option>
	  <flag>--out-of-sample</flag>
	  <effect>gerar predição fora-da-amostra</effect>
        </option>
        <option>
	  <flag>--no-stats</flag>
	  <effect>não mostrar estatísticas de predição</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>não mostrar nada</effect>
        </option>
        <option>
	  <flag>--rolling</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--plot</flag>
	  <optparm optional="true">nome-de-ficheiro</optparm>
	  <effect>ver abaixo</effect>
        </option>
      </options>
      <examples>
        <example>fcast 1997:1 2001:4 f1</example>
	<example>fcast fit2</example>
	<example>fcast 2004:1 2008:3 4 rfcast --rolling</example>
      </examples>
    </usage>

    <description>

      <para context="gui">
	Tem que se seguir a um comando de estimação.  A predições são 
	geradas para o intervalo de observações especificado. Dependendo
	da natureza do modelo, também podem ser gerados erros padrão
	(ver abaixo).
      </para>

      <para context="cli">
	Tem que se seguir a um comando de estimação.  A predições são 
	geradas para um certo intervalo de observações: se tiverem sido
	dados <repl>observações-iniciais</repl> e 
	<repl>observações-finais</repl>, para esse intervalo (se possível); 
	caso contrário se a opção <opt>--out-of-sample</opt> tiver sido dada, 
	para observações a seguir ao intervalo onde o modelo foi estimado;
	caso contrário pelo intervalo de amostragem corrente.  Se uma 
	predição fora-da-amostra for pedida mas não haja observações relevantes,
	será assinalado um erro.  Dependendo da natureza do modelo, poderão ser
	gerados erros padrão; ver abaixo.  Ver também abaixo o efeito especial
	da opção <opt>--rolling</opt>.
      </para>

      <para context="cli">
	Se o último modelo estimado é de uma única equação, então
	o argumento opcional <repl>nome-de-variável</repl> tem o seguinte
	efeito: o valores preditos não são mostrado, mas guardados dentro
	do conjunto de dados no nome fornecido.  Se o último modelo 
	estimado é um sistema de equações, <repl>nome-de-variável</repl> 
	tem um efeito diverente, nomeadamente a seleção de uma variável
	endógena específica para a predição (por omissão são produzidas
	predições para todas as variáveis endógenas).  No caso do sistema,
	ou se não tiver sido dado o <repl>nome-de-variável</repl>, os 
	valores de predição podem ser obtidos usando o acessor 
	<lit>$fcast</lit>, e os erros padrão, se disponíveis, pelo 
	<lit>$fcerr</lit>.
      </para>

      <para>
	A escolha entre predições estáticas ou dinâmicas aplica-se
	apenas no caso de modelos dinâmicos, com processamento 
	autoregressivo de erros ou que incluam um ou mais valores
	desfasados da variável dependente como regressores.  As 
	predições estáticas são um passo à frente, baseadas em valores
	concretizados no período anterior, enquanto que as predições
	dinâmicas usam a regra de encadeamento de predição.  Por 
	exemplo, se uma predição de <math>y</math> em 2008
	requer como entrada um valor de <math>y</math> em 2007, uma
	predição estática é impossível sem dados reais para 2007.  Uma
	predição dinâmica para 2008 é possível se uma predição anterior
	poder ser substítuida no <math>y</math> em 2007.
      </para>

      <para>
	Por omissão o normal é produzir uma predição estática para
	alguma parte do intervalo de predição que abrange o 
	intervalo da amostra onde o modelo foi estimado, e uma predição
	dinâmica (se relevante) para fora-da-amostra.  A opção
	 <lit>dynamic</lit> chama uma predição dinâmica a partir da 
	date mais cedo possível, e a opção <opt>static</opt> chama
	uma predição estática até para fora-da-amosta.
      </para>

      <para context="cli">
	A opção <opt>rolling</opt> está atualmente apenas disponível para
	modelos de um única equação estimados por Mínimos Quadrados (OLS).
	Quando esta opção é dada as predições são recursivas.  Isto é, cada 
	predição é gerada a partir de uma estimativa do modelo dado usando
	dados de uma ponto de partida fixo (nomeadamente, a partir do início 
	do intervalo da amostra da estimação original) até à data de predição
	menos <math>k</math>, onde <math>k</math> é o número de passos à frente
	que têm que ser dados no argumento <repl>passos-à-frente</repl>.  As
	predições serão sempre dinâmicas se isso for aplicável.  Note que o
	argumento <repl>passos-à-frente</repl> deve ser dado apenas em conjunto
	com a opção <opt>rolling</opt>.
      </para>

      <para context="cli">
	A opção <opt>--plot</opt> (disponível apenas no caso de 
	estimação de equação única) invoca a produção de um 
	ficheiro de gráfico, que contém a representação
	gráfica da predição.  Quando não é dado o parâmetro
	<repl>nome-de-ficheiro</repl>, gretl escreve os comandos
	gnuplot para um ficheiro com nomes do tipo
	<lit>gpttmp01.plt</lit> na diretoria de trabalho de gretl
	do utilizador (com o número incrementado em gráficos sucessivos).
	Se o <repl>nome-de-ficheiro</repl> é acrescentado, a sua 
	extensão é usada para determinar o tipo de ficheiro a ser
	escrito (<lit>.eps</lit> para EPS, <lit>.pdf</lit> para PDF, ou
	<lit>.png</lit> para PNG; qualquer outra extensão resulta num
	ficheiro de comandos gnuplot).  Por exemplo,
      </para>
      <code>
	fcast --plot=fc.pdf
      </code>
      <para>
	produzirá um gráfico em formato PDF.  Serão respeitados caminhos
	completos para ficheiros, senão os ficheiros são escritos na
	diretoria de trabalho do gretl.
      </para>
      <para>
	A natureza dos erros padrão de predição (se disponíveis)
	depende da natureza do modelo e da predição.  Para modelos
	lineares estáticos os erros padrão são determinados usando o
	método traçado por <cite key="davidson-mackinnon04">Davidson e MacKinnon
	(2004)</cite>; eles incorporam tanto a incerteza devida aos 
	processos de erro como a incerteza dos parâmetros ( resumidos na
	matriz de covariância das estimativas dos parâmetros).  Para
	modelos dinâmicos, os erros padrão de predição são apenas
	calculados no caso de uma predição dinâmica, e eles não
	incorporam incerteza de parâmetros.  Para modelos não-lineares,
	os erros padrão de predição não estão disponíveis atualmente.
      </para>	

    </description>

    <gui-access>
      <menu-path>Janela de Modelo, /Análise/Predições...</menu-path>
    </gui-access>

  </command>

  <command name="flush" section="Programming" context="cli">

    <description>
     <para>
       This simple command (no arguments, no options) is intended for
       use in time-consuming scripts that may be executed via the
       gretl GUI (it is ignored by the command-line program), to give
       the user a visual indication that things are moving along and
       gretl is not <quote>frozen</quote>.
     </para>
     <para>
       Ordinarily if you launch a script in the GUI no output is shown
       until its execution is completed, but the effect of invoking
       <lit>flush</lit> is as follows:
     </para>
     <ilist>
       <li>
	 <para>
	   On the first invocation, gretl opens a window, displays the
	   output so far, and appends the message
	   <quote>Processing...</quote>.
	 </para>
       </li>
       <li>
	 <para>
	   On subsequent invocations the text shown in the output
	   window is updated, and a new <quote>processing</quote>
	   message is appended.
	 </para>
       </li>
     </ilist>
     <para>
       When execution of the script is completed any remaining output
       is automatically flushed to the text window.
     </para>
     <para>
       Please note, there is no point in using <lit>flush</lit> in
       scripts that take less than (say) 5 seconds to execute. Also
       note that this command should not be used at a point in the
       script where there is no further output to be printed, as the
       <quote>processing</quote> message will then be misleading
       to the user.
     </para>
     <para>
       The following illustrates the intended use of <lit>flush</lit>:
     </para>
     <code>
       set echo off
       scalar n = 10
       loop i=1..n
           # do some time-consuming operation
           loop 100 --quiet
               a = mnormal(200,200)
               b = inv(a)
           endloop
           # print some results
           printf "Iteration %2d done\n", i
           if i &lt; n
               flush
           endif
       endloop
     </code>
    </description>

  </command>

  <command name="foreign" section="Programming" 
    label="Comandos não-nativos" context="cli">

    <usage>
      <syntax><lit>foreign language=</lit><repl>linguagem</repl></syntax>
      <options>
	<option>
	  <flag>--send-data</flag>
	  <effect>pré-carregar o conjunto de dados corrente; ver abaixo</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suprimir a saída do programa estrangeiro</effect>
	</option>
      </options>
    </usage>

    <description>
     <para>
	Este comando inicia um modo especial no qual se aceita os comandos a serem
	executados por outro programa.  Você sai deste modo com <lit>end
	  foreign</lit>; onde neste ponto são executados os comandos acumulados.
      </para>
      <para>
	Presentemente são suportados três programas <quote>estrangeiros</quote>,
 	GNU R (<lit>language=R</lit>), Ox de Jurgen Doornik 
	(<lit>language=Ox</lit>) e GNU Octave (<lit>language=Octave</lit>).
	Os nomes de linguagem são reconhecidos sem considerar capitalização.
      </para>
      <para>
	A opção <opt>--send-data</opt> é válida apenas quando em ligação
	com R e Octave; tem o efeito de tornar o corrente conjunto de dados de
	gretl disponível dentro do programa alvo, usando o nome <lit>gretldata</lit>.
      </para>
      <code>
	list Rlist = x1 x2 x3
	foreign language=R --send-data=Rlist
      </code>
      <para>
	Para detalhes e exemplos ver <guideref targ="chap:gretlR"/>.
      </para>
    </description>

  </command>

  <command name="fractint" section="Statistics" label="Integração fracional">
  
    <usage>
      <arguments>
        <argument>série</argument>
	<argument optional="true">ordem</argument>
      </arguments>
      <options>
        <option>
	  <flag>--gph</flag>
	  <effect>fazer o teste de Geweke e Porter-Hudak</effect>
        </option>
        <option>
	  <flag>--all</flag>
	  <effect>fazer ambos os testes</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>não mostrar resultados</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Testa a integração fracional sobre a série especificada
	(<quote>memória longa</quote>). A hipótese nula é de que a
	ordem de integração da série é zero.  Por omissão é usado o
	estimador local de Whittle <cite key="robinson95" p="true">(Robinson,
	1995)</cite> mas se tiver sido dada a opção <opt>--gph</opt>
	será usado o teste GPH <cite key="GPH83" p="true">(Geweke e
	Porter-Hudak, 1983)</cite>. Se a opção <opt>--all</opt> for 
	dada então serão mostrados os resultados dos dois testes.
      </para>
      <para>
	Para mais detalhes sobre este tipo de testes, ver <cite key="phillips04">Phillips 
	e Shimotsu (2004)</cite>. 
      </para>
      <para>
	Se não tiver sido dado o argumento opcional <repl>ordem</repl>, a ordem para 
	os teste(s) é automaticamente definida como sendo o menor de
	<math>T</math>/2 e <math>T</math><sup>0.6</sup>.
      </para>
      <para>
	Os resultados podem ser obtidos usando os acessores <lit>$test</lit>
	e <lit>$pvalue</lit>. Estes valores baseiam-se no estimador local de
	Whittle exceto quando dada a opção <opt>--gph</opt>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Testes de raiz unitária/Integração fracional</menu-path>
    </gui-access>

  </command>


  <command name="freq" section="Statistics" label="Distribução de frequência">

    <usage>
      <arguments>
        <argument>variável</argument>
      </arguments>
      <options>
        <option>
	  <flag>--nbins</flag>
	  <optparm>n</optparm>
	  <effect>especificar o número de classes</effect>
        </option>
        <option>
	  <flag>--min</flag>
	  <optparm>valor-mínimo</optparm>
	  <effect>especificar o mínimo, ver abaixo</effect>
        </option>
        <option>
	  <flag>--binwidth</flag>
	  <optparm>amplitude</optparm>
	  <effect>especificar a amplitude das classes, ver abaixo</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>não mostrar o gráfico</effect>
        </option>
        <option>
	  <flag>--normal</flag>
	  <effect>testar a distribuição normal</effect>
        </option>
        <option>
	  <flag>--gamma</flag>
	  <effect>testar a distribuição gama</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>não mostrar nada</effect>
        </option>
        <option>
	  <flag>--show-plot</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>nome</optparm>
	  <effect>usar coluna da matriz indicada</effect>
        </option>
      </options>
      <examples>
        <example>freq x</example>
	<example>freq x --normal</example>
	<example>freq x --nbins=5</example>
	<example>freq x --min=0 --binwidth=0.10</example>
      </examples>
    </usage>

    <description context="cli">
      <para>
	Se não forem dadas opções, mostra a distribuição de frequência
	da série <repl>variável</repl> (dada por nome ou por número), com
	o número de classes e respetivo tamanho escolhidos automaticamente.
      </para>
      <para>
	Se tiver sido dada a opção <opt>--matrix</opt>, a <repl>variável</repl>
	(que tem que ser um inteiro) será interpretada com um índice de base 1
	que selecciona a coluna da matriz designada.
      </para>
      <para>
	Para controlar a apresentação da distribuição você pode
	especificar <emphasis>tanto</emphasis> o número de classes
	ou o valor mínimo e ainda a amplitude das classes, tal como
	mostrado nos dois últimos exemplos acima. A opção <opt>--min</opt>
	define o limite inferior da classe mais à esquerda.
      </para>
      <para>
	Se a opção <opt>--normal</opt> tiver sido dada, será calculado
	o teste qui-quadrado para a normalidade de Doornik&ndash;Hansen.
	Se a opção <opt>--gamma</opt> tiver sido dada, o teste de normalidade
	será substituído pelo teste não paramétrico de Locke para a hipótese
	nula de que a variável segue uma distribuição gama; ver <cite
	key="locke76">Locke (1976)</cite>, <cite
	key="shapiro-chen01">Shapiro e Chen (2001)</cite>.  Note que a
	parametrização da distribuição gama utilizadada em gretl é 
	(forma, escala).
      </para>
      <para>
	Em modo interactivo, por omissão é apresentado o gráfico da
	distribuição.  A opção <opt>--quiet</opt> pode ser usada para
	suprimir isto.  Pelo contrário, normalmente não é mostrado o
	gráfico quando se usa a opção <cmd>freq</cmd> dentro de uma
	sequência-de-comandos, mas você pode forçar que seja apresentado
	usando a opção <opt>--show-plot</opt>. (Isto não se aplica quando
	se usa o programa em modo de linha-de-comandos,
	<lit>gretlcli</lit>.)
      </para>
      <para>
	A opção <opt>--silent</opt> suprime toda a saída do programa.
	Isto apenas faz sentido quando em conjunto com uma das opções
	de teste de distribuição: a estatística de teste e o seu valor p
	ficam guardados e podem ser obtidos usando os acessores
	<lit>$test</lit> e <lit>$pvalue</lit>.
      </para>
    </description>

    <description context="gui">
      <para>
	Na janela de diálogo do gráfico de frequência você pode controlar
	as caraterísticas do gráfico em duas maneiras diferentes.
      </para>
      <para>
	Primeiro, você pode escolher o número de classes. Neste caso
	a largura e localização das classes são calculadas automaticamente.
      </para>
      <para>
	Em alternativa, você pode especificar o limite inferior da classe 
	mais à esquerda, e a largura das classes.  Neste caso o número
	de classes é calculado automaticamente.
      </para>
      <para>
	Se você desejar alinhar as classes para números redondos, esta é uma
	maneira possível: comece por especificar o número de classes, e 
	observe o gráfico produzido. Se não fôr do seu agrado, anote a 
	modificação necessária (por exemplo, fazer a classe mais à esquerda
	iniciar em 100 e impor uma largura de classe de 200).
	Então efectue uma segunda passagem onde você especifica o limite
	da classe mais à esquerda e a largura das classes.
      </para>
      <para>
	Este diálogo também permite selecionar a apresentação da curva da
	distribuição teórica dos dados: a normal ou a gama.  Se a opção
	normal for selecionada será determinado o teste para normalidade 
	de Doornik&ndash;Hansen. Se a opção gama é selecionada, gretl
	calcula o teste não paramétrico de Locke para a hipótese nula de que
	variável segue uma distribuição gama.  Note que a
	parametrização da distribuição gama utilizadada em gretl é 
	(forma, escala).
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Distribuição de frequência</menu-path>
    </gui-access>

  </command>

  <command name="function" section="Programming" 
    label="Definir uma função" context="cli">

    <usage>
      <arguments>
        <argument>nome-da-função</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Abre um bloco de declarações no qual é definida a função.  Este
	bloco tem que ser finalizado com <lit>end function</lit>.  Para mais
	detalhes ver <guideref targ="chap:functions"/>.
      </para>
    </description>

  </command>  

  <command name="garch" section="Estimation" label="Modelo GARCH">

    <usage>
      <arguments>
        <argument>p</argument>
	<argument>q</argument>
	<argument separated="true">variável-dependente</argument>
	<argument optional="true">variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>erros padrão robustos</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das iterações</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
        </option>
        <option>
	  <flag>--nc</flag>
	  <effect>não incluir uma constante</effect>
        </option>
        <option>
	  <flag>--stdresid</flag>
	  <effect>normalizar os resíduos</effect>
        </option>
        <option>
	  <flag>--fcp</flag>
	  <effect>usar o algoritmo Fiorentini, Calzolari, Panattoni</effect>
        </option>
        <option>
	  <flag>--arma-init</flag>
	  <effect>parâmetros iniciais da variância a partir de ARMA</effect>
        </option>
      </options>
      <examples>
        <example>garch 1 1 ; y</example>
	<example>garch 1 1 ; y 0 x1 x2 --robust</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Estima um modelo GARCH (GARCH = Autoregressivo Generalizado
        de Heterocedastidade Condicional, "Generalized Autoregressive
	Conditional Heteroskedasticity"), que pode ser um modelo
	univariado, ou multivariado se especificadas as 
	<repl>variáveis-independentes</repl>, incluindo as variáveis 
	exógenas.  Os valores inteiros <repl>p</repl> e <repl>q</repl>
	(que podem ser dados na forma numérica ou como nomes de variáveis
	escalares pré-existentes) representam as ordens de desfasamento 
	na equação de variância condicional:
	<equation status="display"
	  tex="\[h_t = \alpha_0 + \sum_{i=1}^q \alpha_i \varepsilon^2_{t-i} +
	  \sum_{j=1}^p \beta_j h_{t-j}\]"
	  ascii="h(t) = a(0) + sum(i=1 to q) a(i)*u(t-i)^2 + sum(j=1 to p) b(j)*h(t-j)"
	  graphic="garch_h"/>
      </para>
      <para context="cli">
	Portanto, o parâmetro <repl>p</repl> representa a ordem Generalizada
	(ou <quote>AR</quote>), enquanto <repl>q</repl> representa a ordem
	normal ARCH (ou <quote>MA</quote>).  Se <repl>p</repl> for não-nulo,
	<repl>q</repl> tem também que ser não-nulo senão o modelo fica
	não-identificado.  No entanto, você pode estimar um modelo ARCH normal
	ao definir <repl>q</repl> para um valor positivo e <repl>p</repl>
	para zero.  A soma de <repl>p</repl> e <repl>q</repl> não pode ser
	maior que 5.  Note que é automaticamente incluida uma constante na 
        equação da média, exceto se tiver sido dada a opção <opt>--nc</opt>.
      </para>

      <para context="gui">
	Estima um modelo GARCH (GARCH = Autoregressivo Generalizado
        de Heterocedastidade Condicional, "Generalized Autoregressive
	Conditional Heteroskedasticity"), que pode ser um modelo
	univariado, ou multivariado se selecionadas variáveis 
	independentes, incluindo as variáveis exógenas.  Em baixo é
	mostrada a equação de variância condicional:
	<equation status="display" tex="\[h_t = \alpha_0 + 
	\sum_{i=1}^q \alpha_i \varepsilon^2_{t-i} + \sum_{j=1}^p
	\beta_i h_{t-j}\]" ascii="h(t) = a(0) + sum(i=1 to q) a(i)*u(t-i) +
	sum(j=1 to p) b(j)*h(t-j)" graphic="garch_h"/>
      </para>
      <para context="gui">
	Portanto, o parâmetro <repl>p</repl> representa a ordem Generalizada
	(ou <quote>AR</quote>), enquanto <repl>q</repl> representa a ordem
	normal ARCH (ou <quote>MA</quote>).  Se <repl>p</repl> for não-nulo,
	<repl>q</repl> tem também que ser não-nulo senão o modelo fica
	não-identificado.  No entanto, você pode estimar um modelo ARCH normal
	ao definir <repl>q</repl> para um valor positivo e <repl>p</repl>
	para zero.  A soma de <repl>p</repl> e <repl>q</repl> não pode ser
	maior que 5.
      </para>

      <para>
	Por omissão a estimação de modelos GARCH é feita usando código
	nativo gretl, mas você também tem a possibilidade de usar o 
	algoritmo de <cite key="fiorentini96">Fiorentini, Calzolari e Panattoni
	(1996)</cite>.  O primeiro usa o maximizador BFGS enquanto o 
	segundo usa a matriz de informação para maximizar a
	verosimilhança, com aperfeiçoamento por via da Hessiana.
      </para>

      <para context="cli">
	Para este comando estão disponíveis diferentes estimadores da
	matriz de covariância.  Por omissão, usa-se a Hessiana, ou se
	a opção	<opt>--robust</opt> tiver sido dada, será usada a 
	matriz de covariança QML (White).  Outras possibilidades podem
	ser especificadas usando o comando <cmdref targ="set"/> 
	(por exemplo a matriz de informação, ou o estimador 
	Bollerslev&ndash;Wooldridge ).
      </para>

      <para context="gui">
	Para este comando estão disponíveis diferentes estimadores da
	matriz de covariância.  Por omissão, usa-se a Hessiana, ou se
	a opção	<quote>Erros padrão robustos</quote> tiver sido 
	selecionada, será usada a matriz de covariância QML (White).  
	 Outras possibilidades podem ser especificadas usando o 
	comando <cmdref targ="set"/> (por exemplo a matriz de 
	informação, ou o estimador Bollerslev&ndash;Wooldridge ).
      </para>

      <para context="gui">
	A variância condicional estimada, juntamente com os resíduos e 
	outras estatísticas do modelo, podem ser acedidas e acrescentadas
	ao modelo usando o menu <quote>Gravar</quote> na janela
	onde o modelo é apresentado.  Se a opção 
	<quote>Normalizar os resíduos</quote> estiver selecionada, os
	resíduos são divididos pela raiz quadrada da variância condicional.
      </para>

      <para context="cli">
	Por omissão, as estimativas dos parâmetros da variância são
	inicializados usando a variância do erro incondicional da 
	estimação OLS inicial para a constante, e pequenos valores
	positivos para os coeficientes dos valores anteriores do
	quadrado do erro e da variância do erro.  A opção 
	<opt>--arma-init</opt> faz com que os valores iniciais destes
	parâmetros sejam definidos usando inicialmente um modelo ARMA,
	explorando a relação entre GARCH e ARMA demosntrado no 
	Capítulo 21 do livro de Hamilton, <book>Time Series Analysis</book>.
	 Em alguns casos isto pode melhorar as possibilidades de converência.
      </para>

      <para context="cli">
	Os resíduos GARCH e a variância condicional estimada podem ser
	obtidos como <lit>$uhat</lit> e <lit>$h</lit> respectivamente.  Por
	exemplo, para obter a variância condicional:  
      </para>
      <code context="cli">
	genr ht = $h
      </code>
      <para context="cli">
	Se a opção <opt>--stdresid</opt> tiver sido dada, os valores <lit>$uhat</lit>
	são divididos pela raiz quadrada de <math>h</math><sub>t</sub>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/GARCH</menu-path>
    </gui-access>

  </command>

  <command name="genr" section="Dataset"
    label="Gerar uma nova variável">

    <usage>
      <arguments>
        <argument>nova-variável</argument>
        <argument>= expressão</argument>
      </arguments>
    </usage>

    <description>

      <para>
	NOTE: this command has undergone numerous changes and
	enhancements since the following help text was written, so for
	comprehensive and updated info on this command you'll want to
	refer to <guideref targ="chap:genr"/>. On the other hand, this
	help does not contain anything actually erroneous, so take the
	following as <quote>you have this, plus more</quote>.
      </para>

      <para context="cli">
	No contexto apropriado, o nomes; <lit>series</lit>, <lit>scalar</lit>
	e <lit>matrix</lit> são sinónimos para este comando.
      </para>

      <para context="cli">
	Cria novas variáveis, frequentemente a partir de transformações 
	de variáveis já existentes. Ver também os atalhos, <cmdref targ="diff"/>, <cmdref targ="logs"/>,
	<cmdref targ="lags"/>, <cmdref targ="ldiff"/>, 
	<cmdref targ="sdiff"/> e <cmdref targ="square"/>.  No contexto de
	uma expressão <lit>genr</lit>, as variáveis existentes têm que ser 
	referenciadas por nome e não por número ID.  A expressão deve ser uma 
	combinação bem construída de nomes de variáveis, constantes, operadores
	e funções (descrito adiante).  Note que detalhes adicionais sobre alguns
	aspetos deste comando podem ser encontrados em <guideref targ="chap:genr"/>.
      </para>

      <para context="gui">
	Use esta caixa de txto para definir uma nova variável, seguindo o padrão
	<repl>nome</repl> = <repl>expressão</repl>.  A expressão deve ser uma 
	combinação bem construída de nomes de variáveis, constantes, operadores
	e funções (descrito adiante).  Para garantir que o tipo de variável
	criada é o desejado, você pode anteceder o nome com o tipo, como
	sejam, <lit>scalar</lit>, <lit>series</lit> ou <lit>matrix</lit>.  Por
	exemplo, para criar uma série que tenha um valor constante de 10, você
	pode escrever
      </para>
      <code context="gui">
	series c = 10
      </code>
      <para context="gui">
	(de outro modo <lit>c = 10</lit> resultaria numa variável escalar).
      </para>

      <para context="cli">
	Um comando <lit>genr</lit> pode resultar tanto num escalar como numa
	série.  Por exemplo, a expressão <lit>x2 = x * 2</lit> naturalmente
	resulta numa série se a variável <lit>x</lit> for uma série e num 
	escalar se <lit>x</lit> for um escalar.  As expressões <lit>x = 0</lit>
	e <lit>mx = mean(x)</lit> naturalmente retornam escalares.  Em alguma
	circusntâncias você poderá querer ter um resultado escalar expandido 
	numa série ou num vetor.  Você pode fazer isso usando <lit>series</lit>
	como um <quote>aliás</quote> para o comando <lit>genr</lit>.  Por 
	exemplo, <lit>series x = 0</lit> produz uma série em que todos os 
	valores são 0.  Você também pode usar <lit>scalar</lit> como sendo
	um aliás para <lit>genr</lit>.  Não é possível forçar um resultado do
	tipo vetor para um escalar mas o uso desta palavra reservada indica
	que o resultado <emphasis>deve ser</emphasis> um escalar: se não for
	ocorrerá um erro.
      </para>

      <para context="cli">
	Quando uma expressão resulta numa série, o intervalo que será 
	escrito na variável destino depende do actual intervalo de amostragem.
	É assim possível, definir uma série por troços usando o comando 
	<lit>smpl</lit> conjugado com <lit>genr</lit>.
      </para>

      <para>
	Os <emphasis>operadores aritméticos</emphasis> suportados são, por
	ordem de precedência: <lit>^</lit> (potenciação);
	<lit>*</lit>, <lit>/</lit> e <lit>%</lit> (resto da divisão inteira);
	<lit>+</lit> e <lit>-</lit>. 
      </para>

      <para>
	Os <emphasis>operadores Booleanos</emphasis> são (mais uma vez,
	por ordem de precedência): <lit>!</lit> (negação),
	<lit>&amp;&amp;</lit> (E lógico), <lit>||</lit> (OU lógico),
	<lit>&gt;</lit>, <lit>&lt;</lit>, <lit>=</lit>, <lit>&gt;=</lit>
	(maior ou igual), <lit>&lt;=</lit> (menor ou igual) e
	<lit>!=</lit> (diferente).  Os operadores Booleanos podem ser 
	usados na construção de variáveis auxiliares ('dummy'): por
	exemplo <lit>(x > 10)</lit> retorna 1 se <lit>x</lit> &gt; 10,
	0 caso contrário.
      </para>

      <para>
	As constantes pré-definidas são <lit>pi</lit> e <lit>NA</lit>.  Esta última
	representa um valor omisso: você pode inicializar uma variável como tendo
	um valor omisso com <lit>scalar x = NA</lit>.
      </para>

      <para>
	O comando <lit>genr</lit> suporta uma larga gama de funções matemáticas
	e estatísticas, incluindo, para além das usuais, várias que são
	especialmente dedicadas à econometria.  Adicionalmente oferece
	acesso a numerosas variáveis internas que são definidas no 
	decorrer das regressões, testes de hipóteses e outros.
	<refnote xref="false"> 
	  Para uma lista de acessores, escrever 
	  <quote>help functions</quote>. 
	</refnote> 
	<refnote xref="true">
	  Para uma lista de funções e acessores, ver 
	  <gfr targ="chap:funcref"/>. 
	</refnote>
      </para>

      <para>
	Para além dos operadores e funções mencionados acima, existem alguns
	usos especiais de <cmd>genr</cmd>:
      </para>

      <ilist>
	<li>
	  <para>
	    <cmd>genr time</cmd> cria uma variável de tendência temporal (1,2,3,&hellip;)
	    com o nome <cmd>time</cmd>. <cmd>genr index</cmd> faz a mesma coisa exceto
	    em que o nome da variável é <lit>index</lit>.
	  </para>
	</li>
	<li>
	  <para>
	    <cmd>genr dummy</cmd> cria variáveis auxiliares ('dummy')
            até à periodicidade dos dados.  No caso de dados trimestrais
	    (periodicidade 4), o programa cria <lit>dq1</lit> = 1 para
	    o primeiro trimestre 0 nos outros timestres, <lit>dq2</lit> = 1 
	    para o segundo trimestre e 0 para os outros trimestres, e por aí
	    adiante.  No caso de dados mensais as variáveis auxiliares têm os
	    nomes <lit>dm1</lit>, <lit>dm2</lit>, e por aí adiante. No caso de
	    outras frequências os nomes são <lit>dummy_1</lit>, 
	    <lit>dummy_2</lit>, etc.
	  </para>
	</li>
	<li>
	  <para>
	    <cmd>genr unitdum</cmd> e <cmd>genr timedum</cmd> criam 
	    conjuntos de variáveis auxiliares especiais para usar com
	    dados de painel. O primeiro codifica para as seções-cruzadas
	    e o segundo para os períodos temporais das observações.
	  </para>
	</li>
      </ilist>

      <para>
	<emphasis>Nota</emphasis>: No programa de linha-de-comandos, 
	os comandos <cmd>genr</cmd> que obtenham dados de modelo
	referem-se sempre ao modelo que foi estimado mais recentemente.
	Isto também é válido para o programa em ambiente gráfico (GUI),
	ao se usar <cmd>genr</cmd> na <quote>consola gretl</quote> ou
	ao introduzir uma expressão usando <quote>Definir nova variável</quote>
	no menu Acrescentar na janela principal.  No entanto, no GUI, você
	tem a possibilidade de obter dados a partir de qualquer modelo que
	esteja disponível numa janela (independentemente se é ou não o modelo
	mais recente).  Isso é feito no menu <quote>Gravar</quote> na 
	janela do modelo.
      </para>

      <para>
	A variável especial <lit>obs</lit> serve como um índice das
	observações.  Por exemplo <lit>genr dum = (obs=15)</lit> irá gerar
	uma variável auxiliar que tem valor 1 para a observação 15 e 0
	para as outras.  Você também pode usar esta variável para escolher
	certas observações por data ou nome.  Por exemplo, 
	<lit>genr d = (obs&gt;1986:4)</lit>, 
	<lit>genr d = (obs&gt;"2008/04/01")</lit>, ou
	<lit>genr d = (obs="CA")</lit>.  Se se usarem datas diárias ou 
	etiquetas neste contexto, elas devem ser indicadas dentro de aspas.
	Datas trimestrais ou mensais (com um dois-pontos) podem ser usadas
	sem aspas.  Note que no caso de dados de séries temporais anuais, o
	ano não se distingue sintáticamente de um simples inteiro; como tal,
	se você quiser comparar observações com <lit>obs</lit> por ano, você
	tem que usar a função <lit>obsnum</lit> para converter o ano para
	um valor de índice iniciado em 1, tal como em <lit>genr d = (obs&gt;obsnum(1986))</lit>.
      </para>

      <para>
	Valores escalares podem ser extraídos de uma série no contexto de uma
	expressão <lit>genr</lit>, usando a sintaxe 
	<repl>varname</repl><lit>[</lit><repl>obs</repl><lit>]</lit>.  O valor
	<repl>obs</repl> pode ser dado por núnmero ou data. Exemplos:
	<lit>x[5]</lit>, <lit>CPI[1996:01]</lit>.  Para dados diários, deve se
	usar a forma <repl>YYYY/MM/DD</repl>, por exemplo, <lit>ibm[1970/01/23]</lit>.
      </para>

      <para>
	Uma observação individual numa série pode ser modificado usando 
	<lit>genr</lit>.  Para fazer isto, uma observação válida numérica ou de data,
	tem que ser acrescentada dentro de parentesis rectos, ao nome da variável no
	lado esquerdo da expressão.  Por exemplo, <lit>genr x[3] = 30</lit>
	ou <lit>genr x[1950:04] = 303.7</lit>.
      </para>

      <table id="tab-genr" title="Exemplos de uso do comando genr"
	lhead="Expressão" rhead="Comentário" lwidth="100pt" rwidth="300pt" 
	style="rpara">
	<row>
	  <cell><lit>y = x1^3</lit></cell>
	  <cell><lit>x1</lit> ao cubo</cell>
	</row>          
	<row>
	  <cell><lit>y = ln((x1+x2)/x3)</lit></cell>
	  <cell></cell>
	</row>
	<row>
	  <cell><lit>z = x&gt;y</lit></cell>
	  <cell><lit>z(t)</lit> = 1 if <lit>x(t) &gt; y(t)</lit>,
	    caso contrário 0</cell>
	</row> 
	<row>
	  <cell><lit>y = x(-2)</lit></cell>
	  <cell><lit>x</lit> desfasado 2 períodos</cell>
	</row>     
	<row>
	  <cell><lit>y = x(+2)</lit></cell>
	  <cell><lit>x</lit> adiantado 2 períodos</cell>
	</row>
	<row>
	  <cell><lit>y = diff(x)</lit></cell>
	  <cell><lit>y(t) = x(t) - x(t-1)</lit></cell>
	</row>
	<row>
	  <cell><lit>y = ldiff(x)</lit></cell>
	  <cell><lit>y(t) = log x(t) - log x(t-1)</lit>, o
	    rácio de crescimento instantâneo de <lit>x</lit></cell>
	</row>
	<row>
	  <cell><lit>y = sort(x)</lit></cell>
	  <cell>ordena <lit>x</lit> por ordem crescente e guarda em
	    <lit>y</lit></cell>
	</row>
	<row>
	  <cell><lit>y = dsort(x)</lit></cell>
	  <cell>ordena <lit>x</lit> por ordem decrescente</cell>
	</row>
	<row>
	  <cell><lit>y = int(x)</lit></cell>
	  <cell>guarda a parte inteira de <lit>x</lit> em 
	    <lit>y</lit></cell>
	</row>
	<row>
	  <cell><lit>y = abs(x)</lit></cell>
	  <cell>guarda os valores absolutos de <lit>x</lit></cell>
	</row>
	<row>
	  <cell><lit>y = sum(x)</lit></cell>
	  <cell>soma os valores de <lit>x</lit> excluíndo entradas <lit>NA</lit>
	    de valores omissos</cell>
	</row>
	<row>
	  <cell><lit>y = cum(x)</lit></cell>
	  <cell>acumulado: 
		<equation status="inline"
		  tex="$y_t = \sum_{\tau=1}^t x_{\tau}$"
		  ascii="y(t) = a soma de s=1 a s=t de x(s)"
		  graphic="cumulate"/>
	  </cell>
	</row>
	<row>
	  <cell><lit>aa = $ess</lit></cell>
	  <cell>define <lit>aa</lit> igual ao Erro da Soma de Quadrados
	    da última regressão</cell>
	</row>
	<row>
	  <cell><lit>x = $coeff(sqft)</lit></cell>
	  <cell>obtém o coeficiente estimado da variável 
	    <lit>sqft</lit> da última regressão</cell>
	</row>
	<row>
	  <cell><lit>rho4 = $rho(4)</lit></cell>
	  <cell>obtém o coeficiente autoregressivo de quarta-ordem
	    do último modelo (assume um modelo <lit>ar</lit>)</cell>
	</row>
	<row>
	  <cell><lit>cvx1x2 = $vcv(x1, x2)</lit></cell>
	  <cell>obtém a covariância estimada dos coeficientes das
	     variáveis <lit>x1</lit> e <lit>x2</lit> do último modelo</cell>
	</row>
	<row>
	  <cell><lit>foo = uniform()</lit></cell>
	  <cell>variável pseudo-aleatória uniforme no intervalo
	     0&ndash;1</cell>
	</row>
	<row>
	  <cell><lit>bar = 3 * normal()</lit></cell>
	  <cell>variável pseudo-aleatória normal, &mu; = 0, &sigma; =
	    3</cell>
	</row>
	<row>
	  <cell><lit>samp = ok(x)</lit></cell>
	  <cell>= 1 para observações onde <lit>x</lit> não está
	    ausente.</cell>
	</row>
      </table>

    </description>

    <gui-access>
      <menu-path>/Acrescentar/Definir nova variável</menu-path>
      <other-access>Menu de contexto da janela principal</other-access>
    </gui-access>

  </command>

  <command name="genrand" section="Programming" context="gui"
    label="Gerador de variáveis aleatórias">

    <description>
      <para>
	Neste diálogo você tem que fornecer um nome para a variável
	a ser criada, mais alguma informação adicional depedendo da
	distribuição.
      </para>

      <ilist>
	<li>
	  <para>
	    Uniforme: os limites inferior e superior para a distribuição.
	  </para>
	</li>
	<li>
	  <para>
	    Normal: a média e o desvio padrão (positivo).
	  </para>
	</li>
	<li>
	  <para>
	    Qui-quadrado e t de Student: os graus de liberdade, que têm que ser
	    positivos.
	  </para>
	</li>
	<li>
	  <para>
	    F: numerador, denonimador e o grau de liberdade.
	  </para>
	</li>
	<li>
	  <para>
	    gama: parâmetros de forma e de escala (ambos positivos).
	  </para>
	</li>
	<li>
	  <para>
	    Binomial: a probabilidade de <quote>sucesso</quote> e o
	    inteiro positivo para o número de experiências.
	  </para>
	</li>
	<li>
	  <para>
	    Poisson: a média positiva (que também é igual à variância).
	  </para>
	</li>
      </ilist>

      <para>
	Se você quiser gerar sequências repetíveis de números pseudo-aleatórios,
	você pode definir a semente no menu Ferramentas.
      </para>

    </description>
  </command>

  <command name="genseed" section="Programming" context="gui"
    label="Definindo a semente dos números aleatórios">

    <description>
      <para>
	A "semente" controla o ponto inicial da sequência dos
	números pseudo-aleatórios gerados num dada sessão gretl.  Por
	omissão a semente é definida usando a hora do sistema, quando
	o programa é iniciado.  Isto garante que você obtém uma sequência
	diferente de número aleatórios de cada vez que iniciar o programa.
	Se você quiser obter sequências repetíveis, você precisa de
	definir manualmente a semente (tomando nota do valor utilizado).
      </para>
      <para>
	Note que sempre que você clicar "OK" nesta janela de diálogo,
	o gerador é reiniciado usando a semente fornecida.  Portanto,
	por exemplo, se você (a) definir a semente para (digamos) 147;
	(b) gerar uma série com distribuição normal; (c) voltar a esta
	janela de diálogo e clicar "OK" outra vez com a semente ainda a
	147; e depois (d) gerar uma segunda série com a distribuição
	normal, as duas séries serão iguais.
      </para>
    </description>
  </command>

  <command name="gmm" section="Estimation" label="Estimação GMM">

    <usage>
      <options>
	<option>
	  <flag>--two-step</flag>
	  <effect>estimação em duas fases</effect>
	</option>
	<option>
	  <flag>--iterate</flag>
	  <effect>GMM iterado</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das iterações</effect>
	</option>
	<option>
	  <flag>--lbfgs</flag>
	  <effect>usar L-BFGS-B em vez do normal BFGS</effect>
	</option>
      </options>
    </usage>

    <description>

      <para>
	Faz estimação usando o Método dos Momentos Generalizado, 
	'Generalized Method of Moments' (GMM) com o algoritmo 
	BFGS (Broyden, Fletcher, Goldfarb, Shanno). Você tem que
	especificar um ou mais comandos para a atualização das quantidades
	relevantes (tipicamente os resíduos GMM), um ou mais conjuntos
	das condições, uma matriz inicial dos pesos, e uma listagem dos
	parâmetros a serem estimados, tudo entre os marcadores 
	<lit>gmm</lit> e <lit>end gmm</lit>. Quaisquer opções devem ser
	acrescentadas à linha <lit>end gmm</lit> .
      </para>
      <para>
	Por favor veja mais detalhes sobre este comando em <guideref targ="chap:gmm"/>.
	Aqui apenas ilustramos com um exemplo simples.
      </para>
      <code>
	gmm e = y - X*b
	  orthog e ; W
	  weights V
	  params b
	end gmm
      </code>
      <para>
	No exemplo acima nós assumimos que <lit>y</lit> e <lit>X</lit>
	são matrizes, <lit>b</lit> é um vetor de tamanho apropriado dos
	valores dos parâmetros, <lit>W</lit> é a matriz dos instrumentos,
	e <lit>V</lit> é uma matriz adequada de pesos.  A declaração
      </para>
      <code>
	orthog e ; W
      </code>
      <para>
	indica que o vetor dos resíduos <lit>e</lit> é em princípio
	ortognal a cada um dos instromentos que compõem as colunas de
	<lit>W</lit>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/GMM</menu-path>
    </gui-access>

  </command>

  <command name="gnuplot" section="Graphs" 
    label="Criar um gráfico gnuplot" context="cli">

    <usage>
      <arguments>
        <argument>variáveis-y</argument>
        <argument>variável-x</argument>
	<argument optional="true">variável-auxiliar</argument>
      </arguments>
      <options>
        <option>
	  <flag>--with-lines</flag>
	  <optparm optional="true">especificação-de-variáveis</optparm>
	  <effect>usar linhas, e não pontos</effect>
        </option>
        <option>
	  <flag>--with-lp</flag>
	  <optparm optional="true">especificação-de-variáveis</optparm>
	  <effect>usar linhas e pontos</effect>
        </option>
        <option>
	  <flag>--with-impulses</flag>
	  <optparm optional="true">especificação-de-variáveis</optparm>
	  <effect>usar linhas verticais</effect>
        </option>
        <option>
	  <flag>--time-series</flag>
	  <effect>gráfico temporal</effect>
        </option>
        <option>
	  <flag>--suppress-fitted</flag>
	  <effect>não mostrar a linha ajustada</effect>
        </option>
        <option>
	  <flag>--single-yaxis</flag>
	  <effect>forçar o uso de apenas um eixo y</effect>
        </option>
        <option>
	  <flag>--linear-fit</flag>
	  <effect>mostrar o ajustamento por mínimos quadrados</effect>
        </option>
        <option>
	  <flag>--inverse-fit</flag>
	  <effect>mostrar o ajustamento inverso</effect>
        </option>
        <option>
	  <flag>--quadratic-fit</flag>
	  <effect>mostrar o ajustamento quadrático</effect>
        </option>
	<option>
	  <flag>--cubic-fit</flag>
	  <effect>mostrar o ajustamento cúbico</effect>
        </option>
        <option>
	  <flag>--loess-fit</flag>
	  <effect>mostrar o ajustamento loess</effect>
        </option>
        <option>
	  <flag>--semilog-fit</flag>
	  <effect>mostrar o ajustamento semilog</effect>
        </option>
        <option>
	  <flag>--dummy</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>nome</optparm>
	  <effect>representar as colunas da matriz indicada</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>nome-de-ficheiro</optparm>
	  <effect>enviar a saída para o ficheiro especificado</effect>
        </option>
        <option>
	  <flag>--input</flag>
	  <optparm>nome-de-ficheiro</optparm>
	  <effect>obter entrada a partir do ficheiro especificado</effect>
        </option>
      </options>
      <examples>
        <example>gnuplot y1 y2 x</example>
        <example>gnuplot x --time-series --with-lines</example>
	<example>gnuplot wages educ gender --dummy</example>
	<example>gnuplot y x --fit=quadratic</example>
	<example>gnuplot y1 y2 x --with-lines=y2</example>
      </examples>
    </usage>

    <description>
      <para>
	As variáveis na lista <repl>variáveis-y</repl> são representadas
	contra a <repl>variável-x</repl>.  Para um gráfico de série 
	temporal você pode indicar <lit>tempo</lit> como sendo a 
	<repl>variável-x</repl> ou usar a opção <opt>--time-series</opt>.
      </para>
      <para>
	Por omissão os dados são representados como pontos; isto pode ser
	alterado com o uso de uma das opções <opt>--with-lines</opt>, 
	<opt>--with-lp</opt> ou	<opt>--with-impulses</opt>. Se for representada
	mais que uma variável no eixo dos <math>y</math>, o efeito destas opções
	pode ficar confinada a uma subconjunto de variáveis usando o parâmetro
	<repl>especificação-de-variáveis</repl>. Isto deve tomar a forma de uma
	lista separada por vírgulas dos nomes ou números das variáveis a serem
	representadas por linhas ou impulsos respetivamente. O último exemplo 
	mostrado acima, mostra como fazer um gráfico de <lit>y1</lit> e 
	<lit>y2</lit> contra <lit>x</lit>, de modo a que <lit>y2</lit> é representada por
	uma linha, mas <lit>y1</lit> é por pontos.
      </para>
      <para>
	Se a opção <opt>--dummy</opt> tiver sido indicada, terão que ser
	dadas exatamente três variáveis: uma única variável <math>y</math>,
	uma variável <math>x</math>, e uma variável 
	<repl>variável-auxiliar</repl>, uma variável discreta.  O efeito é
	o de representar as <repl>variáveis-y</repl> contra <repl>variável-x</repl>
	com os pontos mostrados com diferentes cores dependendo do valor da
	<repl>variável-auxiliar</repl> na respectiva observação.
      </para>

      <para context="cli">
	Geralmente, as <repl>variáveis-y</repl> e <repl>variável-x</repl> 
	referem-se a séries no conjunto de dados corrente (tanto referenciadas
	por nome como por número ID).  Mas se o nome de uma matriz for indicado
	com a opção <opt>--matrix</opt> estes argumentos (que têm que ser dados
	como valores numéricos) indicam indices de colunas (iniciados em 1) para
	a matriz fornecida. Assim, por exemplo, se você quiser um gráfico X-Y
	da coluna 2 da matriz <lit>M</lit> contra a coluna 1, você deve usar:
      </para>
      <code context="cli">
	gnuplot 2 1 --matrix=M
      </code>
      <para>
	Em modo interativo o gráfico é apresentado imediatamente. Em modo de 
	sequência de comandos o comportamento por omissão é o de criar um 
	ficheiro de comandos gnuplot na directoria de trabalho do utilizador,
	com um nome seguindo o padrão <filename>gpttmpN.plt</filename>, 
	iniciando com N = <lit>01</lit>. Os gráficos podem ser depois gerados
	usando o programa <program>gnuplot</program> (em MS Windows,
	 <program>wgnuplot</program>).  Este comportamento pode ser modificado
	com o uso da opção <opt>--output=</opt><repl>nome-de-ficheiro</repl>.
	Esta opção controla o nome do ficheiro usado, e ao mesmo tempo permite-lhe
	especificar um formato de saída de acordo com a extensão de três letras
	no nome do ficheiro, sendo: <lit>.eps</lit> resultante na produção de um
	ficheiro 'Encapsulated PostScript' (EPS); <lit>.pdf</lit> produz PDF; 
	<lit>.png</lit> produz no formato PNG, <lit>.emf</lit> em formato EMF
	('Enhanced MetaFile'), <lit>.fig</lit> no formato Xfig, e <lit>.svg</lit>
	para o formato SVG ('Scalable Vector Graphics'). Se usado o nome de ficheiro
	<quote><lit>display</lit></quote> o gráfico é apresentado no écran tal como
	em modo interativo. Se o nome de ficheiro tiver outra qualquer extensão que
	não as mencionadas, será escrito um ficheiro de comandos gnuplot.
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>linear</lit>: show the OLS fit regardless of its
	    level of statistical significance.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>none</lit>: don't show any fitted line.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>inverse</lit>, <lit>quadratic</lit>,
	    <lit>cubic</lit>, <lit>semilog</lit> or <lit>linlog</lit>:
	    show a fitted line based on a regression of the specified
	    type. By <lit>semilog</lit>, we mean a regression of log
	    <math>y</math> on <math>x</math>; the fitted line
	    represents the conditional expectation of <math>y</math>,
	    obtained by exponentiation. By <lit>linlog</lit> we mean a
	    regression of <math>y</math> on the log of <math>x</math>.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>loess</lit>: show the fit from a robust locally
	    weighted regression (also is sometimes known as
	    <quote>lowess</quote>).
	  </para>
	</li>	
      </ilist>
      <para>
	As várias opções de <quote>ajustamento</quote> são aplicáveis
	apenas nos caso de gráficos de dispersão bivariados e nos de uma
	única série-temporal. O comportamento por omissão para um gráfico
	de dispersão é o de mostrar a linha de ajustamento de mínimos
	quadrados se e só se o coeficiente do declive fôr significativo
	num nível de 10 porcento. Se a opção <opt>--suppress</opt>
	tiver sido dada, não será mostrada a linha ajustada. O comportamento
	por omissão para um gráfico de série-temporal é o de não mostrar a 
	linha de ajustamento.
	Se a opção <opt>--linear</opt> fôr dada, a linha mínimos quadrados
	será mostrada independentemente de ser significativa ou não.  As outras
	opções de ajustamento (<opt>--inverse</opt>, <opt>--quadratic</opt>, 
	<opt>--cubic</opt>, <opt>--loess</opt> e <opt>--semilog</opt>) produzem 
	respetivamente um ajustamento inverso (regressão de <math>y</math> sobre
	 1/<math>x</math>), um ajustamento quadrático, um ajustamento cúbico, um
	 ajustamento loess e um ajustamento semilog. Loess (também por vezes chamado
	<quote>lowess</quote>) é uma regressão robusta com pesos locais. Por semilog, 
	nós designamos uma regressão do logaritmo de <math>y</math> sobre <math>x</math>
	(ou tempo); a linha ajustada representa o <math>y</math> esperado condicionalmente,
	obtido por exponenciação.
      </para>
      <para>
	Uma outra opção está disponível para este comando:
	a seguir às especificações das variáveis a serem representadas
	e das opções (caso hajam), você pode acrescentar comandos 
	gnuplot para controlar a aparência do gráfico (por exemplo,
	para definir o título e/ou as escalas dos eixos).  Estes comandos
	devem ser colocados dentro de chavetas, e cada comando gnuplot
	tem que ser terminado com um ponto-e-vírgula.  Um '\' pode ser
	usado para continuar uma conjunto de comandos gnuplot por mais que
	uma linha.
	Aqui está um exemplo da sintaxe:
      </para>

      <para>
	<lit>{ set title 'O Meu Título'; set yrange [0:1000]; }</lit>
      </para>

    </description>

    <gui-access>
      <menu-path>/Ver/Gráfico das variáveis</menu-path>
      <other-access>Menu de contexto na janela principal, botão de gráfico na barra de ferramentas</other-access>
    </gui-access>

  </command>

  <command name="graphing" section="Graphs" context="gui"
    label="Representação Gráfica">

    <description>

      <para>Gretl chama um programa separado, designadamente gnuplot,
	para gerar gráficos.  Gnuplot é um programa de gráficos
	completo com muitas de opções.  Gretl dá-lhe acesso direto,
	por intermédio de um interface gráfico, a um subconjunto destas
	opções e tenta escolher valores adequados para si;  também
	permite-lhe controlar completamente os detalhes dos gráficos
	se assim o desejar.</para>

      <para>Com um gráfico apresentado, você pode clicar na janela do
	gráfico para aceder a um menu de contexto com as seguintes opções:
      </para>

      <ilist>
	<li><para>Gravar como 'postscript' (EPS): grava o gráfico no formato
	    'postscript' encapsulado (EPS)</para>
	</li>
	<li><para>Gravar como PNG: grava o gráfico no formato 
	    'Portable Network Graphics'</para>
	</li>
	<li><para>Gravar como PDF: grava o gráfico no formato 
	    'Portable Document Format'</para>
	</li>
	<li><para>Gravar como metaficheiro do Windows (EMF): grava o gráfico no formato 
	    'Enhanced Metafile Format', a cores ou monocromático</para>
	</li>
	<li><para>Guardar para a sessão como ícone: o gráfico aparecerá
	    na forma de ícone quando você selecionar <quote>Ver/Por Ícones</quote> do
            menu da janela principal</para>
	</li>
	<li><para>Ampliar: deixa você selecionar uma área dentro do gráfico
	para melhor inspeção. Depois pode substituir o gráfico inicial ou restaurar
	a vista normal.</para>
	</li>
	<li><para>Imprimir: (apenas no ambiente gráfico Gnome e no MS Windows)
	    permite-lhe imprimir o gráfico diretamente</para>
	</li>
	<li><para>Copia para a memória de edição: (apenas no MS Windows) permite-lhe
	    colar o gráfico em aplicações Windows como seja o MS Word</para>
	</li>
	<li><para>Editar: abre uma janela de controlo do gráfico que 
	    lhe permite ajustar os vários aspetos da sua aparência
	    </para>
	</li>
	<li><para>Fechar: fecha a janela do gráfico</para>
	</li>
      </ilist>

      <para>
	Se você conhece algo sobre o gnuplot e deseja obter melhor controlo
	sobre a aparência sobre o gráfico do que a que é disponibilizada
	no controlador gráfico (<quote>Editar</quote> option), você tem
	outras opção:
      </para>

      <ilist>
	<li>
	  <para>
	    Assim que o gráfico é guardado para a sessão como ícone, você pode clicar com
	    o botão direito sobre o ícone para aceder a um menu de contexto.  Uma das
	    opções é <quote>Editar comandos do gráfico</quote>, o que abre uma janela
            de edição com os comandos gnuplot utilizados nesse gráfico. Você pode editar
	    esses comandos e gravá-los para uso futuro ou enviar para o gnuplot (com o ícone
	    de execução na barra de ferramentas da janela de edição).
	  </para>
	</li>
	<li>
	  <para>
	    Another way to save the plot commands (or to save
	    the displayed plot in formats other than EPS or PNG) is to
	    use <quote>Edit</quote> item on a graph's pop-up menu to
	    invoke the graphical controller, then click on the
	    <quote>Output to file</quote> tab in the controller.  You
	    are then presented with a drop-down menu of formats in
	    which to save the graph.
	  </para>
	</li>
      </ilist>

      <para>
	Para saber mais sobre gnuplot, ver http://www.gnuplot.info
      </para>

    </description>

  </command>

  <command name="graphpg" section="Graphs" label="Página dos gráficos de Gretl">

    <usage>
      <altforms>
        <altform><lit>graphpg add</lit></altform>
	<altform><lit>graphpg fontscale </lit><repl>value</repl></altform>
	<altform><lit>graphpg show</lit></altform>
	<altform><lit>graphpg free</lit></altform>
	<altform><lit>graphpg --output=</lit><repl>filename</repl></altform>
      </altforms>
    </usage>

    <description>

      <para>
	A <quote>página dos gráficos</quote> de sessão apenas funcionará se você
	tiver instalado o sistema de produção de texto &latex;, e puder gerar e 
	visionar documentos PDF ou PostScript.
      </para>
      <para>
	Na janela de sessão por ícones, você pode arrastar até oito gráficos
	para dentro de um ícone de página de gráficos.  Quando você fizer
	duplo-clique na página de gráficos (ou com o botão direito e selecionar 
	<quote>Mostrar</quote>), será produzida uma página com os gráficos
	selecionados e apresentada no visionador adequado.  A partir deste você
	poderá imprimir a página.
      </para>
      <para>
	Para limpar a página de gráficos, clicar com o botão direito no seu ícone e
	selecionar <quote>Limpar</quote>.
      </para>
      <para>
	Note que em sistemas diferentes do MS Windows, você pode ter que
	ajustar as definições dos programas usados para visionar documentos
	PDF ou PostScript. Isso encontra-se dentro do separador <quote>Programas</quote>
	na janela de diálogo das Preferências do gretl (a partir do menu Ferramentas da
	janela principal).
      </para>
      <para>
	Também é possível trabalhar com a página de gráficos a partir
	se sequência de comandos, ou usando a consola (dentro do programa em
	ambiente gráfico). São suportados os seguintes comandos e opções:
      </para>
      <para>
	Para acrescentar um gráfico à página de gráficos, dê o comando
	<lit>graphpg add</lit> depois de o ter gravado como um gráfico com
	nome, tal como
      </para>
      <code>
	grf1 &lt;- gnuplot Y X
	graphpg add
      </code>
       <para>
	Para ver a página de gráficos: <lit>graphpg show</lit>.
      </para>
      <para>
	Para limpar a página de gráficos: <lit>graphpg free</lit>.
      </para>
      <para>
	Para ajustar a escala da fonte usada na página de gráficos, use
	<lit>graphpg fontscale</lit> <repl>escala</repl>, onde
	<repl>escala</repl> é um multiplicador (com o valor 1,0 por omissão).
	Assim, para tornar a o fonte 50 porcento maior que a por inicial você
	pode
      </para>
      <code>
	graphpg fontscale 1.5
      </code>
      <para>
	Para chamar a impressão da página de gráficos para um ficheiro, use a opção
	<opt>output=</opt> mais um nome de ficheiro; o nome do ficheiro deverá ter 
	o sufixo <quote><lit>.pdf</lit></quote>,
	<quote><lit>.ps</lit></quote> ou
	<quote><lit>.eps</lit></quote>. Por exemplo:
      </para>
      <code>
	graphpg --output="meu_ficheiro.pdf"
      </code>
      <para>
	Neste contexto, por omissão o resultado usa linhas coloridas; para
	usar padrões ponto/traço em vez de cores, você pode acrescentar a opção
	<opt>monochrome</opt>.
      </para>

    </description>

  </command>

  <command name="3-D" section="Graphs" context="gui"
    label="Gráficos tridimensionais">

    <description>
      <para>Caso você tenha instalado o gnuplot 3.8 ou superior, pode 
	pode aproveitar a funcionalidade de manipulação de gráficos
	3-D com o rato (rodá-los e expandir ou encolher os eixos).</para>

      <para>Ao criar um gráfico 3-D, note que o eixo Z será mostrado
	como sendo o eixo vertical.  Como tal, se você tiver alguma
	variável dependente que pense poder ser influenciada por duas
	variáveis independentes, você deve colocar a variável dependente
	no eixo Z, e as variáveis independentes nos eixos X e Y.</para>  

      <para>Contrariamente à maior parte dos gráficos de gretl, os gráficos
	3-D são controlados por gnuplot e não pelo gretl.  O menu de edição
	de gráficos do gretl não estará disponível.</para>

    </description>
  </command>

  <command name="gui-funcs" section="Programming" 
	   label="Funções especiais" context="gui">
    <description>
      <para>
	Este diálogo permite-lhe especificar quais as funções de um
	pacote, deverão ter certos papéis especiais.
	Note que uma certa função pode ter apenas um dos seguintes
	papéis, e para se qualificarem como candidatas para um destes
	papéis as funções tem que satisfazer certos critérios.
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>bundle-print</lit>: imprime saídas baseadas no
            conteúdo de um "bundle" produzido pelo seu pacote.
	    Critério: esta função tem que ter como o seu primeiro
            parâmetro um ponteiro-"bundle". Se estiver presente
            um segundo parâmetro ele tem que tomar a forma de
            seletor inteiro que tenha um valor por omissão.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>bundle-plot</lit>: produz um ou mais gráficos usando
            um "bundle" produzido pelo seu pacote. Critério: o mesmo que
	    para <lit>bundle-print</lit>.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>bundle-test</lit>: efetua algum tipo de teste estatístico
            usando um "bundle" produzido pelo seu pacote. Critério: o mesmo que
	    para <lit>bundle-print</lit>.
	  </para>
	</li>	
	<li>
	  <para>
	    <lit>gui-main</lit>: o interface que deve ser apresentado
            aos usuários por omissão quando usado em modo gráfico (GUI).
            Isto é útil apenas quando um pacote tem mais que um interface
            público.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>gui-precheck</lit>: função guardião que retorna
	    0 se a funcionalidade do seu pacote é aplicável no
            contexto corrente, e não-zero caso contrário. Isto destina-se
            a ser utilizado com pacotes que operam num modelo de algum modo,
            para descartar tipos de modelos que não são suportados pelo pacote.
	  </para>
	</li>	
      </ilist>
      <para>
	In addition certain functions may be marked as
	<quote>no-print</quote>. Usually, when a function is invoked
	via the GUI program, gretl opens a window to display its text
	output. By checking this box you are telling gretl not to do
	this, since no text output should be expected.
      </para>
      <para>
	Finally, the <lit>gui-main</lit> function (if any) can be
	marked as <quote>menu-only</quote>. This tells gretl that the
	function in question is specifically designed to be called
	from the GUI menu to which it is attached, and should not be
	presented to users otherwise.
      </para>
    </description>
  </command>

  <command name="gui-htest" section="Tests" context="gui"
    label="Calculador de Estatísticas de Teste">

    <description>
      <para>
	O calculador de estatísticas de teste do gretl determina estatísticas
	de teste e valores p para diversos testes de hipóteses para uma ou duas
        populações.  As entradas necessárias tomam a forma de estatísticas amostrais
        derivadas de uma ou duas amostras, dependendo do teste escolhido.  Estas
        estatísticas podem ser indicadas como valores numéricos.  Em alternativa,
        se você tiver um conjunto de dados aberto, você pode usar gretl para calcular
        estatísticas amostrais para uma ou mais variáveis selecionadas (no caso de
        médias e variâncias, mas não no caso de proporções).
      </para>

      <para>
	Se você quiser basear o seu teste numa variável do conjunto de dados,
        primeiro ative esta opção selecionando a caixa intitulada "Usar variável do
        conjunto de dados".  De seguida o seletor de variáveis fica ativo e você pode
        selecionar a variável.  Quando seleciona uma variável a partir da lista,
        as estatísticas relevantes são automaticamente colocadas nas caixas abaixo.
      </para>

      <para>
	Adicionalmente, além da simples seleção de uma variável, você tem a 
        opção de especificar uma restrição para a variável selecionada (isto é,
        definir uma sub-amostra).  Por exemplo, suponhamos que você tem
	informação de salários na variável "salario" e que também tem uma
	variável auxiliar chamada "sexo" que é igual a 1 para masculino e 0
        para feminino (ou vice versa). Então, no teste para as diferenças de 
        duas médias, você pode selecionar "salario" em ambos os campos, mas
        acrescentar ao campo do topo "(sexo=0)" e ao de baixo "(sexo=1)".
	Isto resultaria num teste para as diferenças entre rendimentos dos
        homens e rendimentos das mulheres.  Note que quando você usa uma
        restrição desta forma, você tem que pressionar a tecla "Enter" para
        obter o cálculo das estatísticas amostrais.  
      </para>

      <para>
	A restrição de sub-amostragem pode ser colocada dentro de parêntesis após
        a variável selecionada, e em geral a restrição toma a forma "var2
	op val", onde var2 é o nome de uma variável no conjunto de dados corrente,
        val é um valor numérico, e op é um operador de comparação escolhido dentre
        =, !=, &lt;, &gt;, &lt;= ou &gt;= (respetivamente igualdade, desigualdade, menor
	que, maior que, menor ou igual, e maior ou igual).  Os espaços separando o operador
        são opcionais.
      </para>

    </description>
  </command>

  <command name="gui-htest-np" section="Tests" context="gui"
    label="Testes Não-paramétricos">

    <description>
      <para>
	No separador <quote>Teste da Diferença</quote> você pode efetuar
        o teste não-paramétrico para a diferença entre duas populações ou
        grupos, o teste específico depende da opção selecionada.
      </para>
      <para>
	Teste dos sinais: Este teste é baseado no fato de que se duas amostras,
	<math>x</math> e <math>y</math>, são obtidas aleatoriamente a partir da
        mesma distribuição, a probabilidade de
	<math>x</math><sub>i</sub> &gt;
	<math>y</math><sub>i</sub>, para cada observação
	<math>i</math>, deve ser igual a 0,5.  A estatística de teste é
	<math>w</math>, o número de observações em que
	<math>x</math><sub>i</sub> &gt;
	<math>y</math><sub>i</sub>. Sob a hipótese nula isto segue uma
        distribuição Binomial com parâmetros
	(<math>n</math>; 0,5), onde <math>n</math> é o número de
        observações.
      </para>
      <para>
	Teste ordinal da soma de Wilcoxon: É executado o teste ordinal da 
        soma de Wilcoxon.  Este teste inicia por ordenar as observações da
        amostra conjunta do menor para o maior, depois obtém a soma das
        ordens para uma das amostras.  As duas amostras não têm que ser da
        mesma dimensão, e se elas forem diferentes será usada a de menor
        tamanho para calcular a soma das ordens.  Sob a hipótese nula de que
        as amostras foram obtidas a partir de populações com igual mediana,
        a distribuição de probabilidade da soma de ordens pode ser calculada
        para quaisquer dimensões de amostras; e para amostras suficientemente
        grandes existe uma aproximação à Normal.
      </para>
      <para>
	Teste ordinal dos sinais de Wilcoxon: É executado o teste ordinal dos 
        sinais de Wilcoxon. Isto é designado pelo emparelhamento de dados de
        modo a que, por exemplo, os valores de uma variável de uma amostra de
        indivíduos antes e depois de algum tratamento.   O teste inicia por
        encontrar as diferenças entre as observações emparelhadas, 
        <math>x</math><sub>i</sub> &minus; <math>y</math><sub>i</sub>, ordenando
        estas diferenças por valor absoluto, depois atribuindo a cada par uma
        ordem com sinal, este sinal concorda com o sinal da diferença.
	Em seguida é calculado <math>W</math><sub>+</sub>, a soma das ordens com
        sinal positivo.  Tal como no teste ordinal da soma, esta estatística tem 
        uma distribuição bem-definida sob a hipótese nula de que a diferença de
        medianas é zero, o que converge para a Normal em amostras de tamanho 
        razoável.
      </para>
      <para>
	No separador <quote>Teste de Aleatoriedade</quote> você pode efetuar
        um teste para a aleatoriedade de uma dada variável, baseada no número
        de sequências consecutivas de valores positivos ou negativos.  Se você
        selecionar a opção <quote>Usar a primeira diferença</quote>, a variável
        é diferenciada antes da análise e como tal as sequências são interpretadas
        como sendo sequências crescentes ou decrescentes de valores da variável
        original.  A estatística de teste é baseada na aproximação normal à
        distribuição do número de sequências segundo a nulidade de aleatoriedade.
      </para>

    </description>
  </command>   
 
  <command name="hausman" section="Tests" label="Diagnósticos de Painel">

    <description>
      <para>
	Este teste apenas está disponível após e estimar um modelo de
        mínimos quadrados (OLS) usando dados de painel (ver também
        <cmd>setobs</cmd>).  Ele testa o modelo de amostragem simples
        ("pooled") contra as alternativas principais, os modelos de efeitos
        fixos e efeitos aleatórios.
      </para>

      <para>
	O modelo de efeitos fixos permite variar a interseção da regressão
        ao longo das unidades de seção cruzada.  Uma estatística 
        teste-<math>F</math> é apresentada segundo a hipótese nula de que
        as interseções não diferem.  O modelo de efeitos aleatórios
        decompõe a variância dos resíduos em duas partes, uma parte 
        específica à unidade de seção cruzada e outra específica para
        a observação em particular. (Este estimador pode ser calculado
        apenas se o número de unidades de seção cruzada nos dados exceder o
        número de parâmetros a serem estimados.) A estatística de teste 
        Breusch&ndash;Pagan LM, testa a hipótese nula de que o estimador
        mínimos quadrados de amostragem ("pooled") é adequado em oposição ao
        da alternativa de efeitos aleatórios.
      </para>

      <para>
	O modelo mínimos quadrados de amostragem ("pooled") pode ser
        rejeitado contra ambas as alternativas, efeitos fixos e efeitos
        aleatórios.  Desde que o erro específico por unidade ou por grupo seja não
        correlacionado com as variáveis independentes, o estimador de 
        efeitos aleatórios é mais eficiente do que o estimador de efeitos
        fixos; caso contrário o estimador de efeitos aleatórios é 
        inconsistente e o estimador de efeitos fixos será preferido.  A
        hipótese nula para o teste de Hausman é de que o erro específico de grupo
        não é tão correlacionado (e como tal o modelo de efeitos aleatórios é
        preferível).  Um valor p baixo para este teste conta contra o modelo de
        efeitos aleatórios e a favor do modelo de efeitos fixos.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Diagnósticos de Painel</menu-path>
    </gui-access>

  </command>

  <command name="hccme" section="Estimation" context="gui"
    label="Erros padrão robustos">

    <description>
      <para>
	Você tem disponível várias variantes de cálculo para erros
        padrão que são robustos na presença de heteroscedasticidade
	(e, no caso do estimador HAC, autocorrelação).
      </para>
      <para>
	HC0 produz os <quote>erros padrão de White</quote> originais;
	HC1, HC2, HC3 e HC3a são variações subsequentes que em geral
        são reconhecidas de produzirem melhores (mais fiáveis) resultados.
	Para detalhes dos estimadores, ver <cite
	key="mackinnon-white85">MacKinnon and White (Journal of
	Econometrics, 1985)</cite> ou <cite
	key="davidson-mackinnon04">Davidson and MacKinnon, Econometric
	Theory and Methods (Oxford, 2004)</cite>.  As etiquetas aqui usadas
        são as que Davidson e MacKinnon usaram.  A variante
	<quote>HC3a</quote> é o canivete ("jackknife"), como descrita em MacKinnon
	e White; HC3 é uma boa aproximação ao canivete.
      </para>
      <para>
	Se você usar o estimador HAC para OLS em dados de séries temporais,
        você pode afinar o comprimento de desfasamento usando o comando 
       <cmd>set</cmd>.  Por favor, para mais detalhes, ver o manual do
        arquivo de ajuda das sequências de comandos.
      </para>
      <para>
	Ao se estimar um modelo OLS usando dados de painel, o estimador
	robusto por omissão da matriz de covariância é o dado por Arellano. 
        A alternativa são os Erros Padrão Corrigidos de Painel de 
        Beck e Katz (PCSE).  Este tem em conta a heteroscedasticidade mas
        não a autocorrelação.  
      </para>
      <para>
	Para modelos GARCH são fornecidos dois estimadores robustos para a 
        matriz de covariância: o estimador de Verosimilhança Quasi-Maximum
        (QML), e estimador de Bollerslev-Wooldridge (BW).
      </para>
      <para>
	By default gretl uses the Student <math>t</math> distribution
	when calculating p-values based on robust standard errors in
	the context of least squares estimators. The option labeled
	<quote>Use the normal distribution for robust p-values</quote>
	can be used to change this behavior.
      </para>
    </description>

  </command>

  <command name="heckit" section="Estimation" context="cli"
    label="Modelo de seleção Heckman">

    <usage>
      <arguments>
        <argument>variável-dependente</argument>
        <argument>variáveis-independentes</argument>
	<argument separated="true">equação de seleção</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>suprimir a escrita de resultados</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <effect>erros padrão QML</effect>
        </option>
        <option>
	  <flag>--two-step</flag>
	  <effect>efetuar estimação de dois passos</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar saídas adicionais</effect>
        </option>
      </options>      
      <examples>
        <example>heckit y 0 x1 x2 ; ys 0 x3 x4</example>
	<demos>
	  <demo>heckit.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Seleção do modelo de tipo Heckman.  Na especificação, a lista
        antes do ponto-e-vírgula representa a equação do resultado.  A 
        variável dependente na equação de seleção (<lit>ys</lit> no 
        exemplo acima) tem que ser uma variável binária.
      </para>
      <para>
	Por omissão, os parâmetros são estimados por máxima verosimilhança.
        A matriz de covariância dos parâmetros é calculada usando a 
        inversão negativa da Hessiana. Se for desejável a estimação,
        de dois passos, use a opção <opt>--two-step</opt>. Neste caso,
        a matriz de covariância dos parâmetros da equação resultante é
        adequadamente ajustada de acordo com <cite key="heckman79">Heckman (1979)</cite>.
      </para>
      <para>
	Repare que na estimação de Máxima Verosimilhança (ML) é usada uma
        aproximação numérica da Hessiana; isto pode levar a inexatidões na
        matriz de covariância estimada se a escala das variáveis explanatórias
        for para alguns dos coeficientes estimados muito pequena em 
        valor absoluto. Este problema pode ser abordado em versões futuras;
        por agora, como solução temporária, pode-se re-escalar a variável 
        ou variáveis explanatórias que estão a causar problemas.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Variável dependente limitada/Heckit...</menu-path>
    </gui-access>

  </command>

  <command name="help" section="Utilities" 
    label="Help on commands" context="cli">

    <usage>
      <altforms>
        <altform><lit>help</lit></altform>
	<altform><lit>help functions</lit></altform>
        <altform><lit>help</lit> <repl>command</repl></altform>
        <altform><lit>help</lit> <repl>function</repl></altform>
      </altforms>
      <options>
	<option>
	  <flag>--func</flag>
	  <effect>select functions help</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	If no arguments are given, prints a list of available commands. If the
	single argument <lit quote="true">functions</lit> is given, prints a
	list of available functions (see <cmdref targ="genr"/>).
      </para>
      <para>
	<lit>help</lit> <repl>command</repl> describes <repl>command</repl>
	(&eg; <lit>help smpl</lit>).  <lit>help</lit> <repl>function</repl> 
	describes <repl>function</repl> (&eg; <lit>help ldet</lit>).
	Some functions have the same names as related commands (&eg;
	<lit>diff</lit>): in that case the default is to print help
	for the command, but you can get help on the function by
	using the <opt>--func</opt> option.
      </para> 
    </description>

    <gui-access>
      <menu-path>/Help</menu-path>
    </gui-access>

  </command>

  <command name="hsk" section="Estimation"
    label="Heteroskedasticity-corrected estimates">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--no-squares</flag>
	  <effect>see below</effect>
	</option>	
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	This command is applicable where heteroskedasticity is present in the
	form of an unknown function of the regressors which can be
	approximated by a quadratic relationship.  In that context it offers
	the possibility of consistent standard errors and more efficient
	parameter estimates as compared with OLS.  
      </para>
      <para>
	The procedure involves (a) OLS estimation of the model of interest,
	followed by (b) an auxiliary regression to generate an estimate of the
	error variance, then finally (c) weighted least squares, using as
	weight the reciprocal of the estimated variance.
      </para>
      <para context="cli">
	In the auxiliary regression (b) we regress the log of the
	squared residuals from the first OLS on the original
	regressors and their squares (by default), or just on the
	original regressors (if the <opt>no-squares</opt> option is
	given).  The log transformation is performed to ensure that the
	estimated variances are all non-negative.  Call the fitted
	values from this regression <math>u</math><sup>*</sup>.  The
	weight series for the final WLS is then formed as
	1/exp(<math>u</math><sup>*</sup>).
      </para>
      <para context="gui">
	In the auxiliary regression (b) we regress the log of the
	squared residuals from the first OLS on the original
	regressors and their squares (by default), or just on the
	original regressors (if the <quote>include squares</quote> box
	is cleared).  The log transformation is performed to ensure
	that the estimated variances are all non-negative.  Call the
	fitted values from this regression <math>u</math><sup>*</sup>.
	The weight series for the final WLS is then formed as
	1/exp(<math>u</math><sup>*</sup>).
      </para>      
    </description>

    <gui-access>
      <menu-path>/Model/Other linear models/Heteroskedasticity corrected</menu-path>
    </gui-access>

  </command>

  <command name="hurst" section="Statistics"
    label="Hurst exponent">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Calculates the Hurst exponent (a measure of persistence or
	long memory) for a time-series variable having at least 128
	observations.
      </para>
      <para>
	The Hurst exponent is discussed by Mandelbrot.  In theoretical
	terms it is the exponent, <math>H</math>, in the
	relationship 
	<equation status="display" 
	  tex="\[\mathrm{RS}(x) = an^H\]" 
	  ascii="RS(x) = an^H" 
	  graphic="hurst"/>where RS is the <quote>rescaled
	  range</quote> of the variable <math>x</math> in
	samples of size <math>n</math> and <math>a</math>
	is a constant. The rescaled range is the range (maximum minus
	minimum) of the cumulated value or partial sum of
	<math>x</math> over the sample period (after subtraction
	of the sample mean), divided by the sample standard deviation.
      </para>
      <para>
	As a reference point, if <math>x</math> is white noise
	(zero mean, zero persistence) then the range of its cumulated
	<quote>wandering</quote> (which forms a random walk), scaled
	by the standard deviation, grows as the square root of the
	sample size, giving an expected Hurst exponent of 0.5.  Values
	of the exponent significantly in excess of 0.5 indicate
	persistence, and values less than 0.5 indicate
	anti-persistence (negative autocorrelation).  In principle the
	exponent is bounded by 0 and 1, although in finite samples it
	is possible to get an estimated exponent greater than 1.  
      </para>
      <para>
	In gretl, the exponent is estimated using binary sub-sampling:
	we start with the entire data range, then the two halves of
	the range, then the four quarters, and so on.  For sample
	sizes smaller than the data range, the RS value is the mean
	across the available samples.  The exponent is then estimated
	as the slope coefficient in a regression of the log of RS on
	the log of sample size.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Hurst exponent</menu-path>
    </gui-access>

  </command>

  <command name="if" section="Programming" label="Flow control" context="cli">

    <description>
      <para>Flow control for command execution.  Three sorts of
	construction are supported, as follows.
      </para>
      <code>
	# simple form
	if condition
	    commands
	endif

	# two branches
	if condition
	    commands1
	else
	    commands2
        endif

	# three or more branches
	if condition1
	    commands1
	elif condition2
	    commands2
	else
	    commands3
	endif
      </code>

      <para>
	<repl quote="true">condition</repl> must be a Boolean expression, for
	the syntax of which see <cmdref targ="genr"/>.  More than one
	<cmd>elif</cmd> block may be included.  In addition, <lit>if</lit>
	&hellip; <lit>endif</lit> blocks may be nested.
      </para>
    </description>

  </command>

  <command name="include" section="Programming" 
    label="Include function definitions" context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
      <examples>
        <example>include myfile.inp</example>
        <example>include sols.gfn</example>
      </examples>
    </usage>

    <description>
      <para>
	Intended for use in a command script, primarily for including
	definitions of functions.  Executes the commands in
	<repl>filename</repl> then returns control to the main script. To
	include a packaged function, be sure to include the filename
	extension.
      </para>
      <para>
	See also <cmdref targ="run"/>.
      </para>
    </description>

  </command>

  <command name="info" section="Dataset" 
    label="Information on data set" context="cli">

    <description>
      <para>
	Prints out any supplementary information stored with the
	current datafile.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Dataset info</menu-path>
      <other-access>Data browser windows</other-access>
    </gui-access>

  </command>

  <command name="install" section="Utilities" context="cli">

    <usage>
      <arguments>
        <argument>pkgname</argument>
      </arguments>
      <options>
        <option>
	  <flag>--local</flag>
	  <effect>install from local file</effect>
        </option>
        <option>
	  <flag>--remove</flag>
	  <effect>see below</effect>
        </option>	
        <option>
	  <flag>--purge</flag>
	  <effect>see below</effect>
        </option>
      </options>
     <examples>
        <example>install armax</example>
        <example>install felogit.gfn</example>
	<example>install /path/to/myfile.gfn --local</example>
	<example>install http://foo.bar.net/gretl/myfile.gfn</example>
     </examples>
    </usage>    

    <description>
      <para>
	Installer for gretl function packages (<lit>gfn</lit> or
	<lit>zip</lit> files).
      </para>
      <para>
	If this command is given the <quote>plain</quote> name of a
	gretl function package (as in the first two examples) the
	action is to download the specified package from the gretl
	server and install it on the local machine. In this case
	it is not necessary to supply a filename extension.
      </para>
      <para>
	If the <opt>--local</opt> option is given, the
	<repl>pkgname</repl> argument should be the path to an
	uninstalled package file on the local machine, with the
	correct extension. The action is to copy the file into place
	(<lit>gfn</lit>), or unzip it into place (<lit>zip</lit>),
	<quote>into place</quote> meaning where the <cmdref
	targ="include"/> command will find it.
      </para>
      <para>
	When no option is given, if <repl>pkgname</repl> begins with
	<lit>http://</lit>, the effect is to download a package file
	from a specified server and install it locally.
      </para>
      <para>
	With the <opt>--remove</opt> or <opt>--purge</opt> option the
	inverse operation is performed; that is, an installed package
	is uninstalled. If just <opt>--remove</opt> is given, the
	specified package is unloaded from memory and is removed from
	the GUI menu to which it is attached, if any. If the
	<opt>--purge</opt> option is given then in addition to the
	actions just mentioned the package file is deleted. (If the
	package is installed in its own subdirectory, the whole
	subdirectory is deleted.)
      </para>      
    </description>

    <gui-access>
      <menu-path>/Tools/Function packages/On server</menu-path>
    </gui-access>

  </command>  

  <command name="intreg" section="Estimation" label="Interval regression model">

    <usage>
      <arguments>
        <argument>minvar</argument>
        <argument>maxvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
      </options>
      <examples>
	<example>intreg lo hi const x1 x2</example>
	<demos>
	  <demo>wtp.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Estimates an interval regression model.  This model arises when the
	dependent variable is imperfectly observed for some (possibly all)
	observations.  In other words, the data generating process is assumed
	to be 
	<equation status="display"
	tex="\[y^*_t = x_t \beta+\epsilon_t\]" ascii="y* = x b + u"/> but we
	only observe 
	<equation status="inline" tex="\[m_t \le
	y_t \le M_t\]" ascii="m &lt;= y* &lt;= M"/> (the interval may be left-
	or right-unbounded). Note that for some observations <math>m</math>
	may equal <math>M</math>.  The variables <repl>minvar</repl> and
	<repl>maxvar</repl> must contain <lit>NA</lit>s for left- and
	right-unbounded observations, respectively.
      </para>

      <para context="gui">
	In the model specification dialog, <repl>minvar</repl> and
	<repl>maxvar</repl> are indentified as the Lower bound variable and
	the Upper bound variable respectively.
      </para>

      <para>
	The model is estimated by maximum likelihood, assuming normality of
	the disturbance term.
      </para>

      <para context="cli">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <opt>--robust</opt> flag is given,
	then QML or Huber&ndash;White standard errors are calculated
	instead. In this case the estimated covariance matrix is a
	<quote>sandwich</quote> of the inverse of the estimated Hessian
	and the outer product of the gradient.
      </para>
      <para context="gui">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the "Robust standard errors" box is
	checked, then QML or Huber&ndash;White standard errors are
	calculated instead. In this case the estimated covariance matrix
	is a <quote>sandwich</quote> of the inverse of the estimated
	Hessian and the outer product of the gradient.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Interval regression</menu-path>
    </gui-access>

  </command>

  <command name="irfboot" section="Graphs" context="gui"
    label="Impulse response plots">

    <description>
      <para>
	If you select the bootstrap option when plotting impulse
	responses, gretl computes a confidence interval for the responses
	using the bootstrap method.  The residuals from the original VAR
	(or VECM) are resampled with replacement; an artificial dataset is
	constructed based on the original parameter estimates and the
	resampled residuals; the system is re-estimated and the impulse
	responses are re-evaluated.  This is repeated 999 times and the
	&alpha;/2 and 1 &minus; &alpha;/2 quantiles for the responses are
	found and plotted along with the point estimates. This option is
	not currently available for restricted VECMs.
      </para>
      <para>
	This dialog also supports reordering of the variables for the
	Cholesky decomposition of the cross-equation covariance matrix.
	The default is given by the order in which the variables are
	entered into the model specification, but the up and down arrows
	can be used to promote or demote a selected variable.
      </para>
    </description>

  </command>

  <command name="join" section="Dataset" label="Manage data sources" 
	   context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
	<argument>varname</argument>
      </arguments>
      <options>
	<option>
	  <flag>--data</flag>
	  <optparm>column-name</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--filter</flag>
	  <optparm>expression</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--ikey</flag>
	  <optparm>inner-key</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--okey</flag>
	  <optparm>outer-key</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--aggr</flag>
	  <optparm>method</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--tkey</flag>
	  <optparm>column-name,format-string</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>report on progress</effect>
	</option>	
      </options>
    </usage>

    <description>
      <para>
	This command imports a data series from the source
	<repl>filename</repl> (which must be either a delimited text
	data file or a <quote>native</quote> gretl data file) under
	the name <repl>varname</repl>. For details please see
	<guideref targ="chap:join"/>; here we just give a brief
	summary of the available options.
      </para>
      <para>
	The <opt>data</opt> option can be used to specify the column
	heading of the data in the source file, if this differs from
	the name by which the data should be known in gretl.
      </para>
      <para>
	The <opt>filter</opt> option can be used to specify a
	criterion for filtering the source data (that is, selecting a
	subset of observations).
      </para> 
      <para>
	The <opt>ikey</opt> and <opt>okey</opt> options can be used to 
	specify a mapping between observations in the current dataset
	and observations in the source data (for example, individuals
	can be matched against the household to which they belong).
      </para>
      <para>
	The <opt>aggr</opt> option is used when the mapping between
	observations in the current dataset and the source is not
	one-to-one.
      </para>
      <para>
	The <opt>tkey</opt> option is applicable only when the current
	dataset has a time-series structure. It can be used to specify
	the name of a column containing dates to be matched to the
	dataset and/or the format in which dates are represented in
	that column.
      </para>     
      <para>
	See also <cmdref targ="append"/> for simpler joining
	operations.
      </para>
    </description>

  </command>

  <command name="kalman" section="Estimation" label="Kalman filter">

    <usage>
      <options>
	<option>
	  <flag>--cross</flag>
	  <effect>allow for cross-correlated disturbances</effect>
	</option>
	<option>
	  <flag>--diffuse</flag>
	  <effect>use diffuse initialization</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Opens a block of statements to set up a Kalman filter.  This block
	should end with the line <lit>end kalman</lit>, to which the options
	shown above may be appended.  The intervening lines specify the
	matrices that compose the filter.  For example,
      </para>
      <code>
	kalman 
	  obsy y
	  obsymat H
	  statemat F
	  statevar Q
	end kalman
      </code>
      <para>
	would set up a linear, time invariant state-space model with
	observation equation 
	<equation status="display" 
	  tex="\[y_t=H'\alpha_t\]"
	  ascii="y(t) = H'a(t)"
	  graphic="kalman1"/> 
	and state transition equation
	<equation status="display" 
	  tex="\[\alpha_{t+1}=F \alpha_t + u_t\]"
	  ascii="a(t+1) = F a(t) + u(t)"
	  graphic="kalman2"/> 
	where <math>V(u) = Q</math>.
      </para>
      <para>
	More complex state-space models are handled via the additional
	keywords <lit>obsx</lit>, <lit>obsxmat</lit>,
	<lit>obsvar</lit>, <lit>inistate</lit>, <lit>inivar</lit> and
	<lit>stconst</lit>. Please see <guideref targ="chap:kalman"/>
	for details.
      </para>
	<para>
	  <seelist>
	    <fncref targ="kfilter"/>
	    <fncref targ="ksimul"/>
	    <fncref targ="ksmooth"/>
	  </seelist>
	</para>
    </description>

  </command>

  <command name="kpss" section="Tests" label="KPSS stationarity test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--trend</flag>
	  <effect>include a trend</effect>
	</option>
	<option>
	  <flag>--seasonals</flag>
	  <effect>include seasonal dummies</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print regression results</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
	</option>
	<option>
	  <flag>--difference</flag>
	  <effect>use first difference of variable</effect>
	</option>
      </options>
      <examples>
	<example>kpss 8 y</example>
        <example>kpss 4 x1 --trend</example>
      </examples>
    </usage>

    <description>

      <para context="gui">
	Computes the KPSS test (Kwiatkowski, Phillips, Schmidt and Shin,
	Journal of Econometrics, 1992) for stationarity of the given
	variable (or its first difference, if the differencing option is
	selected).  The null hypothesis is that the variable in question
	is stationary, either around a level or, if the <quote>include a
	trend</quote> box is checked, around a deterministic linear trend.
      </para>

      <para context="cli">
	For use of this command with panel data please see the final
	section in this entry.
      </para>

      <para context="cli">
	Computes the KPSS test <cite key="KPSS92" p="true">(Kwiatkowski
	et al, Journal of Econometrics, 1992)</cite> for stationarity,
	for each of the specified variables (or their first difference,
	if the <opt>--difference</opt> option is selected). The null
	hypothesis is that the variable in question is stationary,
	either around a level or, if the <opt>--trend</opt> option is
	given, around a deterministic linear trend.
      </para>

      <para context="gui">
	The selected lag order determines the size of the window used
	for Bartlett smoothing.  If the <quote>show regression
	  results</quote> box is checked the results of the auxiliary
	regression are printed, along with the estimated variance of
	the random walk component of the variable.
      </para>

      <para context="cli">
	The <repl>order</repl> argument determines the size of the
	window used for Bartlett smoothing. If a negative value is
	given this is taken as a signal to use an automatic window
	size of 4(<math>T</math>/100)<sup>0.25</sup>, where
	<math>T</math> is the sample size.
      </para>

      <para context="cli">
	If the <opt>--verbose</opt> option is chosen the results of
	the auxiliary regression are printed, along with the estimated
	variance of the random walk component of the variable.
      </para>

      <para>
	The critical values shown for the test statistic are based on
	response surfaces estimated in the manner set out by <cite
	key="sephton95">Sephton (Economics Letters, 1995)</cite>,
	which are more accurate for small samples than the values
	given in the original KPSS article. When the test statistic
	lies between the 10 percent and 1 percent critical values a
	p-value is shown; this is obtained by linear interpolation and
	should not be taken too literally.  See the <fncref
	targ="kpsscrit"/> function for a means of obtaining these
	critical values programmatically.
      </para>

      <subhead context="cli">Panel data</subhead>

      <para context="cli">
	When the <lit>kpss</lit> command is used with panel data, to
	produce a panel unit root test, the applicable options and the
	results shown are somewhat different.  While you may give a list
	of variables for testing in the regular time-series case, with
	panel data only one variable may be tested per command. And the
	<opt>--verbose</opt> option has a different meaning: it produces a
	brief account of the test for each individual time series (the
	default being to show only the overall result).
      </para>
      <para context="cli">
	When possible, the overall test (null hypothesis: the series in
	question is stationary for all the panel units) is calculated
	using the method of <cite key="choi01">Choi (Journal of
	International Money and Finance, 2001)</cite>. This is not
	always straightforward, the difficulty being that while the
	Choi test is based on the p-values of the tests on the
	individual series, we do not currently have a means of
	calculating p-values for the KPSS test statistic; we must
	rely on a few critical values.
      </para>
      <para context="cli">
	If the test statistic for a given series falls between the 10
	percent and 1 percent critical values, we are able to interpolate
	a p-value. But if the test falls short of the 10 percent value, or
	exceeds the 1 percent value, we cannot interpolate and can at best
	place a bound on the global Choi test. If the individual test
	statistic falls short of the 10 percent value for some units but
	exceeds the 1 percent value for others, we cannot even compute
	a bound for the global test.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Unit root tests/KPSS test</menu-path>
    </gui-access>

  </command>

  <command name="labels" section="Dataset" 
    label="Labels for variables" context="cli">

    <usage>
      <altforms>
	<altform><lit>labels [</lit> <repl>varlist</repl> <lit>]</lit></altform>	
	<altform><lit>labels --to-file=</lit><repl>filename</repl></altform>
	<altform><lit>labels --from-file=</lit><repl>filename</repl></altform>
	<altform><lit>labels --delete</lit></altform>
      </altforms>
    </usage>

    <description>
      <para>
	In the first form, prints out the informative labels (if present) for
	the series in <repl>varlist</repl>, or for all series in the dataset
	if <repl>varlist</repl> is not specified.
      </para>
      <para>
	With the option <opt>--to-file</opt>, writes to the named file the
	labels for all series in the dataset, one per line. If no labels are
	present an error is flagged; if some series have labels and others
	do not, a blank line is printed for series with no label.
      </para>
      <para>
	With the option <opt>--from-file</opt>, reads the specified file
	(which should be plain text) and assigns labels to the series in
	the dataset, reading one label per line and taking blank lines
	to indicate blank labels.
      </para>
      <para>
	The <opt>--delete</opt> option does what you'd expect: it
	removes all the series labels from the dataset.
      </para>      
    </description>

    <gui-access>
      <menu-path>/Data/Variable labels</menu-path>
    </gui-access>

  </command>

  <command name="lad" section="Estimation"
    label="Least Absolute Deviation estimation">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Calculates a regression that minimizes the sum of the absolute
	deviations of the observed from the fitted values of the
	dependent variable.  Coefficient estimates are derived using
	the Barrodale&ndash;Roberts simplex algorithm; a warning is
	printed if the solution is not unique.
      </para>
      <para>
	Standard errors are derived using the bootstrap procedure with
	500 drawings. The covariance matrix for the parameter
	estimates, printed when the <opt>--vcv</opt> flag is given, is
	based on the same bootstrap.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Robust estimation/Least Absolute Deviation</menu-path>
    </gui-access>

  </command>

  <command name="lags" section="Transformations" 
    label="Create lags" context="cli">

    <usage>
      <arguments>
        <argument optional="true" separated="true">order</argument>
	<argument>laglist</argument>
      </arguments>
      <examples>
	<example>lags x y</example>
	<example>lags 12 ; x y</example>
      </examples>
    </usage>

    <description>
      <para>
	Creates new series which are lagged values of each of the series in
	<repl>varlist</repl>.  By default the number of lags created equals the
	periodicity of the data. For example, if the periodicity is 4 (quarterly),
	the command <cmd>lags x</cmd> creates
      </para>
      <mono>
	x_1 = x(t-1)
	x_2 = x(t-2)
	x_3 = x(t-3)
	x_4 = x(t-4)
      </mono>
      <para>
	The number of lags created can be controlled by the optional
	first parameter (which, if present, must be followed by a 
	semicolon).
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Lags of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="lags-dialog" section="Estimation" context="gui"
    label="Lag selection box">

    <description>
      <para>
	In this dialog you can select the lag order for the independent
	variables in a time-series model, and in some cases for the dependent
	variable also.  (But note that the common lag order for vector models
	such as VARs and VECMs is handled separately, via a selection spinner in
	the main model dialog box.)
      </para>
      <para>
	The spinners on the left let you select a range of consecutive lags for
	any given variable. To specify non-consecutive lags, click the check box
	next to the entry field titled <quote>specific lags</quote>.  This
	activates the entry box, into which you can type a list of lags,
	separated by spaces.
      </para>
      <para>
	The row marked <quote>default</quote> offers a quick way to set a common
	lag specification for all the independent variables: values set in that
	row are copied to all the others (apart from the dependent variable, if
	present).  
      </para>
      <para>
	The dependent variable is treated specially: the minimum lag must be
	zero, which places the current value of the variable on the left-hand
	side of the model.  Any higher lags appear with the independent
	variables on the right-hand side of the model.
      </para>
      <para>
	Values selected in this dialog are remembered for the duration of your
	session with a given dataset.
      </para>

    </description>

  </command>

  <command name="ldiff" section="Transformations" 
    label="Log-differences" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The first difference of the natural log of each series in
	<repl>varlist</repl> is obtained and the result stored in a
	new series with the prefix <lit>ld_</lit>.  Thus <cmd>ldiff
	  x y</cmd> creates the new variables
      </para>
      <mono>
	ld_x = log(x) - log(x(-1))
	ld_y = log(y) - log(y(-1))
      </mono>
    </description>

    <gui-access>
      <menu-path>/Add/Log differences of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="leverage" section="Tests" label="Influential observations">

    <usage>
      <options>
        <option>
	  <flag>--save</flag>
	  <effect>save variables</effect>
	</option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Must follow an <cmd>ols</cmd> command. Calculates the leverage
	(<math>h</math>, which must lie in the range 0 to 1) for each
	data point in the sample on which the previous model was
	estimated.  Displays the residual (<math>u</math>) for each
	observation along with its leverage and a measure of its
	influence on the estimates, <math>uh</math>/(1 &minus;
	<math>h</math>).  <quote>Leverage points</quote> for which the
	value of <math>h</math> exceeds 2<math>k</math>/<math>n</math>
	(where <math>k</math> is the number of parameters being
	estimated and <math>n</math> is the sample size) are flagged
	with an asterisk.  For details on the concepts of leverage and
	influence see <cite key="davidson-mackinnon93">Davidson and
	MacKinnon (1993)</cite>, Chapter 2.
      </para>
      <para>
	DFFITS values are also computed: these are <quote>studentized
	  residuals</quote> (predicted residuals divided by their
	standard errors) multiplied by 
	  <equation status="inline" 
	  tex="$\sqrt{h/(1 - h)}$"
	  ascii="sqrt[h/(1 - h)]"
	  graphic="dffit"/>. 
	  For discussions of studentized residuals and DFFITS see chapter 
	  12 of <cite key="maddala92">Maddala's Introduction to
	  Econometrics</cite> or <cite key="belsley-etal80">Belsley, 
	  Kuh and Welsch (1980)</cite>.
      </para>
      <para>
	Briefly, a <quote>predicted residual</quote> is the difference
	between the observed value of the dependent variable at
	observation <math>t</math>, and the fitted value for
	observation <math>t</math> obtained from a regression in
	which that observation is omitted (or a dummy variable with
	value 1 for observation <math>t</math> alone has been
	added); the studentized residual is obtained by dividing the
	predicted residual by its standard error.
      </para>
      <para context="cli">
	If the <opt>--save</opt> flag is given with this command, then the
	leverage, influence and DFFITS values are added to the current
	data set. In that context the <opt>--quiet</opt> flag may be used
	to suppress the printing of results.
      </para>
      <para context="gui">
	The "+" icon at the top of the leverage test window brings up
	a dialog box that allows you to save one or more of the test
	variables to the current data set.
      </para>
      <para context="tex">
	After execution, the <lit>$test</lit> accessor returns the
	cross-validation criterion, which is defined as 
        \[
	\sum_{i=1}^n (y_i - \hat{y}_{-i})^2 
        \] 
        where $\hat{y}_{-i}$ is the forecast error for the $i$-th
        observation, after it has been excluded from the sample. The
        criterion is, hence, the sum of the squared forecasting errors
        where all $n$ observations but the $i$-th one are used to
        predict it (the so-called <emphasis>leave-one-out</emphasis>
        estimator).  For a broader discussion of the cross-validation
        criterion, see Davidson and MacKinnon's <book>Econometric
        Theory and Methods</book>, pages 685--686, and the references
        therein.
      </para>
      <para context="notex">
	After execution, the <lit>$test</lit> accessor returns the
	cross-validation criterion, which is defined as the sum of
	squared deviations of the dependent variable from its forecast
	value, the forecast for each observation being based on a
	sample from which that observation is excluded.  (This is
	known as the <emphasis>leave-one-out</emphasis> estimator).
	For a broader discussion of the cross-validation criterion,
	see Davidson and MacKinnon's <book>Econometric Theory and
	Methods</book>, pages 685&ndash;686, and the references therein.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Influential observations</menu-path>
    </gui-access>

  </command>

  <command name="levinlin" section="Tests" label="Levin-Lin-Chu test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--nc</flag>
	  <effect>test without a constant</effect>
	</option>
	<option>
	  <flag>--ct</flag>
	  <effect>with constant and trend</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
	</option>
      </options>
      <examples>
	<example>levinlin 0 y</example>
        <example>levinlin 2 y --ct</example>
        <example>levinlin {2,2,3,3,4,4} y</example>
      </examples>
    </usage>

    <description>
      <para>
	Carries out the panel unit-root test described by <cite
	key="LLC2002">Levin, Lin and Chu (2002)</cite>. The null
	hypothesis is that all of the individual time series exhibit a
	unit root, and the alternative is that none of the series has a
	unit root. (That is, a common AR(1) coefficient is assumed,
	although in other respects the statistical properties of the
	series are allowed to vary across individuals.)
      </para>
	
      <para context="cli">
	By default the test ADF regressions include a constant;
	to suppress the constant use the <opt>--nc</opt> option, or
	to add a linear trend use the <opt>--ct</opt> option.
	(See the <cmdref targ="adf"/> command for explanation of
	ADF regressions.)
      </para>

      <para context="cli">
	The (non-negative) <repl>order</repl> for the test (governing
	the number of lags of the dependent variable to include in the
	ADF regressions) may be given in either of two forms. If a
	scalar value is given, this is applied to all the individuals
	in the panel.  The alternative is to provide a matrix
	containing a specific lag order for each individual; this must
	be a vector with as many elements as there are individuals in
	the current sample range. Such a matrix can be specified by
	name, or constructed using braces as illustrated in the
	last example above.
      </para>

    </description>

    <gui-access>
      <menu-path>/Variable/Unit root tests/Levin-Lin-Chu test</menu-path>
    </gui-access>

  </command>

  <command name="loess" section="Estimation" label="Loess" context="gui">
    <description>
      <para>
	Performs locally-weighted polynomial regression and produces a
	series containing predicted values of the dependent variable for
	each non-missing value of the independent variable. The method is
	as described by <cite key="cleveland79">William Cleveland
	(1979)</cite>.
      </para>
      <para>
	The controls allow you to specify the order of the polynomial
	in the independent variable and the proportion of the data
	points to be used in each local regression (the
	bandwidth). Higher values of the bandwidth produce a smoother
	outcome.
      </para>
      <para>
	If the robust weights box is checked the local regression
	procedure is iterated twice, with the weights being modified
	based on the residuals from the previous iteration so as to
	give less influence to outliers.
      </para>
     </description>
  </command>

  <command name="logistic" section="Estimation" label="Logistic regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--ymax</flag>
	  <optparm>value</optparm>
	  <effect>specify maximum of dependent variable</effect>
	</option>	
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
      <examples>
        <example>logistic y const x</example>
        <example>logistic y const x --ymax=50</example>
      </examples>
    </usage>

    <description>
      <para>
	Logistic regression: carries out an OLS regression using the
	logistic transformation of the dependent variable,
	<equation status="display" 
	  tex="\[\log\left(\frac{y}{y^*-y}\right)\]"
	  ascii="log(y/(y* - y))"
	  graphic="logistic1"/>
      </para>

      <para>
	The dependent variable must be strictly positive.  If all its
	values lie between 0 and 1, the default is to use a
	<math>y</math><sup>*</sup> value (the asymptotic maximum of
	the dependent variable) of 1; if its values lie between 0 and
	100, the default <math>y</math><sup>*</sup> is 100.
      </para>

      <para context="cli">
	If you wish to set a different maximum, use the
	<opt>--ymax</opt> option. Note that the supplied value must be
	greater than all of the observed values of the dependent
	variable.
      </para>

      <para context="gui">
	You may specify a different maximum <math>y</math> value.
	Note that the supplied value must be greater than all of the
	observed values of the dependent variable.
      </para>

      <para>
	The fitted values and residuals from the regression are
	automatically transformed using 	  
	<equation status="display" 
	  tex="\[y=\frac{y^*}{1+e^{-x}}\]"
	  ascii="y = y* / (1 + exp(-x))"
	  graphic="logistic2"/> where <math>x</math> represents
	either a fitted value or a residual from the OLS regression
	using the transformed dependent variable.  The reported values
	are therefore comparable with the original dependent
	variable.
      </para>

      <para>
	Note that if the dependent variable is binary, you should
	use the <cmdref targ="logit"/> command instead.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Logistic</menu-path>
    </gui-access>

  </command>

  <command name="logit" section="Estimation"
    label="Logit regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
	</option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>clustered standard errors</effect>
        </option>
	<option>
	  <flag>--multinomial</flag>
	  <effect>estimate multinomial logit</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
	<option>
	  <flag>--p-values</flag>
	  <effect>show p-values instead of slopes</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	If the dependent variable is a binary variable (all values are 0
	or 1) maximum likelihood estimates of the coefficients on
	<repl>indepvars</repl> are obtained via the Newton&ndash;Raphson
	method. As the model is nonlinear the slopes depend on the
	values of the independent variables.  By default the slopes with
	respect to each of the independent variables are calculated (at
	the means of those variables) and these slopes replace the usual
	p-values in the regression output.  This behavior can be
	suppressed my giving the <opt>--p-values</opt> option. The
	chi-square statistic tests the null hypothesis that all
	coefficients are zero apart from the constant.
      </para>
      <para context="cli">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <opt>--robust</opt> flag is
	given, then QML or Huber&ndash;White standard errors are
	calculated instead. In this case the estimated covariance
	matrix is a <quote>sandwich</quote> of the inverse of the
	estimated Hessian and the outer product of the gradient; see
	chapter 10 of <cite key="davidson-mackinnon04">Davidson and
	MacKinnon (2004)</cite>.  But if the <opt>cluster</opt> option
	is given, then <quote>cluster-robust</quote> standard errors
	are produced; see <guideref targ="chap:robust_vcv"/> for
	details.
      </para>
      <para context="gui">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the "Robust standard errors" box is
	checked, then QML or Huber&ndash;White standard errors are
	calculated instead. In this case the estimated covariance matrix
	is a <quote>sandwich</quote> of the inverse of the estimated
	Hessian and the outer product of the gradient.  See chapter 10 of
	Davidson and MacKinnon for details.
      </para>
      <para>
	If the dependent variable is not binary but is discrete, then by
	default it is interpreted as an ordinal response, and Ordered
	Logit estimates are obtained.  However, if the
	<opt>--multinomial</opt> option is given, the dependent variable
	is interpreted as an unordered response, and Multinomial Logit
	estimates are produced. (In either case, if the variable selected
	as dependent is not discrete an error is flagged.) In the
	multinomial case, the accessor <lit>$mnlprobs</lit> is available
	after estimation, to get a matrix containing the estimated
	probabilities of the outcomes at each observation (observations in
	rows, outcomes in columns).
      </para>
      <para>
	If you want to use logit for analysis of proportions (where
	the dependent variable is the proportion of cases having a
	certain characteristic, at each observation, rather than a 1
	or 0 variable indicating whether the characteristic is present
	or not) you should not use the <cmd>logit</cmd> command, but
	rather construct the logit variable, as in
      </para>
      <code>
	series lgt_p = log(p/(1 - p))
      </code>
      <para>and use this as the dependent variable in an OLS regression.
      See chapter 12 of <cite key="ramanathan02">Ramanathan (2002)</cite>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Logit</menu-path>
    </gui-access>

  </command>

  <command name="logs" section="Transformations" 
    label="Create logs" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The natural log of each of the series in <repl>varlist</repl>
	is obtained and the result stored in a new series with the
	prefix <lit>l_</lit> (<quote>el</quote> underscore).  For example,
	<cmd>logs x y</cmd> creates the new variables <lit>l_x</lit> =
	ln(<lit>x</lit>) and <lit>l_y</lit> = ln(<lit>y</lit>).
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Logs of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="loop" section="Programming" 
    label="Start a command loop" context="cli">

    <usage>
      <arguments>
        <argument>control</argument>
      </arguments>
      <options>
	<option>
	  <flag>--progressive</flag>
	  <effect>enable special forms of certain commands</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>report details of genr commands</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>do not report number of iterations performed</effect>
	</option>
      </options>
      <examples>
        <example>loop 1000</example>
	<example>loop 1000 --progressive</example>
        <example>loop while essdiff > .00001</example>
        <example>loop i=1991..2000</example>
        <example>loop for (r=-.99; r&lt;=.99; r+=.01)</example>
	<example>loop foreach i xlist</example>
      </examples>
    </usage>

    <description>
      <para>
	This command opens a special mode in which the program
	accepts commands to be executed repeatedly.  You exit the mode
	of entering loop commands with <cmd>endloop</cmd>: at this
	point the stacked commands are executed.
      </para>
      <para>
	The parameter <repl quote="true">control</repl> may take any of
	five forms, as shown in the examples: an integer number of times to
	repeat the commands within the loop; <quote><lit>while</lit></quote>
	plus a boolean condition; a range of integer values for index
	variable; <quote><lit>for</lit></quote> plus three expressions in
	parentheses, separated by semicolons (which emulates the
	<lit>for</lit> statement in the C programming language); or
	<quote><lit>foreach</lit></quote> plus an index variable and a list.
      </para>
      <para>
	See <guideref targ="chap:looping"/> for further details and
	examples.  The effect of the <opt>--progressive</opt> option
	(which is designed for use in Monte Carlo simulations) is
	explained there. Not all gretl commands may be used within
	a loop; the commands available in this context are also
	set out there.
      </para>
    </description>

  </command>

  <command name="mahal" section="Statistics" label="Mahalanobis distances">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print anything</effect>
        </option>
	<option>
	  <flag>--save</flag>
	  <effect>add distances to the dataset</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Computes the Mahalanobis distances between the series in
	<repl>varlist</repl>.  The Mahalanobis distance is the
	distance between two points in a <math>k</math>-dimensional
	space, scaled by the statistical variation in each dimension
	of the space.  For example, if <math>p</math> and
	<math>q</math> are two observations on a set of <math>k</math>
	variables with covariance matrix <math>C</math>, then the
	Mahalanobis distance between the observations is given by
	<equation status="display"
        tex="\[\sqrt{(p-q)^{\prime}C^{-1}(p-q)}\]"
        ascii="sqrt((p - q)' * C-inverse * (p - q))"
          graphic="mahal"/>
	where (<math>p</math> &minus; <math>q</math>) is a
	<math>k</math>-vector. This reduces to Euclidean
	distance if the covariance matrix is the identity
	matrix.
      </para>
      <para>
	The space for which distances are computed is defined by
	the selected variables.  For each observation in the current
	sample range, the distance is computed between the observation
	and the centroid of the selected variables.  This distance is
	the multidimensional counterpart of a standard
	<math>z</math>-score, and can be used to judge whether a
	given observation <quote>belongs</quote> with a group of other
	observations.
      </para>
      <para context="cli">
	If the <opt>--vcv</opt> option is given, the
	covariance matrix and its inverse are printed.  If the
	<opt>--save</opt> option is given, the distances are saved to
	the dataset under the name <lit>mdist</lit> (or
	<lit>mdist1</lit>, <lit>mdist2</lit> and so on if there is
	already a variable of that name).
      </para>
      <para context="gui">
	If the number of variables selected is 4 or
	less, the covariance matrix and its inverse are printed.
	Clicking the "+" button at the top of the window displaying
	the distances give you the option of adding the distances to
	the dataset as a new variable.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Mahalanobis distances</menu-path>
    </gui-access>

  </command>  

  <command name="makepkg" section="Programming" context="cli"
    label="Make function package">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
      <options>
        <option>
	  <flag>--index</flag>
	  <effect>write auxiliary index file</effect>
        </option>
        <option>
	  <flag>--translations</flag>
	  <effect>write auxiliary strings file</effect>
        </option>
      </options>
    </usage>    

    <description>
      <para>
	Supports creation of a gretl function package via the command
	line. The mode of operation of this command depends on the
	extension of <repl>filename</repl>, which must be either
	<lit>.gfn</lit> or <lit>.zip</lit>.
      </para>
      <subhead>Gfn mode</subhead>
      <para>
	Writes a gfn file. It is assumed that a package specification
	file, with the same basename as <repl>filename</repl> but with
	the extension <lit>.spec</lit>, is accessible, along with any
	auxiliary files that it references. It is also assumed that
	all the functions to be packaged have been read into memory.
      </para>
      <subhead>Zip mode</subhead>
      <para>
	Writes a zip package file (gfn plus other materials). If
	a gfn file of the same basename as <repl>filename</repl>
	is found, it forms the basis of the zip package. If no
	gfn file is found, the program first attempts to build
	the gfn, as described above.
      </para>
      <subhead>Gfn options</subhead>
      <para>
	The option flags support the writing of auxiliary files,
	intended for use with gretl <quote>addons</quote>. The index
	file is a short XML document containing basic information
	about the package; it has the same basename as the package and
	the extension <lit>.xml</lit>. The translations file contains
	strings from the package that may be suitable for translation,
	in C format; for package <lit>foo</lit> this file is named
	<lit>foo-i18n.c</lit>. These files are not produced if the
	command is operating in zip mode and a pre-existing gfn
	file is used.
      </para>
      <para>
	For details on all of this, see the the <book>Gretl Function
	Package Guide</book>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/Function packages/New package</menu-path>
    </gui-access>

  </command>

  <command name="markers" section="Dataset" label="Observation markers" context="cli">

    <usage>
      <altforms>
	<altform><lit>markers --to-file=</lit><repl>filename</repl></altform>
	<altform><lit>markers --from-file=</lit><repl>filename</repl></altform>
	<altform><lit>markers --delete</lit></altform>
      </altforms>
    </usage>

    <description>
      <para>
	With the option <opt>--to-file</opt>, writes to the named file
	the observation marker strings from the current dataset, one
	per line. If no such strings are present an error is flagged.
      </para>
      <para>
	With the option <opt>--from-file</opt>, reads the specified
	file (which should be plain text) and assigns observation
	markers to the rows in the dataset, reading one marker per
	line. In general there should be at least as many markers in
	the file as observations in the dataset, but if the dataset is
	a panel it is also acceptable if the number of markers in the
	file matches the number of cross-sectional units (in which
	case the markers are repeated for each time period.)
      </para>
      <para>
	The <opt>--delete</opt> option does what you'd expect: it
	removes the observation marker strings from the dataset.
      </para>      
    </description>

    <gui-access>
      <menu-path>/Data/Observation markers</menu-path>
    </gui-access>

  </command>

  <command name="meantest" section="Tests" label="Difference of means">

    <usage>
      <arguments>
        <argument>series1</argument>
        <argument>series2</argument>
      </arguments>
      <options>
        <option>
	  <flag>--unequal-vars</flag>
	  <effect>assume variances are unequal</effect>
        </option>
      </options>
    </usage>

    <description>
      <para context="cli">
	Calculates the <math>t</math> statistic for the null
	hypothesis that the population means are equal for the
	variables <repl>series1</repl> and <repl>series2</repl>, and
	shows its p-value.
      </para>
      <para>
	By default the test statistic is calculated on the assumption
	that the variances are equal for the two variables. With the
	<opt>--unequal-vars</opt> option the variances are assumed to
	be different; in this case the degrees of freedom for the test
	statistic are approximated as per <cite
	key="satter46">Satterthwaite (1946)</cite>.
      </para>
      <para context="gui">
	Calculates the t-test for the null hypothesis that the
	population means are equal for two selected series, and shows
	its p-value.  The command may be called with or without the
	assumption that the variances are equal for the two variables.
	In the latter case the degrees of freedom for the test are
	approximated as per <cite key="satter46">Satterthwaite
	(1946)</cite>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/Test statistic calculator</menu-path>
    </gui-access>

  </command>

  <command name="missing" section="Dataset" context="gui"
    label="Missing data values">

    <description>
      <para>
	Set a numerical value that will be interpreted as
	<quote>missing</quote> or <quote>not available</quote>, either for
	a particular data series (under the Variable menu) or globally for
	the entire data set (under the Sample menu).
      </para> 
      <para>
	Gretl has its own internal coding for missing values, but
	sometimes imported data may employ a different code.  For
	example, if a particular series is coded such that a value of
	-1 indicates <quote>not applicable</quote>, you can select
	<quote>Set missing value code</quote> under the Variable menu
	and type in the value <quote>-1</quote> (without the quotes).
	Gretl will then read the -1s as missing observations.
      </para>
    </description>
  </command>

  <command name="menu-attach" section="Programming" 
	   label="Menu attachment" context="gui">
    <description>
      <para>
	This dialog enables you to specify a menu attachment for a
	function package. To do this you must complete the following
	three fields in the dialog box.
      </para>
      <subhead>1. Label</subhead>
      <para>
	This requires a short label string, which will appear as
	the menu entry for the package.
      </para>
      <subhead>2. Window</subhead>
      <para>
	Select <quote>model window</quote> for a function package that
	does something with a gretl model, and should appear in the
	menu bar in a gretl model window. Otherwise, select
	<quote>main window</quote>.
      </para>
      <subhead>3. Menu tree</subhead>
      <para>
	Select the position within the menu tree (for either the
	main window or the model window, as chosen above) where the
	entry for the package should appear.
      </para>
      <subhead>Optional elements</subhead>
      <para>
	In addition you can use the <quote>GUI help text</quote>
	button to add or edit GUI-specific help text, to be shown when
	the package is called from a menu. And if the package is
	intended to be called from a model window you can specify a
	certain type of model (identified by its gretl command-word)
	as a requirement.
      </para>
    </description>
  </command>

  <command name="mle" section="Estimation"
    label="Maximum likelihood estimation">

    <usage>
      <arguments>
        <argument>log-likelihood function</argument>
	<argument optional="true">derivatives</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't show estimated model</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--hessian</flag>
	  <effect>base covariance matrix on the Hessian</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <effect>QML covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
	<option>
	  <flag>--no-gradient-check</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--lbfgs</flag>
	  <effect>use L-BFGS-B instead of regular BFGS</effect>
	</option>
      </options>
      <examples>
	<demos>
	  <demo>weibull.inp</demo>
	</demos>
      </examples>
    </usage>

    <description context="gui">
      <para>
	Performs Maximum Likelihood (ML) estimation using either the
	BFGS (Broyden, Fletcher, Goldfarb, Shanno) algorithm or
	Newton's method. You must specify the log-likelihood function;
	it is recommended that you also supply expressions for the
	derivatives of this function with respect to each of the
	parameters if possible.
      </para>
      <para>
	Simple example: Suppose we have a series <lit>X</lit> with values 0
	or 1 and we wish to obtain the maximum likelihood estimate of the
	probability, <lit>p</lit>, that <lit>X</lit> = 1.  (In this simple case
	we can guess in advance that the ML estimate of <lit>p</lit> will simply
	equal the proportion of Xs equal to 1 in the sample.)
      </para>
      <para>
	The parameter <lit>p</lit> must first be added to the dataset and
	given an initial value.  This can be done using the genr command or via
	menu choices.  Appropriate <quote>genr</quote> lines may be typed into
	the MLE specification window prior to the specification of the
	log-likelihood function.
      </para>
      <para>
	In the MLE window we type the following lines:
      </para>
      <code>
	loglik = X*log(p) + (1-X)*log(1-p)
	deriv p = X/p - (1-X)/(1-p)
      </code>
      <para>
	The first line specifies the log-likelihood function, and the
	next line supplies the derivative of that function with
	respect to the parameter p.  If no "deriv" lines are given, a
	numerical approximation to the derivatives is computed.
      </para>
      <para>
	If the parameter p was not previously declared we could
	preface the above lines with something like the following:
      </para>
      <code>
	scalar p = 0.5
      </code>
      <para>
	By default, standard errors are based on the Outer Product of the
	Gradient.  If the robust standard errors box is checked, a QML
	estimator is used (namely, a sandwich of the negative inverse of
	the Hessian and the covariance matrix of the gradient).  The
	Hessian is approximated numerically.
      </para>
      <para>
	For a much more in-depth description of <cmd>mle</cmd>, please
	refer to <guideref targ="chap:mle"/>.
      </para>
    </description>

    <description context="cli">
      <para>
	Performs Maximum Likelihood (ML) estimation using either the
	BFGS (Broyden, Fletcher, Goldfarb, Shanno) algorithm or
	Newton's method. The user must specify the log-likelihood
	function.  The parameters of this function must be declared
	and given starting values (using the <cmd>genr</cmd> command)
	prior to estimation.  Optionally, the user may specify the
	derivatives of the log-likelihood function with respect to
	each of the parameters; if analytical derivatives are not
	supplied, a numerical approximation is computed.
      </para>
      <para>
	Simple example: Suppose we have a series <lit>X</lit> with
	values 0 or 1 and we wish to obtain the maximum likelihood
	estimate of the probability, <lit>p</lit>, that <lit>X</lit> = 1.
	(In this simple case we can guess in advance that the ML estimate
	of <lit>p</lit> will simply equal the proportion of Xs equal to 1
	in the sample.)
      </para>
      <para>
	The parameter <lit>p</lit> must first be added to the dataset
	and given an initial value.  For example, 
        <lit>scalar p = 0.5</lit>.
      </para>
      <para>
	We then construct the MLE command block:
      </para>
      <code>
	mle loglik = X*log(p) + (1-X)*log(1-p)
	  deriv p = X/p - (1-X)/(1-p)
	end mle
      </code>
      <para>
	The first line above specifies the log-likelihood function. It
	starts with the keyword <lit>mle</lit>, then a dependent
	variable is specified and an expression for the log-likelihood
	is given (using the same syntax as in the <cmd>genr</cmd>
	command).  The next line (which is optional) starts with the
	keyword <lit>deriv</lit> and supplies the derivative of the
	log-likelihood function with respect to the parameter
	<lit>p</lit>. If no derivatives are given, you should include
	a statement using the keyword <lit>params</lit> which
	identifies the free parameters: these are listed on one line,
	separated by spaces and can be either scalars, or vectors, or
	any combination of the two.  For example, the above could be
	changed to:
      </para>
      <code>
	mle loglik = X*log(p) + (1-X)*log(1-p)
	  params p
	end mle
      </code>
      <para>
	in which case numerical derivatives would be used.
      </para>
      <para>
	Note that any option flags should be appended to the ending line
	of the MLE block.
      </para>
      <para>
	By default, estimated standard errors are based on the Outer
	Product of the Gradient.  If the <opt>--hessian</opt> option is
	given, they are instead based on the negative inverse of the
	Hessian (which is approximated numerically).  If the
	<opt>--robust</opt> option is given, a QML estimator is used
	(namely, a sandwich of the negative inverse of the Hessian and the
	covariance matrix of the gradient). 
      </para>
      <para>
	If you supply analytical derivatives, by default gretl runs a
	numerical check on their plausibility.  Occasionally this may
	produce false positives, instances where correct derivatives
	appear to be wrong and estimation is refused. To counter this,
	or to achieve a little extra speed, you can give the option
	<opt>--no-gradient-check</opt>.  Obviously, you should do
	this only if you are quite confident that the gradient you
	have specified is right.
      </para>
      <para>
	For a much more in-depth description of <cmd>mle</cmd>, please
	refer to <guideref targ="chap:mle"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Maximum likelihood</menu-path>
    </gui-access>

  </command>

  <command name="modeltab" section="Utilities" label="The model table">

    <usage>
      <altforms>
        <altform><lit>modeltab add</lit></altform>
	<altform><lit>modeltab show</lit></altform>
	<altform><lit>modeltab free</lit></altform>
	<altform><lit>modeltab --output=</lit><repl>filename</repl></altform>
      </altforms>
    </usage>

    <description context="gui"> 
      <para>
	In econometric research it is common to estimate several
	models with a common dependent variable&mdash;the models
	differing in respect of which independent variables are
	included, or perhaps in respect of the estimator used.  In
	this situation it is convenient to present the regression
	results in the form of a table, where each column contains the
	results (coefficient estimates and standard errors) for a
	given model, and each row contains the estimates for a given
	variable across the models.
      </para>

      <para>
	Gretl provides a means of constructing such a table (and
	copying it in plain text, &latex; or Rich Text Format).  Here is
	how to do it:
      </para>

      <nlist>
	<li><para>Estimate a model which you wish to include in the
	    table, and in the model display window, under the File
	    menu, select <quote>Save to session as icon</quote> or
	    <quote>Save as icon and close</quote>.</para>
	</li>
	<li><para>Repeat step 1 for the other models to be included in
	    the table (up to a total of six models).</para>
	</li>
	<li><para>When you are done estimating the models, open the
	    icon view of your gretl session (by selecting <quote>icon
	      view</quote> under the Session menu in the main gretl
	    window, or by clicking the <quote>session icon
	      view</quote> icon on the gretl toolbar).</para>
	</li>
	<li><para>In session icon view, there is an icon labeled
	    <quote>Model table</quote>. Decide which model you wish to
	    appear in the left-most column of the model table and add
	    it to the table, either by dragging its icon onto the
	    Model table icon, or by right-clicking on the model icon
	    and selecting <quote>Add to model table</quote> from the
	    pop-up menu.</para>
	</li>
	<li><para>Repeat step 4 for the other models you wish to
	    include in the table.  The second model selected will
	    appear in the second column from the left, and so
	    on.</para>
	</li>
	<li><para>When you are finished composing the model table,
	    display it by double-clicking on its icon.  Under the Edit
	    menu in the window which appears, you have the option of
	    copying the table to the clipboard in various
	    formats.</para>
	</li>
	<li><para>If the ordering of the models in the table is not
	    what you wanted, right-click on the model table icon and
	    select <quote>Clear table</quote>.  Then go back to step 4
	    above and try again.</para>
	</li>
      </nlist>
    </description>

    <description context="cli">
      <para>
	Manipulates the gretl <quote>model table</quote>. See 
	<guideref targ="chap:modes"/> for details. The sub-commands have
	the following effects: <cmd>add</cmd> adds the last model
	estimated to the model table, if possible; <cmd>show</cmd>
	displays the model table in a window; and <cmd>free</cmd>
	clears the table.
      </para>
      <para>
	To call for printing of the model table, use the flag
	<opt>--output=</opt> plus a filename. If the filename has the
	suffix <quote><lit>.tex</lit></quote>, the output will be in
	&tex; format; if the suffix is <quote><lit>.rtf</lit></quote>
	the output will be RTF; otherwise it will be plain text.
	In the case of &tex; output the default is to produce a
	<quote>fragment</quote>, suitable for inclusion in a
	document; if you want a stand-alone document instead,
	use the <opt>complete</opt> option, for example
      </para>
      <code>
	modeltab --output="myfile.tex" --complete
      </code>
    </description>

    <gui-access>
      <menu-path>Session icon window, Model table icon</menu-path>
    </gui-access>

  </command>

  <command name="modprint" section="Printing"
    label="Print a user-defined model" context="cli">

    <usage>
      <arguments>
        <argument>coeffmat</argument>
        <argument>names</argument>
	<argument optional="true">addstats</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Prints the coefficient table and optional additional statistics for a
	model estimated <quote>by hand</quote>. Mainly useful for user-written
	functions.
      </para>
      <para>
	The argument <repl>coeffmat</repl> should be a <math>k</math>
	by 2 matrix containing <math>k</math> coefficients and
	<math>k</math> associated standard errors, and
	<repl>names</repl> should be a string containing at least
	<math>k</math> names for the coefficients, separated by commas
	or spaces. (The <repl>names</repl> argument may be either the
	name of a string variable or a literal string, enclosed in
	double quotes.)
      </para>
      <para>
	The optional argument <repl>addstats</repl> is a vector containing
	<math>p</math> additional statistics to be printed under the
	coefficient table.  If this argument is given, then <repl>names</repl>
	should contain <math>k + p</math> comma-separated strings, the
	additional <math>p</math> strings to be associated with the additional
	statistics.
      </para>
    </description>

  </command>

  <command name="modtest" section="Tests" label="Model tests"
    context="cli">

    <usage>
      <arguments>
        <argument optional="true">order</argument>
      </arguments>
      <options>
        <option>
	  <flag>--normality</flag>
	  <effect>normality of residual</effect>
        </option>
        <option>
	  <flag>--logs</flag>
	  <effect>non-linearity, logs</effect>
        </option>
        <option>
	  <flag>--autocorr</flag>
	  <effect>serial correlation</effect>
        </option>
        <option>
	  <flag>--arch</flag>
	  <effect>ARCH</effect>
        </option>
        <option>
	  <flag>--squares</flag>
	  <effect>non-linearity, squares</effect>
        </option>
        <option>
	  <flag>--white</flag>
	  <effect>heteroskedasticity, White's test</effect>
        </option>
        <option>
	  <flag>--white-nocross</flag>
	  <effect>White's test, squares only</effect>
        </option>
        <option>
	  <flag>--breusch-pagan</flag>
	  <effect>heteroskedasticity, Breusch&ndash;Pagan</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust variance estimate for Breusch&ndash;Pagan</effect>
        </option>
        <option>
	  <flag>--panel</flag>
	  <effect>heteroskedasticity, groupwise</effect>
        </option>
        <option>
	  <flag>--comfac</flag>
	  <effect>common factor restriction, AR1 models only</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print details</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Must immediately follow an estimation command. Depending on
	the option given, this command carries out one of the
	following: the Doornik&ndash;Hansen test for the normality of
	the error term; a Lagrange Multiplier test for nonlinearity
	(logs or squares); White's test (with or without
	cross-products) or the Breusch&ndash;Pagan test (<cite
	key="breusch-pagan79">Breusch and Pagan, 1979</cite>) for
	heteroskedasticity; the LMF test for serial correlation <cite
	key="kiviet86" p="true">(Kiviet, 1986)</cite>; a test for ARCH
	(Autoregressive Conditional Heteroskedasticity; see also the
	<cmd>arch</cmd> command); or a test of the common factor
	restriction implied by AR(1) estimation.  With the exception
	of the normality and common factor test most of the options
	are only available for models estimated via OLS, but see below
	for details regarding two-stage least squares.
      </para>
      <para>
	The optional <lit>order</lit> argument is relevant only in case
	the <opt>--autocorr</opt> or <opt>--arch</opt> options are
	selected.  The default is to run these tests using a lag order
	equal to the periodicity of the data, but this can be adjusted by
	supplying a specific lag order.
      </para>
      <para>
	The <opt>--robust</opt> option applies only when the
	Breusch&ndash;Pagan test is selected; its effect is to use the
	robust variance estimator proposed by <cite
	key="koenker81">Koenker (1981)</cite>, making the test less
	sensitive to the assumption of normality.
      </para>
      <para>
	The <opt>--panel</opt> option is available only when the model
	is estimated on panel data: in this case a test for groupwise
	heteroskedasticity is performed (that is, for a differing
	error variance across the cross-sectional units).
      </para>
      <para>
	The <opt>--comfac</opt> option is available only when the model is
	estimated via an AR(1) method such as Hildreth&ndash;Lu.  The
	auxiliary regression takes the form of a relatively unrestricted
	dynamic model, which is used to test the common factor restriction
	implicit in the AR(1) specification.
      </para>
      <para>
	By default, the program prints the auxiliary regression on
	which the test statistic is based, where applicable.  This may
	be suppressed by using the <opt>--quiet</opt> flag (minimal
	printed output) or the <opt>--silent</opt> flag (no printed
	output).  The test statistic and its p-value may be retrieved
	using the accessors <lit>$test</lit> and <lit>$pvalue</lit>
	respectively.
      </para>
      <para>
	When a model has been estimated by two-stage least squares (see
	<cmdref targ="tsls"/>), the LM principle breaks down and gretl
	offers some equivalents: the <flag>--autocorr</flag> option
	computes Godfrey's test for autocorrelation <cite key="godfrey94"
	p="true">(Godfrey, 1994)</cite> while the <flag>--white</flag>
	option yields the HET1 heteroskedasticity test <cite
	key="pesaran99" p="true">(Pesaran and Taylor, 1999)</cite>.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests</menu-path>
    </gui-access>

  </command>

  <command name="mpols" section="Estimation" label="Multiple-precision OLS">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
	<option>
	  <flag>--simple-print</flag>
	  <effect>do not print auxiliary statistics</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Computes OLS estimates for the specified model using multiple
	precision floating-point arithmetic, with the help of the
	Gnu Multiple Precision (GMP) library.  By default 256 bits of
	precision are used for the calculations, but this can be increased
	via the environment variable <lit>GRETL_MP_BITS</lit>.  For
	example, when using the bash shell one could issue the following
	command, before starting gretl, to set a precision of 1024 bits.
      </para>
      <code>
	export GRETL_MP_BITS=1024
      </code>

      <para context="cli">
	A rather arcane option is available for this command (primarily
	for testing purposes): if the <repl>indepvars</repl> list is
	followed by a semicolon and a further list of numbers, those
	numbers are taken as powers of <repl>x</repl> to be added to the
	regression, where <repl>x</repl> is the last variable in
	<repl>indepvars</repl>.  These additional terms are computed and
	stored in multiple precision.  In the following example
	<lit>y</lit> is regressed on <lit>x</lit> and the second, third
	and fourth powers of <lit>x</lit>:
      </para>
      <code context="cli">
	mpols y 0 x ; 2 3 4
      </code>
    </description>

    <gui-access>
      <menu-path>/Model/Other linear models/High precision OLS</menu-path>
    </gui-access>

  </command>

  <command name="nadarwat" section="Estimation" label="Nadaraya-Watson" 
	   context="gui">
    <description>
      <para>
	Computes the Nadaraya&ndash;Watson nonparametric estimator of
	the conditional mean of the dependent variable,
	<math>m(x)</math>, for each non-missing value of the
	independent variable.
      </para>
      <para>
	The kernel function <math>K</math> is given by <math>K =
	exp(-x</math><sup>2</sup><math> / 2h)</math> for <math>|x|
	&lt; T</math> and zero otherwise.
      </para>
      <para>
	The bandwidth, usually a small number, controls the smoothness
	of <math>m(x)</math> (higher values producing a smoother
	series); the default value is <math>n</math><sup>-0.2</sup>.
      </para>
      <para>
	If the <quote>leave-one-out</quote> box is checked, a variant
	of the estimator is employed in which the <math>i</math>-th
	observation is not used in evaluating
	<math>m(x</math><sub>i</sub><math>)</math>. This makes the
	Nadaraya&ndash;Watson estimator more robust numerically and
	its usage is normally advised when the estimator is computed
	for inference purposes.
      </para>
     </description>
  </command>

  <command name="negbin" section="Estimation"
    label="Negative Binomial regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
	<argument separated="true" optional="true">offset</argument>
      </arguments>
      <options>
	<option>
	  <flag>--model1</flag>
	  <effect>use NegBin 1 model</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <effect>QML covariance matrix</effect>
	</option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
	<option>
	  <flag>--opg</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
      </options>
    </usage>  

    <description>
      <para>
	Estimates a Negative Binomial model.  The dependent variable is taken
	to represent a count of the occurrence of events of some sort, and
	must have only non-negative integer values. By default the model
	NegBin 2 is used, in which the conditional variance of the count is
	given by &mu;(1 + &alpha;&mu;), where &mu; denotes the conditional
	mean.  But if the <opt>--model1</opt> option is given the conditional
	variance is &mu;(1 + &alpha;).
      </para>
      <para>
	The optional <lit>offset</lit> series works in the same way as for the
	<cmdref targ="poisson"/> command.  The Poisson model is a restricted
	form of the Negative Binomial in which &alpha; = 0 by construction.
      </para>
      <para>
	By default, standard errors are computed using a numerical
	approximation to the Hessian at convergence.  But if the
	<opt>--opg</opt> option is given the covariance matrix is based on
	the Outer Product of the Gradient (OPG), or if the
	<opt>--robust</opt> option is given QML standard errors are
	calculated, using a <quote>sandwich</quote> of the inverse of the
	Hessian and the OPG.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Count data...</menu-path>
    </gui-access>
  </command>

  <command name="nls" section="Estimation"
    label="Nonlinear Least Squares">

    <usage>
      <arguments>
        <argument>function</argument>
        <argument optional="true">derivatives</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't show estimated model</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
      </options>
      <examples>
	<demos>
	  <demo>wg_nls.inp</demo>
	</demos>
      </examples>
    </usage>

    <description context="gui">
      <para>
	Performs Nonlinear Least Squares (NLS) estimation using a
	modified version of the Levenberg&ndash;Marquardt
	algorithm. You must supply a function specification; it is
	recommended but not required that you also supply expressions
	for the derivatives of this function with respect to each of
	the parameters if possible.  If you do not supply derivatives
	you should instead give a list of the parameters to be
	estimated (separated by spaces or commas), preceded by the
	keyword <lit>params</lit>; these can be either scalars, or
	vectors, or any combination of the two.
      </para>
      <para>
	Example: Suppose we have a data set with variables
	<math>C</math> and <math>Y</math> (&eg;
	<lit>greene11_3.gdt</lit>) and we wish to estimate a nonlinear
	consumption function of the form
	<equation status="display"
	  tex="\[C = \alpha + \beta Y^{\gamma}\]"
	  ascii="C = alpha + beta * Y^gamma"
	  graphic="greene_Cfunc"/>
      </para>
      <para>
	The parameters alpha, beta and gamma must first be added to
	the dataset and given initial values.  Appropriate lines may
	be typed into the NLS specification window prior to the
	function specification.
      </para>
      <para>
	In the NLS window we type the following lines:
      </para>
      <code>
	C = alpha + beta * Y^gamma
	deriv alpha = 1
	deriv beta = Y^gamma
	deriv gamma = beta * Y^gamma * log(Y)
      </code>
      <para>
	The first line specifies the regression function, and the next
	three lines supply the derivatives of that function with respect
	to each of the parameters in turn. If the "deriv" lines are not
	given, a numerical approximation to the Jacobian is computed.
      </para>
      <para>
	If the parameters alpha, beta and gamma were not previously
	declared we could preface the above lines with something like the
	following:
      </para>
      <code>
	scalar alpha = 1
	scalar beta = 1
	scalar gamma = 1
      </code>
      <para>For further details on NLS estimation please see 
	<guideref targ="chap:nls"/>.
      </para>
    </description>

    <description context="cli">
      <para>
	Performs Nonlinear Least Squares (NLS) estimation using a modified
	version of the Levenberg&ndash;Marquardt algorithm.  You must
	supply a function specification.  The parameters of this function
	must be declared and given starting values (using the
	<cmd>genr</cmd> command) prior to estimation.  Optionally, you may
	specify the derivatives of the regression function with respect to
	each of the parameters.  If you do not supply derivatives you
	should instead give a list of the parameters to be estimated
	(separated by spaces or commas), preceded by the keyword
	<lit>params</lit>.  In the latter case a numerical approximation
	to the Jacobian is computed.
      </para>
      <para>
	It is easiest to show what is required by example.  The
	following is a complete script to estimate the nonlinear
	consumption function set out in William Greene's
	<book>Econometric Analysis</book> (Chapter 11 of the 4th
	edition, or Chapter 9 of the 5th).  The numbers to the left of
	the lines are for reference and are not part of the commands.
	Note that any option flags, such as <opt>--vcv</opt> for
	printing the covariance matrix of the parameter estimates,
	should be appended to the final command, <lit>end nls</lit>.
      </para>
      <code>
	1   open greene11_3.gdt
	2   ols C 0 Y
	3   scalar a = $coeff(0)
	4   scalar b = $coeff(Y)
	5   scalar g = 1.0
	6   nls C = a + b * Y^g
	7    deriv a = 1
	8    deriv b = Y^g
	9    deriv g = b * Y^g * log(Y)
	10  end nls --vcv
      </code>
      <para>
	It is often convenient to initialize the parameters by
	reference to a related linear model; that is accomplished here
	on lines 2 to 5.  The parameters alpha, beta and gamma could
	be set to any initial values (not necessarily based on a model
	estimated with OLS), although convergence of the NLS procedure
	is not guaranteed for an arbitrary starting point.
      </para>
      <para>
	The actual NLS commands occupy lines 6 to 10. On line 6 the
	<cmd>nls</cmd> command is given: a dependent variable is
	specified, followed by an equals sign, followed by a function
	specification.  The syntax for the expression on the right is
	the same as that for the <cmd>genr</cmd> command.  The next
	three lines specify the derivatives of the regression function
	with respect to each of the parameters in turn.  Each line
	begins with the keyword <cmd>deriv</cmd>, gives the name of a
	parameter, an equals sign, and an expression whereby the
	derivative can be calculated (again, the syntax here is the
	same as for <cmd>genr</cmd>). As an alternative to supplying
	numerical derivatives, you could substitute the following for
	lines 7 to 9:
      </para>
      <code>
	params a b g
      </code>
      <para>
	Line 10, <cmd>end nls</cmd>, completes the command and calls for
	estimation. Any options should be appended to this line.
      </para>
      <para>
	For further details on NLS estimation please see
	<guideref targ="chap:nls"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Nonlinear Least Squares</menu-path>
    </gui-access>

  </command>

  <command name="normtest" section="Tests" label="Normality test">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--dhansen</flag>
	  <effect>Doornik&ndash;Hansen test, the default</effect>
        </option>
	<option>
	  <flag>--swilk</flag>
	  <effect>Shapiro&ndash;Wilk test</effect>
        </option>
	<option>
	  <flag>--lillie</flag>
	  <effect>Lilliefors test</effect>
        </option>
	<option>
	  <flag>--jbera</flag>
	  <effect>Jarque&ndash;Bera test</effect>
        </option>
	<option>
	  <flag>--all</flag>
	  <effect>do all tests</effect>
        </option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printed output</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Carries out a test for normality for the given
	<repl>series</repl>.  The specific test is controlled by the
	option flags (but if no flag is given, the Doornik&ndash;Hansen
	test is performed).  Note: the Doornik&ndash;Hansen and
	Shapiro&ndash;Wilk tests are recommended over the others, on
	account of their superior small-sample properties.
      </para>
      <para>
	The test statistic and its p-value may be retrieved
	using the accessors <lit>$test</lit> and <lit>$pvalue</lit>.
	Please note that if the <opt>--all</opt> option is given,
	the result recorded is that from the Doornik&ndash;Hansen
	test.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Normality test</menu-path>
    </gui-access>

  </command>

  <command name="nulldata" section="Dataset"
    label="Creating a blank dataset">

    <usage>
      <arguments>
        <argument>series_length</argument>
      </arguments>
      <options>
	<option>
	  <flag>--preserve</flag>
	  <effect>preserve matrices</effect>
        </option>
      </options>
      <examples>
        <example>nulldata 500</example>
      </examples>
    </usage>

    <description>
      <para>
	Establishes a <quote>blank</quote> data set, containing only a
	constant and an index variable, with periodicity 1 and the
	specified number of observations. This may be used for
	simulation purposes: some of the <cmd>genr</cmd> commands
	(&eg; <cmd>genr uniform()</cmd>, <cmd>genr normal()</cmd>)
	will generate dummy data from scratch to fill out the data
	set. This command may be useful in conjunction with
	<cmd>loop</cmd>.  See also the <quote>seed</quote> option to
	the <cmdref targ="set"/> command.
      </para>
      <para>
	By default, this command cleans out all data in gretl's current
	workspace.  If you give the <opt>--preserve</opt> option, however,
	any currently defined matrices are retained.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/New data set</menu-path>
    </gui-access>

  </command>

  <command name="ols" section="Estimation" label="Ordinary Least Squares">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>clustered standard errors</effect>
        </option>
        <option>
	  <flag>--jackknife</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--simple-print</flag>
	  <effect>do not print auxiliary statistics</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
        <option>
	  <flag>--anova</flag>
	  <effect>print an ANOVA table</effect>
        </option>
        <option>
	  <flag>--no-df-corr</flag>
	  <effect>suppress degrees of freedom correction</effect>
        </option>
        <option>
	  <flag>--print-final</flag>
	  <effect>see below</effect>
        </option>
      </options>
      <examples>
        <example>ols 1 0 2 4 6 7</example>
	<example>ols y 0 x1 x2 x3 --vcv</example>
	<example>ols y 0 x1 x2 x3 --quiet</example>
      </examples>
    </usage>

    <description>
      <para context="gui">
        Computes ordinary least squares (OLS) estimates for the
	specified model.
      </para>

      <para context="cli">
        Computes ordinary least squares (OLS) estimates with
	<repl>depvar</repl> as the dependent variable and
	<repl>indepvars</repl> as the list of independent variables.
	Variables may be specified by name or number; use the number
	zero for a constant term. 
      </para>

      <para>Besides coefficient estimates and standard errors, the
	program also prints p-values for <math>t</math>
	(two-tailed) and <math>F</math>-statistics.  A p-value
	below 0.01 indicates statistical significance at the 1 percent
	level and is marked with <lit>***</lit>. <lit>**</lit>
	indicates significance between 1 and 5 percent and
	<lit>*</lit> indicates significance between the 5 and 10
	percent levels. Model selection statistics (the Akaike
	Information Criterion or AIC and Schwarz's Bayesian Information
	Criterion) are also printed.  The formula used for the AIC is
	that given by <cite key="akaike74">Akaike (1974)</cite>, namely 
	minus two times the maximized log-likelihood plus two times the 
	number of parameters estimated.</para>

      <para context="cli">If the option <opt>--no-df-corr</opt> is
	given, the usual degrees of freedom correction is not applied
	when calculating the estimated error variance (and hence also
	the standard errors of the parameter estimates).</para>

      <para context="cli">
	The option <opt>--print-final</opt> is applicable only in the
	context of a <cmdref targ="loop"/>.  It arranges for the
	regression to be run silently on all but the final iteration
	of the loop. See <guideref targ="chap:looping"/> for details.
      </para>

      <para context="cli">
	Various internal variables may be retrieved following
	estimation. For example
      </para>
      <code context="cli">
	series uh = $uhat
      </code>
      <para context="cli">
	saves the residuals under the name <lit>uh</lit>.  See the
	<quote>accessors</quote> section of the gretl function
	reference for details.
      </para>

      <para context="cli">
	The specific formula (<quote>HC</quote> version) used for
	generating robust standard errors when the <opt>--robust</opt>
	option is given can be adjusted via the <cmdref targ="set"/>
	command.  The <opt>--jackknife</opt> option has the effect of
	selecting an <lit>hc_version</lit> of <lit>3a</lit>. The
	<opt>cluster</opt> overrides the selection of HC version, and
	produces robust standard errors by grouping the observations
	by the distinct values of <repl>clustvar</repl>; see <guideref
	targ="chap:robust_vcv"/> for details.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Ordinary Least Squares</menu-path>
      <other-access>Beta-hat button on toolbar</other-access>
    </gui-access>

  </command>

  <command name="omit" section="Tests" label="Omit variables">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--test-only</flag>
	  <effect>don't replace the current model</effect>
	</option>
	<option>
	  <flag>--chi-square</flag>
	  <effect>give chi-square form of Wald test</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>print only the basic test result</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix for reduced model</effect>
	</option>
	<option>
	  <flag>--auto</flag>
	  <optparm optional="true">alpha</optparm>
	  <effect>sequential elimination, see below</effect>
	</option>
      </options>
      <examples>
        <example>omit 5 7 9</example>
        <example>omit seasonals --quiet</example>
        <example>omit --auto</example>
        <example>omit --auto=0.05</example>
      </examples>
    </usage>

    <description context="gui">
      <para>
	This command re-estimates the given model after omitting the specified
	variables, or after sequentially omitting insignificant variables if
	the relevant box is available and is checked.  Besides the usual model
	output, it prints a test for the joint significance of the omitted
	variables. The null hypothesis is that the true coefficients on all
	the omitted variables equal zero.
      </para>
      <para>
	Sequential elimination works as follows: at each step the variable
	with the highest p-value is omitted, until all remaining variables
	have a p-value no greater than some cutoff.  The default cutoff is 10
	percent (two-sided); this can be adjusted via the spin button.  By
	default this process operates on all variables in the model (apart
	from the constant).  If you want to confine it to a subset of the
	variables, check the box labeled <quote>Test only selected
	  variables</quote> and make a selection.
      </para>
    </description>

    <description context="cli">
      <para>
	This command must follow an estimation command.  It calculates
	a Wald test for the joint significance of the variables in
	<repl>varlist</repl>, which should be a subset of the
	independent variables in the model last estimated. The
	results of the test may be retrieved using the accessors
	<lit>$test</lit> and <lit>$pvalue</lit>.
      </para>
      <para>
	By default the restricted model is estimated and it replaces
	the original as the <quote>current model</quote> for the
	purposes of, for example, retrieving the residuals as
	<lit>$uhat</lit> or doing further tests. This behavior may be
	suppressed via the <opt>--test-only</opt> option.
      </para>
      <para>
	By default the <math>F</math>-form of the Wald test is
	recorded; the <opt>--chi-square</opt> option may be used to
	record the chi-square form instead.
      </para>
      <para>
	If the restricted model is both estimated and printed, the
	<opt>--vcv</opt> option has the effect of printing its
	covariance matrix, otherwise this option is ignored.
      </para>
      <para>
	Alternatively, if the <opt>--auto</opt> flag is given,
	sequential elimination is performed: at each step the variable
	with the highest p-value is omitted, until all remaining
	variables have a p-value no greater than some cutoff.  The
	default cutoff is 10 percent (two-sided); this can be adjusted
	by appending <quote><lit>=</lit></quote> and a value between 0
	and 1 (with no spaces), as in the fourth example above.  If
	<repl>varlist</repl> is given this process is confined to the
	listed variables, otherwise all variables are treated as
	candidates for omission. Note that the <opt>--auto</opt>
	and <opt>--test-only</opt> options cannot be combined.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Omit variables</menu-path>
    </gui-access>

  </command>

  <command name="online" section="Dataset" context="gui"
    label="Access online databases">

    <description>
      <para>
	Gretl is able to access databases at Wake Forest University
	(your computer must be connected to the internet for this to
	work).
      </para>
      <para>
	Under the <quote>File, Browse databases</quote> menu,
	select the item <quote>on database server</quote>. A window
	should appear, showing a listing of the gretl databases
	available at Wake Forest. (Depending on your location and the
	speed of your internet connection, this may take a few
	seconds.)  Along with the name of the database and a short
	description, there will appear a <quote>Local status</quote>
	entry: this shows whether you have the database installed
	locally (on the hard drive of your computer) and if so,
	whether or not it is up to date with the version on the
	server.
      </para>
      <para>
	If you have a given database installed locally, and it is
	up to date, there is no advantage in accessing it via the
	server.  But for a database that is not already installed and
	up to date, you may wish to get a listing of the data series:
	click on <quote>Get series listing</quote>.  This brings up a
	further window, from which you can display the values of a
	chosen data series, graph those values, or import them into
	gretl's workspace.  These tasks can be accomplished using the
	<quote>Series</quote> menu, or via the popup menu that appears
	when you click the right mouse button on a given series.  You
	can also search the listing for a variable of interest (the
	<quote>Find</quote> menu item).
      </para>
      <para>
	If you want faster access to the data, or wish to access
	the database offline, then select the line showing the
	database you want, in the initial database window, and press
	the <quote>Install</quote> button.  This will download the
	database in compressed format, then uncompress it and install
	it on your hard drive. Thereafter you should be able to find
	it under the <quote>File, Browse databases, gretl
	  native</quote> menu.
      </para>
    </description>
  </command>

  <command name="open" section="Dataset" 
    label="Open a data file" context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print list of series</effect>
	</option>
	<option>
	  <flag>--preserve</flag>
	  <effect>preserve variables other than series</effect>
	</option>
	<option>
	  <flag>--frompkg</flag>
	  <optparm>pkgname</optparm>
	  <effect>see below</effect>
	</option>	
	<option>
	  <flag>--www</flag>
	  <effect>use a database on the gretl server</effect>
	</option>
	<option>
	  <note>See below for additional specialized options</note>
	</option>
      </options>
      <examples>
        <example>open data4-1</example>
        <example>open voter.dta</example>
	<example>open fedbog --www</example>
      </examples>
    </usage>

    <description>
      <para>
	Opens a data file or database.  If a data file is already
	open, it is replaced by the newly opened one. To add data to
	the current dataset, see <cmdref targ="append"/> and (for
	greater flexibility) <cmdref targ="join"/>.
      </para>
      <para>
	If a full path is not given, the program will search some relevant
	paths to try to find the file.  If no filename suffix is given (as
	in the first example above), gretl assumes a native datafile with
	suffix <lit>.gdt</lit>.  Based on the name of the file and various
	heuristics, gretl will try to detect the format of the data file
	(native, plain text, CSV, MS Excel, Stata, SPSS, etc.).
      </para>
      <para>
	If the <opt>frompkg</opt> option is used, gretl will look for
	the specified data file in the subdirectory associated with
	the function package specified by <repl>pkgname</repl>.
      </para>
      <para>
	If the <repl>filename</repl> argument takes the form of a
	URI starting with <lit>http://</lit>, then gretl will attempt
	to download the indicated data file before opening it.
      </para>
      <para>
	By default, opening a new data file clears the current gretl
	session, which includes deletion of all named variables,
	including matrices, scalars and strings.  If you wish to keep
	your currently defined variables (other than series, which are
	necessarily cleared out), use the <opt>--preserve</opt>
	option.
      </para>
      <para>
	The <lit>open</lit> command can also be used to open a database
	(gretl, RATS 4.0 or PcGive) for reading.  In that case it should be
	followed by the <cmdref targ="data"/> command to extract particular
	series from the database.  If the <lit>www</lit> option is given, the
	program will try to access a database of the given name on the gretl
	server &mdash; for instance the Federal Reserve interest rates
	database in the third example above.
      </para>
      <para>
	When opening a spreadsheet file (Gnumeric, Open Document or MS
	Excel), you may give up to three additional parameters
	following the filename.  First, you can select a particular
	worksheet within the file.  This is done either by giving its
	(1-based) number, using the syntax, &eg;,
	<opt>--sheet=2</opt>, or, if you know the name of the sheet,
	by giving the name in double quotes, as in
	<opt>--sheet="MacroData"</opt>. The default is to read the
	first worksheet. You can also specify a column and/or row
	offset into the worksheet via, &eg;,
      </para>
      <code>
	--coloffset=3 --rowoffset=2
      </code>
      <para>
	which would cause gretl to ignore the first 3 columns and the first 2
	rows.  The default is an offset of 0 in both dimensions, that is, to
	start reading at the top-left cell.
      </para>
      <para>
	With plain text files, gretl generally expects to find the data
	columns delimited in some standard manner.  But there is also a
	special facility for reading <quote>fixed format</quote> files, in
	which there are no delimiters but there is a known specification of
	the form, &eg;, <quote>variable <math>k</math> occupies 8 columns
	starting at column 24</quote>.  To read such files, you should append
	a string <opt>--fixed-cols=</opt><repl>colspec</repl>, where
	<repl>colspec</repl> is composed of comma-separated integers.  These
	integers are interpreted as a set of pairs.  The first element of each
	pair denotes a starting column, measured in bytes from the beginning
	of the line with 1 indicating the first byte; and the second element
	indicates how many bytes should be read for the given field.  So, for
	example, if you say
      </para>
      <code>
	open fixed.txt --fixed-cols=1,6,20,3
      </code>
      <para>
	then for variable 1 gretl will read 6 bytes starting at column 1; and
	for variable 2, 3 bytes starting at column 20.  Lines that are blank,
	or that begin with <lit>#</lit>, are ignored, but otherwise the
	column-reading template is applied, and if anything other than a valid
	numerical value is found an error is flagged.  If the data are read
	successfully, the variables will be named <lit>v1</lit>,
	<lit>v2</lit>, etc.  It's up to the user to provide meaningful names
	and/or descriptions using the commands <cmdref targ="rename"/> and/or 
	<cmdref targ="setinfo"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Open data</menu-path>
      <other-access>Drag a data file onto gretl's main window</other-access>
    </gui-access>

  </command>

  <command name="orthdev" section="Transformations" 
    label="Orthogonal deviations" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Applicable with panel data only. A series of forward orthogonal
	deviations is obtained for each variable in <repl>varlist</repl> and
	stored in a new variable with the prefix <lit>o_</lit>. Thus
	<cmd>orthdev x y</cmd> creates the new variables <lit>o_x</lit> and
	<lit>o_y</lit>.
      </para>
      <para>
	The values are stored one step ahead of their true temporal location
	(that is, <lit>o_x</lit> at observation <math>t</math> holds the
	deviation that, strictly speaking, belongs at <math>t</math> &minus;
	1).  This is for compatibility with first differences: one loses the
	first observation in each time series, not the last.
      </para>
    </description>

  </command>

  <command name="outfile" section="Printing" 
    label="Direct printing to file" context="cli">

    <usage>
      <altforms>
	<altform><lit>outfile</lit> <repl>filename</repl> <repl>option</repl></altform>
	<altform><lit>outfile --close</lit></altform>
      </altforms>
      <options>
        <option>
	  <flag>--append</flag>
	  <effect>append to file</effect>
        </option>
        <option>
	  <flag>--write</flag>
	  <effect>overwrite file</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>see below</effect>
        </option>	
      </options>
      <examples>
        <example>outfile regress.txt --write</example>
        <example>outfile --close</example>
      </examples>
    </usage>

    <description>
      <para>
	Diverts output to <repl>filename</repl>, until further
	notice.  Use the flag <opt>--append</opt> to append output to
	an existing file or <opt>--write</opt> to start a new file
	(or overwrite an existing one).  Only one file can be opened
	in this way at any given time.
      </para>
      <para>
	The <opt>--close</opt> flag is used to close an output
	file that was previously opened as above.  Output will then
	revert to the default stream. Note that since only one file
	can be opened via <lit>outfile</lit> at any given time, no
	filename argument need (nor should) be supplied with this
	variant of the command.
      </para>
      <para>
	In the first example command above, the file
	<filename>regress.txt</filename> is opened for writing, and in
	the second it is closed.  This would make sense as a sequence
	only if some commands were issued before the
	<opt>--close</opt>.  For example if an <cmd>ols</cmd> command
	intervened, its output would go to
	<filename>regress.txt</filename> rather than the screen.
      </para>
      <para>
	Three special variants on the above are available. If you give
	the keyword <lit>null</lit> in place of a real filename along
	with the <opt>--write</opt> option, the effect is to suppress
	all printed output until redirection is ended. If either of
	the keywords <lit>stdout</lit> or <lit>stderr</lit> are given
	in place of a regular filename the effect is to redirect
	output to standard output or standard error output
	respectively.
      </para>
      <para>
	The <opt>--quiet</opt> option is for use with <opt>--write</opt>
	or <opt>--append</opt>: its effect is to turn off the echoing
	of commands and the printing of auxiliary messages while
	output is redirected. It is equivalent to doing
      </para>
      <code>
	set echo off
	set messages off
      </code>
      <para>
	except that when redirection is ended the original values of
	the <lit>echo</lit> and <lit>messages</lit> variables are
	restored.
      </para>
    </description>

  </command>

  <command name="panel" section="Estimation" label="Panel models">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
        <option>
	  <flag>--fixed-effects</flag>
	  <effect>estimate with group fixed effects</effect>
        </option>
        <option>
	  <flag>--random-effects</flag>
	  <effect>random effects or GLS model</effect>
        </option>
        <option>
	  <flag>--nerlove</flag>
	  <effect>use the Nerlove transformation</effect>
        </option>
        <option>
	  <flag>--between</flag>
	  <effect>estimate the between-groups model</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors; see below</effect>
        </option>
        <option>
	  <flag>--time-dummies</flag>
	  <effect>include time dummy variables</effect>
        </option>
        <option>
	  <flag>--unit-weights</flag>
	  <effect>weighted least squares</effect>
        </option>
        <option>
	  <flag>--iterate</flag>
	  <effect>iterative estimation</effect>
        </option>
        <option>
	  <flag>--matrix-diff</flag>
	  <effect>use matrix-difference method for Hausman test</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>less verbose output</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>more verbose output</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Estimates a panel model.  By default the fixed effects estimator is
	used; this is implemented by subtracting the group or unit means from
	the original data.
      </para>
      <para context="cli">
	If the <opt>--random-effects</opt> flag is given, random
	effects estimates are computed, by default using the method of
	<cite key="swamy72">Swamy and Arora (1972)</cite>. In this
	case (only) the option <opt>--matrix-diff</opt> forces use of
	the matrix-difference method (as opposed to the regression
	method) for carrying out the Hausman test for the consistency
	of the random effects estimator. Also specific to the random
	effects estimator is the <opt>--nerlove</opt> flag, which
	selects the method of <cite key="nerlove71">Nerlove
	(1971)</cite> as opposed to Swamy and Arora.
      </para>
      <para context="cli">
	Alternatively, if the <opt>--unit-weights</opt> flag is given, the
	model is estimated via weighted least squares, with the weights
	based on the residual variance for the respective cross-sectional
	units in the sample.  In this case (only) the <opt>--iterate</opt>
	flag may be added to produce iterative estimates: if the
	iteration converges, the resulting estimates are Maximum
	Likelihood.
      </para>
      <para context="cli">
	As a further alternative, if the <opt>--between</opt> flag is
	given, the between-groups model is estimated (that is, an OLS
	regression using the group means).
      </para>
      <para context="cli">
	The <opt>--robust</opt> option is available only for fixed
	effects models. The default variant is the Arellano HAC
	estimator, but Beck&ndash;Katz <quote>Panel Corrected Standard
	Errors</quote> can be selected via the command <lit>set pcse
	on</lit>. When the robust option is specified the joint
	<math>F</math> test on the fixed effects is performed using
	the robust method of <cite key="welch51">Welch (1951)</cite>.
      </para>
      <para context="gui">
	If the "Random effects" button is checked, random effects
	(GLS) estimates are computed. By default the method of Swamy
	and Arora is used for the GLS transformation, but the Nerlove
	method is available as an option.
      </para>
      <para>
	For more details on panel estimation, please see <guideref
	  targ="chap:panel"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Panel</menu-path>
    </gui-access>

  </command>

  <command name="panel-between" section="Estimation" context="gui"
    label="Between groups model">

    <description>
      <para>
	This dialog allows you to enter a specification for the
	<quote>between model</quote> in the context of panel data.  This
	regression uses the group-means of the data, thereby ignoring the
	variation within the groups.  This model is rarely of great
	interest in its own right, but may be useful for purposes of
	comparison (for example, with the fixed effects model).
      </para>
    </description>

  </command>    

  <command name="panel-mode" section="Dataset" context="gui"
    label="Panel data organization">

    <description>
      <para>
	This dialog offers up to three options with regard to defining a
	data set as a panel.  The first two options require that the data
	set is already organized in a panel format (although this may not
	yet be recognized by gretl).  The third option requires that the
	data set contains variables that represent the panel structure.
      </para>
      <para>
	<emphasis>Stacked time series</emphasis>: Let there be <repl>N</repl>
	cross-sectional units in the data set, and let <repl>T</repl> = the
	number of time-series observations per unit.  By selecting this option
	you are telling gretl that the data set is currently composed of
	<repl>N</repl> consecutive blocks of <repl>T</repl> time-series
	observations, one for each cross-sectional unit.  The next step will
	be to specify the value of <repl>N</repl>.
      </para>
      <para>
	<emphasis>Stacked cross sections</emphasis>: You are telling gretl
	that the data set is currently composed of <repl>T</repl> consecutive
	blocks of <repl>N</repl> cross-sectional observations, one for each
	time period. The next step, again, will be to specify the value of
	<repl>N</repl>.
      </para>
      <para>
	If the total number of observations in the current dataset is
	prime, the above options are not available.
      </para>
      <para>
	<emphasis>Use index variables</emphasis>: You are saying that the data
	set is currently organized any old way (it doesn't matter how), but
	that it contains two variables that index the cross-sectional units
	and the time periods respectively.  The next step will be to select
	those two variables.  Panel index variables must have nothing but
	non-negative integer values, with no missing values.  If there are no
	such variables in the dataset this option is not available.
      </para>
    </description>

  </command>

  <command name="panel-wls" section="Estimation" context="gui"
    label="Groupwise weighted least squares">

    <description>
      <para>
	Groupwise weighted least squares for panel data.  Computes
	weighted least squares (WLS) estimates, with the weights based on
	the estimated error variances for the respective cross-sectional
	units in the sample.
      </para>
      <para>
	If the iteration option is selected, the procedure is iterated: at
	each round the residuals are re-computed using the current WLS
	parameter estimates, which gives rise to a new set of estimates of
	the error variances, and a hence a new set of weights. Iteration
	stops when the maximum difference in the parameter estimates from
	one round to the next falls below 0.0001 or the number of
	iterations reaches 20.  If the iteration converges, the resulting
	estimates are Maximum Likelihood.
      </para>
    </description>

  </command>

  <command name="pca" section="Statistics"
    label="Principal Components Analysis">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--covariance</flag>
	  <effect>use the covariance matrix</effect>
        </option>	
        <option>
	  <flag>--save</flag>
	  <optparm optional="true">n</optparm>
	  <effect>save major components</effect>
        </option>	
        <option>
	  <flag>--save-all</flag>
	  <effect>save all components</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
      </options>
    </usage>

    <description context="gui">
      <para>
	Principal Components Analysis.  Prints the eigenvalues of the
	correlation matrix (or the covariance matrix if the option box is
	checked) for the variables in <repl>varlist</repl>, along with the
	proportion of the joint variance accounted for by each component.
	Also prints the corresponding eigenvectors (or <quote>component
	  loadings</quote>).
      </para>
      <para>
	In the window displaying the results, you have the option of
	saving the principal components to the dataset as series.
      </para> 
    </description>

    <description context="cli">
      <para>
	Principal Components Analysis. Unless the <opt>--quiet</opt>
	option is given, prints the eigenvalues of the correlation
	matrix (or the covariance matrix if the
	<opt>--covariance</opt> option is given) for the variables
	in <repl>varlist</repl>, along with the proportion of the
	joint variance accounted for by each component. Also prints
	the corresponding eigenvectors (or <quote>component
	loadings</quote>).
      </para>
      <para>
	If you give the <opt>--save-all</opt> option then all
	components are saved to the dataset as series, with names
	<lit>PC1</lit>, <lit>PC2</lit> and so on. These artificial
	variables are formed as the sum of (component loading) times
	(standardized <math>X</math><sub>i</sub>), where
	<math>X</math><sub>i</sub> denotes the <math>i</math>th
	variable in <repl>varlist</repl>.
      </para>
      <para>
	If you give the <opt>--save</opt> option without a parameter
	value, components with eigenvalues greater than the mean
	(which means greater than 1.0 if the analysis is based on the
	correlation matrix) are saved to the dataset as described
	above. If you provide a value for <repl>n</repl> with this
	option then the most important <repl>n</repl> components are
	saved.
      </para>
      <para>
	See also the <fncref targ="princomp"/> function.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Principal components</menu-path>
      <other-access>Main window pop-up (multiple selection)</other-access>
    </gui-access>

  </command>

  <command name="pergm" section="Statistics" label="Periodogram">

    <usage>
      <arguments>
        <argument>series</argument>
        <argument optional="true">bandwidth</argument>
      </arguments>
      <options>
        <option>
	  <flag>--bartlett</flag>
	  <effect>use Bartlett lag window</effect>
        </option>
        <option>
	  <flag>--log</flag>
	  <effect>use log scale</effect>
        </option>
        <option>
	  <flag>--radians</flag>
	  <effect>show frequency in radians</effect>
        </option>
        <option>
	  <flag>--degrees</flag>
	  <effect>show frequency in degrees</effect>
        </option>
	<option>
	  <flag>--plot</flag>
	  <optparm>mode-or-filename</optparm>
	  <effect>see below</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Computes and displays the spectrum of the specified series.
	By default the sample periodogram is given, but optionally a
	Bartlett lag window is used in estimating the spectrum (see,
	for example, Greene's <book>Econometric Analysis</book> for a
	discussion of this).  The default width of the Bartlett window
	is twice the square root of the sample size but this can be
	set manually using the <repl>bandwidth</repl> parameter, up to
	a maximum of half the sample size.
      </para>
      <para>
	If the <opt>--log</opt> option is given the spectrum is
	represented on a logarithmic scale.
      </para>
      <para>
	The (mutually exclusive) options <opt>--radians</opt> and
	<opt>--degrees</opt> influence the appearance of the frequency
	axis when the periodogram is graphed. By default the frequency
	is scaled by the number of periods in the sample, but these
	options cause the axis to be labeled from 0 to &pi; radians
	or from 0 to 180&deg;, respectively.
      </para>
      <para>
	By default, if the program is not in batch mode a plot of the
	periodogram is shown.  This can be adjusted via the
	<opt>plot</opt> option. The acceptable parameters to this
	option are <lit>none</lit> (to suppress the plot);
	<lit>display</lit> (to display a plot even when in batch
	mode); or a file name. The effect of providing a file name is
	as described for the <opt>output</opt> option of the <cmdref
	targ="gnuplot"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Periodogram</menu-path>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="plot" section="Graphs" context="cli">
    <usage>
      <arguments>
        <argument>data</argument>
      </arguments>
      <options>
        <option>
	  <flag>--with-lines</flag>
	  <optparm optional="true">varspec</optparm>
	  <effect>use lines, not points</effect>
        </option>
        <option>
	  <flag>--with-lp</flag>
	  <optparm optional="true">varspec</optparm>
	  <effect>use lines and points</effect>
        </option>
        <option>
	  <flag>--with-impulses</flag>
	  <optparm optional="true">varspec</optparm>
	  <effect>use vertical lines</effect>
        </option>
        <option>
	  <flag>--time-series</flag>
	  <effect>plot against time</effect>
        </option>
        <option>
	  <flag>--single-yaxis</flag>
	  <effect>force use of just one y-axis</effect>
        </option>
        <option>
	  <flag>--dummy</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--fit</flag>
	  <optparm>fitspec</optparm>
	  <effect>see below</effect>
        </option>	
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>
      </options>
    </usage>
    
    <description>
      <para>
	The <lit>plot</lit> block provides an alternative to the
	<cmdref targ="gnuplot"/> command which may be more convenient
	when you are producing an elaborate plot (with several options
	and/or gnuplot commands to be inserted into the plot file).
      </para>
      <para>
	A <lit>plot</lit> block starts with the command-word
	<lit>plot</lit> followed by the required argument,
	<repl>data</repl>, which specifies the data to be plotted:
	this should be the name of a list, a matrix, or a single
	series.
      </para>
      <para>
	If a list or matrix is given, the last element (list) or
	column (matrix) is assumed to be the <math>x</math>-axis
	variable and the other(s) the <math>y</math>-axis variable(s),
	unless the <opt>time-series</opt> option is given in which
	case all the specified data go on the <math>y</math> axis.
      </para>
      <para>
	The option of supplying a single series name is restricted to
	time-series data, in which case it is assumed that a
	time-series plot is wanted; otherwise an error is flagged.
      </para>
      <para>
	The starting line may be prefixed with
	the <quote><repl>savename</repl> <lit>&lt;-</lit></quote>
	apparatus to save a plot as an icon in the GUI program.
	The block ends with <lit>end plot</lit>.
      </para>
      <para>
	Inside the block you have zero or more lines of these types,
	identified by an initial keyword:
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>option</lit>: specify a single option.
	  </para>
	</li>
	<li>
	  <para>
	  <lit>options</lit>: specify multiple options on a single
	  line, separated by spaces.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>literal</lit>: a command to be passed to gnuplot
	    literally.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>printf</lit>: a printf statement whose result will be
	    passed to gnuplot literally.
	  </para>
	</li>
      </ilist>
      <para>
	Note that when you specify an option using the
	<lit>option</lit> or <lit>options</lit> keywords, it is not
	necessary to supply the customary double-dash before the
	option specifier. For details on the effects of the various
	options please see <cmdref targ="gnuplot"/>.
      </para>
      <para>
	The intended use of the <lit>plot</lit> block is best
	illustrated by example:
      </para>
      <code>
	string title = "My title"
	string xname = "My x-variable"
	plot plotmat
	    options with-lines fit=none
	    literal set linetype 3 lc rgb "#0000ff"
	    literal set nokey
	    printf "set title \"%s\"", title
	    printf "set xlabel \"%s\"", xname
	end plot --output=display
      </code>
      <para>
	This example assumes that <lit>plotmat</lit> is the name of a
	matrix with at least 2 columns (or a list with at least two
	members).  Note that it is considered good practice to place
	the <opt>output</opt> option (only) on the last line of the
	block.
      </para>
    </description>

  </command>

  <command name="polyweights" section="Transformations" context="gui"
    label="Polynomial trend fitting">

    <description>
      <para>
	In fitting a polynomial trend to a time series it may be
	desirable to give extra weight to the observations at the
	start and end of the sample. (Points in the middle of the
	sample range have neighbours on both sides that are likely to
	be pulling the fit in the same general direction.)
      </para>
      <para>
	The weighting schemes offered here (quadratic, cosine-bell and
	steps) can be used to this effect. If you select one of these
	schemes two additional settings must be chosen: first, what
	maximum weight should be used (the minimum, baseline weight is
	1.0)?  Second, what central fraction of the sample should be
	given a uniform (minimal) weighting?
      </para>
      <para>
	Suppose, for example, you select a maximum weight of 3.0 and a
	central fraction of 0.4. This means that the middle 40 percent
	of the data get a weight of 1.0. If the <quote>steps</quote>
	shape is selected the first and last 30 percent of the
	observations get a weight of 3.0; otherwise, for the first 30
	percent of observations the weights decline gradually from 3.0
	to 1.0; and for the last 30 percent the weights increase from
	1.0 to 3.0.
      </para>
    </description>

  </command>

  <command name="poisson" section="Estimation" 
    label="Poisson estimation">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
        <argument separated="true" optional="true">offset</argument>
      </arguments>
      <options>
        <option>
          <flag>--robust</flag>
          <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
        <option>
          <flag>--vcv</flag>
          <effect>print covariance matrix</effect>
        </option>
        <option>
          <flag>--verbose</flag>
          <effect>print details of iterations</effect>
        </option>
      </options>      
      <examples>
        <example>poisson y 0 x1 x2</example>
	<example>poisson y 0 x1 x2 ; S</example>
      </examples>
    </usage>

    <description>
      <para>
	Estimates a poisson regression.  The dependent variable is taken to
	represent the occurrence of events of some sort, and must take on only
	non-negative integer values.
      </para>
      <para>
	If a discrete random variable <math>Y</math> follows
	the Poisson distribution, then
        <equation status="display" 
          tex="\[\mathrm{Pr}(Y = y) = \frac{e^{-v} v^y}{y!}\]"
          ascii="Pr(Y = y) = exp(-v) * v^y / y!"
          graphic="poisson1"/>
	for <math>y</math> = 0, 1,
      2,&hellip;.  The mean and variance of the distribution are both
      equal to <math>v</math>.  In the Poisson regression model,
      the parameter <math>v</math> is represented as a function
      of one or more independent variables.  The most common version
      (and the only one supported by gretl) has
        <equation status="display" 
          tex="\[v = \mathrm{exp}(\beta_0+\beta_1 x_1+\beta_2 x_2 + \cdots)\]"
          ascii="v = exp(b0 + b1*x1 + b2*x2 + ...)"
          graphic="poisson2"/>
	or in other words the log of
      <math>v</math> is a linear function of the independent
      variables.
      </para>
      <para>
	Optionally, you may add an <quote>offset</quote> variable to the
	specification.  This is a scale variable, the log of which is added to
	the linear regression function (implicitly, with a coefficient of
	1.0).  This makes sense if you expect the number of occurrences of the
	event in question to be proportional, other things equal, to some
	known factor.  For example, the number of traffic accidents might be
	supposed to be proportional to traffic volume, other things equal, and
	in that case traffic volume could be specified as an
	<quote>offset</quote> in a Poisson model of the accident rate. The
	offset variable must be strictly positive.  
      </para>
      <para>
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <opt>--robust</opt> flag is given,
	then QML or Huber&ndash;White standard errors are calculated
	instead. In this case the estimated covariance matrix is a
	<quote>sandwich</quote> of the inverse of the estimated Hessian
	and the outer product of the gradient.
      </para>
      <para>
	See also <cmdref targ="negbin"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Count data...</menu-path>
    </gui-access>

  </command>

  <command name="print" section="Printing" 
    label="Print data or strings" context="cli">

    <usage>
      <altforms>
        <altform><lit>print</lit> <repl>varlist</repl></altform>
	<altform><lit>print</lit></altform>
	<altform><lit>print</lit> <repl>object_name</repl></altform>
        <altform><lit>print</lit> <repl>string_literal</repl></altform>
      </altforms>
      <options>
	<option>
	  <flag>--byobs</flag>
	  <effect>by observations</effect>
	</option>
	<option>
	  <flag>--no-dates</flag>
	  <effect>use simple observation numbers</effect>
	</option>
      </options>
      <examples>
	<example>print x1 x2 --byobs</example>
	<example>print my_matrix</example>
	<example>print "This is a string"</example>
      </examples>
    </usage>

    <description>
      <para>
	If <repl>varlist</repl> is given, prints the values of the specified
	series, or if no argument is given, prints the values of all series in
	the current dataset. If the <opt>--byobs</opt> flag is added the data
	are printed by observation, otherwise they are printed by variable.
	When printing by observation, the default is to show the date (with
	time-series data) or the observation marker string (if any) at the
	start of each line. The <opt>--no-dates</opt> option suppresses the
	printing of dates or markers; a simple observation number is shown
	instead.
      </para>
      <para>
	Besides printing series, you may give the name of a (single) matrix or
	scalar variable for printing. Or you may give a literal string
	argument, enclosed in double quotes, to be printed as is. In these
	case the option flags are not applicable.
      </para>
      <para>
	Note that you can gain greater control over the printing format (and
	so, for example, expose a greater number of digits than are shown by
	default) by using <cmdref targ="printf"/>.
      </para>	
    </description>

    <gui-access>
      <menu-path>/Data/Display values</menu-path>
    </gui-access>

  </command>

  <command name="printf" section="Printing" 
    label="Formatted printing" context="cli">

    <usage>
      <arguments>
        <argument>format</argument>
	<argpunct>, </argpunct>
        <argument>args</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Prints scalar values, series, matrices, or strings under the control
	of a format string (providing a subset of the <lit>printf()</lit>
	statement in the C programming language).  Recognized numeric formats
	are <lit>%e</lit>, <lit>%E</lit>, <lit>%f</lit>, <lit>%g</lit>,
	<lit>%G</lit> and <lit>%d</lit>, in each case with the various
	modifiers available in C.  Examples: the format <lit>%.10g</lit>
	prints a value to 10 significant figures; <lit>%12.6f</lit> prints a
	value to 6 decimal places, with a width of 12 characters.  The format
	<lit>%s</lit> should be used for strings.
      </para>
      <para>
	The format string itself must be enclosed in double quotes.  The
	values to be printed must follow the format string, separated by
	commas.  These values should take the form of either (a) the names of
	variables, (b) expressions that are valid for the <cmd>genr</cmd>
	command, or (c) the special functions <lit>varname()</lit> or
	<lit>date()</lit>.  The following example prints the values of two
	variables plus that of a calculated expression:
      </para>
      <code>
	ols 1 0 2 3
	scalar b = $coeff[2]
	scalar se_b = $stderr[2]
	printf "b = %.8g, standard error %.8g, t = %.4f\n", 
          b, se_b, b/se_b
      </code>
      <para>
	The next lines illustrate the use of the varname and date functions,
	which respectively print the name of a variable, given its ID number,
	and a date string, given a 1-based observation number.
      </para>
      <code>
	printf "The name of variable %d is %s\n", i, varname(i)
	printf "The date of observation %d is %s\n", j, date(j)
      </code>
      <para>
	If a matrix argument is given in association with a numeric
	format, the entire matrix is printed using the specified
	format for each element.  The same applies to series, except
	that the range of values printed is governed by the current
	sample setting.
      </para>
      <para>
	The maximum length of a format string is 127 characters.  The
	escape sequences <lit>\n</lit> (newline), <lit>\t</lit> (tab),
	<lit>\v</lit> (vertical tab) and <lit>\\</lit> (literal
	backslash) are recognized.  To print a literal percent sign,
	use <lit>%%</lit>.
      </para>
      <para>
	As in C, numerical values that form part of the format (width and or
	precision) may be given directly as numbers, as in <lit>%10.4f</lit>,
	or they may be given as variables.  In the latter case, one puts
	asterisks into the format string and supplies corresponding arguments
	in order.  For example,
      </para>
      <code>
	scalar width = 12
	scalar precision = 6
	printf "x = %*.*f\n", width, precision, x
      </code>
    </description>

  </command>

  <command name="probit" section="Estimation"
    label="Probit model">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
	</option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
	<option>
	  <flag>--p-values</flag>
	  <effect>show p-values instead of slopes</effect>
	</option>
	<option>
	  <flag>--random-effects</flag>
	  <effect>estimates a random effects panel probit model</effect>
	</option>
	<option>
	  <flag>--quadpoints</flag>
	  <optparm>k</optparm>
	  <effect>number of quadrature points for RE estimation</effect>
	</option>
      </options>
      <examples>
	<demos>
	  <demo>ooballot.inp</demo>
	  <demo>oprobit.inp</demo>
	  <demo>reprobit.inp</demo>
	</demos>
      </examples>      
    </usage>

    <description>
      <para>
	If the dependent variable is a binary variable (all values are 0
	or 1) maximum likelihood estimates of the coefficients on
	<repl>indepvars</repl> are obtained via the Newton&ndash;Raphson
	method. As the model is nonlinear the slopes depend on the
	values of the independent variables.  By default the slopes with
	respect to each of the independent variables are calculated (at
	the means of those variables) and these slopes replace the usual
	p-values in the regression output.  This behavior can be
	suppressed my giving the <opt>--p-values</opt> option. The
	chi-square statistic tests the null hypothesis that all
	coefficients are zero apart from the constant.
      </para>
      <para context="cli">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <opt>--robust</opt> flag is given,
	then QML or Huber&ndash;White standard errors are calculated
	instead. In this case the estimated covariance matrix is a
	<quote>sandwich</quote> of the inverse of the estimated Hessian
	and the outer product of the gradient. See chapter 10 of Davidson
	and MacKinnon for details.
      </para>
      <para context="gui">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the "Robust standard errors" box is
	checked, then QML or Huber&ndash;White standard errors are
	calculated instead. In this case the estimated covariance matrix
	is a <quote>sandwich</quote> of the inverse of the estimated
	Hessian and the outer product of the gradient.  See chapter 10 of
	Davidson and MacKinnon for details.
      </para>
      <para>
	If the dependent variable is not binary but is discrete, then Ordered
	Probit estimates are obtained.  (If the variable selected as dependent
	is not discrete, an error is flagged.)
      </para>
      <subhead>Probit for panel data</subhead>
      <para>
	With the <opt>--random-effects</opt> option, the error
	term is assumed to be composed of two normally distributed
	components: one time-invariant term that is specific to the
	cross-sectional unit or <quote>individual</quote> (and is
	known as the individual effect); and one term that is specific
	to the particular observation.
      </para>
      <para>
	Evaluation of the likelihood for this model involves the use
	of Gauss-Hermite quadrature for approximating the value of
	expectations of functions of normal variates. The number of
	quadrature points used can be chosen through the
	<opt>--quadpoints</opt> option (the default is 32). Using more
	points will increase the accuracy of the results, but at the
	cost of longer compute time; with many quadrature points and a
	large dataset estimation may be quite time consuming.
      </para>
      <para>
	Besides the usual parameter estimates (and associated
	statistics) relating to the included regressors, certain
	additional information is presented on estimation of this
	sort of model:
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>lnsigma2</lit>: the maximum likelihood estimate of
	    the log of the variance of the individual effect;
	  </para>
	</li>
	<li>
	  <para>
	    <lit>sigma_u</lit>: the estimated standard deviation of
	    the individual effect; and
	  </para>
	</li>
	<li>
	  <para>
	    <lit>rho</lit>: the estimated share of the individual
	    effect in the composite error variance (also known as the
	    intra-class correlation).
	  </para>
	</li>
      </ilist>
      <para>
	The Likelihood Ratio test of the null hypothesis that
	<lit>rho</lit> equals zero provides a means of assessing
	whether the random effects specification is needed. If
	the null is not rejected that suggests that a simple
	pooled probit specification is adequate.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Probit</menu-path>
    </gui-access>

  </command>

  <command name="pvalue" section="Utilities" 
    label="Compute p-values" context="cli">

    <usage>
      <arguments>
        <argument>dist</argument>
        <argument optional="true">params</argument>
	<argument>xval</argument>
      </arguments>
      <examples>
        <example>pvalue z zscore</example>
	<example>pvalue t 25 3.0</example>
	<example>pvalue X 3 5.6</example>
	<example>pvalue F 4 58 fval</example>
	<example>pvalue G shape scale x</example>
	<example>pvalue B bprob 10 6</example>
	<example>pvalue P lambda x</example>
	<example>pvalue W shape scale x</example>
      </examples>
    </usage>

    <description>
      <para>
	Computes the area to the right of <repl>xval</repl> in the
	specified distribution (<lit>z</lit> for Gaussian, <lit>t</lit>
	for Student's <math>t</math>, <lit>X</lit> for chi-square,
	<lit>F</lit> for <math>F</math>, <lit>G</lit> for gamma,
	<lit>B</lit> for binomial, <lit>P</lit> for Poisson, or
	<lit>W</lit> for Weibull).  
      </para>
      <para>
	Depending on the distribution, the following information must be
	given, before the <repl>xval</repl>: for the <math>t</math>
	and chi-square distributions, the degrees of freedom; for
	<math>F</math>, the numerator and denominator degrees of
	freedom; for gamma, the shape and scale parameters; for the
	binomial distribution, the <quote>success</quote> probability and
	the number of trials; for the Poisson distribution, the
	parameter &lgr; (which is both the mean and the variance); and
	for the Weibull distribution, shape and scale parameters. As
	shown in the examples above, the numerical parameters may be given
	in numeric form or as the names of variables.
      </para>
      <para>
	The parameters for the gamma distribution are sometimes given as
	mean and variance rather than shape and scale. The mean is the
	product of the shape and the scale; the variance is the product of
	the shape and the square of the scale.  So the scale may be found
	as the variance divided by the mean, and the shape as the mean
	divided by the scale.  
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/P-value finder</menu-path>
    </gui-access>

  </command>

  <command name="qlrtest" section="Tests" label="Quandt likelihood ratio test">

    <usage>
      <options>
	<option>
	  <flag>--limit-to</flag>
	  <optparm>list</optparm>
	  <effect>limit test to subset of regressors</effect>
	</option>
	<option>
	  <flag>--plot</flag>
	  <optparm>mode-or-filename</optparm>
	  <effect>see below</effect>
	</option>
      </options>      
    </usage>
    
    <description>
      <para>
	For a model estimated on time-series data via OLS, performs the
	Quandt likelihood ratio (QLR) test for a structural break at an
	unknown point in time, with 15 percent trimming at the beginning
	and end of the sample period.
      </para>
      <para>
	For each potential break point within the central 70 percent
	of the observations, a Chow test is performed. See <cmdref
	targ="chow"/> for details; as with the regular Chow test, this
	is a robust Wald test if the original model was estimated with
	the <opt>robust</opt> option, an F-test otherwise. The QLR
	statistic is then the maximum of the individual test
	statistics.
      </para>
      <para>
	An asymptotic p-value is obtained using the method of <cite
	key="hansen97">Bruce Hansen (1997)</cite>.
      </para>
      <para>
	Besides the standard hypothesis test accessors <fncref
	targ="$test"/> and <fncref targ="$pvalue"/>, <fncref
	targ="$qlrbreak"/> can be used to retrieve the index of the
	observation at which the test statistic is maximized.
      </para>      
      <para context="cli">
	The <opt>limit-to</opt> option can be used to limit the set of
	interactions with the split dummy variable in the Chow tests
	to a subset of the original regressors. The parameter for this
	option must be a named list, all of whose members are among
	the original regressors. The list should not include the
	constant.
      </para>
      <para>
	When this command is run interactively (only), a plot of the
	Chow test statistic is displayed by default.  This can be
	adjusted via the <opt>plot</opt> option. The acceptable
	parameters to this option are <lit>none</lit> (to suppress the
	plot); <lit>display</lit> (to display a plot even when not in
	interactive mode); or a file name. The effect of providing a
	file name is as described for the <opt>output</opt> option of
	the <cmdref targ="gnuplot"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/QLR test</menu-path>
    </gui-access>

  </command>

  <command name="qqplot" section="Graphs" label="Q-Q plot">

    <usage>
      <altforms>
	<altform><lit>qqplot</lit> <repl>y</repl></altform>
	<altform><lit>qqplot</lit> <repl>y</repl> <repl>x</repl></altform>
      </altforms>
      <options>
	<option>
	  <flag>--z-scores</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--raw</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>
      </options>
    </usage>

    <description>
      <para context="gui">
	With just one series selected, displays a plot of the empirical
	quantiles of the given series against the quantiles of the normal
	distribution. The series must include at least 20 valid observations
	in the current sample range. By default the empirical quantiles are
	plotted against quantiles of the normal distribution having the same
	mean and variance as the sample data, but two alternatives are
	available: the data may be standardized (converted to z-scores) before
	plotting, or the <quote>raw</quote> empirical quantiles may be plotted
	against the quantiles of the standard normal distribution.
      </para>
      <para context="cli">
	Given just one series argument, displays a plot of the empirical
	quantiles of the selected series (given by name or ID number)
	against the quantiles of the normal distribution. The series must
	include at least 20 valid observations in the current sample
	range. By default the empirical quantiles are plotted against
	quantiles of the normal distribution having the same mean and
	variance as the sample data, but two alternatives are available:
	if the <opt>--z-scores</opt> option is given the data are
	standardized, while if the <opt>--raw</opt> option is given the
	<quote>raw</quote> empirical quantiles are plotted against the
	quantiles of the standard normal distribution. 
      </para>
      <para>
	The option <opt>--output</opt> has the effect to send the
	output to the desiderd filename; use <quote>display</quote> to
	force output to the screen, for example during a loop.
      </para>
      <para>
	Given two series arguments, <repl>y</repl> and <repl>x</repl>,
	displays a plot of the empirical quantiles of <repl>y</repl> against
	those of <repl>x</repl>. The data values are not standardized.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Normal Q-Q plot</menu-path>
      <menu-path>/View/Graph specified vars/Q-Q plot</menu-path>
    </gui-access>

  </command>

  <command name="quantreg" section="Estimation" 
    label="Quantile regression">

    <usage>
      <arguments>
	<argument>tau</argument>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--intervals</flag>
	  <optparm optional="true">level</optparm>
	  <effect>compute confidence intervals</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
      </options>
      <examples>
	<example>quantreg 0.25 y 0 xlist</example>
	<example>quantreg 0.5 y 0 xlist --intervals</example>
	<example>quantreg 0.5 y 0 xlist --intervals=.95</example>
	<example>quantreg tauvec y 0 xlist --robust</example>
	<demos>
	  <demo>mrw_qr.inp</demo>
	</demos>
      </examples>
    </usage>

    <description context="gui">
      <para>
	Quantile regression.  By default standard errors are computed
	according to the asymptotic formula given by <cite
	key="koenker-bassett78">Koenker and Bassett
	(<book>Econometrica</book>, 1978)</cite>, but if the
	<quote>robust</quote> box is checked we use the
	heteroskedasticity-robust variant from <cite
	key="koenker-zhao94">Koenker and Zhao (<book>Journal of
	Nonparametric Statistics</book>, 1994)</cite>.
      </para>
      <para>
	If the <quote>Compute confidence intervals</quote> option is
	checked gretl will calculate confidence intervals for the
	coefficients, in place of standard errors. The
	<quote>robust</quote> check-box still has an effect: if it is not
	checked, the intervals are computed on the assumption of IID
	errors; with it, gretl uses the robust estimator developed by
	<cite key="koenker-machado99">Koenker and Machado (<book>Journal
	of the American Statistical Association</book>, 1999)</cite>.
	Note that these intervals are not just <quote>plus or minus so
	many standard errors</quote>; in general, they are asymmetrical
	about the point estimates of the coefficients.
      </para>	
      <para>
	You may give a list of quantiles (see the drop-down list for some
	pre-defined possibilities).  In that case gretl will calculate
	quantile estimates and either standard errors or confidence intervals
	for each of the specified values.
      </para>
      <para>
	To Follow up on the references given above, please see
	<guideref targ="chap:quantreg"/>.
      </para>
    </description>

    <description context="cli">
      <para>
	Quantile regression.  The first argument, <repl>tau</repl>, is the
	conditional quantile for which estimates are wanted.  It may be given
	either as a numerical value or as the name of a pre-defined scalar
	variable; the value must be in the range 0.01 to 0.99. (Alternatively,
	a vector of values may be given for <repl>tau</repl>; see below for
	details.) The second and subsequent arguments compose a regression
	list on the same pattern as <cmdref targ="ols"/>.
      </para>
      <para>
	Without the <opt>--intervals</opt> option, standard errors are
	printed for the quantile estimates.  By default, these are
	computed according to the asymptotic formula given by <cite
	key="koenker-bassett78">Koenker and Bassett (1978)</cite>, but if
	the <opt>--robust</opt> option is given, standard errors that are
	robust with respect to heteroskedasticity are calculated using the
	method of <cite key="koenker-zhao94">Koenker and Zhao
	(1994)</cite>.
      </para>
      <para>
	When the <opt>--intervals</opt> option is chosen, confidence
	intervals are given for the parameter estimates instead of
	standard errors.  These intervals are computed using the rank
	inversion method, and in general they are asymmetrical about the
	point estimates.  The specifics of the calculation are inflected
	by the <opt>--robust</opt> option: without this, the intervals are
	computed on the assumption of IID errors <cite key="koenker94"
	p="true">(Koenker, 1994)</cite>; with it, they use the robust
	estimator developed by <cite key="koenker-machado99">Koenker and
	Machado (1999)</cite>.
      </para>
      <para>
	By default, 90 percent confidence intervals are produced.  You can
	change this by appending a confidence level (expressed as a decimal
	fraction) to the intervals option, as in <opt>--intervals=0.95</opt>.
      </para>
      <para>
	Vector-valued <repl>tau</repl>:  instead of supplying a scalar, you
	may give the name of a pre-defined matrix.  In this case estimates are
	computed for all the given <repl>tau</repl> values and the results are
	printed in a special format, showing the sequence of quantile
	estimates for each regressor in turn.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Robust estimation/Quantile regression</menu-path>
    </gui-access>

  </command>

  <command name="quit" section="Utilities" 
    label="Exit the program" context="cli">

    <description>
      <para>
	Exits from the program, giving you the option of saving the
	output from the session on the way out.  
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Exit</menu-path>
    </gui-access>

  </command>

  <command name="rename" section="Dataset" 
    label="Rename variables" context="cli">

    <usage>
      <arguments>
	<argument>series</argument>
	<argument>newname</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Changes the name of <repl>series</repl> (identified by name or ID
	number) to <repl>newname</repl>.  The new name must be of 31
	characters maximum, must start with a letter, and must be composed of
	only letters, digits, and the underscore character.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Edit attributes</menu-path>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="reprobit" section="Estimation" label="Random effects probit"
	   context="gui">

    <description>
      <para>
	The random effects probit estimator provides a means of
	estimating a (binary) probit model for panel data. The error
	term is assumed to be composed of two normally distributed
	components: one time-invariant term that is specific to the
	cross-sectional unit or <quote>individual</quote> (and is
	known as the individual effect); and one term that is specific
	to the particular observation.
      </para>
      <para>
	Evaluation of the likelihood for this model involves the use
	of Gauss-Hermite quadrature for approximating the value of
	expectations of functions of normal variates. In this dialog
	you can select the number of quadrature points used. Using
	more points will increase the accuracy of the results, but at
	the cost of longer compute time; with many quadrature points
	and a large dataset estimation may be quite time consuming.
      </para>
      <para>
	Besides the usual parameter estimates (and associated
	statistics) relating to the included regressors, certain
	additional information is presented on estimation of this
	sort of model:
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>lnsigma2</lit>: the maximum likelihood estimate of
	    the log of the variance of the individual effect;
	  </para>
	</li>
	<li>
	  <para>
	    <lit>sigma_u</lit>: the estimated standard deviation of
	    the individual effect; and
	  </para>
	</li>
	<li>
	  <para>
	    <lit>rho</lit>: the estimated share of the individual
	    effect in the composite error variance (also known as the
	    intra-class correlation).
	  </para>
	</li>
      </ilist>
      <para>
	The Likelihood Ratio test of the null hypothesis that
	<lit>rho</lit> equals zero provides a means of assessing
	whether the random effects specification is needed. If
	the null is not rejected that suggests that a simple
	pooled probit specification is adequate.
      </para>
      <para>
	In scripting mode, the random effects probit model is 
	estimated using the <lit>probit</lit> command with 
	the <opt>random-effects</opt> option. 
      </para>
    </description>

  </command>
  

  <command name="reset" section="Tests" label="Ramsey's RESET">

    <usage>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print the auxiliary regression</effect>
	</option>
	<option>
	  <flag>--squares-only</flag>
	  <effect>compute the test using only the squares</effect>
	</option>
	<option>
	  <flag>--cubes-only</flag>
	  <effect>compute the test using only the cubes</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Must follow the estimation of a model via OLS. Carries out
	Ramsey's RESET test for model specification (non-linearity) by
	adding the square and/or the cube of the fitted values to the
	regression and calculating the <math>F</math> statistic for the
	null hypothesis that the parameters on the added terms are zero.
      </para>
      <para context="cli">
	Both the square and the cube are added, unless one of the options
	<opt>--squares-only</opt> or <opt>--cubes-only</opt> is given.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Ramsey's RESET</menu-path>
    </gui-access>

  </command>

  <command name="restrict" section="Tests" context="cli"
    label="Testing restrictions">

    <usage>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print restricted estimates</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
	</option>
	<option>
	  <flag>--wald</flag>
	  <effect>system estimators only &ndash; see below</effect>
	</option>
	<option>
	  <flag>--bootstrap</flag>
	  <effect>bootstrap the test if possible</effect>
	</option>
	<option>
	  <flag>--full</flag>
	  <effect>OLS and VECMs only, see below</effect>
	</option>
      </options>
    </usage>    

    <description>
      <para>
	Imposes a set of (usually linear) restrictions on either (a)
	the model last estimated or (b) a system of equations
	previously defined and named. In all cases the set of
	restrictions should be started with the keyword
	<quote>restrict</quote> and terminated with <quote>end
	restrict</quote>.
      </para>
      <para>
	In the single equation case the restrictions are always
	implicitly to be applied to the last model, and they are
	evaluated as soon as the <lit>restrict</lit> block is closed.
      </para>
      <para>
	In the case of a system of equations (defined via the <cmdref
	targ="system"/> command), the initial <quote>restrict</quote>
	may be followed by the name of a previously defined system of
	equations. If this is omitted and the last model was a system
	then the restrictions are applied to the last model. By
	default the restrictions are evaluated when the system is next
	estimated, using the <cmdref targ="estimate"/> command. But if
	the <opt>--wald</opt> option is given the restriction is
	tested right away, via a Wald chi-square test on the
	covariance matrix. Note that this option will produce an error
	if a system has been defined but not yet estimated.
      </para>
      <para>
	Depending on the context, the restrictions to be tested may be
	expressed in various ways.  The simplest form is as follows: each
	restriction is given as an equation, with a linear combination of
	parameters on the left and a scalar value to the right of the equals
	sign (either a numerical constant or the name of a scalar variable).
      </para>
      <para>
	In the single-equation case, parameters may be referenced in the form
	<lit>b[</lit><repl>i</repl><lit>]</lit>, where <repl>i</repl>
	represents the position in the list of regressors (starting at 1), or
	<lit>b[</lit><repl>varname</repl><lit>]</lit>, where
	<repl>varname</repl> is the name of the regressor in question. In the
	system case, parameters are referenced using <lit>b</lit> plus two
	numbers in square brackets. The leading number represents the position
	of the equation within the system and the second number indicates
	position in the list of regressors.  For example <lit>b[2,1]</lit>
	denotes the first parameter in the second equation, and
	<lit>b[3,2]</lit> the second parameter in the third equation.
	The <lit>b</lit> terms in the equation representing a restriction may
	be prefixed with a numeric multiplier, for example
	<lit>3.5*b[4]</lit>.
      </para>
      <para>
	Here is an example of a set of restrictions for a previously estimated
	model:
      </para>
      <code>
	restrict
	 b[1] = 0
	 b[2] - b[3] = 0
	 b[4] + 2*b[5] = 1
	end restrict
      </code>
      <para>
	And here is an example of a set of restrictions to be applied to a
	named system.  (If the name of the system does not contain spaces, the
	surrounding quotes are not required.)
      </para>
      <code>
	restrict "System 1"
	 b[1,1] = 0
	 b[1,2] - b[2,2] = 0
	 b[3,4] + 2*b[3,5] = 1
	end restrict
      </code>
      <para>
	In the single-equation case the restrictions are by default evaluated
	via a Wald test, using the covariance matrix of the model in question.
	If the original model was estimated via OLS then the restricted
	coefficient estimates are printed; to suppress this, append the
	<opt>--quiet</opt> option flag to the initial <lit>restrict</lit>
	command.  As an alternative to the Wald test, for models estimated via
	OLS or WLS only, you can give the <opt>--bootstrap</opt> option to
	perform a bootstrapped test of the restriction.
      </para>
      <para>
	In the system case, the test statistic depends on the estimator
	chosen: a Likelihood Ratio test if the system is estimated using a
	Maximum Likelihood method, or an asymptotic <math>F</math>-test
	otherwise.
      </para>
      <para>
	There are two alternatives to the method of expressing restrictions
	discussed above.  First, a set of <math>g</math> linear restrictions
	on a <math>k</math>-vector of parameters, &bgr;, may be written
	compactly as <math>R</math>&bgr; &minus; <math>q</math> = 0, where
	<math>R</math> is an <by r="g" c="k"/> matrix and <math>q</math> is a
	<math>g</math>-vector.  You can specify a restriction by giving the
	names of pre-defined, conformable matrices to be used as
	<math>R</math> and <math>q</math>, as in
      </para>
      <code>
	restrict 
	  R = Rmat
	  q = qvec
	end restrict
      </code>
      <para>
	Secondly, if you wish to test a nonlinear restriction (this is
	currently available for single-equation models only) you should give
	the restriction as the name of a function, preceded by
	<quote><lit>rfunc = </lit></quote>, as in
      </para>
      <code>
	restrict
	  rfunc = myfunction
	end restrict
      </code>
      <para>
	The constraint function should take a single <lit>const matrix</lit>
	argument; this will be automatically filled out with the parameter
	vector.  And it should return a vector which is zero under the null
	hypothesis, non-zero otherwise.  The length of the vector is the
	number of restrictions. This function is used as a
	<quote>callback</quote> by gretl's numerical Jacobian routine, which
	calculates a Wald test statistic via the delta method.
      </para>
      <para>
	Here is a simple example of a function suitable for testing one
	nonlinear restriction, namely that two pairs of parameter values have
	a common ratio. 
      </para>
      <code>
	function matrix restr (const matrix b)
	  matrix v = b[1]/b[2] - b[4]/b[5]
	  return v
	end function
      </code>
      <para>
	On successful completion of the <lit>restrict</lit> command
	the accessors <lit>$test</lit> and <lit>$pvalue</lit> give the
	test statistic and its p-value.
      </para>
      <para>
	When testing restrictions on a single-equation model estimated
	via OLS, or on a VECM, the <opt>--full</opt> option can be
	used to set the restricted estimates as the <quote>last
	model</quote> for the purposes of further testing or the use
	of accessors such as <lit>$coeff</lit> and <lit>$vcv</lit>.
	Note that some special considerations apply in the case of
	testing restrictions on Vector Error Correction Models. Please
	see <guideref targ="chap:vecm"/> for details.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Linear restrictions</menu-path>
    </gui-access>

  </command>

  <command name="restrict-model" section="Tests" context="gui"
    label="Restrictions on a model">

    <description>
      <para>
	Each restriction in the set should be expressed as an equation, with a
	linear combination of parameters on the left and a numeric value to
	the right of the equals sign. Parameters may be referenced in the form
	<lit>b[</lit><repl>i</repl><lit>]</lit>, where <repl>i</repl>
	represents the position in the list of regressors (starting at 1), or
	<lit>b[</lit><repl>varname</repl><lit>]</lit>, where
	<repl>varname</repl> is the name of the regressor in question.
      </para>
      <para>
	The <lit>b</lit> terms in the equation representing a restriction may
	be prefixed with a numeric multiplier, using <lit>*</lit> to represent
	multiplication, for example <lit>3.5*b[4]</lit>.
      </para>
      <para>
	Here is an example of a set of restrictions:
      </para>
      <code>
	b[1] = 0
	b[2] - b[3] = 0
	b[4] + 2*b[5] = 1
      </code>
    </description>

  </command>

  <command name="restrict-system" section="Tests" context="gui"
    label="Restrictions on a system of equations">

    <description>
      <para>
	Each restriction in the set should be expressed as an equation, with a
	linear combination of parameters on the left and a numeric value to
	the right of the equals sign.  Parameters are referenced using
	<lit>b</lit> plus two numbers in square brackets. The leading number
	represents the position of the equation within the system and the
	second number indicates position in the list of regressors, starting
	at 1 in both cases.  For example <lit>b[2,1]</lit> denotes the first
	parameter in the second equation, and <lit>b[3,2]</lit> the second
	parameter in the third equation.
      </para>
      <para>
	The <lit>b</lit> terms in the equation representing a restriction may
	be prefixed with a numeric multiplier, using <lit>*</lit> to represent
	multiplication, for example <lit>3.5*b[1,4]</lit>.
      </para>
      <para>Here is an example of a set of restrictions:
      </para>
      <code>
	b[1,1] = 0
	b[1,2] - b[2,2] = 0
	b[3,4] + 2*b[3,5] = 1
      </code>
    </description>

  </command>

  <command name="restrict-vecm" section="Tests" context="gui"
    label="Restrictions on a VECM">

    <description>
      <para>
	Use this command to place linear restrictions on the cointegrating
	relations (beta) and/or adjustment coefficients (alpha) in a vector
	error-correction model (VECM).
      </para>
      <para>
	Each restriction should be expressed as an equation, with a linear
	combination of parameters to the left of the equals sign and a
	numerical value on the right.  Restrictions on beta may be
	non-homogeneous (non-zero on the right), but alpha restrictions must
	be homogeneous (zero on the right).  
      </para>
      <para>
	If the VECM is of rank 1, the elements of beta are referenced in the
	form <lit>b[</lit><repl>i</repl><lit>]</lit>, where <repl>i</repl>
	represents position in the cointegrating vector, starting at 1. For
	example, <lit>b[2]</lit> denotes the second element in beta. If the
	rank is greater than 1, use <lit>b</lit> plus two numbers in square
	brackets.  For example, <lit>b[2,1]</lit> denotes the first element in
	the second cointegrating vector.
      </para>
      <para>
	To reference elements of alpha, use <lit>a</lit> instead of
	<lit>b</lit>.
      </para>
      <para>
	The parameter identifiers in the equation representing a restriction
	may be prefixed with a numeric multiplier, using <lit>*</lit> to
	represent multiplication, for example <lit>3.5*b[4]</lit>.
      </para>
      <para>Here is an example of a set of restrictions on a VECM of rank 1.
      </para>
      <code>
	b[1] + b[2] = 0
	b[1] + b[3] = 0
      </code>
      <para>
	See also <guideref targ="chap:vecm"/>.
      </para>
    </description>

  </command>

  <command name="rmplot" section="Graphs" label="Range-mean plot">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--trim</flag>
	  <effect>see below</effect>
	</option>	
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printed output</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Range&ndash;mean plot: this command creates a simple graph to help in
	deciding whether a time series, <math>y</math>(t), has constant
	variance or not.  We take the full sample t=1,...,T and divide it into
	small subsamples of arbitrary size <math>k</math>. The first subsample
	is formed by <math>y</math>(1),...,<math>y</math>(k), the second is
	<math>y</math>(k+1), ..., <math>y</math>(2k), and so on.  For each
	subsample we calculate the sample mean and range (= maximum minus
	minimum), and we construct a graph with the means on the horizontal
	axis and the ranges on the vertical. So each subsample is represented
	by a point in this plane.  If the variance of the series is constant
	we would expect the subsample range to be independent of the subsample
	mean; if we see the points approximate an upward-sloping line this
	suggests the variance of the series is increasing in its mean; and if
	the points approximate a downward sloping line this suggests the
	variance is decreasing in the mean.
      </para>
      <para>
	Besides the graph, gretl displays the means and ranges for each
	subsample, along with the slope coefficient for an OLS regression of
	the range on the mean and the p-value for the null hypothesis that
	this slope is zero.  If the slope coefficient is significant at the 10
	percent significance level then the fitted line from the regression of
	range on mean is shown on the graph.  The <math>t</math>-statistic for
	the null, and the corresponding p-value, are recorded and may be
	retrieved using the accessors <lit>$test</lit> and <lit>$pvalue</lit>
	respectively.
      </para>
      <para context="cli">
	If the <opt>--trim</opt> option is given, the minimum and maximum
	values in each sub-sample are discarded before calculating the
	mean and range. This makes it less likely that outliers will
	distort the analysis.
      </para>
      <para context="cli">
	If the <opt>--quiet</opt> option is given, no graph is shown and no
	output is printed; only the <math>t</math>-statistic and p-value
	are recorded.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Range-mean graph</menu-path>
    </gui-access>

  </command>

  <command name="run" section="Programming" 
    label="Execute a script" context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Executes the commands in <repl>filename</repl> then returns
	control to the interactive prompt.  This command is intended
	for use with the command-line program
	<program>gretlcli</program>, or at the <quote>gretl
	console</quote> in the GUI program.
      </para>
      <para>
	See also <cmdref targ="include"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>Run icon in script window</menu-path>
    </gui-access>

  </command>

  <command name="runs" section="Tests" label="Runs test">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--difference</flag>
	  <effect>use first difference of variable</effect>
	</option>
	<option>
	  <flag>--equal</flag>
	  <effect>positive and negative values are equiprobable</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Carries out the nonparametric <quote>runs</quote> test for
	randomness of the specified <repl>series</repl>, where runs are
	defined as sequences of consecutive positive or negative values.
	If you want to test for randomness of deviations from the median,
	for a variable named <lit>x1</lit> with a non-zero median, you can
	do the following:
      </para>
      <code>
	series signx1 = x1 - median(x1)
	runs signx1
      </code>
      <para>
	If the <opt>--difference</opt> option is given, the variable is
	differenced prior to the analysis, hence the runs are interpreted
	as sequences of consecutive increases or decreases in the
	value of the variable.
      </para>
      <para>
	If the <opt>--equal</opt> option is given, the null hypothesis
	incorporates the assumption that positive and negative values
	are equiprobable, otherwise the test statistic is invariant
	with respect to the <quote>fairness</quote> of the process
	generating the sequence, and the test focuses on independence
	alone.  
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/Nonparametric tests</menu-path>
    </gui-access>

  </command>

  <command name="sampling" section="Dataset" context="gui"
    label="Setting the sample">

    <description>
      <para>
	The Sample menu offers several ways of selecting a sub-sample from the
	current dataset.
      </para>
      <para>
	If you choose <quote>Sample/Restrict based on criterion...</quote> you
	need to supply a Boolean (logical) expression, of the same sort that
	you would use to define a dummy variable.  For example the expression
	<quote>sqft > 1400</quote> will select only cases for which the
	variable sqft has a value greater than 1400. Conditions may be
	concatenated using the logical operators <quote>&amp;&amp;</quote>
	(AND) and <quote>||</quote> (OR), and may be negated using
	<quote>!</quote> (NOT). If the dataset already contains dummy
	variables, you are also given the option of selecting one of these to
	define the sample (observations with a value of 1 for the selected
	dummy will be included, and others excluded).
      </para>
      <para>
	The menu item <quote>Sample/Drop all obs with missing values</quote>
	redefines the sample to exclude all observations for which values of
	one or more variables are missing (leaving only complete cases).
      </para>  
      <para>
	To select observations for which a particular variable has no missing
	values, use <quote>Restrict based on criterion...</quote> and
	supply the Boolean condition <quote>!missing(varname)</quote> (replace
	<quote>varname</quote> with the name of the variable you want to use).
      </para>  
      <para>
	If the observations are labeled, you can exclude particular
	observations using, for example, <lit>obs!="France"</lit> as the
	Boolean criterion.  The observation name must be enclosed in double
	quotes.
      </para>
      <para>
	One point should be noted about defining a sample based on a dummy
	variable, a Boolean expression, or on the missing values criterion:
	Any <quote>structural</quote> information in the data header file
	(regarding the time series or panel nature of the data) is lost.  You
	may reimpose structure with <quote>Sample/Set frequency,
	  startobs...</quote>.
      </para>
      <para>
	Please see <guideref targ="chap:sampling"/> for further details.
      </para>
    </description>
  </command>

  <command name="save-labels" section="Utilities" 
    label="Save or remove series labels" context="gui">
    <description>
      <para>
	If you choose Export here, gretl will write a file containing 
	the descriptive labels of any series in the current dataset
	that have such labels. This is a plain text file with one
	line per variable. The line will be empty for variables that
	have no descriptive label.
      </para>
      <para>
	If you choose Remove, the descriptive labels will be removed
	for all series that have such labels. This would be
	appropriate only if the current labels have somehow been
	added in error.
      </para>
    </description>
  </command>

  <command name="add-labels" section="Utilities" 
    label="Add series labels" context="gui">
    <description>
      <para>
	If you choose Yes here, you are offered a file-open dialog box to
	select a plain text file containing descriptive labels for the
	series in the current dataset. The file should contain one label
	per line; a blank line means no label. Gretl will attempt to read
	as many labels as there are series in the dataset, excluding the
	constant.
      </para>
    </description>
  </command>

  <command name="save-script" section="Utilities" 
    label="Save commands?" context="gui">
    <description>
      <para>
	If you choose Yes here, gretl will write a file containing a
	record of the commands you executed in the current session.  Most
	commands that you execute via <quote>point and click</quote> have
	a <quote>script</quote> counterpart, and it is these script
	commands that will be saved.  You could take the file as the basis
	for writing a gretl command script.
      </para>
      <para>
	If you don't care to be prompted to save a record of commands
	on exit, uncheck the tick box in the save commands dialog.
      </para>
    </description>
  </command>

  <command name="save-session" section="Utilities" 
    label="Save this gretl session?" context="gui">
    <description>
      <para>
	If you choose Yes here, gretl will write a file containing a
	<quote>snapshot</quote> of the current session, including a copy of
	the working dataset along with any models, graphs or other objects
	that you have saved <quote>as icons</quote>.  You can re-open this
	file later to recreate the state of gretl as of the time you quit the
	session (see the <quote>File/Session files</quote> menu).
      </para>
      <para>
	If you mostly work with gretl using command scripts (which we
	recommend for <quote>serious</quote> econometric work) you
	probably don't need to save the session, but you should be
	sure to save any changes to your script that you wish to keep.
	You may also want to save any changes to your dataset, unless
	these are of a sort that can easily be recreated by running
	a script.
      </para>
      <para>
	If you work with scripts and don't care to be prompted to
	save your session on exit, uncheck the tick box in the
	save session dialog.
      </para>
    </description>
  </command>

  <command name="scatters" section="Graphs"
    label="Multiple pairwise graphs">

    <usage>
      <arguments>
        <argument>yvar</argument>
        <argument separated="true">xvars</argument>
	<argument alternate="true">yvars ; xvar</argument>
      </arguments>
      <options>
	<option>
	  <flag>--with-lines</flag>
	  <effect>create line graphs</effect>
	</option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>name</optparm>
	  <effect>plot columns of named matrix</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>
      </options>
      <examples>
        <example>scatters 1 ; 2 3 4 5</example>
        <example>scatters 1 2 3 4 5 6 ; 7</example>
	<example>scatters y1 y2 y3 ; x --with-lines</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Generates pairwise graphs of <repl>yvar</repl> against all the
	variables in <repl>xvars</repl>, or of all the variables in
	<repl>yvars</repl> against <repl>xvar</repl>.  The first
	example above puts variable 1 on the <math>y</math>-axis
	and draws four graphs, the first having variable 2 on the
	<math>x</math>-axis, the second variable 3 on the
	<math>x</math>-axis, and so on.  The second example
	plots each of variables 1 through 6 against variable 7 on the
	<math>x</math>-axis. Scanning a set of such plots can be
	a useful step in exploratory data analysis.  The maximum
	number of plots is 16; any extra variable in the list will be
	ignored.
      </para>
      <para context="cli">
	By default the graphs are scatterplots, but if you give the
	<opt>--with-lines</opt> flag they will be line graphs.
      </para>
      <para context="cli">
	For details on usage of the <opt>--output</opt> option, please see
	the <cmdref targ="gnuplot"/> command.
      </para>
      <para context="cli">
	If a named matrix is specified as the data source the
	<repl>x</repl> and <repl>y</repl> lists should be given as
	1-based column numbers; or alternatively, if no such numbers
	are given, all the columns are plotted against time or an
	index variable.
      </para>
      <para context="gui">
	Generates pairwise graphs of the selected <quote>Y-axis
	  variable</quote> against each of the selected <quote>X-axis
	  variables</quote> in turn.  (Or you can select several variables
	for the Y-axis and one for the X-axis.)  Scanning a set of such
	plots can be a useful step in exploratory data analysis.  The
	maximum number of plots is 16; any extra variables will be
	ignored.
      </para>
      <para>
	If the dataset is time-series, then the second sub-list can be
	omitted, in which case it will implicitly be taken as "time",
	so you can plot multiple time series in separated sub-graphs.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Multiple graphs</menu-path>
    </gui-access>

  </command>

  <command name="sdiff" section="Transformations" 
    label="Seasonal differencing" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The seasonal difference of each variable in <repl>varlist</repl> is
	obtained and the result stored in a new variable with the prefix
	<lit>sd_</lit>.  This command is available only for seasonal time
	series.  
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Seasonal differences of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="set" section="Programming" 
    label="Set program parameters" context="cli">

    <usage>
      <altforms>
	<altform><lit>set</lit> <repl>variable</repl> <repl>value</repl></altform>
	<altform><lit>set --to-file=</lit><repl>filename</repl></altform>
	<altform><lit>set --from-file=</lit><repl>filename</repl></altform>
	<altform><lit>set stopwatch</lit></altform>
	<altform><lit>set</lit></altform>
      </altforms>
      <examples>
        <example>set svd on</example>
        <example>set csv_delim tab</example>
	<example>set horizon 10</example>
	<example>set --to-file=mysettings.inp</example>
      </examples>    
    </usage>

    <description>
      <para>
	The most common use of this command is the first variant shown above,
	where it is used to set the value of a selected program parameter.
	This is discussed in detail below.  The other uses are: with
	<opt>--to-file</opt>, to write a script file containing all the
	current parameter settings; with <opt>--from-file</opt> to read a
	script file containing parameter settings and apply them to the
	current session; with <lit>stopwatch</lit> to zero the gretl
	<quote>stopwatch</quote> which can be used to measure CPU time (see
	the entry for the <lit>$stopwatch</lit> accessor in the gretl function
	reference); or, if the word <lit>set</lit> is given alone, to print
	the current settings.
      </para>
      <para>
	Values set via this comand remain in force for the duration of
	the gretl session unless they are changed by a further call to
	<cmd>set</cmd>. The parameters that can be set in this way are
	enumerated below. Note that the settings of
	<lit>hc_version</lit>, <lit>hac_lag</lit> and
	<lit>hac_kernel</lit> are used when the <opt>--robust</opt>
	option is given to an estimation command.
      </para>
      <para>
	The available settings are grouped under the following categories:
	program interaction and behavior, numerical methods, random number
	generation, robust estimation, filtering, time series 
	estimation, and interaction with GNU R.
      </para>

      <subhead>Program interaction and behavior</subhead>

      <para>
	These settings are used for controlling various aspects of the way
	gretl interacts with the user.
      </para>
      <ilist>
	<li>
	  <para><lit>csv_delim</lit>: either <lit>comma</lit> (the default),
	    <lit>space</lit>, <lit>tab</lit> or <lit>semicolon</lit>.  Sets
	    the column delimiter used when saving data to file in CSV format.
	  </para>
	</li>
	<li>
	  <para><lit>csv_write_na</lit>: the string used to represent
	  missing values when writing data to file in CSV format.
	  Maximum 7 characters; the default is <lit>NA</lit>.
	  </para>
	</li>
	<li>
	  <para><lit>csv_read_na</lit>: the string taken to represent
	  missing values (NAs) when reading data in CSV
	  format. Maximum 7 characters. The default depends on whether
	  a data column is found to contain numerical data (mostly) or
	  string values. For numerical data the following are taken as
	  indicating NAs: an empty cell, or any of the strings
	  <lit>NA</lit>, <lit>N.A.</lit>, <lit>na</lit>,
	  <lit>n.a.</lit>, <lit>N/A</lit>, <lit>#N/A</lit>,
	  <lit>NaN</lit>, <lit>.NaN</lit>, <lit>.</lit>,
	  <lit>..</lit>, <lit>-999</lit>, and <lit>-9999</lit>.  For
	  string-valued data only a blank cell, or a cell containing
	  an empty string, is counted as NA. These defaults can be
	  reimposed by giving <lit>default</lit> as the value for
	  <lit>csv_read_na</lit>. To specify that only empty cells
	  are read as NAs, give a value of <lit>""</lit>. Note that
	  empty cells are always read as NAs regardless of the setting
	  of this variable.
	  </para>
	</li>
	<li>
	  <para><lit>csv_digits</lit>: a positive integer specifying
	  the number of significant digits to use when writing data in
	  CSV format. By default up to 15 digits are used depending on
	  the precision of the original data. Note that CSV output
	  employs the C library's <lit>fprintf</lit> function with
	  <quote><lit>%g</lit></quote> conversion, which means that
	  trailing zeros are dropped.
	  </para>
	</li>
	<li>
	  <para><lit>mwrite_g</lit>: <lit>on</lit> or <lit>off</lit>
	  (the default). When writing a matrix to file as text, gretl
	  by default uses scientific notation with 18-digit precision,
	  hence ensuring that the stored values are a faithful
	  representation of the numbers in memory. When writing
	  primary data with no more than 6 digits of precision it may
	  be preferable to use <lit>%g</lit> format for a more compact
	  and human-readable file; you can make this switch via
	  <lit>set mwrite_g on</lit>.
	  </para>
	</li>
	<li>
	  <para><lit>echo</lit>: <lit>off</lit> or <lit>on</lit> (the
	  default). Suppress or resume the echoing of commands in gretl's
	  output.
	  </para>
	</li>
	<li>
	  <para><lit>force_decpoint</lit>: <lit>on</lit> or <lit>off</lit>
	    (the default).  Force gretl to use the decimal point
	    character, in a locale where another character (most likely
	    the comma) is the standard decimal separator.
	  </para>
	</li>
	<li>	  
	  <para><lit>loop_maxiter</lit>: one non-negative integer
	  value (default 100000).  Sets the maximum number of
	  iterations that a <lit>while</lit> loop is allowed before
	  halting (see <cmdref targ="loop"/>). Note that this setting
	  only affects the <lit>while</lit> variant; its purpose is to
	  guard against inadvertently infinite loops. Setting this
	  value to 0 has the effect of disabling the limit; use with
	  caution.
	  </para>
	</li>
	<li>
	  <para><lit>max_verbose</lit>: <lit>on</lit> or
	  <lit>off</lit> (the default). Toggles verbose output for the
	  <lit>BFGSmax</lit> and <lit>NRmax</lit> functions (see the
	  User's Guide for details).
	</para>
	</li>
	<li>
	  <para><lit>messages</lit>: <lit>off</lit> or <lit>on</lit> (the
	  default). Suppress or resume the printing of non-error messages
	  associated with various commands, for example when a new variable is
	  generated or when the sample range is changed.
         </para>
	</li>
	<li>
	  <para><lit>warnings</lit>: <lit>off</lit> or <lit>on</lit> (the
	  default). Suppress or resume the printing of warning messages
	  issued when arithmetical operations produce non-finite values. 
         </para>
	</li>
	<li>
	  <para><lit>debug</lit>: <lit>1</lit>, <lit>2</lit> or <lit>0</lit>
	    (the default).  This is for use with user-defined functions.
	    Setting <lit>debug</lit> to 1 is equivalent to turning
	    <lit>messages</lit> on within all such functions; setting this
	    variable to <lit>2</lit> has the additional effect of turning on
	    <lit>max_verbose</lit> within all functions.
	  </para>
	</li>
	<li>
	  <para><lit>shell_ok</lit>: <lit>on</lit> or <lit>off</lit>
	  (the default). Enable launching external programs from
	  gretl via the system shell. This is disabled by default for
	  security reasons, and can only be enabled via the graphical
	  user interface (Tools/Preferences/General). However, once
	  set to on, this setting will remain active for future
	  sessions until explicitly disabled.
	  </para>
	</li>
	<li>
	  <para><lit>shelldir</lit>: <repl>path</repl>.  Sets the current
	  working directory for shell commands.
	  </para>
	</li>
	<li>
	  <para><lit>use_cwd</lit>: <lit>on</lit> or <lit>off</lit>
	  (the default). This setting affects the behavior of the
	  <cmdref targ="outfile"/> and <cmdref targ="store"/>
	  commands, which write external files. Normally, the file
	  will be written in the user's default data directory; if
	  <lit>use_cwd</lit> is <lit>on</lit>, on the contrary, the
	  file will be created in the working directory when gretl was
	  started.
	  </para>
	</li>
	<li>
	  <para><lit>bfgs_verbskip</lit>: one integer. This setting
	  affects the behavior of the <opt>--verbose</opt> option to
	  those commands that use BFGS as an optimization algorithm and
	  is used to compact output. if <lit>bfgs_verbskip</lit> is
	  set to, say, 3, then the <opt>--verbose</opt> switch will
	  only print iterations 3, 6, 9 and so on.
	  </para>
	</li>
	<li>
	  <para><lit>skip_missing</lit>: <lit>on</lit> (the default)
	  or <lit>off</lit>. Controls gretl's behavior when
	  contructing a matrix from data series: the default is to
	  skip data rows that contain one or more missing values but
	  if <lit>skip_missing</lit> is set <lit>off</lit> missing
	  values are converted to NaNs.
	  </para>
	</li>
	<li>
	  <para><lit>matrix_mask</lit>: the name of a series, or the
	  keyword <lit>null</lit>. Offers greater control than 
	  <lit>skip_missing</lit> when constructing matrices from
	  series: the data rows selected for matrices are those
	  with non-zero (and non-missing) values in the specified
	  series. The selected mask remains in force until it is 
	  replaced, or removed via the <lit>null</lit> keyword.
	  </para>
	</li>
	<li>
	  <para><lit>huge</lit>: a large positive number (by default,
	  1.0E100). This setting controls the value returned by the
	  accessor <fncref targ="$huge"/>.
	  </para>
	</li>
      </ilist>

      <subhead>Numerical methods</subhead>

      <para>
	These settings are used for controlling the numerical
	algorithms that gretl uses for estimation.
      </para>
      <ilist>
	<li>	
	  <para><lit>optimizer</lit>: either <lit>auto</lit> (the
	  default), <lit>BFGS</lit> or <lit>newton</lit>. Sets the
	  optimization algorithm used for various ML estimators, in
	  cases where both BFGS and Newton&ndash;Raphson are
	  applicable. The default is to use Newton&ndash;Raphson
	  where an analytical Hessian is available, otherwise BFGS.
	  </para>
	</li>
	<li>	
	  <para><lit>bhhh_maxiter</lit>: one integer, the maximum number of
	    iterations for gretl's internal BHHH routine, which is used in
	    the <cmd>arma</cmd> command for conditional ML estimation. If
	    convergence is not achieved after <lit>bhhh_maxiter</lit>, the
	    program returns an error. The default is set at 500.
	  </para>
	</li>	  
	<li>	  
	  <para><lit>bhhh_toler</lit>: one floating point value, or the
	    string <lit>default</lit>.  This is used in gretl's internal
	    BHHH routine to check if convergence has occurred. The
	    algorithm stops iterating as soon as the increment in the
	    log-likelihood between iterations is smaller than
	    <lit>bhhh_toler</lit>.  The default value is 1.0E&minus;06;
	    this value may be re-established by typing <lit>default</lit>
	    in place of a numeric value.
	  </para>
	</li>
	<li>
	  <para><lit>bfgs_maxiter</lit>: one integer, the maximum number of
	    iterations for gretl's BFGS routine, which is used for
	    <cmd>mle</cmd>, <cmd>gmm</cmd> and several specific
	    estimators. If convergence is not achieved in the specified
	    number of iterations, the program returns an error. The
	    default value depends on the context, but is typically
	    of the order of 500.
	  </para>
	</li>	  
	<li>	  
	  <para><lit>bfgs_toler</lit>: one floating point value, or the
	    string <lit>default</lit>.  This is used in gretl's BFGS
	    routine to check if convergence has occurred. The algorithm
	    stops as soon as the relative improvement in the objective
	    function between iterations is smaller than
	    <lit>bfgs_toler</lit>.  The default value is the machine
	    precision to the power 3/4; this value may be re-established
	    by typing <lit>default</lit> in place of a numeric value.
	  </para>
	</li>
	<li>	  
	  <para><lit>bfgs_maxgrad</lit>: one floating point value. This is
	    used in gretl's BFGS routine to check if the norm of the gradient
	    is reasonably close to zero when the <lit>bfgs_toler</lit>
	    criterion is met.  A warning is printed if the norm of the
	    gradient exceeds 1; an error is flagged if the norm exceeds
	    <lit>bfgs_maxgrad</lit>. At present the default is the 
	    permissive value of 5.0.
	  </para>
	</li>
	<li>
	  <para><lit>bfgs_richardson</lit>: <lit>on</lit> or
	  <lit>off</lit> (the default). Use Richardson extrapolation
	  when computing numerical derivatives in the context of BFGS
	  maximization.
	  </para>
	</li>
	<li>	  
	  <para><lit>initvals</lit>: either <lit>auto</lit> (the
	  default) or the name of a pre-specified matrix. Allows
	  manual setting of the initial parameter estimates for
	  numerical optimization problems (such as ARMA
	  estimation). For details see <guideref
	  targ="chap:timeseries"/>.
	  </para>
	</li>
	<li>
	  <para><lit>lbfgs</lit>: <lit>on</lit> or <lit>off</lit> (the
	    default). Use the limited-memory version of BFGS (L-BFGS-B)
	    instead of the ordinary algorithm. This may be advantageous when
	    the function to be maximized is not globally concave.
	  </para>
	</li>
	<li>
	  <para><lit>lbfgs_mem</lit>: an integer value in the range 3 to 20
	    (with a default value of 8).  This determines the number of
	    corrections used in the limited memory matrix when L-BFGS-B
	    is employed.  
	  </para>
	</li>
	<li>
	<para>
	  <lit>nls_toler</lit>: a floating-point value (the default is the
	  machine precision to the power 3/4).  Sets the tolerance used in
	  judging whether or not convergence has occurred in nonlinear least
	  squares estimation using the <cmdref targ="nls"/> command.
	</para>
	</li>
	<li>
	  <para>
	    <lit>svd</lit>: <lit>on</lit> or <lit>off</lit> (the default). Use
	    SVD rather than Cholesky or QR decomposition in least squares
	    calculations.  This option applies to the <lit>mols</lit> function
	    as well as various internal calculations, but not to the regular
	    <cmdref targ="ols"/> command.
	  </para>
	</li>
	<li>
	  <para><lit>fcp</lit>: <lit>on</lit> or <lit>off</lit> (the
	    default). Use the algorithm of Fiorentini, Calzolari and
	    Panattoni rather than native gretl code when computing
	    GARCH estimates.</para>
	</li>
	<li>
	  <para><lit>gmm_maxiter</lit>: one integer, the maximum number of
	    iterations for gretl's <cmdref targ="gmm"/> command when in iterated
	    mode (as opposed to one- or two-step).  The default value is
	    250.
	  </para>
	</li>
	<li>
	  <para><lit>nadarwat_trim</lit>: one integer, the trim
	  parameter used in the  <fncref targ="nadarwat"/> function.
	  </para>
	</li>
	<li>
	  <para><lit>fdjac_quality</lit>: one integer between 0 and 2,
	  the algorithm used by the <fncref targ="fdjac"/> function.
	  </para>
	</li>
      </ilist>

      <subhead>Random number generation</subhead>

      <ilist>
	<li>
	  <para><lit>seed</lit>: an unsigned integer.  Sets the seed for
	    the pseudo-random number generator.  By default this is set from the
	    system time; if you want to generate repeatable sequences of random
	    numbers you must set the seed manually.
	  </para>
	</li>
	<li>
	  <para><lit>normal_rand</lit>: <lit>ziggurat</lit> (the default) or
	    <lit>box-muller</lit>.  Sets the method for generating random
	    normal samples based on uniform input.
	  </para>
	</li>
      </ilist>

      <subhead>Robust estimation</subhead>

      <ilist>
	<li>
	  <para><lit>bootrep</lit>: an integer. Sets the number of
	  replications for the <cmdref targ="restrict"/> command with
	  the <opt>--bootstrap</opt> option.</para>
	</li>
	<li>
	  <para><lit>garch_vcv</lit>: <lit>unset</lit>,
	    <lit>hessian</lit>, <lit>im</lit> (information matrix) ,
	    <lit>op</lit> (outer product matrix), <lit>qml</lit> (QML
	    estimator), <lit>bw</lit> (Bollerslev&ndash;Wooldridge). Specifies
	    the variant that will be used for estimating the coefficient
	    covariance matrix, for GARCH models.  If <lit>unset</lit> is given
	    (the default) then the Hessian is used unless the
	    <quote>robust</quote> option is given for the garch command, in
	    which case QML is used.
	  </para>
	</li>
	<li>
	  <para><lit>arma_vcv</lit>: <lit>hessian</lit> (the default) or
	    <lit>op</lit> (outer product matrix). Specifies the variant
	    to be used when computing the covariance matrix for ARIMA
	    models.
	  </para>
	</li>
	<li>
	  <para><lit>force_hc</lit>: <lit>off</lit> (the default) or
	  <lit>on</lit>.  By default, with time-series data and when
	  the <opt>--robust</opt> option is given with <lit>ols</lit>,
	  the HAC estimator is used.  If you set <lit>force_hc</lit>
	  to <quote>on</quote>, this forces calculation of the regular
	  Heteroskedasticity Consistent Covariance Matrix (HCCM),
	  which does not take autocorrelation into account. Note that
	  VARs are treated as a special case: when the <opt>robust</opt>
	  option is given the default method is regular HCCM, but the
	  <opt>robust-hac</opt> flag can be used to force the use of a
	  HAC estimator.
	  </para>
	</li>
	<li>
	  <para><lit>robust_z</lit>: <lit>off</lit> (the default) or
	  <lit>on</lit>. This controls the distribution used when
	  calculating p-values based on robust standard errors in
	  the context of least-squares estimators. By default gretl
	  uses the Student <math>t</math> distribution but if
	  <lit>robust_z</lit> is turned on the normal distribution
	  is used.
	  </para>
	</li>
	<li>
	  <para><lit>hac_lag</lit>: <lit>nw1</lit> (the default),
	    <lit>nw2</lit>, <lit>nw3</lit> or an integer.  Sets the
	    maximum lag value or bandwidth, <math>p</math>, used when
	    calculating HAC (Heteroskedasticity and Autocorrelation
	    Consistent) standard errors using the Newey-West approach, for
	    time series data.  <lit>nw1</lit> and <lit>nw2</lit> represent
	    two variant automatic calculations based on the sample size,
	    <math>T</math>: for nw1, 
	    <equation status="inline"
	      tex="$p = 0.75 \times T^{1/3}$" 
	      ascii="p = 0.75 * T^(1/3)"
	      graphic="nw1"/>, and for nw2, 
	    <equation status="inline"
	      tex="$p = 4 \times (T/100)^{2/9}$" 
	      ascii="p = 4 * (T/100)^(2/9)" 
	      graphic="nw2"/>. <lit>nw3</lit> calls for data-based
	    bandwidth selection.  See also <lit>qs_bandwidth</lit> and
	    <lit>hac_prewhiten</lit> below.
	  </para>
	</li>
	<li>
	  <para><lit>hac_kernel</lit>: <lit>bartlett</lit> (the default),
	    <lit>parzen</lit>, or <lit>qs</lit> (Quadratic Spectral). Sets
	    the kernel, or pattern of weights, used when calculating HAC
	    standard errors.
	  </para>
	</li>
	<li>
	  <para><lit>hac_prewhiten</lit>: <lit>on</lit> or <lit>off</lit>
	    (the default). Use Andrews-Monahan prewhitening and
	    re-coloring when computing HAC standard errors.  This also
	    implies use of data-based bandwidth selection.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>hc_version</lit>: 0 (the default), 1, 2, 3 or 3a. Sets the
	    variant used when calculating Heteroskedasticity Consistent
	    standard errors with cross-sectional data.  The first four options
	    correspond to the HC0, HC1, HC2 and HC3 discussed by
	    Davidson and MacKinnon in <book>Econometric Theory and
	      Methods</book>, chapter 5.  HC0 produces what are usually called
	    <quote>White's standard errors</quote>.  Variant 3a is
	    the MacKinnon&ndash;White <quote>jackknife</quote> procedure.
	  </para>
	</li>
	<li>
	  <para><lit>pcse</lit>: <lit>off</lit> (the default) or
	    <lit>on</lit>.  By default, when estimating a model using
	    pooled OLS on panel data with the <opt>--robust</opt> option,
	    the Arellano estimator is used for the covariance matrix.  If
	    you set <lit>pcse</lit> to <quote>on</quote>, this forces use
	    of the Beck and Katz Panel Corrected Standard Errors (which do
	    not take autocorrelation into account).
	  </para>
	</li>
	<li>
	  <para><lit>qs_bandwidth</lit>: Bandwidth for HAC estimation in
	    the case where the Quadratic Spectral kernel is selected.
	    (Unlike the Bartlett and Parzen kernels, the QS bandwidth need
	    not be an integer.)
	  </para>
	</li> 
      </ilist>

      <subhead>Time series</subhead>

      <ilist>
	<li>
	  <para>
	    <lit>horizon</lit>: one integer (the default is based on the
	    frequency of the data).  Sets the horizon for impulse responses
	    and forecast variance decompositions in the context of vector
	    autoregressions.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>vecm_norm</lit>: <lit>phillips</lit> (the default),
	    <lit>diag</lit>, <lit>first</lit> or <lit>none</lit>. Used in the
	    context of VECM estimation via the <cmdref targ="vecm"/> command
	    for identifying the cointegration
	    vectors. See the <guideref targ="chap:vecm"/> for
	    details.
	  </para>
	</li>
      </ilist>

      <para>
	<emphasis>Interaction with R</emphasis>
      </para>

      <ilist>
	<li>
	  <para><lit>R_lib</lit>: <lit>on </lit>(the default) or
	    <lit>off</lit>.  When sending instructions to be executed by R,
	    use the R shared library by preference to the R executable, if the
	    library is available.
	  </para>
	</li>
	<li>
	  <para><lit>R_functions</lit>: <lit>off</lit> (the default) or
	    <lit>on</lit>. Recognize functions defined in R as if they were
	    native functions (the namespace prefix
	    <quote><lit>R.</lit></quote> is required). See <guideref
	      targ="chap:gretlR"/> for details on this and the
	    previous item.
	  </para>
	</li>
      </ilist>

    </description>
  </command>

  <command name="setinfo" section="Dataset" label="Edit attributes of variable">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--description</flag>
	  <optparm>string</optparm>
	  <effect>set description</effect>
	</option>
	<option>
	  <flag>--graph-name</flag>
	  <optparm>string</optparm>
	  <effect>set graph name</effect>
	</option>
	<option>
	  <flag>--discrete</flag>
	  <effect>mark series as discrete</effect>
	</option>
	<option>
	  <flag>--continuous</flag>
	  <effect>mark series as continuous</effect>
	</option>
      </options>
      <examples>
        <example>setinfo x1 --description="Description of x1"</example>
        <example>setinfo y --graph-name="Some string"</example>
	<example>setinfo z --discrete</example>
      </examples>
    </usage>

    <description context="cli">
      <para>
	Sets up to three attributes of <repl>series</repl>, given by name
	or ID number, as follows.
      </para>

      <para>
	If the <opt>--description</opt> flag is given followed by a
	string in double quotes, that string is used to set the
	variable's descriptive label. This label is shown in response
	to the <cmdref targ="labels"/> command, and is also shown in
	the main window of the GUI program.
      </para>

      <para>
	If the <opt>--graph-name</opt> flag is given followed by a
	quoted string, that string will be used in place of the
	variable's name in graphs.
      </para>

      <para>
	If one or other of the <opt>--discrete</opt> or
	<opt>--continuous</opt> option flags is given, the variable's
	numerical character is set accordingly.  The default is to treat
	all series as continuous; setting a series as discrete
	affects the way the variable is handled in frequency plots.
      </para>

    </description>

    <description context="gui">

      <para>
	In this dialog box you can:</para>

      <para>* Rename a (series) variable.</para>

      <para>* Add or edit a description of the variable: this appears
	next to the variable name in the gretl main window.</para>

      <para>* Add or edit the "display name" for the variable (if the
	variable is a series, not a scalar).  This string (maximum 19
	characters) is shown in place of the variable name when the
	variable is displayed in a graph.  Thus for instance you can
	associate a more comprehensible string such as "T-bill rate" with
	a cryptically named variable such as "tb3".</para>

      <para>* (For time-series data) set the compaction method for the
	variable.  This method will be used if you decide to reduce the
	frequency of the dataset, or if you update the variable by
	importing from a database where the variable is at a higher
	frequency than in the working dataset.
      </para>

      <para>* Mark a variable as discrete (for series with integer values
	only).  This affects the way the variable is handled when you ask
	for a frequency plot.
      </para>

    </description>

    <gui-access>
      <menu-path>/Variable/Edit attributes</menu-path>
      <other-access>Main window pop-up menu</other-access>
    </gui-access>

  </command>

  <command name="setmiss" section="Dataset"
    label="Missing value code">

    <usage>
      <arguments>
        <argument>value</argument>
        <argument optional="true">varlist</argument>
      </arguments>
      <examples>
        <example>setmiss -1</example>
        <example>setmiss 100 x2</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Get the program to interpret some specific numerical data
	value (the first parameter to the command) as a code for
	<quote>missing</quote>, in the case of imported data.  If this
	value is the only parameter, as in the first example above,
	the interpretation will be applied to all series in the data
	set.  If <repl quote="true">value</repl> is followed by a list
	of variables, by name or number, the interpretation is
	confined to the specified variable(s). Thus in the second
	example the data value 100 is interpreted as a code for
	<quote>missing</quote>, but only for the variable
	<lit>x2</lit>.
      </para>
      
      <para context="gui">
	Set a numerical value that will be interpreted as "missing" or
	"not applicable", either for a particular data series (under
	the Variable menu) or globally for the entire data set (under
	the Sample menu).</para> 
      
      <para context="gui">
	Gretl has its own internal coding for missing values, but
	sometimes imported data may employ a different code.  For
	example, if a particular series is coded such that a value of
	-1 indicates "not applicable", you can select "Set missing
	value code" under the Variable menu and type in the value "-1"
	(without the quotes).  Gretl will then read the -1s as missing
	observations.</para>

    </description>

    <gui-access>
      <menu-path>/Data/Set missing value code</menu-path>
    </gui-access>

  </command>

  <command name="setobs" section="Dataset" context="cli"
    label="Set frequency and starting observation">

    <usage>
      <altforms>
        <altform><lit>setobs</lit> <repl>periodicity</repl> <repl>startobs</repl></altform>
	<altform><lit>setobs</lit> <repl>unitvar</repl> <repl>timevar</repl> <lit>--panel-vars</lit></altform>
      </altforms>
      <options>
        <option>
	  <flag>--cross-section</flag>
	  <effect>interpret as cross section</effect>
        </option>
        <option>
	  <flag>--time-series</flag>
	  <effect>interpret as time series</effect>
        </option>
        <option>
	  <flag>--stacked-cross-section</flag>
	  <effect>interpret as panel data</effect>
        </option>
        <option>
	  <flag>--stacked-time-series</flag>
	  <effect>interpret as panel data</effect>
        </option>
        <option>
	  <flag>--panel-vars</flag>
	  <effect>use index variables, see below</effect>
        </option>
        <option>
	  <flag>--panel-time</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--panel-groups</flag>
	  <effect>see below</effect>
        </option>	
      </options>
      <examples>
        <example>setobs 4 1990:1 --time-series</example>
        <example>setobs 12 1978:03</example>
	<example>setobs 1 1 --cross-section</example>
        <example>setobs 20 1:1 --stacked-time-series</example>
	<example>setobs unit year --panel-vars</example>
      </examples>
    </usage>

    <description>
      <para>
	This command forces the program to interpret the current data
	set as having a specified structure.
      </para>
      <para>
	In the first form of the command the <repl>periodicity</repl>,
	which must be an integer, represents frequency in the case of
	time-series data (1 = annual; 4 = quarterly; 12 = monthly; 52 =
	weekly; 5, 6, or 7 = daily; 24 = hourly).  In the case of panel
	data the periodicity means the number of lines per data block:
	this corresponds to the number of cross-sectional units in the
	case of stacked cross-sections, or the number of time periods in
	the case of stacked time series.  In the case of simple
	cross-sectional data the periodicity should be set to 1.
      </para>
      <para>
	The starting observation represents the starting date in the
	case of time series data.  Years may be given with two or four
	digits; subperiods (for example, quarters or months) should be
	separated from the year with a colon.  In the case of panel
	data the starting observation should be given as 1:1; and in
	the case of cross-sectional data, as 1.  Starting observations
	for daily or weekly data should be given in the form
	YYYY-MM-DD (or simply as 1 for undated data).
      </para>
      <para>
	If no explicit option flag is given to indicate the structure
	of the data the program will attempt to guess the structure
	from the information given.
      </para>
      <para>
	The second form of the command (which requires the
	<opt>--panel-vars</opt> flag) may be used to impose a panel
	interpretation when the data set contains variables that uniquely
	identify the cross-sectional units and the time periods.  The data
	set will be sorted as stacked time series, by ascending values of
	the units variable, <repl>unitvar</repl>.
      </para>
      <subhead>Panel-specific options</subhead>
      <para>
	The <opt>panel-time</opt> and <opt>panel-groups</opt> options
	can only be used with a dataset which has already been defined
	as a panel.
      </para>
      <para>
	The purpose of <opt>panel-time</opt> is to set extra
	information regarding the time dimension of the panel. This
	should be given on the pattern of the first form of
	<lit>setobs</lit> noted above. For example, the following may
	be used to indicate that the time dimension of a panel is
	quarterly, starting in the first quarter of 1990.
      </para>
      <code>
	setobs 4 1990:1 --panel-time
      </code>
      <para>
	The purpose of <opt>panel-groups</opt> is to create a
	string-valued series holding names for the groups
	(individuals, cross-sectional units) in the panel. (This will
	be used where appropriate in panel graphs.) With this
	option you supply either one or two arguments as follows.
      </para>
      <para>
	First case: the (single) argument is the name of a
	string-valued series. If the number of distinct values equals
	the number of groups in the panel this series is used to
	define the group names. If necessary, the numerical content of
	the series will be adjusted such that the values are all 1s
	for the first group, all 2s for the second, and so on. If the
	number of string values doesn't match the number of groups an
	error is flagged.
      </para>
      <para>
	Second case: the first argument is the name of a series and
	the second is a string literal or variable holding a name for
	each group. The series will be created if it does not already
	exist. If the second argument is a string literal or string
	variable the group names should be separated by spaces; if a
	name includes spaces it should be wrapped in backslash-escaped
	double-quotes. Alternatively the second argument may be an
	array of strings.
      </para>
      <para>
	For example, the following will create a series named
	<lit>country</lit> in which the names in <lit>cstrs</lit> are
	each repeated <math>T</math> times, <math>T</math> being the
	time-series length of the panel.
      </para>
      <code>
	string cstrs = sprintf("France Germany Italy \"United Kingdom\"")
	setobs country cstrs --panel-groups
      </code>
    </description> 

    <gui-access>
      <menu-path>/Data/Dataset structure</menu-path>
    </gui-access>
      
  </command>

  <command name="setopt" section="Programming" context="cli"
    label="Set options for next command">

    <usage>
      <arguments>
	<argument>command</argument>
	<argument optional="true">action</argument>
	<argument>options</argument>
      </arguments>
      <examples>
        <example>setopt mle --hessian</example>
        <example>setopt ols persist --quiet</example>
	<example>setopt ols clear</example>
      </examples>
    </usage>

    <description>
      <para>
	This command enables the pre-setting of options for a
	specified command. Ordinarily this is not required, but it may
	be useful for the writers of hansl functions when they wish to
	make certain command options conditional on the value of an
	argument supplied by the caller.
      </para>
      <para>
	For example, suppose a function offers a boolean
	<quote><lit>quiet</lit></quote> switch, whose intended effect
	is to suppress the printing of results from a certain
	regression executed within the function. In that case one
	might write:
      </para>
      <code>
	if quiet
	  setopt ols --quiet
	endif
	ols ...
      </code>
      <para>
	The <opt>quiet</opt> option will then be applied to the
	next <lit>ols</lit> command if and only if the variable
	<lit>quiet</lit> has a non-zero value.
      </para>
      <para>
	By default, options set in this way apply only to the
	following instance of <repl>command</repl>; they are not
	persistent. However if you give <lit>persist</lit> as the
	value for <repl>action</repl> the options will continue to
	apply to the given command until further notice.  The antidote
	to the <lit>persist</lit> action is <lit>clear</lit>: this
	erases any stored setting for the specified command.
      </para>
      <para>
	It should be noted that options set via <lit>setopt</lit> are
	compounded with any options attached to the target command
	directly. So for example one might append the
	<opt>hessian</opt> option to an <lit>mle</lit> command
	unconditionally but use <lit>setopt</lit> to add
	<opt>quiet</opt> conditionally.
      </para>
    </description> 
  </command>

  <command name="shell" section="Utilities" 
    label="Execute shell commands" context="cli">

    <usage>
      <arguments>
        <argument>shellcommand</argument>
      </arguments>
      <examples>
        <example>! ls -al</example>
	<example>! notepad</example>
	<example>launch notepad</example>
      </examples>
    </usage>

    <description>
      <para>
	An exclamation mark, <cmd>!</cmd>, or the keyword
	<cmd>launch</cmd>, at the beginning of a command line is
	interpreted as an escape to the user's shell.  Thus arbitrary
	shell commands can be executed from within
	<program>gretl</program>.  When <cmd>!</cmd> is used, the
	external command is executed synchronously.  That is,
	<program>gretl</program> waits for it to complete before
	proceeding.  If you want to start another program from within
	<program>gretl</program> and not wait for its completion
	(asynchronous operation), use <cmd>launch</cmd> instead.
      </para>
      <para>
	For reasons of security this facility is not enabled by default.
	To activate it, check the box titled <quote>Allow shell
	commands</quote> under the File, Preferences menu in the GUI
	program.  This also makes shell commands available in the
	command-line program (and is the only way to do so).
      </para>
    </description>

  </command>

  <command name="smpl" section="Dataset" 
    label="Set the sample range" context="cli">

    <!-- don't break the lines below or the text version will get messed
    up -->

    <usage>
      <altforms>
	<altform><lit>smpl</lit> <repl>startobs endobs</repl></altform>
	<altform><lit>smpl</lit> <repl>+i -j</repl></altform>
	<altform><lit>smpl</lit> <repl>dumvar</repl> <lit>--dummy</lit></altform>
	<altform><lit>smpl</lit> <repl>condition</repl> <lit>--restrict</lit></altform>
	<altform><lit>smpl</lit> <lit>--no-missing [ </lit><repl>varlist</repl> <lit>]</lit></altform>
	<altform><lit>smpl</lit> <lit>--no-all-missing [ </lit><repl>varlist</repl> <lit>]</lit></altform>
	<altform><lit>smpl</lit> <lit>--contiguous [ </lit><repl>varlist</repl> <lit>]</lit></altform>
	<altform><lit>smpl</lit> <repl>n</repl> <lit>--random</lit></altform>
	<altform><lit>smpl full</lit></altform>
      </altforms>
      <options>
        <option>
	  <flag>--dummy</flag>
	  <effect>argument is a dummy variable</effect>
        </option>
        <option>
	  <flag>--restrict</flag>
	  <effect>apply boolean restriction</effect>
        </option>
        <option>
	  <flag>--replace</flag>
	  <effect>replace any existing boolean restriction</effect>
        </option>
        <option>
	  <flag>--no-missing</flag>
	  <effect>restrict to valid observations</effect>
        </option>
        <option>
	  <flag>--no-all-missing</flag>
	  <effect>omit empty observations (see below)</effect>
        </option>
        <option>
	  <flag>--contiguous</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--random</flag>
	  <effect>form random sub-sample</effect>
        </option>
        <option>
	  <flag>--permanent</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--balanced</flag>
	  <effect>panel data: try to retain balanced panel</effect>
        </option>
      </options>	
      <examples>
        <example>smpl 3 10</example>
	<example>smpl 1960:2 1982:4</example>
	<example>smpl +1 -1</example>
	<example>smpl x > 3000 --restrict</example>
	<example>smpl y > 3000 --restrict --replace</example>
	<example>smpl 100 --random</example>
      </examples>
    </usage>

    <description>
      <para>
	Resets the sample range.  The new range can be defined in
	several ways.  In the first alternate form (and the first two
	examples) above, <repl>startobs</repl> and <repl>endobs</repl>
	must be consistent with the periodicity of the data.  Either
	one may be replaced by a semicolon to leave the value
	unchanged.  In the second form, the integers <repl>i</repl>
	and <repl>j</repl> (which may be positive or negative, and
	should be signed) are taken as offsets relative to the
	existing sample range. In the third form <repl>dummyvar</repl>
	must be an indicator variable with values 0 or 1 at each
	observation; the sample will be restricted to observations
	where the value is 1. The fourth form, using
	<opt>--restrict</opt>, restricts the sample to observations
	that satisfy the given Boolean condition (which is specified
	according to the syntax of the <cmdref targ="genr"/> command).
      </para>

      <para>
	The options <opt>--no-missing</opt> and
	<opt>--no-all-missing</opt> may be used to exclude from the
	sample observations for which data are missing. The first
	variant excludes those rows in the dataset for which at least
	one variable has a missing value, while the second excludes
	just those rows on which <emphasis>all</emphasis> variables
	have missing values. In each case the test is confined to the
	variables in <repl>varlist</repl> if this argument is given,
	otherwise it is applied to all series&mdash;with the
	qualification that in the case of <opt>--no-all-missing</opt>
	and no <repl>varlist</repl>, the generic variables
	<lit>index</lit> and <lit>time</lit> are ignored.
      </para>

      <para>
	The <opt>--contiguous</opt> form of <lit>smpl</lit> is intended for
	use with time series data.  The effect is to trim any observations
	at the start and end of the current sample range that contain
	missing values (either for the variables in <repl>varlist</repl>, or
	for all data series if no <repl>varlist</repl> is given).  Then a
	check is performed to see if there are any missing values in the
	remaining range; if so, an error is flagged.
      </para>

      <para>
	With the <opt>--random</opt> flag, the specified number of cases
	are selected from the current dataset at random (without
	replacement).  If you wish to be able to replicate this selection
	you should set the seed for the random number generator first (see
	the <cmdref targ="set"/> command).
      </para>

      <para>
	The final form, <lit>smpl full</lit>, restores the full data range.
      </para>

      <para>
	Note that sample restrictions are, by default, cumulative: the
	baseline for any <lit>smpl</lit> command is the current sample. If you
	wish the command to act so as to replace any existing restriction you
	can add the option flag <opt>--replace</opt> to the end of the
	command. (But this option is not compatible with the
	<opt>--contiguous</opt> option.)
      </para>

      <para>
	The internal variable <lit>obs</lit> may be used with the
	<opt>--restrict</opt> form of <lit>smpl</lit> to exclude particular
	observations from the sample.  For example
      </para>
      <code>
	smpl obs!=4 --restrict
      </code> 
      <para>
	will drop just the fourth observation. If the data points are
	identified by labels,
      </para>
      <code>
	smpl obs!="USA" --restrict
      </code>
      <para>will drop the observation with label <quote>USA</quote>.
      </para>

      <para>
	One point should be noted about the <opt>--dummy</opt>,
	<opt>--restrict</opt> and <opt>--no-missing</opt> forms of
	<lit>smpl</lit>: <quote>structural</quote> information in the data
	file (regarding the time series or panel nature of the data) is likely
	to be lost when this command is issued.  You may reimpose structure
	with the <cmdref targ="setobs"/> command.  A related option, for use
	with panel data, is the <opt>--balanced</opt> flag: this requests that
	a balanced panel is reconstituted after sub-sampling, via the
	insertion of <quote>missing rows</quote> if need be.  But note that it
	is not always possible to comply with this request.  
      </para>

      <para>
	By default, restrictions on the current sample range are
	undoable: by doing <lit>smpl full</lit> you can restore
	the unrestricted dataset. However, the <opt>permanent</opt>
	flag can be used to substitute the restricted dataset for the
	original. This option is only available in conjunction
	with the <opt>restrict</opt>, <opt>dummy</opt>,
	<opt>no-missing</opt>, <opt>no-all-missing</opt> or
	<opt>random</opt> forms of <lit>smpl</lit>.
      </para>

      <para>
	Please see <guideref targ="chap:sampling"/> for further details.
      </para>

    </description>

    <gui-access>
      <menu-path>/Sample</menu-path>
    </gui-access>

  </command>

  <command name="spearman" section="Statistics"
    label="Spearmans's rank correlation">

    <usage>
      <arguments>
        <argument>series1</argument>
        <argument>series2</argument>
      </arguments>
      <options>
        <option>
	  <flag>--verbose</flag>
	  <effect>print ranked data</effect>
        </option>
      </options>
    </usage>

    <description>
      <para context="cli">
	Prints Spearman's rank correlation coefficient for the
	series <repl>series1</repl> and <repl>series2</repl>. The
	variables do not have to be ranked manually in advance; the
	function takes care of this.
      </para>
      <para context="gui">
	Prints Spearman's rank correlation coefficient for a specified
	pair of series.  The series do not have to be ranked
	manually in advance; the function takes care of this.
      </para>
      <para>
	The automatic ranking is from largest to smallest (&ie; the
	largest data value gets rank 1).  If you need to invert this
	ranking, create a new variable which is the negative of the
	original.  For example:
      </para>
      <code>
	series altx = -x
	spearman altx y
      </code>
    </description>

    <gui-access>
      <menu-path>/Model/Robust estimation/Rank correlation</menu-path>
    </gui-access>

  </command>

  <command name="sprintf" section="Printing" 
    label="Printing to a string" context="cli">

    <usage>
      <arguments>
	<argument>stringvar</argument>
        <argument>format</argument>
	<argpunct>, </argpunct>
        <argument>args</argument>
      </arguments>
    </usage>

    <description>
      <para>
	This command works exactly like the <cmdref targ="printf"/>
	command, printing the given arguments under the control of the
	format string, except that the result is written into the named
	string, <repl>stringvar</repl>.
      </para>
    </description>

  </command>

  <command name="square" section="Transformations" 
    label="Create squares of variables" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--cross</flag>
	  <effect>generate cross-products as well as squares</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Generates new series which are squares of the series in
	<repl>varlist</repl> (plus cross-products if the
	<opt>--cross</opt> option is given).  For example, <cmd>square
	  x y</cmd> will generate <lit>sq_x</lit> = <lit>x</lit>
	squared, <lit>sq_y</lit> = <lit>y</lit> squared and
	(optionally) <lit>x_y</lit> = <lit>x</lit> times <lit>y</lit>.
	If a particular variable is a dummy variable it is not squared
	because we will get the same variable.  
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Squares of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="store" section="Dataset" label="Save data">

    <usage>
      <arguments>
        <argument>filename</argument>
        <argument optional="true">varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--csv</flag>
	  <effect>use CSV format</effect>
        </option>
        <option>
	  <flag>--omit-obs</flag>
	  <effect>see below, on CSV format</effect>
        </option>
        <option>
	  <flag>--no-header</flag>
	  <effect>see below, on CSV format</effect>
        </option>
        <option>
	  <flag>--gnu-octave</flag>
	  <effect>use GNU Octave format</effect>
        </option>
        <option>
	  <flag>--gnu-R</flag>
	  <effect>use GNU R format</effect>
        </option>
        <option>
	  <flag>--gzipped</flag>
	  <optparm optional="true">level</optparm>
	  <effect>apply gzip compression</effect>
        </option>
        <option>
	  <flag>--jmulti</flag>
	  <effect>use JMulti ASCII format</effect>
        </option>
        <option>
	  <flag>--dat</flag>
	  <effect>use PcGive ASCII format</effect>
        </option>
        <option>
	  <flag>--decimal-comma</flag>
	  <effect>use comma as decimal character</effect>
        </option>
        <option>
	  <flag>--database</flag>
	  <effect>use gretl database format</effect>
        </option>
        <option>
	  <flag>--overwrite</flag>
	  <effect>see below, on database format</effect>
        </option>
        <option>
	  <flag>--comment</flag>
	  <optparm>string</optparm>
	  <effect>see below</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Save data to <repl>filename</repl>. By default all currently
	defined series are saved but the optional <repl>varlist</repl>
	argument can be used to select a subset of series. If the
	dataset is sub-sampled, only the observations in the current
	sample range are saved.
      </para>
      <para>
	The format in which the data are written may be controlled in
	the first instance by the extension or suffix of
	<repl>filename</repl>, as follows:
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>.gdt</lit>, or no extension: gretl's native XML
	    data format. (If no extension is provided,
	    <quote><lit>.gdt</lit></quote> is added automatically.)
	  </para>
	</li>
	<li>
	  <para>
	    <lit>.gtdb</lit>: gretl's native binary data format.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>.csv</lit>: comma-separated values (CSV).
	  </para>
	</li>
	<li>
	  <para>
	    <lit>.txt</lit> or <lit>.asc</lit>: space-separated
	    values.
	  </para>
	</li>	
	<li>
	  <para>
	    <lit>.R</lit>: GNU R format.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>.m</lit>: GNU Octave format.
	  </para>
	</li>	
      </ilist>
      <para>
	The format-related option flags shown above can be used to
	force the issue of the save format independently of the
	filename (or to get gretl to write in the formats of PcGive or
	JMulTi). However, if <repl>filename</repl> has extension
	<lit>.gdt</lit> or <lit>.gdtb</lit> this necessarily implies
	use of native format and the addition of a conflicting
	option flag will generate an error.
      </para>
      <para>
	When data are saved in native format (only), the
	<opt>gzipped</opt> option may be used for data compression,
	which can be useful for large datasets. The optional parameter
	for this flag controls the level of compression (from 0 to 9):
	higher levels produce a smaller file, but compression takes
	longer. The default level is 1; a level of 0 means that no
	compression is applied.
      </para>
      <para>
	The option flags <opt>--omit-obs</opt> and <opt>--no-header</opt>
	are applicable only when saving data in CSV format.  By default,
	if the data are time series or panel, or if the dataset includes
	specific observation markers, the CSV file includes a first column
	identifying the observations (&eg; by date).  If the
	<opt>--omit-obs</opt> flag is given this column is omitted. The
	<opt>--no-header</opt> flag suppresses the usual printing of the
	names of the variables at the top of the columns.
      </para>
      <para>
	The option flag <opt>--decimal-comma</opt> is also confined to
	the case of saving data in CSV format. The effect of this option
	is to replace the decimal point with the decimal comma; in
	addition the column separator is forced to be a semicolon.
      </para>
      <para>
	The option of saving in gretl database format is intended to help
	with the construction of large sets of series, possibly having
	mixed frequencies and ranges of observations.  At present this
	option is available only for annual, quarterly or monthly
	time-series data. If you save to a file that already exists, the
	default action is to append the newly saved series to the existing
	content of the database.  In this context it is an error if one or
	more of the variables to be saved has the same name as a variable
	that is already present in the database. The
	<opt>--overwrite</opt> flag has the effect that, if there are
	variable names in common, the newly saved variable replaces the
	variable of the same name in the original dataset. 
      </para>
      <para>
	The <opt>--comment</opt> option is available when saving data
	as a database or in CSV format. The required parameter is a
	double-quoted one-line string, attached to the option flag
	with an equals sign. The string is inserted as a comment into
	the database index file or at the top of the CSV output.
      </para>
      <para>
	The <lit>store</lit> command behaves in a special manner in
	the context of a <quote>progressive loop</quote>.  See
	<guideref targ="chap:looping"/> for details.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Save data; /File/Export data</menu-path>
    </gui-access>

  </command>

  <command name="summary" section="Statistics" 
    label="Descriptive statistics" context="cli">

    <usage>
      <altforms>
	<altform><lit>summary [</lit> <repl>varlist</repl> ]</altform>
	<altform><lit>summary --matrix=</lit><repl>matname</repl></altform>
      </altforms>
      <options>
        <option>
	  <flag>--simple</flag>
	  <effect>basic statistics only</effect>
        </option>
        <option>
	  <flag>--weight</flag>
	  <optparm>wvar</optparm>
	  <effect>weighting variable</effect>
        </option>
        <option>
	  <flag>--by</flag>
	  <optparm>byvar</optparm>
	  <effect>see below</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	In its first form, this command prints summary statistics for
	the variables in <repl>varlist</repl>, or for all the
	variables in the data set if <repl>varlist</repl> is omitted.
	By default, output consists of the mean, standard deviation
	(sd), coefficient of variation (= sd/mean), median, minimum,
	maximum, skewness coefficient, and excess kurtosis.  If the
	<opt>--simple</opt> option is given, output is restricted to
	the mean, minimum, maximum and standard deviation.
      </para>
      <para>
	If the <opt>--by</opt> option is given (in which case the
	parameter <repl>byvar</repl> should be the name of a discrete
	variable), then statistics are printed for sub-samples
	corresponding to the distinct values taken on by
	<repl>byvar</repl>.  For example, if <repl>byvar</repl> is a
	(binary) dummy variable, statistics are given for the cases
	<lit>byvar = 0</lit> and <lit>byvar = 1</lit>. Note: at
	present, this option is incompatible with the
	<opt>--weight</opt> option.
      </para>
      <para>
	If the alternative form is given, using a named matrix, then
	summary statistics are printed for each column of the matrix.
	The <opt>--by</opt> option is not available in this case.
      </para>      
    </description>

    <gui-access>
      <menu-path>/View/Summary statistics</menu-path>
      <other-access>Main window pop-up menu</other-access>
    </gui-access>

  </command>

  <command name="system" section="Estimation" label="Systems of equations">

    <usage>
      <altforms>
	<altform><lit>system method=</lit><repl>estimator</repl></altform>
	<altform><repl>sysname</repl><lit> &lt;- system</lit></altform>
      </altforms>
      <examples>
	<example>"Klein Model 1" &lt;- system</example>
        <example>system method=sur</example>
	<example>system method=3sls</example>
	<demos>
	  <demo>klein.inp</demo>
	  <demo>kmenta.inp</demo>
	  <demo>greene14_2.inp</demo>
	</demos>	  
      </examples>
    </usage>

    <description>

      <para context="gui">
	In this window you can define a system of equations and choose an
	estimator for the system.  Four sorts of statement may be given here, as
	follows:
      </para>

      <para context="cli">
	Starts a system of equations.  Either of two forms of the
	command may be given, depending on whether you wish to save
	the system for estimation in more than one way or just
	estimate the system once.</para>

      <para context="cli">
	To save the system you should assign it a name, as in the first
	example (if the name contains spaces it must be surrounded by
	double quotes).  In this case you estimate the system using
	the <cmdref targ="estimate"/> command.  With a saved system of
	equations, you are able to impose restrictions (including
	cross-equation restrictions) using the <cmdref
	targ="restrict"/> command.
      </para>

      <para context="cli">
	Alternatively you can specify an estimator for the system
	using <lit>method=</lit> followed by a string identifying one
	of the supported estimators: <cmd>ols</cmd> (Ordinary Least
	Squares), <cmd>tsls</cmd> (Two-Stage Least Squares)
	<cmd>sur</cmd> (Seemingly Unrelated Regressions),
	<cmd>3sls</cmd> (Three-Stage Least Squares), <cmd>fiml</cmd>
	(Full Information Maximum Likelihood) or <cmd>liml</cmd>
	(Limited Information Maximum Likelihood).  In this case the
	system is estimated once its definition is complete.  
      </para>

      <para context="cli">
	An equation system is terminated by the line <cmd>end system</cmd>.
	Within the system four sorts of statement may be given, as follows.
      </para>

      <ilist>
	<li><para><cmdref targ="equation"/>: specify an equation
	    within the system.  At least two such statements must be
	    provided.</para>
	</li>
	<li><para><cmd>instr</cmd>: for a system to be estimated via
	    Three-Stage Least Squares, a list of instruments (by
	    variable name or number). Alternatively, you can put this
	    information into the <cmd>equation</cmd> line using the
	    same syntax as in the <cmdref targ="tsls"/>
	    command.</para>
	</li>
	<li><para><cmd>endog</cmd>: for a system of simultaneous
	    equations, a list of endogenous variables.  This is
	    primarily intended for use with FIML estimation, but with
	    Three-Stage Least Squares this approach may be used
	    instead of giving an <cmd>instr</cmd> list; then all the
	    variables not identified as endogenous will be used as
	    instruments.</para>
	</li>
	<li><para><cmd>identity</cmd>: for use with FIML, an identity
	    linking two or more of the variables in the system.  This
	    sort of statement is ignored when an estimator other than
	    FIML is used.
	  </para>
	</li>
      </ilist>
	
      <para context="cli">
	After estimation using the <cmd>system</cmd> or
	<cmd>estimate</cmd> commands the following accessors can be used to
	retrieve additional information:
      </para>

      <ilist context="cli">
	<li><para><lit>$uhat</lit>: the matrix of residuals, one column
	    per equation.
	  </para>
	</li>
	<li><para><lit>$yhat</lit>: matrix of fitted values, one column
	    per equation.
	  </para>
	</li>
	<li><para><lit>$coeff</lit>: column vector of coefficients (all
	    the coefficients from the first equation, followed by those
	    from the second equation, and so on).
	  </para>
	</li>
	<li><para><lit>$vcv</lit>: covariance matrix of the coefficients.
	    If there are <math>k</math> elements in the
	    <lit>$coeff</lit> vector, this matrix is <math>k</math>
	    by <math>k</math>.
	  </para>
	</li>
	<li><para><lit>$sigma</lit>: cross-equation residual covariance
	    matrix.
	  </para>
	</li>
	<li><para><lit>$sysGamma</lit>, <lit>$sysA</lit> and <lit>$sysB</lit>: 
	    structural-form coefficient matrices (see below).
	  </para>
	</li>
      </ilist>

      <para context="cli">
	If you want to retrieve the residuals or fitted values for a
	specific equation as a data series, select a column from the
	<lit>$uhat</lit> or <lit>$yhat</lit> matrix and assign it to
	a series, as in
      </para>
      <code context="cli">
	series uh1 = $uhat[,1]
      </code>

      <para context="cli">
	The structural-form matrices correspond to the following
	representation of a simultaneous equations model:
	<equation status="display"
	  tex="\[\Gamma y_t=Ay_{t-1}+Bx_t+\epsilon_t\]"
	  ascii="Gamma y(t) = A y(t-1) + B x(t) + e(t)"
	  graphic="structural"/> 
	If there are <math>n</math> endogenous variables and
	<math>k</math> exogenous variables, 
	&Gamma; is an <by r="n" c="n"/> matrix and <math>B</math>
	is <by r="n" c="k"/>. If the system contains no lags of the endogenous
	variables then the <math>A</math> matrix is not present.  If the
	maximum lag of an endogenous regressor is <math>p</math>,
	the <math>A</math> matrix is <by r="n" c="np"/>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Simultaneous equations</menu-path>
    </gui-access>

  </command>

  <command name="tabprint" section="Printing" 
    label="Print model in tabular form" context="cli">

    <usage>
      <options>
        <option>
	  <flag>--rtf</flag>
	  <effect>Produce RTF instead of &latex;</effect>
        </option>
        <option>
	  <flag>--csv</flag>
	  <effect>Produce CSV instead of &latex;</effect>
        </option>
        <option>
	  <flag>--complete</flag>
	  <effect>Create a complete document</effect>
        </option>
        <option>
	  <flag>--format="f1|f2|f3|f4"</flag>
	  <effect>Specify a custom format</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>	
      </options>
    </usage>

    <description>
      <para>
	Must follow the estimation of a model.  Prints the estimated
	model in tabular form &mdash; by default as &latex;, but as
	RTF if the <opt>--rtf</opt> flag is given or as CSV is the
	<opt>--csv</opt> flag is given.  If a filename is specified
	using the <opt>output</opt> option output goes to that file,
	otherwise it goes to a file with a name of the form
	<filename>model_N</filename> followed by the extension
	<lit>tex</lit>, <lit>rtf</lit> or <lit>csv</lit>, where
	<lit>N</lit> is the number of models estimated to date in the
	current session.
      </para>
      <para> 
	If CSV format is selected, values are comma-separated unless
	the decimal comma is in force, in which case the separator is
	the semicolon. Note that CSV output may be less complete than
	the other formats.
      </para>
      <para>
	The further options discussed below are available only when
	printing the model as &latex;.
      </para>
      <para>
	If the <opt>--complete</opt> flag is given the &latex; file is
	a complete document, ready for processing; otherwise it must
	be included in a document.
      </para>
      <para>
	If you wish alter the appearance of the tabular output, you can
	specify a custom row format using the <opt>--format</opt> flag.
	The format string must be enclosed in double quotes and must be
	tied to the flag with an equals sign.  The pattern for the format
	string is as follows.  There are four fields, representing the
	coefficient, standard error, <math>t</math>-ratio and
	p-value respectively.  These fields should be separated by
	vertical bars; they may contain a <lit>printf</lit>-type
	specification for the formatting of the numeric value in question,
	or may be left blank to suppress the printing of that column
	(subject to the constraint that you can't leave all the columns
	blank).  Here are a few examples:
      </para>
      <code>
	--format="%.4f|%.4f|%.4f|%.4f"
	--format="%.4f|%.4f|%.3f|"
	--format="%.5f|%.4f||%.4f"
	--format="%.8g|%.8g||%.4f"
      </code>
      <para>
	The first of these specifications prints the values in all columns
	using 4 decimal places.  The second suppresses the p-value and
	prints the <math>t</math>-ratio to 3 places.  The third
	omits the <math>t</math>-ratio.  The last one again omits
	the <math>t</math>, and prints both coefficient and standard
	error to 8 significant figures.
      </para>
      <para>
	Once you set a custom format in this way, it is remembered and
	used for the duration of the gretl session.  To revert to
	the default format you can use the special variant
	<opt>--format=default</opt>.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /LaTeX</menu-path>
    </gui-access>

  </command>

  <command name="textplot" section="Graphs" 
    label="ASCII plot" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--time-series</flag>
	  <effect>plot by observation</effect>
        </option>
        <option>
	  <flag>--one-scale</flag>
	  <effect>force a single scale</effect>
        </option>
        <option>
	  <flag>--tall</flag>
	  <effect>use 40 rows</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Quick and simple ASCII graphics.  Without the <opt>--time-series</opt>
	flag, <repl>varlist</repl> must contain at least two series, the last
	of which is taken as the variable for the <math>x</math> axis, and a
	scatter plot is produced. In this case the <opt>--tall</opt> option
	may be used to produce a graph in which the <math>y</math> axis is
	represented by 40 rows of characters (the default is 20 rows).
      </para>
      <para>
	With the <opt>--time-series</opt>, a plot by observation is produced.
	In this case the option <opt>--one-scale</opt> may be used to force
	the use of a single scale; otherwise if <repl>varlist</repl> contains
	more than one series the data may be scaled. Each line represents an
	observation, with the data values plotted horizontally.  
      </para>
      <para>
	See also <cmdref targ="gnuplot"/>.
      </para>
    </description>

  </command>


  <command name="tobit" section="Estimation" label="Tobit model">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--llimit</flag>
	  <optparm>lval</optparm>
	  <effect>specify left bound</effect>
        </option>
        <option>
	  <flag>--rlimit</flag>
	  <optparm>rval</optparm>
	  <effect>specify right bound</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Estimates a Tobit model, which may be appropriate when the
	dependent variable is <quote>censored</quote>.  For example,
	positive and zero values of purchases of durable goods on the part
	of individual households are observed, and no negative values, yet
	decisions on such purchases may be thought of as outcomes of an
	underlying, unobserved disposition to purchase that may be
	negative in some cases. 
      </para>
      <para context="cli">
	By default it is assumed that the dependent variable is
	censored at zero on the left and is uncensored on the
	right. However you can use the options <opt>--llimit</opt>
	and <opt>--rlimit</opt> to specify a different pattern
	of censoring. Note that if you specify a right bound only,
	the assumption is then that the dependent variable is
	uncensored on the left.
      </para>
      <para context="gui">
	By default it is assumed that the dependent variable is
	censored at zero on the left and is uncensored on the
	right. However you can use the entry boxes marked <quote>left
	bound</quote> and <quote>right bound</quote> to specify a
	different pattern of censoring. Enter either a numerical value
	or <lit>NA</lit> for no censoring.
      </para>
      <para>
	The Tobit model is a special case of interval regression, which
	is supported via the <cmdref targ="intreg"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Tobit</menu-path>
    </gui-access>

  </command>

  <command name="transpos" section="Dataset" label="Transpose data"
    context="gui">

    <description>
      <para>
	Transposes the current data set.  That is, each observation
	(row) in the current data set will be treated as a variable
	(column), and each variable as an observation.  This command
	may be useful if data have been read from some external source 
	in which the rows of the data table represent variables.
      </para>
      <para>
	See also <cmdref targ="dataset"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Transpose data</menu-path>
    </gui-access>

  </command>

  <command name="tsls" section="Estimation"
    label="Instrumental variables regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
	<argument separated="true">instruments</argument>
      </arguments>
      <options>
        <option>
	  <flag>--no-tests</flag>
	  <effect>don't do diagnostic tests</effect>
        </option>	
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>clustered standard errors</effect>
        </option>
	<option>
	  <flag>--liml</flag>
	  <effect>use Limited Information Maximum Likelihood</effect>
        </option>
	<option>
	  <flag>--gmm</flag>
	  <effect>use the Generalized Method of Moments</effect>
        </option>
      </options>      
      <examples>
        <example>tsls y1 0 y2 y3 x1 x2 ; 0 x1 x2 x3 x4 x5 x6</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Computes Instrumental Variables (IV) estimates, by default using
	two-stage least squares (TSLS) but see below for further options.  The
	dependent variable is <repl>depvar</repl>, <repl>indepvars</repl> is
	the list of regressors (which is presumed to include at least one
	endogenous variable); and <repl>instruments</repl> is the list of
	instruments (exogenous and/or predetermined variables). If the
	<repl>instruments</repl> list is not at least as long as
	<repl>indepvars</repl>, the model is not identified.
      </para>

      <para context="cli">
	In the above example, the <lit>y</lit>s are endogenous and the
	<lit>x</lit>s are the exogenous variables. Note that exogenous
	regressors should appear in both lists.
      </para>

      <para context="gui">
	This command requires the selection of two lists of variables: the
	independent variables to appear in the given model and a set of
	instruments.  Note that any exogenous regressors should appear in both
	lists.
      </para>

      <para>
	Output for two-stage least squares estimates includes the Hausman
	test and, if the model is over-identified, the Sargan
	over-identification test.  In the Hausman test, the null
	hypothesis is that OLS estimates are consistent, or in other words
	estimation by means of instrumental variables is not really
	required.  A model of this sort is over-identified if there are
	more instruments than are strictly required.  The Sargan test is
	based on an auxiliary regression of the residuals from the
	two-stage least squares model on the full list of instruments.
	The null hypothesis is that all the instruments are valid, and
	suspicion is thrown on this hypothesis if the auxiliary regression
	has a significant degree of explanatory power. For a good
	explanation of both tests see chapter 8 of <cite
	key="davidson-mackinnon04">Davidson and MacKinnon (2004)</cite>.
      </para>

      <para>
	For both TSLS and LIML estimation, an additional test result is
	shown provided that the model is estimated under the assumption of
	i.i.d. errors (that is, the <opt>--robust</opt> option is not
	selected). This is a test for weakness of the instruments.  Weak
	instruments can lead to serious problems in IV regression: biased
	estimates and/or incorrect size of hypothesis tests based on the
	covariance matrix, with rejection rates well in excess of the
	nominal significance level <cite key="stock-wright-yogo02"
	p="true">(Stock, Wright and Yogo, 2002)</cite>.  The test
	statistic is the first-stage <math>F</math>-test if the model
	contains just one endogenous regressor, otherwise it is the
	smallest eigenvalue of the matrix counterpart of the first stage
	<math>F</math>. Critical values based on the Monte Carlo analysis
	of <cite key="stock-yogo03">Stock and Yogo (2003)</cite> are shown
	when available.
      </para>

      <para>
	The R-squared value printed for models estimated via two-stage least
	squares is the square of the correlation between the dependent
	variable and the fitted values.
      </para>

      <para context="cli">
	For details on the effects of the <opt>robust</opt> and
	<opt>cluster</opt> options, please see the help for
	<cmdref targ="ols"/>.
      </para>

      <para context="cli">
	As alternatives to TSLS, the model may be estimated via Limited
	Information Maximum Likelihood (the <opt>--liml</opt> option) or via
	the Generalized Method of Moments (<opt>--gmm</opt> option). Note that
	if the model is just identified these methods should produce the same
	results as TSLS, but if it is over-identified the results will differ
	in general.
      </para>

      <para context="cli">
	If GMM estimation is selected, the following additional options become
	available:
      </para>

      <ilist context="cli">
	<li>
	  <para>
	    <opt>--two-step</opt>: perform two-step GMM rather than the
	    default of one-step.
	  </para>
	</li>
	<li>
	  <para>
	    <opt>--iterate</opt>: Iterate GMM to convergence.
	  </para>
	</li>
	<li>
	  <para>
	    <opt>--weights=</opt><repl>Wmat</repl>: specify a square matrix of
	    weights to be used when computing the GMM criterion function. The
	    dimension of this matrix must equal the number of instruments. The
	    default is an appropriately sized identity matrix.
	  </para>
	</li>	
      </ilist>

    </description>

    <gui-access>
      <menu-path>/Model/Instrumental variables</menu-path>
    </gui-access>

  </command>

  <command name="var" section="Estimation"
    label="Vector Autoregression">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>ylist</argument>
	<argument separated="true" optional="true">xlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--nc</flag>
	  <effect>do not include a constant</effect>
        </option>
        <option>
	  <flag>--trend</flag>
	  <effect>include a linear trend</effect>
        </option>
        <option>
	  <flag>--seasonals</flag>
	  <effect>include seasonal dummy variables</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--robust-hac</flag>
	  <effect>HAC standard errors</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>skip output of individual equations</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
        </option>
        <option>
	  <flag>--impulse-responses</flag>
	  <effect>print impulse responses</effect>
        </option>
        <option>
	  <flag>--variance-decomp</flag>
	  <effect>print variance decompositions</effect>
        </option>
        <option>
	  <flag>--lagselect</flag>
	  <effect>show criteria for lag selection</effect>
        </option>
      </options>
      <examples>
        <example>var 4 x1 x2 x3 ; time mydum</example>
	<example>var 4 x1 x2 x3 --seasonals</example>
	<example>var 12 x1 x2 x3 --lagselect</example>
      </examples>
    </usage>

    <description>

      <para context="gui">
	This command requires specification of:
      </para>
      <ilist context="gui">
	<li><para context="gui">- the lag order, that is, the number of
	    lags of each variable that should be included in the
	    system;</para>
	</li>
	<li><para context="gui">- any exogenous variables (but note that a
	    constant is included automatically unless you specify otherwise, a
	    trend can be added using the trend checkbox, and seasonal dummy
	    variables can be added using the seasonals checkbox); and
	  </para>
	</li>
	<li><para context="gui">- a list of endogenous variables, lags
	    of which will be included on the right-hand side of each
	    equation (note: do not include lagged variables in this
	    list -- they will be added automatically).</para>
	</li>
      </ilist>
      <para context="gui">
	A separate regression will be run for each variable in the system.
	Output for each equation includes F-tests for zero restrictions on
	all lags of each of the variables and an F-test for the maximum
	lag, along with (optionally) forecast variance decompositions and
	impulse response functions.
      </para>

      <para context="cli">
	Sets up and estimates (using OLS) a vector autoregression
	(VAR).  The first argument specifies the lag order &mdash; or
	the maximum lag order in case the <opt>--lagselect</opt>
	option is given (see below).  The order may be given
	numerically, or as the name of a pre-existing scalar variable.
	Then follows the setup for the first equation.  Do not include
	lags among the elements of <repl>ylist</repl> &mdash; they
	will be added automatically.  The semi-colon separates the
	stochastic variables, for which <repl>order</repl> lags will
	be included, from any exogenous variables in
	<repl>xlist</repl>.  Note that a constant is included
	automatically unless you give the <opt>--nc</opt> flag, a
	trend can be added with the <opt>--trend</opt> flag, and
	seasonal dummy variables may be added using the
	<opt>--seasonals</opt> flag.
      </para>

      <para context="cli">
	While a VAR specification usually includes all lags from 1
	to a given maximum, it is possible to select a specific
	set of lags. To do this, substitute for the regular
	(scalar) <repl>order</repl> argument either the name of
	a predefined vector or a comma-separated list of lags,
	enclosed in braces. We show below two ways of specifying
	that a VAR should include lags 1, 2 and 4 (but not lag 3):
      </para>
      <code context="cli">
	var {1,2,4} ylist
	matrix p = {1,2,4}
	var p ylist
      </code>      

      <para context="cli">
	A separate regression is reported for each variable in
	<repl>ylist</repl>.  Output for each equation includes
	<math>F</math>-tests for zero restrictions on all lags of each
	of the variables, an <math>F</math>-test for the significance
	of the maximum lag, and, if the <opt>--impulse-responses</opt>
	flag is given, forecast variance decompositions and impulse
	responses.
      </para>

      <para>
	Forecast variance decompositions and impulse responses are
	based on the Cholesky decomposition of the contemporaneous
	covariance matrix, and in this context the order in which the
	(stochastic) variables are given matters.  The first variable
	in the list is assumed to be <quote>most exogenous</quote>
	within-period. The horizon for variance decompositions and
	impulse responses can be set using the <cmdref targ="set"/>
	command.  For retrieval of a specified impulse response
	function in matrix form, see the <fncref targ="irf"/>
	function.
      </para> 

      <para context="cli">
	If the <opt>--robust</opt> option is given, standard errors
	are corrected for heteroskedasticity. Alternatively, the
	<opt>--robust-hac</opt> option can be given to produce
	standard errors that are robust with respect to both
	heteroskedasticity and autocorrelation (HAC). In general
	the latter correction should not be needed if the VAR
	includes sufficient lags.
      </para>

      <para context="cli">
	If the <opt>--lagselect</opt> option is given, the first parameter to
	the <lit>var</lit> command is taken as the maximum lag order.  Output
	consists of a table showing the values of the Akaike (AIC), Schwarz
	(BIC) and Hannan&ndash;Quinn (HQC) information criteria computed from
	VARs of order 1 to the given maximum.  This is intended to help
	with the selection of the optimal lag order.  The usual VAR output is
	not presented. The table of information criteria may be retrieved
	as a matrix via the <lit>$test</lit> accessor.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Time series/Vector autoregression</menu-path>
    </gui-access>

  </command>

  <command name="VAR-lagselect" section="Tests" context="gui"
    label="VAR lag-length selection">

    <description>
      <para>
	In this dialog box you specify a VAR as usual, but use the lag order
	spin button to set the maximum number of lags to test.
      </para>
      <para>
	Output will consist of a table showing the values of the Akaike (AIC),
	Schwarz (BIC) and Hannan&ndash;Quinn (HQC) information criteria computed
	from VARs of order 1 to the chosen maximum.  This is intended to help with
	the selection of the optimal lag order.  
      </para>
    </description>

  </command>

  <command name="VAR-omit" section="Tests" context="gui"
    label="Test exogenous variables in VAR">

    <description>
      <para>
	Use this dialog box to specify a subset of exogenous variables in a VAR.
	These variables will be omitted from the original VAR, and the system
	re-estimated.
      </para>
      <para>
	A Likelihood Ratio test is reported, where the null hypothesis is that
	the true parameter values are zero, in all equations of the VAR, for the
	omitted variables.  The test is based on the difference between the
	log-determinant of the variance matrix for the unrestricted system, and
	that for the restricted system with the selected variables omitted.
      </para>
    </description>

  </command>

  <command name="varlist" section="Dataset" 
    label="Listing of variables" context="cli">

    <usage>
      <options>
	<option>
	  <flag>--scalars</flag>
	  <effect>list scalars</effect>
	</option>
	<option>
	  <flag>--accessors</flag>
	  <effect>list accessor variables</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	By default, prints a listing of the (series) variables
	currently available; <cmd>ls</cmd> may be used as an alias for
	this command.
      </para>
      <para>
	If the <opt>--scalars</opt> option is given, prints a listing
	of any currently defined scalar variables and their
	values. Otherwise, if the <opt>--accessors</opt> option is
	given, prints a list of the internal variables currently
	available via accessors such as <fncref targ="$nobs"/> and
	<fncref targ="$uhat"/>.
      </para>
    </description>

  </command>

  <command name="vartest" section="Tests"
    label="Difference of variances">

    <usage>
      <arguments>
        <argument>series1</argument>
        <argument>series2</argument>
      </arguments>
    </usage>

    <description>
      <para context="cli">
	Calculates the <math>F</math> statistic for the null
	hypothesis that the population variances for the variables
	<repl>series1</repl> and <repl>series2</repl> are equal, and
	shows its p-value.
      </para>
      <para context="gui">
	Calculates the <math>F</math> statistic for the null
	hypothesis that the population variances are equal for the
	two selected series, and shows its p-value.
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/Test statistic calculator</menu-path>
    </gui-access>

  </command>

  <command name="vecm" section="Estimation"
    label="Vector Error Correction Model">

    <usage>
      <arguments>
        <argument>order</argument>
	<argument>rank</argument>
        <argument>ylist</argument>
	<argblock optional="true" separated="true">
	  <argument>xlist</argument>
	</argblock>
	<argblock optional="true" separated="true">
	  <argument>rxlist</argument>
	</argblock>
      </arguments>
      <options>
        <option>
	  <flag>--nc</flag>
	  <effect>no constant</effect>
        </option>
        <option>
	  <flag>--rc</flag>
	  <effect>restricted constant</effect>
        </option>
        <option>
	  <flag>--uc</flag>
	  <effect>unrestricted constant</effect>
        </option>
        <option>
	  <flag>--crt</flag>
	  <effect>constant and restricted trend</effect>
        </option>
        <option>
	  <flag>--ct</flag>
	  <effect>constant and unrestricted trend</effect>
        </option>
        <option>
	  <flag>--seasonals</flag>
	  <effect>include centered seasonal dummies</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>skip output of individual equations</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
        </option>
        <option>
	  <flag>--impulse-responses</flag>
	  <effect>print impulse responses</effect>
        </option>
        <option>
	  <flag>--variance-decomp</flag>
	  <effect>print variance decompositions</effect>
        </option>
      </options>
      <examples>
        <example>vecm 4 1 Y1 Y2 Y3</example>
        <example>vecm 3 2 Y1 Y2 Y3 --rc</example>
	<example>vecm 3 2 Y1 Y2 Y3 ; X1 --rc</example>
	<demos>
	  <demo>denmark.inp</demo>
	  <demo>hamilton.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	A VECM is a form of vector autoregression or VAR (see <cmdref
	  targ="var"/>), applicable where the variables in the model are
	individually integrated of order 1 (that is, are random walks, with or
	without drift), but exhibit cointegration.  This command is closely
	related to the Johansen test for cointegration (see <cmdref
	  targ="coint2"/>).
      </para>

      <para context="cli">
	The <repl>order</repl> parameter to this command represents the lag
	order of the VAR system.  The number of lags in the VECM itself (where
	the dependent variable is given as a first difference) is one less than
	<repl>order</repl>.
      </para>

      <para context="gui">
	The lag order selected in the VECM dialog box is that of the VAR system.
	The number of lags in the VECM itself (where the dependent variable is
	given as a first difference) is one less than this number.
      </para>

      <para context="cli">
	The <repl>rank</repl> parameter represents the cointegration rank, or in
	other words the number of cointegrating vectors.  This must be greater
	than zero and less than or equal to (generally, less than) the number of
	endogenous variables given in <repl>ylist</repl>.
      </para>

      <para context="gui">
	The <quote>rank</quote> represents the number of cointegrating
	vectors.  This must be greater than zero and less than or
	equal to (generally, less than) the number of endogenous
	variables selected.
      </para>

      <para context="cli">
	<repl>ylist</repl> supplies the list of endogenous variables, in
	levels. The inclusion of deterministic terms in the model is controlled
	by the option flags.  The default if no option is specified is to
	include an <quote>unrestricted constant</quote>, which allows for the
	presence of a non-zero intercept in the cointegrating relations as well
	as a trend in the levels of the endogenous variables.  In the literature
	stemming from the work of Johansen (see for example his 1995 book) this
	is often referred to as <quote>case 3</quote>.  The first four options
	given above, which are mutually exclusive, produce cases 1, 2, 4 and 5
	respectively.  The meaning of these cases and the criteria for selecting
	a case are explained in <guideref targ="chap:vecm"/>.
      </para>

      <para context="cli">
	The optional lists <repl>xlist</repl> and <repl>rxlist</repl>
	allow you to specify sets of exogenous variables which enter the
	model either unrestrictedly (<repl>xlist</repl>) or restricted to
	the cointegration space (<repl>rxlist</repl>). These lists are
	separated from <repl>ylist</repl> and from each other by
	semicolons.
      </para>

      <para context="gui">
	In the <quote>Endogenous variables</quote> box you select the
	vector of endogenous variables, in levels. The inclusion of
	deterministic terms in the model is controlled by the option
	buttons.  The default is to include an <quote>unrestricted
	constant</quote>, which allows for the presence of a non-zero
	intercept in the cointegrating relations as well as a trend in the
	levels of the endogenous variables.  In the literature stemming
	from the work of Johansen (see for example his 1995 book) this is
	often referred to as <quote>case 3</quote>.  The other four
	options produce cases 1, 2, 4 and 5 respectively.  The meaning of
	these cases and the criteria for selecting a case are explained in
	<guideref targ="chap:vecm"/>.
      </para>

      <para context="gui">
	In the <quote>Exogenous variables</quote> box you may add specific
	exogenous variables.  By default these enter the model in
	unrestricted form (indicated by a <lit>U</lit> next to the name of
	the variable).  If you want a certain exogenous variable to be
	restricted to the cointegrating space, right-click on it and
	select <quote>Restricted</quote> from the pop-up menu.  The symbol
	next to the variable will change to R.
      </para>

      <para context="cli">
	The <opt>--seasonals</opt> option, which may be combined with any of the
	other options, specifies the inclusion of a set of centered seasonal
	dummy variables.  This option is available only for quarterly or monthly
	data.
      </para>

      <para context="gui">
	If the data are quarterly or monthly, a check box is shown that allows
	you to include a set of centered seasonal dummy variables.  In all
	cases, an additional check box (<quote>Show details</quote>) allows
	for the printing of the auxiliary regressions that form the starting
	point of the Johansen maximum likelihood estimation procedure.
      </para>

      <para context="cli">
	The first example above specifies a VECM with lag order 4 and a single
	cointegrating vector.  The endogenous variables are <lit>Y1</lit>,
	<lit>Y2</lit> and <lit>Y3</lit>.  The second example uses the same
	variables but specifies a lag order of 3 and two cointegrating vectors;
	it also specifies a <quote>restricted constant</quote>, which is
	appropriate if the cointegrating vectors may have a non-zero intercept
	but the <lit>Y</lit> variables have no trend.
      </para>

      <para context="cli">
	Following estimation of a VECM some special accessors are
	available: <lit>$jalpha</lit>, <lit>$jbeta</lit> and
	<lit>$jvbeta</lit> retrieve, respectively, the &agr; and &bgr;
	matrices and the estimated variance of &bgr;.  For retrieval
	of a specified impulse response function in matrix form, see
	the <fncref targ="irf"/> function.
      </para>      
    </description>

    <gui-access>
      <menu-path>/Model/Time series/VECM</menu-path>
    </gui-access>

  </command>

  <command name="vif" section="Tests" context="cli"
    label="Variance Inflation Factors">

    <description>
      <para>
	Must follow the estimation of a model which includes at least
	two independent variables. Calculates and displays the
	Variance Inflation Factors (VIFs) for the regressors.  The VIF
	for regressor <math>j</math> is defined as
	<equation status="display" 
	  tex="\[\frac{1}{1-R_j^2}\]"
	  ascii="1/(1 - Rj^2)"
	  graphic="vif"/> where <math>R</math><sub>j</sub> is
	the coefficient of multiple correlation between regressor
	<math>j</math> and the other regressors. The factor has
	a minimum value of 1.0 when the variable in question is
	orthogonal to the other independent variables.  
	<cite key="neter-etal90">Neter,	Wasserman, and Kutner (1990)</cite> 
	suggest inspecting the largest VIF as a diagnostic for collinearity; 
	a value greater than 10 is sometimes taken as indicating a 
	problematic degree of collinearity.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Collinearity</menu-path>
    </gui-access>

  </command>

  <command name="wls" section="Estimation"
    label="Weighted Least Squares">

    <usage>
      <arguments>
        <argument>wtvar</argument>
        <argument>depvar</argument>
	<argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
      </options> 
    </usage>

    <description>
      <para context="cli">
	Computes weighted least squares (WLS) estimates using
	<repl>wtvar</repl> as the weight, <repl>depvar</repl> as the
	dependent variable, and <repl>indepvars</repl> as the list of
	independent variables.  Let <repl>w</repl> denote the positive
	square root of <lit>wtvar</lit>; then WLS is basically equivalent
	to an OLS regression of <repl>w</repl> <lit>*</lit>
	<repl>depvar</repl> on <repl>w</repl> <lit>*</lit>
	<repl>indepvars</repl>.  The <emphasis>R</emphasis>-squared,
	however, is calculated in a special manner, namely as
	<equation status="display"
	  tex="\[R^2 = 1 - \frac{\rm ESS}{\rm WTSS}\]"
	  ascii="R^2 = 1 - ESS / WTSS"
	  graphic="wlsr2"/> where ESS is the error sum of squares (sum of
	squared residuals) from the weighted regression and WTSS denotes
	the <quote>weighted total sum of squares</quote>, which equals the
	sum of squared residuals from a regression of the weighted
	dependent variable on the weighted constant alone.
      </para>

      <para context="cli">
	If <repl>wtvar</repl> is a dummy variable, WLS estimation is
	equivalent to eliminating all observations with value zero for
	<repl>wtvar</repl>.
      </para>

      <para context="gui">
	Let "wtvar" denote the variable selected in the "Weight variable"
	box.  An OLS regression is run, where the dependent variable is
	the product of the positive square root of wtvar and the selected
	dependent variable, and the independent variables are also
	multiplied by the square root of wtvar. Statistics such as
	<emphasis>R</emphasis>-squared are based on the weighted
	data.  If wtvar is a dummy variable, weighted least squares
	estimation is equivalent to eliminating all observations with
	value zero for wtvar.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Other linear models/Weighted Least Squares</menu-path>
    </gui-access>

  </command>

  <command name="workdir" section="Utilities" label="Working directory"
    context="gui">

    <description>
      <para>
	The <quote>working directory</quote> is where gretl looks
	by default when reading or writing data files or scripts
	via the file Open and Save dialogs.
      </para>
      <para>
	In addition the working directory is the default location for
      </para>
      <ilist>
	<li>
	  <para>
	    reading files via the script commands <lit>append</lit>,
	    <lit>open</lit>, <lit>run</lit> and <lit>include</lit>; and
	  </para>
	</li>
	<li>
	  <para>
	    writing files via the commands <lit>eqnprint</lit>,
	    <lit>tabprint</lit>, <lit>gnuplot</lit>, <lit>outfile</lit>
	    and <lit>store</lit>.
	  </para>
	</li>
      </ilist>
      <para>
	The option of having gretl use the current directory (as
	determined via the shell) at start-up may be useful to people who
	are in the habit of launching gretl from a command prompt rather
	than a menu or icon.
      </para>
      <para>
	This dialog also allows you to set the behavior of the GUI file
	selector: when you open or save a file in a given folder, should the
	selector remember and return to the same folder on the next
	invocation?  Or should the selector always visit the chosen working
	directory?
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Working directory</menu-path>
    </gui-access>

  </command>

  <command name="x12a" section="Utilities" context="gui"
    label="X-12-ARIMA">

    <description>
      <para>
	There are two procedural options here, controlled by the 
	lower set of radio-buttons.
      </para>
      <para>
	If you select <quote>Execute X-12-ARIMA directly</quote> then
	gretl writes a command file for X-12-ARIMA and calls the
	x12a program to execute the commands. In this case you have the
	option of producing a graph and/or saving selected output series
	to the gretl dataset.
      </para>
      <para>
	If you select <quote>Make X-12-ARIMA command file</quote>
	gretl writes a command file for X-12-ARIMA, as above, but then
	opens this file in an editor window. In that window you are
	able to make changes and to save the file under a chosen
	name. You are also able to send the file for execution by x12a
	(by clicking the <quote>Run</quote> button on the editor
	window toolbar) and view the output. But in this case you do
	not have the option of saving data as gretl series or
	producing a gretl graph.
      </para>
    </description>

  </command>

  <command name="xcorrgm" section="Statistics" label="Cross-correlogram">

    <usage>
      <arguments>
        <argument>series1</argument>
        <argument>series2</argument>
        <argument optional="true">order</argument>
      </arguments>
      <options>
       <option>
	  <flag>--plot</flag>
	  <optparm>mode-or-filename</optparm>
	  <effect>see below</effect>
        </option>	  
      </options>
       <examples>
        <example>xcorrgm x y 12</example>
      </examples>
    </usage>

    <description>
      <para>
	Prints and graphs the cross-correlogram for
	<repl>series1</repl> and <repl>series2</repl>, which may be
	specified by name or number.  The values are the sample
	correlation coefficients between the current value of
	<repl>series1</repl> and successive leads and lags of
	<repl>series2</repl>.
      </para>
      <para>
	If an <repl>order</repl> value is specified the length of the
	cross-correlogram is limited to at most that number of leads and
	lags, otherwise the length is determined automatically, as a
	function of the frequency of the data and the number of
	observations.
      </para>
      <para>
	By default, a plot of the cross-correlogram is produced: a
	gnuplot graph in interactive mode or an ASCII graphic in batch
	mode.  This can be adjusted via the <opt>plot</opt>
	option. The acceptable parameters to this option are
	<lit>none</lit> (to suppress the plot); <lit>ascii</lit> (to
	produce a text graphic even when in interactive mode);
	<lit>display</lit> (to produce a gnuplot graph even when in
	batch mode); or a file name. The effect of providing a file
	name is as described for the <opt>output</opt> option of the
	<cmdref targ="gnuplot"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Cross-correlogram</menu-path>
      <other-access>Main window pop-up menu (multiple selection)</other-access>
    </gui-access>

  </command>

  <command name="xtab" section="Statistics" 
    label="Cross-tabulate variables">

    <usage>
      <arguments>
        <argument>ylist</argument>
	<argument optional="true" separated="true">xlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--row</flag>
	  <effect>display row percentages</effect>
        </option>
        <option>
	  <flag>--column</flag>
	  <effect>display column percentages</effect>
        </option>
        <option>
	  <flag>--zeros</flag>
	  <effect>display zero entries</effect>
        </option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>matname</optparm>
	  <effect>use frequencies from named matrix</effect>
        </option>
      </options>
    </usage>

    <description context="cli">
      <para>
        Displays a contingency table or cross-tabulation for each
	combination of the variables included in <repl>ylist</repl>; if a
	second list <repl>xlist</repl> is given, each variable in
	<repl>ylist</repl> is cross-tabulated by row against each variable
	in <repl>xlist</repl> (by column).  Variables in these lists can
	be referenced by name or by number.  Note that all the variables
	must have been marked as discrete.  Alternatively, if the 
	<opt>--matrix</opt> option is given, treat the named matrix as
	a precomputed set of frequencies and display this as a 
	cross-tabulation.
      </para>
      <para>
	By default the cell entries are given as frequency counts. The
	<opt>--row</opt> and <opt>--column</opt> options (which are
	mutually exclusive), replace the counts with the percentages for
	each row or column, respectively.  By default, cells with a zero
	count are left blank; the <opt>--zeros</opt> option, which has the
	effect of showing zero counts explicitly, may be useful for
	importing the table into another program, such as a spreadsheet.
      </para>
      <para>
        Pearson's chi-square test for independence is displayed if the
        expected frequency under independence is at least 1.0e-7 for all
        cells.  A common rule of thumb for the validity of this statistic is
        that at least 80 percent of cells should have expected frequencies
        of 5 or greater; if this criterion is not met a warning is printed.
      </para>
      <para>
	If the contingency table is 2 by 2, Fisher's Exact Test for
	independence is computed.  Note that this test is based on the
	assumption that the row and column totals are fixed, which may or
	may not be appropriate depending on how the data were generated.
	The left p-value should be used when the alternative to
	independence is negative association (values tend to cluster in
	the lower left and upper right cells); the right p-value should be
	used if the alternative is positive association.  The two-tailed
	p-value for this test is calculated by method (b) in section 2.1
	of <cite key="agresti92">Agresti (1992)</cite>: it is the sum of
	the probabilities of all possible tables having the given row and
	column totals and having a probability less than or equal to that
	of the observed table.
      </para>
    </description>

    <description context="gui">
      <para>
        Displays a contingency table or cross-tabulation for each
	combination of the selected variables.  Note that all the
	variables must be discrete.
      </para>
      <para>
	By default, frequency count values are shown in the cells and on
	the margins of the table.  However, you can choose to display
	either row or column percentages instead.
      </para>
      <para>
	By default, cells with a zero count are shown as empty, but you
	can choose to show zero values explicitly.
      </para>
      <para>
        Pearson's chi-square test for independence is displayed if the
        expected frequency under independence is at least 1.0e-7 for all
        cells.  A common rule of thumb for the validity of this statistic is
        that at least 80 percent of cells should have expected frequencies
        of 5 or greater; if this criterion is not met a warning is printed.
      </para>
      <para>
	If the contingency table is 2 by 2, Fisher's Exact Test for
	independence is computed.  Note that this test is based on the
	assumption that the row and column totals are fixed, which may or
	may not be approriate depending on how the data were generated.
	The left p-value should be used when the alternative to
	independence is negative association (values tend to cluster in
	the lower left and upper right cells); the right p-value should be
	used if the alternative is positive association.  The two-tailed
	p-value for this test is calculated by method (b) in section 2.1
	of <cite key="agresti92">Agresti (1992)</cite>: it is the sum of
	the probabilities of all possible tables having the given row and
	column totals and having a probability less than or equal to that
	of the observed table.
      </para>
    </description>

  </command>

</commandref>

