<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE commandref SYSTEM "gretl_commands.dtd">

<commandref language="portuguese">

<?PSGML NOFILL label code altforms altform menu-path equation other-access?>

  <command name="add" section="Tests" label="Acrescentar variáveis ao modelo">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--lm</flag>
	  <effect>fazer um teste LM, apenas para MQO</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>mostrar apenas o resultado básico do teste</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>não mostrar nada</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar matriz de covariância para o modelo aumentado</effect>
	</option>
	<option>
	  <flag>--both</flag>
	  <effect>apenas para estimação com variáveis instrumentais, ver abaixo</effect>
	</option>
      </options>
      <examples>
        <example>add 5 7 9</example>
        <example>add xx yy zz --quiet</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Tem que ser invocado após um comando de estimação. Realiza um
	teste conjunto para a adição das variáveis especificadas no
	último modelo. Os resultados desse teste podem ser recuperados
	via funções de acesso <fncref targ="$test"/> e <fncref targ="$pvalue"/>.
      </para>
      <para context="cli">
	Por omissão é efetuada uma estimação da versão aumentada do modelo
    original, com a inclusão das variáveis da <repl>lista-de-variáveis</repl>.
    É realizado um teste de Wald sobre o modelo aumentado, que substitui
    o original como o <quote>modelo corrente</quote> para o propósito de,
    por exemplo, obter os resíduos como <lit>$uhat</lit> ou realizar
    testes adicionais.
      </para>
      <para context="cli">
	Alternativamente, utilizando a opção <opt>lm</opt> (disponível
    apenas no caso de modelos estimados via MQO), será efetuado um
    teste LM. Uma regressão auxiliar é executada, na qual a variável
    dependente é o resíduo do modelo anterior e as variáveis
    independentes são as mesmas do modelo anterior mais as da
    <repl>lista-de-variáveis</repl>. De acordo com a hipótese nula de
    que as variáveis acrescentadas não aumentam o poder explicativo,
    o tamanho da amostra vezes o R quadrado não-ajustado desta
    regressão tem uma distribuição qui-quadrado com graus de
    liberdade iguais ao número de regressores adicionados. Neste
    caso o modelo original não é substituído.
      </para>
      <para context="cli">
	A opção <opt>both</opt> é específica para o método dos mínimos
	quadrados de dois estágios: indica que as novas variáveis devem
	ser acrescentadas tanto à lista de regressores quanto à lista
	de instrumentos, por padrão, neste caso, apenas serão
	acrescentadas à lista de regressores.
      </para>
      <para context="gui">
	As variáveis seleccionadas/selecionadas são acrescentadas ao modelo
	anterior e o novo modelo é estimado. É apresentada a estatística de
	teste para a significância conjunta das variáveis acrescentadas, assim
	como o respectivo p-valor.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Acrescentar variáveis</menu-path>
    </gui-access>

  </command>

  <command name="addline" section="Graphs" label="Acrescentar linha ao gráfico"
    context="gui">

    <description>
      <para>
	Esta caixa de diálogo permite acrescentar uma linha a um
	gráfico, definida por uma expressão. A expressão tem que
	ser aceitável pelo Gnuplot. Use <lit>x</lit> para designar
	o valor da variável no eixo x. Note que o gnuplot usa
	<lit>**</lit> para potenciação e que o símbolo de
	decimais deve ser <quote>.</quote>. Exemplos:
      </para>
      <code>
	10+0.35*x
	100+5.3*x-0.12*x**2
	sin(x)
	exp(sqrt(pi*x))
      </code>
    </description>
  </command>

  <command name="adf" section="Tests" label="Teste de Dickey-Fuller aumentado">

    <usage>
      <arguments>
        <argument>ordem</argument>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--nc</flag>
	  <effect>teste sem constante</effect>
	</option>
	<option>
	  <flag>--c</flag>
	  <effect>apenas com constante</effect>
	</option>
	<option>
	  <flag>--ct</flag>
	  <effect>com constante e tendência</effect>
	</option>
	<option>
	  <flag>--ctt</flag>
	  <effect>com constante, tendência e quadrado da tendência</effect>
	</option>
	<option>
	  <flag>--seasonals</flag>
	  <effect>incluir variáveis dummy sazonais</effect>
	</option>
	<option>
	  <flag>--gls</flag>
	  <effect>remover média ou tendência usando GLS</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar resultados da regressão</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar resultados</effect>
	</option>
	<option>
	  <flag>--difference</flag>
	  <effect>usar a primeira diferença da variável</effect>
	</option>
	<option>
	  <flag>--test-down</flag>
	  <optparm optional="true">critério</optparm>
	  <effect>quantidade de defasagens automática</effect>
	</option>
	<option>
	  <flag>--perron-qu</flag>
	  <effect>ver abaixo</effect>
	</option>	
      </options>
      <examples>
	<example>adf 0 y</example>
        <example>adf 2 y --nc --c --ct</example>
        <example>adf 12 y --c --test-down</example>
	<demos>
	  <demo>jgm-1996.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para context="gui">
	Este comando requer um número de defasagens inteiro. Se o número for
	0 é executado um teste Dickey&ndash;Fuller padrão (não aumentado).
	Calcula um conjunto de testes de Dickey&ndash;Fuller sobre a variável
	especificada, com a hipótese nula de que a variável tem uma raiz
	unitária. Mas se a opção de diferenciação tiver sido dada, a primeira
	diferença da variável é obtida antes do teste, e a discussão abaixo
	deve ser interpretada como sendo referente à variável transformada.
      </para>

      <para context="cli">
	As opções mostradas acima e a discussão que se segue é sobre o uso do
	comando <lit>adf</lit> com dados de séries temporais típicas. Para usar
	este comando com dados em painel, ver mais abaixo.
      </para>

      <para context="cli">
	Calcula um conjunto de testes de Dickey&ndash;Fuller sobre cada
	uma das variáveis listadas, sendo a hipótese nula de que a variável
	em questão tem uma raiz unitária. Mas quando é dada a opção
	<opt>difference</opt>, é calculada sobre as primeiras diferenças,
	assim, a discussão abaixo deve ser entendida como sendo sobre a
	variável transformada.
      </para>

      <para context="cli">
	Por padrão, são apresentadas duas variantes do teste: uma baseada
	na regressão contendo uma constante e uma usando uma constante e
	uma tendência linear. Você pode controlar as variantes que são
	apresentadas ao especificar uma ou mais opções.
      </para>

      <para context="cli">
	A opção <opt>gls</opt> pode ser utilizada em conjunto com uma ou
	mais opções, <opt>c</opt> e <opt>ct</opt> (ou seja, o modelo com
	constante ou modelo com constante e tendência). O efeito desta
	opção é que a remoção da média ou da tendênciação da variável a
	ser testada é feita usando o procedimento de Mínimos Quadrados
	Generalizados (GLS) sugerido por <cite key="ERS96">Elliott,
	Rothenberg e Stock (1996)</cite>, o que resulta num teste com
	maior força do que a abordagem padrão de Dickey&ndash;Fuller.
	Esta opção não é compatível com a opção <opt>nc</opt>,
	<opt>ctt</opt> ou <opt>seasonals</opt>.
      </para>      

      <para>
	Em todos os casos a variável dependente é a primeira diferença da
	variável especificada, <math>y</math>, e a variável independente
	chave é a primeira desfasagem de <math>y</math>. O modelo é
	construido de modo que o coeficiente da defasagem de <math>y</math>
	seja igual a raiz em questão menos 1. Por exemplo, o modelo com
	uma constante pode ser escrito como
	<equation status="display"
	tex="\[(1-L)y_t=\beta_0+(\alpha-1)y_{t-1}+\epsilon_t\]"
	ascii="(1 - L)y(t) = b0 + (a-1)y(t-1) + e(t)"
	  graphic="adf1"/> Sobre a hipótese nula de que o coeficiente da
	variável desfasada <math>y</math> é igual a zero. Sobre a alternativa
	de que <math>y</math> é estacionária, este coeficiente é negativo.
      </para>

      <subhead context="cli">Seleção do nível desfasamentos</subhead>

      <para context="cli">
	Se o <repl>número</repl> de defasagens (daqui em diante, 
	<math>k</math>) é maior que 0, então <math>k</math> desfasagens
	da variável dependente são incluidas no lado direito das regressões
	de teste. Se a ordem for dada como &minus;1, o valor de
	<math>k</math> segue a recomendação de <cite key="schwert89">Schwert
	(1989)</cite>, ou seja, a parte inteira de 12(<math>T</math>/100)
	<sup>0.25</sup>, onde <math>T</math> é o tamanho da amostra. Em
	ambos os casos, se a opção <opt>test-down</opt> for dada,
	<math>k</math> é considerada como sendo a desfasagem <emphasis>máxima
	</emphasis> e a ordem de desfasamento efetivamente usada é obtida
	testando os modelos da maior para a menor defasagem. O critério para
	a escolha da defasagem pode ser selecionado usando o parâmetro
	opcional, que deve ser <lit>AIC</lit>, <lit>BIC</lit> ou
	<lit>tstat</lit>. Por padrão é utilizado <lit>AIC</lit>.
      </para>

      <para context="gui">
	Se o número de defasagens (<math>k</math>) é maior que 0, então incluem-se
	<math>k</math> desfasagens da variável dependente no lado direito de cada uma
	das regressões necessárias para calcular as estatísticas dos testes, sujeito
	ao seguinte requisito. Com a caixa de seleção <quote>Testar para baixo a
	partir da ordem máxima de defasagens</quote> estiver ativa, o número de
	desfasagens é considerado como o máximo e o número efetivo de desfasagens
	utilizado é obtido testando para baixo (de acordo com o critério que foi
	selecionado na lista).
      </para>

      <para context="cli">
	Quando se testa para baixo usando AIC ou BIC, a quantidade final
	de defasagens para a equação ADF é tal que otimiza o critério de
	informação escolhido (de Akaike ou Bayesiano de Schwarz). O
	procedimento exato depende se foi indicada ou não a opção
	<opt>gls</opt>: quando a remoção de tendência no GLS é
	especificada, os AIC e BIC são versões <quote>modificadas</quote>
	descritas em <cite key="ng-perron01">Ng e Perron (2001)</cite>,
	caso contrário, elas são as versões padrão. No caso GLS existe
	um refinamento: se a opção adicional <opt>perron-qu</opt> tiver
	sido dada, o critério de informação modificado é calculado de
	acordo com o método revisto recomendado por
	<cite key="perron-qu07">Perron e Qu (2007)</cite>.
      </para>

      <para context="gui">
	Quando se testa para baixo usando AIC ou BIC, o númerol final de
	desfasagens para a equação ADF é tal que otimiza o critério de
	informação escolhido (de Akaike ou Bayesiano de Schwarz).
      </para>      
	
      <para>
	Quando se testa para baixo usando o método da estatística
	<math>t</math>, o procedimento é o seguinte:
      </para>

      <nlist>
	<li><para>Estimar a regressão de Dickey&ndash;Fuller com
	    <math>k</math> desfasagens da variável dependente.
	  </para>
	</li>
	<li><para>A última desfasagem é significante? Se sim, executar
	o teste com <math>k</math> defasagens. Caso contrário, fazer 
	<math>k</math> = <math>k</math> &minus; 1. Se <math>k</math>
	for igual a 0, executar o teste com a 0 defasagens, caso
	contrário ir para o passo 1.
	  </para>
	</li>
      </nlist>

      <para>No contexto do passo 2 acima, <quote>significante</quote>
	quer dizer que para o último desfasamento, a estatística <math>t</math>
	para a última defasagem tem um <emphasis>p</emphasis>-valor bilateral
	assintótico, contra uma distribuição normal, menor ou igual a 0,10.
      </para> 

      <para>
	Os <emphasis>p</emphasis> valores para os testes de
	Dickey&ndash;Fuller baseiam-se em <cite key="mackinnon96">
	MacKinnon (1996)</cite>. O código relevante é incluído
	com a generosa permissão do autor. No caso de teste usando
	a tendência linear (GLS) estes <emphasis>p</emphasis>
	valores não são aplicáveis. Neste caso usam-se os valores
	críticos da Tabela 1 em <cite key="ERS96">Elliott,
	Rothenberg e Stock (1996)</cite>.
      </para>

      <subhead context="cli">Dados em painel</subhead>

      <para context="cli">
	Quando o comando <lit>adf</lit> é usado com dados em painel, para
	produzir testes de raiz unitária em painel, as opções disponíveis
	e os resultados apresentados são ligeiramente diferentes.
      </para>
      <para context="cli">
	Em primeiro lugar, enquanto vocIe pode fornecer uma lista de variáveis
	para serem testadas no caso de séries temporais regulares, com dados em
	painels apenas uma variável pode ser testada por comando. Segundo, as
	opções que ontrolam a inclusão de termos determinísticos passam a ser
	mutuamente exclusivas: você deve escolher entre sem constante, apenas
	com constante e constante mais tendência. O padrão é apenas constante.
	Adicionalmente, a opção <opt>seasonals</opt> não está disponível.
	Terceiro, a opção <opt>verbose</opt> possui um significado diferente:
	ela prduz um breve detalhamento do teste para cada uma das variáveis
	de séries temporais de forma individual (sendo o padrão mostrar apenas
	o resultado geral).
      </para>
      <para context="cli">
	O teste geral (hipótese nula: a série em questão possui uma raiz
	unitária para todas as unidades de painel) é calculado de uma ou de
	ambas as maneiras: utilizando o método de <cite key="IPS03">Im,
	Pesaran and Shin (Journal of Econometrics, 2003)</cite> ou
	o de <cite key="choi01">Choi (Journal of International
	Money and Finance, 2001)</cite>. O teste de Choi requer que os
	<emphasis>p</emphasis>-valores estejam disponíveis para os testes
	individuais, se esse não for o caso (dependendo das opções selecionadas)
	ele é omitido. A estatística particular dada para o teste de
	Im, Pesaran, Shin varia como se segue: se o número de defasagens para
	o teste for diferente de zero a estatística <math>W</math> é mostrada,
	caso contrário, se os tamanhos das séries de tempo forem diferentes
	entre os indivíduos a estatístca <math>Z</math> é mostrada, caso
	contrário a estatística <math>t</math>-barra é mostrada.
      </para>

    </description>

    <gui-access>
      <menu-path>/Variável/Teste de Dickey-Fuller aumentado</menu-path>
    </gui-access>

  </command>

  <command name="anova" label="ANOVA" section="Statistics">
    <usage>
      <arguments>
        <argument>resposta</argument>
        <argument>tratamento</argument>
        <argument optional="true">controlo</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar resultados</effect>
	</option>
      </options>
    </usage>
    <description>
      <para>
	Análise de Variância: <repl>resposta</repl> é uma série que mede
	um efeito com interesse e <repl>tratamento</repl> tem que ser uma
	variável discreta que codifica dois ou mais tipos de tratamento
	(ou não-tratamento).  Para ANOVA de duas-vias, a variável 
	<repl>controlo</repl> (que também deve ser discreta) codifica os 
	valores de uma variável de controlo.
      </para>
      <para context="cli">
	Quando não se usa a opção <opt>--quiet</opt>, este comando mostra a
	tabela das somas de quadrados e médias quadradas juntamente com um
	teste <math>F</math>.  O teste <math>F</math> e o seu valor P podem 
	ser obtidos usando os acessores <lit>$test</lit> e <lit>$pvalue</lit>
	respetivamente.
      </para>
      <para>
	A hipótese nula do teste <math>F</math> é de que a resposta média é
	invariante com o tipo de tratamento, ou por outras palavras, que o 
	tratamento não produz efeito.  Em termos formais, o teste é apenas
	válido se a variância da resposta for igual para todos os tipos de 
	tratamentos.
      </para>
      <para>
	Note que os resultados apresentados por este comando pertencem a um
	subconjunto da informação resultante do procedimento seguinte, que é
	facilmente implementável em gretl.  Crie um conjunto de variáveis
	auxiliares que codifiquem todos os tipos de tratamentos, exceto um.
	No caso da ANOVA de duas-vias, adicionalmente, crie um conjunto de
	variáveis auxiliares que codifiquem todos os <quote>controlos</quote>
    , exceto um.
	De seguida efectue uma regressão sobre <repl>resposta</repl> com uma
	constante e com as variáveis auxiliares usando <cmdref targ="ols"/>.
	No caso da ANOVA singular (ou uma-via) a  tabela é produzida passando
	a opção <opt>--anova</opt> para <lit>ols</lit>.  No caso da ANOVA de 
	duas-vias o teste F relevante, é obtido usando o comando <cmdref
	targ="omit"/>.  Por exemplo (assumindo que <lit>y</lit> é a resposta,
	<lit>xt</lit> codifica os tratamentos, e <lit>xb</lit> codifica os 
	controlos):
      </para>
      <code>
	# uma-via
	list dxt = dummify(xt)
	ols y 0 dxt --anova
	# duas-vias
	list dxb = dummify(xb)
	ols y 0 dxt dxb
	# teste da significância conjunta de dxt
	omit dxt --quiet
      </code>
    </description>

    <gui-access>
      <menu-path>/Modelo/Outros modelos lineares/ANOVA</menu-path>
    </gui-access>

  </command>

  <command name="append" section="Dataset" label="Acrescentar dados" context="cli">

    <usage>
      <arguments>
        <argument>ficheiro-de-dados</argument>
      </arguments>
      <options>
	<option>
	  <flag>--time-series</flag>
	  <effect>ver abaixo</effect>
	</option>
	<option>
	  <flag>--fixed-sample</flag>
	  <effect>ver abaixo</effect>
	</option>
	<option>
	  <flag>--update-overlap</flag>
	  <effect>ver abaixo</effect>
	</option>

	<optnote>Ver abaixo para opções especializadas adicionais</optnote>
      </options>
    </usage>

    <description>
      <para>
	Abre um ficheiro de dados e acrescenta esse conteúdo ao conjunto 
        de dados actual, se os novos dados forem compatíveis.  O programa
        tentará determinar o formato do ficheiro de dados (nativo, texto 
        simples, CSV, Gnumeric, Excel, etc.).
      </para>
	<para>
	Os dados acrescentados podem tomar a forma de observações
        adicionais em variáveis já existentes, ou em novas variáveis.
	Caso sejam novas variáveis, estas terão que ser compatíveis de
	acordo com:
        (a) o número de observações dos novos dados seja o mesmo que nos
	dados existentes, ou
        (b) que os novos dados estejam acompanhados de informação clara
	sobre as observações de modo que gretl possa decidir onde colocar
	os valores.  
      </para>
      <para>
	Existe uma funcionalidade especial para acrescentar a um conjunto de
	dados de painel. Seja <math>n</math> o número de unidades de secção 
	cruzada no painel, <math>T</math> o número de períodos temporais, e
	<math>m</math> o número de observações dos novos dados. Se <math>m =
	  n</math> os novos dados serão tomados como invariantes-temporais,
	e serão copiados para a posição em cada período temporal. Por outro
	lado, se <math>m = T</math> os dados serão tratados como sendo 
	não-variantes a longo das unidades de painel, e serão copiados para a
	posição em cada unidade. Se o painel é <quote>quadrado</quote>, e
	<math>m</math> é igual a <math>n</math> e a <math>T</math>, acontece
	uma ambiguidade. Neste caso, por omissão, trata-se cada novos dados
	como sendo invariantes-temporais, mas você pode forçar gretl para
	tratar os novos dados como sendo série temporal ao fornecer a opção
	<opt>--time-series</opt>.  (Esta opção é ignorada nos outros casos.)
      </para>
      <para>
	When a data file is selected for appending, there may be an
	area of overlap with the existing dataset; that is, one or
	more series may have one or more observations in common across
	the two sources. If the option <opt>update-overlap</opt> is
	given, the <lit>append</lit> operation will replace any
	overlapping observations with the values from the selected
	data file, otherwise the values currently in place will be
	unaffected.
      </para>
      <para>
	The additional specialized options <opt>sheet</opt>,
	<opt>coloffset</opt>, <opt>rowoffset</opt> and
	<opt>fixed-cols</opt> work in the same way as with <cmdref
	targ="open"/>; see that command for explanations.
      </para>
      <para>
	Ver também o comando <cmdref targ="join"/> para um manuseamento
        mais sofisticado com várias fontes de dados.
      </para>
    </description>

    <gui-access>
      <menu-path>/Ficheiro/Acrescentar dados</menu-path>
    </gui-access>

  </command>

  <command name="ar" section="Estimation" label="Estimação autoregressiva">

    <usage>
      <arguments>
        <argument>desfasamentos</argument>
	<argument separated="true">variável-dependente</argument>
        <argument>variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar matriz de covariância</effect>
	</option>
      </options>
      <examples>
        <example>ar 1 3 4 ; y 0 x1 x2 x3</example>
      </examples>
    </usage>

    <description>
      <para>
	Determina estimativas para os parâmetros usando o
        procedimento iteractivo e generalizado de 
        Cochrane&ndash;Orcutt (ver a Secção 9.5 de Ramanathan, 
        2002). A iteração termina quando os erros das somas de 
        quadrados sucessivos não difiram em mais que 0,005 porcento 
        ou após 20 iterações.</para>

      <para context="gui">
	A <quote>lista de desfasamentos AR</quote> especifica a estrutura do
	processo de erro.  Por exemplo, a entrada <quote>1 3
	  4</quote> corresponde ao processo: 
	<equation status="display" 
	  tex="\[u_t = \rho_1u_{t-1} + \rho_3 u_{t-3} +
	    \rho_4 u_{t-4} + e_t\]"
	  ascii="u(t) = rho1*u(t-1) + rho3*u(t-3) + rho4*u(t-4)"
	  graphic="arlags"/>
      </para>

      <para context="cli">
	<repl quote="true">desfasamentos</repl> é uma lista de desfasamentos nos 
        resíduos, terminada por um ponto-e-vírgula. No exemplo acima
        o termo do erro é especificado como
	<equation status="display" 
	  tex="\[u_t = \rho_1u_{t-1} + \rho_3 u_{t-3} +
	    \rho_4 u_{t-4} + e_t\]"
	  ascii="u(t) = rho(1)*u(t-1) + rho(3)*u(t-3) + rho(4)*u(t-4)"
	  graphic="arlags"/>
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/Estimação autoregressiva</menu-path>
    </gui-access>

  </command>

  <command name="ar1" section="Estimation" label="Estimação AR(1)">

    <usage>
      <arguments>
	<argument>variável-dependente</argument>
        <argument>variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--hilu</flag>
	  <effect>usar o procedimento Hildreth&ndash;Lu</effect>
	</option>
	<option>
	  <flag>--pwe</flag>
	  <effect>usar o estimador Prais&ndash;Winsten</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
	</option>
	<option>
	  <flag>--no-corc</flag>
	  <effect>não aperfeiçoar os resultados com Cochrane-Orcutt</effect>
	</option>
	<option>
	  <flag>--loose</flag>
	  <effect>usar critério de convergência mais relaxado</effect>
	</option>
      </options>
      <examples>
        <example>ar1 1 0 2 4 6 7</example>
	<example>ar1 y 0 xlist --pwe</example>
	<example>ar1 y 0 xlist --hilu --no-corc</example>
      </examples>
    </usage>

    <description>
      <para>
	Determina estimativas admissíveis GLS para um modelo em que se
	assume que o termo de erro segue um processo autoregressivo de
	primeira-ordem.
      </para>
      <para>
	O método por omissão é o procedimento iterativo de Cochrane&ndash;Orcutt
	(ver, por exemplo, a Secção 9.4 de Ramanathan, 2002). A iteração termina
	quando as estimativas sucessivas do coeficiente de autocorrelação não 
	diferirem por mais de 0.001 ou após 20 iterações.
      </para>
      <para>
	Se tiver sido dada a opção <lit>--hilu</lit>, é utilizado o 
	procedimento de pesquisa de Hildreth&ndash;Lu.  Os resultados são depois
	aperfeiçoados usando o método Cochrane&ndash;Orcutt, exceto se tiver sido
	indicada a opção <lit>--no-corc</lit>.  (Esta última opção é ignorada se
	 não tiver sido usado <lit>--hilu</lit>).
      </para>
      <para>
	Se tiver sido dada a opção <lit>--pwe</lit>, é usado o estimador 
	de Prais&ndash;Winsten.  Isto involve uma iteração semelhante à de
	Cochrane&ndash;Orcutt; a diferença é que equanto 
	Cochrane&ndash;Orcutt descarta a primeira observação, a de 
	Prais&ndash;Winsten faz uso dela. Para mais detalhes ver, por 
	exemplo, o Capítulo 13 do livro de Greene, <book>Econometric Analysis</book>
	(2000).
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/AR(1)</menu-path>
    </gui-access>

  </command>

  <command name="arch" section="Estimation" label="Modelo ARCH">

    <usage>
      <arguments>
        <argument>ordem</argument>
        <argument>variável-dependente</argument>
	<argument>variáveis-independentes</argument>
      </arguments>
      <examples>
        <example>arch 4 y 0 x1 x2 x3</example>
      </examples>
    </usage>

    <description>
	<para>
	This command is retained at present for backward
	compatibility, but you are better off using the maximum
	likelihood estimator offered by the <cmdref targ="garch"/>
	command; for a plain ARCH model, set the first GARCH
	parameter to 0.
      </para>
      <para>
	Estima a especificação do modelo fornecido aceitando em ARCH
	(Heterosquedicidade Condicional Autoregressiva).  O modelo é
	primeiramente estimado em OLS, e depois é efectuada uma 
	regressão auxiliar, na qual, o quadrado dos resíduos da primeira
	fase é regredido com os seus próprios valores desfasados.  A fase
	final é uma estimação por mínimos quadrados com pesos, usando como
	pesos os recíprocos das variâncias de erro ajustadas da regressão
	auxiliar.  (Se a variância predita de de alguma observação na 
	regressão auxiliar for não positiva, então será usada o 
	correspondente resíduo quadrado).
      </para>
      <para>
	Os valores <lit>alpha</lit> mostrados abaixo dos coeficientes
	são os parâmetros estimados do processo ARCH da regressão auxiliar.
      </para>
      <para>
	Ver também <cmdref targ="garch"/> e <cmdref targ="modtest"/> (a opção
	<opt>--arch</opt>).  
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/ARCH</menu-path>
    </gui-access>

  </command>

  <command name="arima" section="Estimation" label="Modelo ARMA">

    <usage>
      <arguments>
	<argblock>
	  <argument>p</argument>
	  <argument>d</argument>
	  <argument>q</argument>
	</argblock>
	<argblock separated="true" optional="true">
	  <argument>P</argument>
	  <argument>D</argument>
	  <argument>Q</argument>
	</argblock>
	<argument separated="true">variável-dependente</argument>
	<argument optional="true">variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das iterações</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>mostrar matriz de covariância</effect>
        </option>
	<option>
	  <flag>--hessian</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--opg</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--nc</flag>
	  <effect>não incluir uma constante</effect>
        </option>
        <option>
	  <flag>--conditional</flag>
	  <effect>usar verosimilhança máxima condicional</effect>
        </option>
        <option>
	  <flag>--x-12-arima</flag>
	  <effect>usar X-12-ARIMA para estimação</effect>
        </option>
	<option>
	  <flag>--lbfgs</flag>
	  <effect>usar maximizador L-BFGS-B</effect>
	</option>
	<option>
	  <flag>--y-diff-only</flag>
	  <effect>ARIMAX especial, ver abaixo</effect>
	</option>
	<option>
	  <flag>--save-ehat</flag>
	  <effect>ver abaixo</effect>
	</option>
      </options>
      <examples>
        <example>arima 1 0 2 ; y</example>
	<example>arima 2 0 2 ; y 0 x1 x2 --verbose</example>
	<example>arima 0 1 1 ; 0 1 1 ; y --nc</example>
      </examples>
    </usage>

    <description>

      <para>
	Advertência: <lit>arma</lit> é uma alcunha aceitável para esta instrução.
      </para>

      <para context="cli">
	Se não for dada a lista de <repl>variáveis-independentes</repl>,
        é estimado um modelo ARIMA (Média Móvel, Autoregressiva, Integrada)
        univariado.  O valores inteiros <repl>p</repl>, <repl>d</repl> e 
        <repl>q</repl> representam respectivamente, a ordem autoregressiva 
        (AR), a ordem de diferenciação, e ordem da média móvel (MA). 
	  Estes valores podem ser fornecidos na forma numérica, ou como
        nome de variáveis escalares pré-existentes.  Por exemplo, um valor
        de 1 em <repl>d</repl>, significa que a primeira diferença da variável
        dependente deve ser obtida antes de estimar os parâmetros ARMA.
      </para>

      <para context="cli">
	Se você pretender apenas incluir no modelo desfasamentos específicos
	AR ou MA (e não todos os desfasamentos até uma certa ordem) você pode
	substituir <repl>p</repl> e/ou <repl>q</repl> de acordo com:
	(a) o nome de uma matriz pré-definida contendo um conjunto de valores
	inteiros, ou
	(b) uma expressão tal como <lit>{1 4}</lit>; ou seja, um conjunto de 
	desfasamentos separados por espaços dentro de chavetas.
      </para>

      <para context="cli">
	Os valores inteiros opcionais,<repl>P</repl>, <repl>D</repl> e
	<repl>Q</repl> representam respectivamente, a sazonalidade AR,
        a ordem para diferenciação de sazonalidade e a ordem de 
        sazonalidade MA.  Estes são apenas aplicáveis se os dados tiverem
        uma frequência superior a 1 (por exemplo, trimestral ou 
        mensal). Mais uma vez, estas ordens podem ser dadas na forma 
        numérica ou como variáveis.
      </para>

      <para context="cli">
	No caso univariado é incluído no modelo por omissão, um interceptor,  
        mas isto pode ser suprimido com a opção	<lit>--nc</lit>.  Se forem
        fornecidas <repl>variáveis-independentes</repl>, o modelo passa a 
        ser ARMAX; neste caso a constante deve ser explicitamente incluída
        se você pretender um interceptor (tal como no segundo exemplo acima).
      </para>

      <para context="cli">
	Existe outra forma alternativa para este comando: se você não 
        pretende aplicar diferenciação (seja sazonal ou não-sazonal), 
        você pode omitir ambos os parâmetros <repl>d</repl> e 
        <repl>D</repl>, em vez de entrar explicitamente zeros.  Além 
        disso, <lit>arma</lit> é um sinónimo ou aliás para 
        <lit>arima</lit>.  Assim, por exemplo, o comando seguinte é 
        válido para especificar o modelo ARMA(2, 1):
      </para>
      <code context="cli">
	arma 2 1 ; y
      </code>

      <para context="gui">
	Estima um modelo ARMA, com ou sem regressores exógenos.  Se a ordem
        de diferenciação for superior a zero o modelo passa a ser ARIMA.  Se
        os dados têm uma frequência superior a 1, é apresentada a opção de 
        inclusão de uma componente sazonal.
      </para>
   
      <para context="gui">
	Se você pretender apenas incluir no modelo desfasamentos específicos
	AR ou MA (e não todos os desfasamentos até uma certa ordem) ative a
	caixa de seleção à direita do seletor de números e escreva no campo de
	texto uma lista	de desfasamentos, separados por espaços.
	Alternativamente, se existir uma matriz contendo o conjunto de
	desfasamentos pretendidos, você pode introduzir o seu nome no campo de
	texto.
      </para>

      <para>
	O normal é usar a funcionalidade <quote>nativa</quote> gretl
        ARMA, com estimação de Máxima Verosimilhança (ML) exata 
        (usando o filtro de Kalman).  Outras opções são: código nativo,
         ML condicional; <program>X-12-ARIMA</program>, ML exata; e 
	<program>X-12-ARIMA</program>, ML condicional.  (As últimas
        duas opções estão disponíveis apenas se o programa
	<program>X-12-ARIMA</program> estiver instalado.)  Para detalhes
        sobre estas opções, veja por favor <guideref targ="chap:timeseries"/>.
      </para>

      <para context="cli">
	Quando o código nativo ML é usado, os erros padrão são por omissão 
	baseados numa aproximação numérica da (inversa negativa da) Hessiana,
	ou em recurso, no produto externo do gradiente (OPG) caso falhe o 
	cálculo da Hessiana numérica. Podem ser usadas duas opções 
	(mutualmente exclusivas) para forçar a situação: a opção <opt>--opg</opt> 
	força o uso do método OPG, sem tentar obter a Hessiana, enquanto a
	opção <opt>--hessian</opt> desativa o OPG em último recurso.
	Note que a falha na determinação da Hessiana numérica indica, em geral
	um modelo incorretamente especificado.
      </para>

      <para context="cli">
	A opção <opt>--lbfgs</opt> é específica para estimação usando
	código nativo ARMA e ML exata:
	significa o uso do algoritmo de <quote>memória limitada</quote>
	L-BFGS-B em vez do usual maximizador BFGS.  Isto pode ajudar em
	alguns casos onde a convergência é difícil de atingir.
      </para>

      <para context="cli">
	A opção <opt>--y-diff-only</opt> é específica na estimação de modelos
	ARIMAX (modelos com uma ordem de integração não-nula e que incluam 
	regressores exógenos), e aplica-se apenas quando se usa a ML exata e
	nativa de gretl. Para esses modelos o comportamento normal é de 
	diferenciar tanto as variáveis dependentes como as regressoras,
	mas quando esta opção é fornecida, apenas é diferenciada a variável
	dependente, mantendo-se as variáveis regressoras na forma de nível.
      </para>

      <para context="cli">
	A opção <opt>--save-ehat</opt> é aplicável apenas quando se
	usa estimação ML nativa e exata. O efeito é o de disponibilizar
	um vector contendo a estimativa óptima de período
	<math>t</math> da perturbação data-<math>t</math> ou inovação:
	isto pode ser recuperado com o uso do acessor <lit>$ehat</lit>.
	Estes valores diferem da série dos resíduos (<lit>$uhat</lit>),
	que contém os erros de predição um-passo-à-frente.
      </para>

      <para>O valor AIC retornado em ligação com os modelos ARIMA é
	calculado conforme a definição usada no programa 
	<program>X-12-ARIMA</program>, nomeadamente
	  <equation status="display" 
	  tex="\[\mbox{AIC}=-2\ell + 2k\]"
	  ascii="AIC = -2L + 2k"
	  graphic="aic"/> onde 
	<equation status="inline" 
	  tex="$\ell$" ascii="L"
	  graphic="ell"/> é o
	logaritmo da verosimilhança e <math>k</math> é o número
        total de parâmetros estimados.  Note-se que o programa 
        <program>X-12-ARIMA</program> não produz critérios de informação
        tal como o AIC quando a estimação é por ML condicional.
      </para>

      <para context="tex">
	As raízes AR e MA apresentadas em ligação com a estimação ARMA
	são baseadas na seguinte representação de um processo ARMA($p,q$):
	\[
	(1-\phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p)Y =
          c + (1 + \theta_1 L + \theta_2 L^2 + \cdots +
         \theta_q L^q)\varepsilon_t
        \]
        As raízes AR são portanto as soluções de
        \[
         1 - \phi_1 z - \phi_2 z^2 - \cdots - \phi_p L^p = 0
        \]
        e a estabilidade requer que estas raízes estejam fora do círculo
	unitário.
      </para>

      <para context="tex">
	A imagem da <quote>frequência</quote> apresentada
	em ligação com as raízes AR e MA é o valor $\lambda$ que solve 
	$z=re^{i2\pi\lambda}$, onde $z$ é a raiz em questão
	e $r$ é o seu módulo.
      </para>

      <para context="notex">
	As raízes AR e MA apresentadas em ligação com a estimação ARMA
	são baseadas na seguinte representação de um processo ARMA(p,q):
      </para>
      <mono context="notex">
	(1 - a_1*L - a_2*L^2 - ... - a_p*L^p)Y =
          c + (1 + b_1*L + b_2*L^2 + ... + b_q*L^q) e_t
      </mono>
      <para context="notex">
        As raízes AR são portanto as soluções de
      </para>
      <mono context="notex">
         1 - a_1*z - a_2*z^2 - ... - a_p*L^p = 0
      </mono>
      <para context="notex">
        e a estabilidade requer que estas raízes estejam fora do círculo
	unitário.
      </para>

      <para context="notex">
	A imagem da <quote>frequência</quote> apresentada
	em ligação com as raízes AR e MA é o valor &lgr; que resolve
	<math>z</math> = <math>r</math> * exp(i*2*&pi;*&lgr;) onde 
	<math>z</math> é a raiz em questão e <math>r</math>
	 o seu módulo.
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/ARIMA</menu-path>
      <other-access>Menu de contexto da janela principal (selecção singular)</other-access>
    </gui-access>

  </command>

  <command name="bfgs-config" section="Estimation" label="Opções do maximizador BFGS"
    context="gui">
    <description>
      <para>
	Esta janela de diálogo permite-lhe controlar alguns aspetos de operação
	do maximizador BFGS.  Se o maximizador falhar em convergir, em alguns casos, 
	pode ajudar aumentar o número máximo  de iterações e, ou, aumentar a tolerância
	de convergência (ser mais permissivo).
	No entanto, se tiver sido usada uma tolerância muito alta, esses resultados 
	devem ser encarados com desconfiança pois é possível que o modelo que se está
	a estimar não tenha sido bem especificado.
      </para>
      <para>
	Na maior parte das aplicações nós recomendamos o uso do maximizador 
	BFGS normal, mas para algums problemas a variante do algoritmo com 
	<quote>memória limitada</quote>, L-BFGS-B, pode produzir uma 
	convergência mais rápida.  Quando se seleciona L-BFGS-B, tem-se a 
	opção de definir o número de correções usadas na matriz de memória 
	limitada (entre 3 e 20, com um valor por omissão de 8).
      </para>
    </description>
  </command>

  <command name="biprobit" section="Estimation" label="Bivariate probit"
    context="cli">
    <usage>
      <arguments>
        <argument>variável-dependente1</argument>
	<argument>variável-dependente2</argument>
        <argument>variáveis-independentes1</argument>
	<argument separated="true" optional="true">variáveis-independentes2</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
        </option>
	<option>
	  <flag>--robust</flag>
	  <effect>erros padrão robustos</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>variável-agrupada</optparm>
	  <effect>ver a explicação em <cmdref targ="logit"/></effect>
        </option>
	<option>
	  <flag>--opg</flag>
	  <effect>ver abaixo</effect>
        </option>
	<option>
	  <flag>--save-xbeta</flag>
	  <effect>ver abaixo</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar informação adicional</effect>
        </option>
      </options>      
      <examples>
        <example>biprobit y1 y2 0 x1 x2</example>
	<example>biprobit y1 y2 0 x11 x12 ; 0 x21 x22</example>
	<demos>
	  <demo>biprobit.inp</demo>
	</demos>
      </examples>
    </usage>
    <description>
      <para>
	Estima um modelo probit bivariado, usando o método de
	Newton&ndash;Raphson para maximizar a verosimilhança.
      </para>
      <para>
	A lista de argumentos começa com as duas variáveis dependentes
	(binárias), seguidas pela lista de regressores.  Se a segunda 
	lista tiver sido dada, separada por um ponto-e-vírgula, ela 
	será interpretada como sendo o conjunto de regressores para a
	segunda equação, e as <repl>variáveis-independentes1</repl> são
	específicas para a primeira equação; caso contrário as 
	<repl>variáveis-independentes1</repl> são consideradas 
	representando o conjunto de regressores comum.
      </para>
      <para>
	Por omissão, os erros padrão são calculados usando uma aproximação
	númerica por convergência da Hessiana.  Mas se tiver sido dada a 
	opção <opt>--opg</opt> a matriz de covariância será baseada no
	produto externo do gradiente (OPG), ou se a opção 
	<opt>--robust</opt> tiver sido dada, o erros padrão QML serão calulados
	usando a <quote>sandwich</quote> entre a inversa da Hessiana e a OPG.
      </para>
      <para>
	Depois duma estimação com sucesso, o acessor <lit>$uhat</lit>
	obtém uma matriz de duas colunas que são os resíduos
	generalizados das duas equações; ou seja, os valores esperados
	das perturbações condicionadas pelos resultados observados e
	covariados.  Por omissão o <lit>$yhat</lit> obtém a matriz
	com quatro colunas, que são as probabilidades estimadas para os
	quatro possíveis resultados conjuntos para (<math>y</math><sub>1</sub>,
	<math>y</math><sub>2</sub>), e pela ordem (1,1), (1,0), (0,1),
	(0,0). Alternativamente, se tiver sido usada a opção
	<lit>--save-xbeta</lit>, o <lit>$yhat</lit> tem duas colunas e 
	contém os valores das funções de índice para as respectivas equações.
      </para>
      <para>
	A saída inclui o teste de razões de verosimilhança para a hipótese
	nula de que as perturbações nas duas equações são não-correlacionadas.
      </para>
    </description>
  </command>

  <command name="bootstrap" section="Tests" label="Opções do Bootstrap"
    context="gui">

    <description>

      <para>Nesta janela de diálogo você pode escolher:</para>

      <ilist>
	<li>
	  <para>
	    A variável ou coeficiente a examinar.  (Com este método, você
	    apenas pode testar um coeficiente de cada vez.)
	  </para>
	</li>
	<li>
	  <para>
	    O tipo de análise a efectuar. O intervalo de confiança 
	    (95 porcento) omissão baseia-se diretamente nos quantis
	    das estimativas dos coeficientes bootstrap.  A versão
	    <quote>studentized</quote> segue o texto de Davidson e
	    MacKinnon <book>Economic Theory and Methods</book> (ETM),
	    Capítulo 5: em cada replicação bootstrap uma razão-<math>t</math>
	    é construída como (a) a diferença entre as estimativas do
	    coeficiente corrente e da linha-base, dividida pela (b) 
	    estimativa do erro padrão da linha-base. Então o intervalo 
	    de confiança é construído segundo os quantis desta 
	    razão-<math>t</math>, tal como explicado no ETM. A opção
	    valor-P baseia-se na distribuição da razão-<math>t</math>
	    do bootstrap:
	    é a proporção de replicações onde o valor absoluto desta 
	    estatística excede o valor absoluto da razão-<math>t</math>
	    da linha-base.
	  </para>
	</li>
	<li>
	  <para>Resíduos reamostrados versus erros normalizados simulados.
	    No primeiro caso os resíduos originais (re-escalados como 
	    sugerido em ETM) são reamostrados com substituição.  No segundo
	    caso são gerados valores normalizados pseudo-aleatórios a partir
	    da variância original dos resíduos.
	  </para>
	</li>
	<li>
	  <para>O número de replicações a realizar.  Note que quando você
	    está a construir um intervalo de confiança a 95 porcento, é
	    desejável que 0.05(<math>B</math> + 1)/2 resulte num valor
	    inteiro (onde <math>B</math> é o número de replicações).
	    Portanto o gretl pode ajustar o número de replicações de modo a
	    garantir isso.
	  </para>
	</li>
	<li>
	  <para>Se se produz ou não um gráfico da distribuição bootstrap.
	    Esta opção usa o mecanismo de estimação de densidade de núcleo
	    do gretl.
	  </para>
	</li>
      </ilist>

    </description>
  </command>

  <command name="boxplot" section="Graphs" label="Gráficos caixa-com-bigodes">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--notches</flag>
	  <effect>mostrar intervalo de 90 porcento para a mediana delimitado por entalhes</effect>
	</option>
	<option>
	  <flag>--factorized</flag>
	  <effect>ver abaixo</effect>
	</option>
	<option>
	  <flag>--panel</flag>
	  <effect>ver abaixo</effect>
	</option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>nome</optparm>
	  <effect>mostra o gráfico das colunas da matriz indicada</effect>
        </option>
	<option>
	  <flag>--output</flag>
	  <optparm>nome-de-ficheiro</optparm>
	  <effect>envia a saída para o ficheiro especificado</effect>
	</option>
      </options>
    </usage>

    <description>

      <para>Estes gráficos apresentam a distribuição de uma variável.  A
	caixa central contém os 50 porcento dos dados centrais, &ie; está
	limitada pelos primeiro e terceiro quartis.  Os <quote>bigodes</quote>
        estendem-se até aos valores mínimo e máximo.  É desenhada uma linha
	que corta a caixa na mediana.  Um símbolo <quote>+</quote> indica a
	média.  Se tiver sido selecionada a opção de mostrar o intervalo de
	confiança para a média, ele é obtido pelo método 'bootstrap' e 
	apresentado na forma de linhas a tracejado acima e abaixo da mediana.
      </para> 

      <para context="gui">
	A opção <opt>factorized</opt> permite examinar a distribuição 
	de uma variável condicionada pelo valor de um fator discreto. Por
	exemplo, se um conjunto de dados contém salários e uma variável
	auxiliar para o sexo, você pode selecionar a variável salário como
	alvo e o sexo como fator, para visualizar lado a lado gráficos
	caixas-com-bigodes dos salários de homens e mulheres.
      </para>

       <para context="cli">
	A opção <opt>factorized</opt> permite examinar a distribuição 
	de uma variável condicionada pelo valor de um fator discreto. Por
	exemplo, se um conjunto de dados contém salários e uma variável
	auxiliar para o sexo, você pode selecionar a variável salário como
	alvo e o sexo como fator para visualizar lado a lado gráficos
	caixas-com-bigodes dos salários de homens e mulheres, como em
      </para>
      <code context="cli">
	boxplot salario sexo --factorized
      </code>
      <para context="cli">
	Note que neste caso você tem que especificar exatamente duas
	variáveis, com a variável fator em segundo lugar.
      </para>

      <para context="cli">
	Se o conjunto de dados atual é de painel, e apenas tiver sido 
	especificada uma variável, a opção <opt>panel</opt> produz uma
	série de gráficos caixas-com-bigodes lado a lado, para cada uma
	das <quote>unidades</quote> do painel ou grupo.
      </para>

      <para context="cli">
	Em modo interativo o resultado é mostrado imediatamente. Em
	modo lote ('batch') o comportamento normal é o de criar um
	ficheiro de script gnuplot na diretoria de trabalho do
	utilizador, cujo nome segue a forma <filename>gpttmpN.plt</filename>,
   	iniciando com N = <lit>01</lit>. Esses gráficos poderão ser 
	posteriormente gerados com o programa <program>gnuplot</program> 
	(em MS Windows, <program>wgnuplot</program>).  Este comportamento
	pode ser modificado usando a opção <opt>--output=</opt>
	<repl>nome-de-ficheiro</repl>.  Para mais detalhes, ver o 
	comando <cmdref targ="gnuplot"/>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Ver/Gráfico das variáveis/Caixa com bigodes</menu-path>
    </gui-access>

  </command>

  <command name="break" section="Programming" 
    label="Sair de um ciclo" context="cli">

    <description>

      <para>Sai de um ciclo.  Este comando pode apenas ser usado
        dentro de um ciclo; ele termina a execução de comandos e
        sai de dentro do ciclo (o mais interior).  
        Ver também <cmdref targ="loop"/>.
      </para> 
    </description>
  </command>

  <command name="bwfilter" section="Transformations" context="gui"
    label="Filtro de Butterworth">

    <description>
      <para>
	O filtro de Butterworth é uma aproximação a um filtro ideal
	de onda-quadrada que deixa passar frequências de uma certa
	gama com potência máxima enquanto bloqueia todas as outras.
      </para>
      <para>
	Em princípio, valores altos do parâmetro de ordem, <math>n</math>, produzem
	uma melhor aproximação ao filtro ideal, mas tem uma possível
	penalidade de instabilidade numérica.  O valor de <quote>corte</quote> 
	define a banda de passagem e a banda de bloqueio.  Ele é expresso em graus,
	e tem que ser maior que 0 e menor que 180&deg; (ou &pi;	radianos, 
	correspondendo à maior frequência nos dados).  Valores menores do corte 
	produzem uma tendência mais suave (alisamento).
      </para>
      <para>
	Observar o periodograma da série em estudo é uma análise preliminar que é
	útil quando se pretende aplicar este filtro.  Para mais detalhes ver
	<guideref targ="chap:genr"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Filtro/Butterworth</menu-path>
    </gui-access>

  </command>

  <command name="catch" section="Programming" 
    label="Capturar erros" context="cli">
    <usage>
      <syntax><lit>catch </lit><repl>command</repl></syntax>
    </usage>
    <description>
      <para>
	Isto não é um comando em sentido estrito mas pode ser usado como 
	um prefixo na maior parte dos comandos: o efeito é o de prevenir
	a eventual interrupção da execução de comandos (ou de um ficheiro
	de comandos) quando ocorra um erro num comando.  Se acontecer um 
	erro, este fica registado como sendo um erro interno que pode ser
	acedido com <lit>$error</lit> (um valor de zero indica sucesso).
	O valor de <lit>$error</lit> deve ser sempre verificado 
	imediatamente a seguir ao uso de <lit>catch</lit>, e deve ser 
	tomada a ação adequada se o comando falhou.
      </para> 
      <para>
	A palavra reservada <lit>catch</lit> não pode ser usada antes de <lit>if</lit>,
	<lit>elif</lit> ou <lit>endif</lit>.
      </para>
    </description>
  </command>

  <command name="chow" section="Tests" label="Teste de Chow">

<usage>
      <altforms>
        <altform><lit>chow</lit> <repl>observação</repl></altform>
        <altform><lit>chow</lit> <repl>variável-auxiliar</repl> <lit>--dummy</lit></altform>
      </altforms>
      <options>
	<option>
	  <flag>--dummy</flag>
	  <effect>usar uma variável auxiliar pré-existente</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar estimativas para o modelo aumentado</effect>
	</option>
	<option>
	  <flag>--limit-to</flag>
	  <optparm>list</optparm>
	  <effect>limit test to subset of regressors</effect>
	</option>	
      </options>
      <examples>
        <example>chow 25</example>
        <example>chow 1988:1</example>
	<example>chow female --dummy</example>
      </examples>
    </usage>

    <description>
      <para context="gui">
	Este comando requer um número de uma observação (ou uma data, 
        em dados cronológicos).
      </para>

      <para>
	Tem que se seguir a uma regressão de Mínimos Quadrados (OLS). Se um número de observação ou uma data tiver sido dado, produz um teste sobre a hipótese nula de não haver quebra estrutural no ponto de separação indicado. O procedimento cria uma variável auxiliar que é igual a 1 a partir do ponto
        especificado por <repl>observação</repl> até ao final da amostra,
        caso contrário é 0, e cria também termos de interação entre
        esta variável auxiliar e as variáveis regressoras originais. Se tiver sido dada uma variável auxiliar, será testada a hipótese nula de homogeneidade estrutural no que diz respeito a essa variável auxiliar. Mais uma vez, os termos de interação são acrescentados. Em quaisquer dos casos, é executada uma regressão aumentada que inclui estes termos e
        é calculada a estatística <math>F</math>, considerando 
        a regressão aumentada como não restringida e a original como
        restringida.  Mas se o modelo original usou um estimador robusto para a matriz de covariância, a estatística de teste é um valor de qui-quadrado de Wald baseada num estimador robusto da matriz de covariância da regressão aumentada.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Teste de Chow</menu-path>
    </gui-access>
  </command>

  <command name="clear" section="Programming" context="cli">
    <usage>
      <options>
	<option>
	  <flag>--dataset</flag>
	  <effect>apagar apenas o conjunto de dados</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Sem opções, apaga da memória todos os objetos gravados, incluindo o conjunto
	de dados corrente.  Note que ao abrir um novo conjunto de dados, ou ao usar
	o comando <cmd>nulldata</cmd> para criar um conjunto de dados vazio,
	obterá o mesmo efeito, por isso o uso de <cmd>clear</cmd> normalmente não
	é necessário.
      </para>
      <para>
	Se tiver sido dada a opção <lit>--dataset</lit>, então apenas o conjunto
	de dados é apagado; os outros objetos gravados como matrizes e escalares
	serão preservados.
      </para>
    </description>
  </command>

  <command name="cluster" section="Estimation" 
	   label="Estimação de variância robusta" context="gui">
    <description>
      <para>
	Se você selecionar a segunda opção então tem que fornecer o
	nome de uma variável agrupadora. Esta variável deve ter pelo
        menos dois valores distintos mas geralmente deve ter 
        substancialmente menos valores distintos do que há observações
	no intervalo da amostra.
      </para>
      <para>
	O estimador de variância <quote>agrupação-robusta</quote> 
        divide a amostra num número de subconjuntos ou agrupamentos
        de acordo com o valor tomado na variável selecionada. Em vez da
        assunção clássica de que o erro é independente e identicamente
        distribuído, este estimador permite que a variância do erro
        seja diferente por agrupamento e também permite algum grau de
        dependência do erro dentro de cada agrupamento.
      </para>
    </description>
  </command>

  <command name="coeffsum" section="Tests" label="Soma de coeficientes">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <examples>
        <example>coeffsum xt xt_1 xr_2</example>
	<demos>
	  <demo>restrict.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para context="gui">Este comando requer uma lista de variáveis,
        escolhidas a partir do conjunto de variáveis independentes de um
        dado modelo.</para>
      <para context="gui">
	Calcula a soma dos coeficientes nas variáveis indicadas na lista.
        Apresenta esta soma juntamente com o seu erro padrão e o p-value
        para a hipótese nula de que a soma é zero.
      </para>
      <para context="cli">
	Tem que se seguir a uma regressão.  Calcula a soma dos 
        coeficientes nas variáveis indicadas na <repl>lista-de-variáveis</repl>.
        Apresenta esta soma juntamente com o seu erro padrão e o p-value
        para a hipótese nula de que a soma é zero.
      </para>
      <para>Note-se a diferença entre este teste e <cmdref
	  targ="omit"/>, que testa a hipótese nula de que os coeficientes
        num conjunto especificado de variáveis independentes são
        <emphasis>todos</emphasis> iguais a zero.</para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Soma de coeficientes</menu-path>
    </gui-access>

  </command>

  <command name="coint" section="Tests" 
    label="Teste de cointegração Engle-Granger">

    <usage>
      <arguments>
        <argument>ordem</argument>
        <argument>variável-dependente</argument>
	<argument>variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--nc</flag>
	  <effect>não incluir uma constante</effect>
	</option>
	<option>
	  <flag>--ct</flag>
	  <effect>incluir constante e tendência</effect>
	</option>
	<option>
	  <flag>--ctt</flag>
	  <effect>incluir constante e tendência quadrática</effect>
	</option>
	<option>
	  <flag>--skip-df</flag>
	  <effect>não efectuar testes DF nas variáveis individuais</effect>
	</option>
	<option>
	  <flag>--test-down</flag>
	  <optparm optional="true">criterion</optparm>
	  <effect>ordem de desfasamento automática</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes adicionais das regressões</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
	</option>
      </options>
      <examples>
	<example>coint 4 y x1 x2</example>
	<example>coint 0 y x1 x2 --ct --skip-df</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	O teste de cointegração Engle&ndash;Granger.  O procedimento 
        por omissão é: (1) efetuar testes de Dickey&ndash;Fuller (DF) 
        segundo a hipótese nula de que cada variável listada tem uma raiz 
        unitária; (2) estima a regressão de cointegração; e (3) executar um 
        teste DF sobre os resíduos da regressão de cointegração. Se for 
        dada a opção <lit>--skip-df</lit>, o passo (1) é omitido.
      </para>
      <para context="cli">
	Se a ordem de desfasamento especificada é positiva, todos os testes
	Dickey&ndash;Fuller usam essa ordem, com esta qualificação: se a 
	opção <opt>--test-down</opt> for dada, o valor indicado é tomado 
	como sendo o máximo e a ordem de desfasamento efetivamente usada em
	cada caso é obtida testando para baixo. Ver o comando 
	<cmdref targ="adf"/> para detalhes sobre este procedimento.
      </para>
      <para context="cli">
	Por omissão, a regressão de cointegração contém uma constante.  Se
        você deseja suprimir a constante, acrescente a opção <lit>--nc</lit>.
	Se você deseja aumentar a lista de termos determinísticos na regressão
        de cointegração com uma tendência linear ou quadrática, use a opção
        <lit>--ct</lit> ou <lit>--ctt</lit>.  Estas opções são mutualmente
        exclusivas.
      </para>

      <para context="gui">
	O teste de cointegração Engle&ndash;Granger.  O procedimento
        por omissão é: (1) efetuar testes de Dickey&ndash;Fuller (DF)
        segundo a hipótese nula de que cada variável listada tem uma
        raiz unitária; (2) estima a regressão de cointegração; e (3)  
        executar um teste DF sobre os resíduos da regressão de 
        cointegração. No entanto, se estiver selecionada a opção
        <quote>ignorar os testes iniciais DF</quote> os testes no 
        passo (1) são omitidos.
      </para>
      <para context="gui">
        Se a ordem de desfasamento <math>k</math>, é maior que zero, então
        são incluídos <math>k</math> desfasamentos no lado direito de cada
        regressão de teste, excepto se estiver selecionada a opção 
        <quote>Testar para baixo a partir do desfasamento de maior ordem</quote>:
        nesse caso ela é encarada como sendo o máximo desfasamento e a 
        efetivamente usada em cada caso é obtida testando para baixo.  
        Ver o comando <cmdref targ="adf"/> para detalhes sobre este 
        procedimento.
      </para>
      <para context="gui">
	Por omissão, as regressões de cointegração incluem uma constante.
        Se você deseja suprimir a constante, ou juntar uma tendência linear
        ou quadrática, selecione a opção apropriada a partir do conjunto de 
        botões de rádio disponíveis na janela do teste de cointegração.
      </para>

      <para>Os <emphasis>P-</emphasis>values para este teste são baseados em
	MacKinnon (1996).  O código relevante é incluído com a
        generosa permissão do autor.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/Testes de Cointegração/Engle-Granger</menu-path>
    </gui-access>

  </command>

  <command name="coint2" section="Tests" label="Teste de cointegração de Johansen">

    <usage>
      <arguments>
        <argument>ordem</argument>
        <argument>listaY</argument>
      <argblock optional="true" separated="true">
	  <argument>listaX</argument>
	</argblock>
	<argblock optional="true" separated="true">
	  <argument>listaRx</argument>
	</argblock>
      </arguments>
      <options>
        <option>
	  <flag>--nc</flag>
	  <effect>sem constante</effect>
        </option>
        <option>
	  <flag>--rc</flag>
	  <effect>constante restringida</effect>
        </option>
	<option>
	  <flag>--uc</flag>
	  <effect>constante não restringida</effect>
        </option>
        <option>
	  <flag>--crt</flag>
	  <effect>constante e tendência restringida</effect>
        </option>
        <option>
	  <flag>--ct</flag>
	  <effect>constante e tendência não restringida</effect>
        </option>
        <option>
	  <flag>--seasonals</flag>
	  <effect>incluir auxiliares sazonais centradas</effect>
        </option>
	<option>
	  <flag>--asy</flag>
	  <effect>registar valores p assimtóticos</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>apenas mostrar os testes</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>não mostrar nada</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das regressões auxiliares</effect>
        </option>
      </options>
      <examples>
        <example>coint2 2 y x</example>
	<example>coint2 4 y x1 x2 --verbose</example>
	<example>coint2 3 y x1 x2 --rc</example>
      </examples>
    </usage>

    <description>
      <para>
	Executa o teste de Johansen para a cointegração entre as
	variáveis em <repl>listaY</repl> para a dada ordem de 
	desfasamento.  Para detalhes sobre este teste ver 
	<guideref targ="chap:vecm"/> ou <cite key="hamilton94">Hamilton (1994)</cite>,
	Capítulo 20. Os valores p são calculados usando a aproximação
	gama de Doornik <cite key="doornik98" p="true">(Doornik, 1998)</cite>.
	 São mostrados dois conjuntos de valores p para o teste traço,
	valores assintóticos imediatos e valores ajustados para o 
	tamanho da amostra.  Por omissão, o acessor <lit>$pvalue</lit>
	obtém a variante ajustada, mas se opção <opt>--asy</opt> for 
	dada, pode ser usado para registar os valores assintóticos.
      </para>

      <para context="gui">
	Efetua o teste de Johansen para a cointegração entre as
	variáveis listadas para a ordem de desfasamento selecionada.
	  Para detalhes sobre este teste ver, por exemplo, o livro 
	de Hamilton, <book>Time Series Analysis</book> (1994), 
	Capítulo 20.  Os valores p são calculados usando a aproximação
	gama de  Doornik.
      </para>

      <para context="cli">
	A inclusão de termos determinísticos no modelo é controlada por
       intermédio das opções.  Por omissão, se não tiver sido indicada nenhuma
       opção, será incluída uma <quote>constante não restringida</quote>, o
       que permite a presença de um interceptor não-nulo nas relações
       cointegrantes assim como uma tendência nos níveis das variáveis
       endógenas.  Na literatura derivada do trabalho de Johansen (ver por
       exemplo o livro dele de 1995) isto é frequentemente referido como sendo
       o <quote>caso 3</quote>.  As primeiras quatro opções apresentadas
       acima, que são mutualmente exclusivas, produzem respectivamente os
       casos 1, 2, 4, e 5.  O significado destes casos e os critérios para
       seleccionar um caso estão explicados no 
       <guideref targ="chap:vecm"/>.
      </para>

      <para context="gui">
        A inclusão de termos determinísticos no modelo é controlada por
        intermédio dos botões das opções.  Por omissão, se não tiver sido
        indicada nenhuma opção, será incluída uma <quote>constante não
        restringida</quote>, o que permite a presença de um interceptor 
        não-nulo nas relações cointegrantes assim como uma tendência nos
        níveis das variáveis endógenas.  Na literatura derivada do trabalho
        de Johansen (ver por exemplo o livro dele de 1995) isto é
        frequentemente referido como sendo o <quote>caso 3</quote>.  Os 
        outros quatro botões de opções produzem respectivamente os casos 1,
        2, 4, e 5.  O significado destes casos e os critérios para 
        selecionar um caso estão explicados no
        <guideref targ="chap:vecm"/>.
      </para>

<para context="cli">
	As listas opcionais <repl>listaX</repl> e <repl>listaRx</repl>
	permite-lhe controlar as variáveis exógenas: estas entram no 
	sistema como não restringidas (<repl>listaX</repl>) ou
	restringidas ao espaço de cointegração (<repl>listaRx</repl>).
	 Estas listas são separadas da  <repl>listaY</repl> e entre 
	elas usando ponto-e-vírgulas.
      </para>

      <para context="gui">
	Você pode controlar as variáveis exógenas adicionando-as à 
	lista na caixa inferior.  Por omissão estas entram no modelo 
	na forma não restringida  (indicada por um <lit>U</lit> junto
	do nome da variável).  Se você pretender que uma certa variável
	seja restringida ao espaço de cointegração, clique com botão 
	direito sobre ela e selecione <quote>Restringida</quote> do 
	menu de contexto.  O símbolo junto à variável mudará para R.
      </para>

      <para context="cli">
	A opção <lit>--seasonals</lit>, que pode ser combinada com qualquer
        outra opção, especifica a inclusão de um conjunto de variáveis
        auxiliares sazonais.  Esta opção apenas está disponível para dados
        trimestrais ou mensais.
      </para>

      <para context="gui">
        Se os dados são trimestrais ou mensais, fica disponível a seleção
        de uma opção que lhe permite incluir um conjunto de variáveis
        auxiliares sazonais centradas.  Em todos os casos, a opção
        (<quote>Mostrar detalhes</quote>) permite apresentar as regressões
        auxiliares que formam o ponto de partida do procedimento da estimação
        de máxima verosimilhança de Johansen.
      </para>

      <para context="notex">
	A seguinte tabela serve como um guia à interpretação dos resultados
        apresentados pelo teste, num caso de 3-variáveis.  <lit>H0</lit>
        significa a hipótese nula, <lit>H1</lit> a hipótese alternativa, e
        <lit>c</lit> o número de relações cointegrantes.</para>
      <mono context="notex">
         Ordem    Teste Traço        Teste Lmax
                  H0     H1          H0     H1 
         ---------------------------------------
          0      c = 0  c = 3       c = 0  c = 1
          1      c = 1  c = 3       c = 1  c = 2
          2      c = 2  c = 3       c = 2  c = 3
         ---------------------------------------
      </mono>
      <para context="tex">
	A seguinte tabela serve como um guia
        à interpretação dos resultados
        apresentados pelo teste, num caso de
        3-variáveis.  $H_0$ significa a hipótese
        nula, $H_1$ hipótese alternativa, e $c$ o 
        número de relações cointegrantes	

	\begin{center}
	\begin{tabular}{cllll}
	&amp; \multicolumn{2}{c}{Teste Traço} &amp;
	   \multicolumn{2}{c}{Teste $\lambda$-max} \\
	Rank &amp;  \multicolumn{1}{c}{$H_0$} &amp; 
	       \multicolumn{1}{c}{$H_1$} &amp; 
	       \multicolumn{1}{c}{$H_0$} &amp; 
	       \multicolumn{1}{c}{$H_1$} \\ [4pt]
 	0 &amp; $c$ = 0 &amp; $c$ = 3 &amp; $c$ = 0 &amp; $c$ = 1 \\
	1 &amp; $c$ = 1 &amp; $c$ = 3 &amp; $c$ = 1 &amp; $c$ = 2 \\
	2 &amp; $c$ = 2 &amp; $c$ = 3 &amp; $c$ = 2 &amp; $c$ = 3 
	\end{tabular}
	\end{center}
      </para>

      <para>
	Ver também o comando <cmdref targ="vecm"/>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/Testes de Cointegração/Johansen</menu-path>
    </gui-access>

  </command>

  <command name="compact" section="Dataset" context="gui"
    label="Compactar dados">

    <description>

      <para>Quando você acrescenta a um conjunto de dados uma série temporal
        cuja frequência é superior, é necessário <quote>compactar</quote> a
        nova série.  Por exemplo, uma série mensal terá que ser compactada
        para caber num conjunto de dados trimestral.</para>  

      <para>Adicionalmente, por vezes você pode querer compactar um conjunto
        de dados para uma frequência mais baixa (eventualmente, antes de
        acrescentar uma variável de baixa-frequência ao conjunto de dados).</para>

      <para>Gretl oferece quatro opções de compactação:</para>

      <ilist>
	<li><para>Média: O valor escrito no conjunto de dados 
            será a média aritmética dos valores relevantes da série
            temporal.  Por exemplo, o valor escrito para o primeiro
            trimestre de 1990 será a média dos valores de janeiro, 
            fevereiro e março de 1990.</para>
	</li>

	<li><para>Soma: O valor escrito no conjunto de dados 
            será a soma dos valores de alta-frequência relevantes.
            Por exemplo, o valor escrito para o primeiro
            trimestre será a soma dos valores de janeiro, 
            fevereiro e março.</para>
	</li>

	<li><para>Valores fim-do-período: O valor escrito no conjunto
            de dados será o último valor relevante dos dados de 
            alta-frequência.  Por exemplo, o valor escrito para o
            primeiro trimestre de 1990 será o valor de março de 1990.</para>
	</li>

	<li><para>Valores início-do-período: O valor escrito no conjunto
            de dados será o primeiro valor relevante dos dados de 
            alta-frequência.  Por exemplo, o valor escrito para o
            primeiro trimestre de 1990 será o valor de janeiro de 1990.</para>
	</li>
      </ilist>

      <para>Caso esteja a compactar a totalidade de um conjunto de
        dados, a escolha que você fizer neste diálogo, define o método
        por omissão.  Mas se definiu um método de compactação 
        individualmente para uma variável (no menu, 
        <quote>Variável/Editar características</quote>), esse método
        será usado e não o por omissão.  Se o método de compactação
        já estiver definido para todas as variáveis, não será 
        apresentada a escolha do método.</para>

    </description>
   <gui-access>
      <menu-path>/Dados/Compactar Dados</menu-path>
    </gui-access>
  </command>

  <command name="controlled" section="Graphs" context="gui"
    label="Gráfico X-Y com controlo">

    <description>
      <para>
	Este comando requer a seleção de três variáveis, uma para o eixo 
        dos X, uma para o eixo dos Y, e outra para a que você deseja controlar
        (chamemos-lhe Z). O gráfico apresenta Y ajustado contra X ajustado, 
        onde a versão ajustada da variável são os resíduos de uma regressão
        por Mínimos Quadrados (OLS) sobre Z.  
      </para>
      <para>
	Exemplo: Você tem uma amostra com dados de salários, experiência e 
	nível de educação de um conjunto de pessoas.  Você pretende ver 
        um gráfico dos salários contra o nível de educação controlando pela
        experiência.  Nesse caso você seleciona salários para o eixo dos Y,
        nível de educação para o eixo dos X, e experiência como controlo.
	O gráfico apresenta os salários contra a educação, com ambas as 
        variáveis <quote>purgadas</quote> do efeito experiência.
      </para>
    </description>

  </command>

  <command name="corr" section="Statistics" 
    label="Coeficientes de correlação" context="cli">

    <usage>
      <arguments>
        <argument optional="true">lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--uniform</flag>
	  <effect>garante amostra uniforme</effect>
	</option>
	<option>
	  <flag>--spearman</flag>
	  <effect>Ró de Spearman</effect>
	</option>
	<option>
	  <flag>--kendall</flag>
	  <effect>Tau de Kendall</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostra classificações ('rankings')</effect>
	</option>
      </options>
      <examples>
        <example>corr y x1 x2 x3</example>
	<example>corr ylist --uniform</example>
	<example>corr x y --spearman</example>
	<example>corr --matrix=X --plot=display</example>
      </examples>
    </usage>

    <description context="gui">
      <para>
	Apresenta os coeficientes de correlação emparelhados 
        (correlação momento-produto de Pearson) para as variáveis indicadas.
        O comportamento normal é usar todas as observações para calcular 
        cada coeficiente emparelhado, mas se a caixa de seleção estiver
        selecionada a amostra será limitada (caso seja necessário) de modo
        que o mesmo conjunto de observações seja usado em todos os 
        coeficientes.  Esta opção apenas tem efeito se existirem valores
        omissos em quantidades diferentes para as variáveis usadas.
      </para>
    </description>

    <description context="cli">
      <para>
	Por omissão, apresenta os coeficientes de correlação emparelhados 
        (correlação momento-produto de Pearson) para as variáveis 
        <repl>lista-de-variáveis</repl>, ou para todas as variáveis no 
        conjunto de dados se não tiver sido indicada <repl>lista-de-variáveis</repl>.
          O comportamento normal é o de usar todas as observações disponíveis para calcular 
        cada coeficiente emparelhado, mas se tiver sido dada a opção <flag>--uniform</flag> 
        a amostra será limitada (caso seja necessário) de modo
        que o mesmo conjunto de observações seja usado em todos os 
        coeficientes.  Esta opção apenas tem efeito se existirem valores
        omissos em quantidades diferentes para as variáveis usadas.
      </para>
      <para>
	As opções (mutualmente exclusivas) <opt>--spearman</opt> e 
	<opt>--kendall</opt> produzem, respetivamente, o Ró de correlação
        de Spearman e o Tau de correlação de ordem de Kendall em vez do 
        usual coeficiente de Pearson.  Quando uma destas opções é dada,
        a <repl>lista-de-variáveis</repl> deve apenas conter duas variáveis.
      </para>
      <para>
	Ao calcular uma correlação de ordem, pode ser dada a opção 
        <opt>verbose</opt> para mostrar os dados originais e ordenados
        (caso contrário esta opção será ignorada).
      </para>
    </description>

    <gui-access>
      <menu-path>/Ver/Matriz de correlação</menu-path>
      <other-access>Menu de contexto da janela principal (selecção múltipla)</other-access>
    </gui-access>

  </command>

  <command name="corrgm" section="Statistics" label="Correlograma">

    <usage>
      <arguments>
        <argument>série</argument>
        <argument optional="true">ordem</argument>
      </arguments>
      <options>
	<option>
	  <flag>--bartlett</flag>
	  <effect>use Bartlett standard errors</effect>
        </option>	
	<option>
	  <flag>--plot</flag>
	  <optparm>modo ou nome-de-ficheiro</optparm>
	  <effect>ver abaixo</effect>
        </option>
      </options>
      <examples>
        <example>corrgm x 12</example>
      </examples>
    </usage>

    <description>
      <para context="notex">
	Apresenta os valores da função de autocorrelação para a
	<repl>série</repl>, que pode ser especificada por nome ou por
        número.
	Os valores são definidos como &rgr;(<math>u</math><sub>t</sub>,
	<math>u</math><sub>t-s</sub>) onde <math>u</math><sub>t</sub>
	é a <math>t</math>&ndash;ésima observação da variável <math>u</math> e <math>s</math>
        é o número de desfasamentos.
      </para>
      <para context="tex">
	Apresenta os valores da função de autocorrelação para a
	<repl>série</repl>, que pode ser especificada por nome ou por
        número.  Os valores são definidos como $\hat{\rho}(u_t, u_{t-s})$, onde
	$u_t$ é a $t$&ndash;ésima observação da variável $u$ e
	$s$ é o número de desfasamentos.
      </para>
      <para>
	Também são apresentadas as autocorrelações parciais (obtidas segundo
        o algoritmo de Durbin&ndash;Levinson): estas constituem a rede dos
        efeitos dos desfasamentos intervenientes.  Adicionalmente, é 
        apresentada a estatística de teste <math>Q</math> de 
        Ljung&ndash;Box. Esta pode ser usada para testar a hipótese nula de que
        a série é <quote>ruído branco</quote>: terá uma distribuição
        qui-quadrado assimptótico com os graus de liberdade iguais ao número de
        desfasamentos usados.
      </para>
      <para>
	Se o valor <repl>ordem</repl> for especificado o comprimento do
        correlograma fica limitado a esse máximo número de desfasamentos, senão o 
        comprimento é determinado automaticamente, como uma função da
        frequência dos dados e do número de observações.
      </para>
      <para>
	Por omissão é apresentado um gráfico do correlograma:
        um gráfico gnuplot em modo interativo ou um gráfico ASCII em
        modo de lote de comandos. Isto pode ser ajustado por via da opção
        <opt>plot</opt>. Os parâmetros válidos para esta opção são <lit>none</lit>
	(para suprimir o gráfico); <lit>ascii</lit> (para produzir um gráfico
        de texto mesmo em modo interativo); <lit>display</lit> (para produzir
	um gráfico gnuplot mesmo em modo de lote de comandos); ou o nome de um
        ficheiro. O efeito de se fornecer um nome de ficheiro é como descrito para
	a opção <opt>output</opt> do comando <cmdref targ="gnuplot"/>.
      </para>
      <para>
	Depois de completar com sucesso, os acessores <lit>$test</lit> e
	<lit>$pvalue</lit> contêm os respectivos valores do teste de
	Ljung&ndash;Box para a maior ordem apresentada. Note que se você
	apenas quiser determinar a estatística <math>Q</math>, provavelmente
	você quererá usar a função <fncref targ="ljungbox"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Correlograma</menu-path>
      <other-access>Menu de contexto da janela principal (selecção singular)</other-access>
    </gui-access>

  </command>

  <command name="count-model" section="Estimation" context="gui"
    label="Modelos para a contagem de dados">

    <description>
      <para>
	A variável dependente é tida como representando a contagem de 
	ocorrência de eventos de alguma natureza, e tem que ter apenas
	valores inteiros não-negativos. Por omissão, usa-se a 
	distribuição de Poisson, mas o seletor de menu permite escolher
	a distribuição Binomial Negativa. (A variante 'NegBin 2' é 
	normalmente usada em econometria, mas também está disponível a 
	menos usada 'NegBin 1').
      </para>
      <para>
	Opcionalmente, você pode acrescentar na especificação uma variável
	<quote>offset</quote>.  Esta é uma variável de escala, cujo logarítmo
	é acrescentado à função de regressão linear (implicitamente com um 
	coeficiente de 1.0).  Isto faz sentido quando você espera que o número
	de ocorrências do evento em questão seja proporcional a algum factor 
	conhecido (TODO: other things equal).  Por exemplo, o número de acidentes
	de trânsito pode ser assumido como proporcional ao volume de trânsito
	(TODO: other things equal), e nesse caso o volume de trânsito pode ser
	especificado como um <quote>offset</quote> num modelo de taxa de acidentes.
	A variável 'offset' tem que ser estritamente positiva.
      </para>
      <para>
	Por omissão os erros padrão são calculados usando uma aproximação numérica
	da Hessiana por convergência.  Mas se estiver seleccionada a caixa dos
	<quote>Erros padrão robustos</quote> então serão calculados os erros padrão
	QML, usando uma <quote>sandwich</quote> da inversa da Hessiana e do produto
	externo do gradiente.
      </para>
    </description>
  </command>

  <command name="curve" section="Graphs" label="Desenha uma curva"
    context="gui">

    <description>
      <para>
	Esta caixa de diálogo permite-lhe criar um gráfico gnuplot
        a partir de uma expressão. Esta tem que ser uma expressão 
        válida para o gnuplot.  Use <lit>x</lit> como o identificador
	dos valores da variável no eixo dos X. Tenha em atenção que o
	gnuplot usa <lit>**</lit> para potenciação, e que o caratere
	dos decimais tem que ser o <quote>.</quote>.  Exemplos:
      </para>
      <code>
	10+0.35*x
	100+5.3*x-0.12*x**2
	sin(x)
	exp(sqrt(pi*x))
      </code>
      <para>
	Para acrescentar uma linha adicional a um gráfico criado por 
        este processo, clicar no gráfico e selecionar <quote>Editar</quote>, 
        selecionar o separador <quote>Linhas</quote> na janela de edição
        do gráfico, e use o botão <quote>Acrescentar linha...</quote>.
      </para>
    </description>
  </command>

  <command name="cusum" section="Tests" label="Teste CUSUM">

    <usage>
      <options>
	<option>
	  <flag>--squares</flag>
	  <effect>executa o teste CUSUMSQ</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>apenas mostra o teste Harvey&ndash;Collier</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Tem que se seguir à estimação de um modelo por via de OLS.  Executa 
        o teste CUSUM &mdash;ou se for dada a opção <lit>--squares</lit>,
        o teste CUSUMSQ &mdash;para a estabilidade dos parâmetros.  É obtida
        uma série temporal de erros de predição um passo-à-frente, pela 
        execução de séries de regressões: a primeira regressão usa as 
        primeiras <math>k</math> observações e é usada para gerar a
        predição da variável dependente na observação
        <math>k</math> + 1; a segunda usa a primeira predição para a
        observação <math>k</math> + 2, e por aí a diante 
	(onde <math>k</math> é o número de parâmetros no modelo original).
      </para>
      <para>
	A soma acumulada dos erros de predição escalados, ou os quadrados desses
        erros, é mostrada e apresentada em gráfico.  A hipótese nula para a
        estabilidade dos parâmetros é rejeitada ao nível de cinco porcento, 
        se a soma acumulada se desviar do intervalo de confiança de 95 porcento.
      </para>
      <para>
	No caso do teste  CUSUM, é também apresentada a estatística de teste
        <math>t</math> de Harvey&ndash;Collier, para a hipótese nula
        da estabilidade dos parâmetros.  Ver o livro 
        <book>Econometric Analysis</book> de Greene para mais detalhes.  Para
        o teste CUSUMSQ, o intervalo de confiança a 95 porcento é calculado de
        acordo com o algoritmo apresentado por 
	<cite key="edgerton94">Edgerton e Wells (1994)</cite>.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Teste CUSUM(SQ)</menu-path>
    </gui-access>

  </command>

  <command name="daily-purge" section="Dataset" 
	   label="Purge daily data" context="gui">
    <description>
      <para>
	If a daily dataset is nominally on a 7-day calendar but
	in fact only includes business-day data, it is recommended
	that you delete the blank weekend rows, thereby switching
	to a 5-day calendar.
      </para>
      <para>
	If a business-daily dataset contains a relatively small number
	of rows with no data entries (presumably due to trading
	holidays) you may wish to delete these rows. In effect, this
	means treating the missing values for holidays as non-existent
	rather than truly <quote>missing</quote>, and treating the
	trading days as forming a continuous time-series.
      </para>
      <para>
	Note that if you take either of these options gretl will
	nonetheless preserve the date information, and it will be
	possible to reconstruct the full calendar dataset later
	if that's required.
      </para>
    </description>
  </command>

  <command name="data" section="Dataset" 
    label="Importar de uma base de dados" context="cli">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--compact</flag>
	  <optparm>method</optparm>
	  <effect>specify compaction method</effect>
	</option>
	<option>
	  <flag>--interpolate</flag>
	  <effect>do interpolation for low-frequency data</effect>
	</option>	
	<option>
	  <flag>--quiet</flag>
	  <effect>não reportar resultados exceto quando hajam erros</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Lê as variáveis indicadas na <repl>lista-de-variáveis</repl> a
	partir de uma base de dados (gretl, RATS 4.0 ou PcGive), que
	deve ter sido préviamente aberta usando o comando 
	<cmdref targ="open"/>.  A frequência dos dados e o intervalo da
	amostra podem ser definidos pelos comandos <cmdref targ="setobs"/> 
	e <cmdref targ="smpl"/> antes de usar este comando. 
	Apresenta-se um exemplo completo:</para>
      <code>
	open macrodat.rat
	setobs 4 1959:1
	smpl ; 1999:4
	data GDP_JP GDP_UK
      </code>
      <para>
	Os comandos acima abrem a base de dados com o nome 
	<filename>macrodat.rat</filename>, definem um conjunto de dados
	trimestral iniciando no primeiro trimestre de 1959 e terminando
	no quarto trimestre de 1999, e depois importam as séries temporais
	com os nomes <lit>GDP_JP</lit> e <lit>GDP_UK</lit>.</para>
      <para>
	Se os comandos <lit>setobs</lit> e <lit>smpl</lit> não tiverem sido
	especificados deste modo, a frequência dos dados e o intervalo da 
	amostra serão definidos usando a primeira variável lida da base de
	dados.
      </para>
      <para>
	Se as séries temporais a serem lidas forem de frequência 
	superior à do conjunto de dados em uso, você pode especificar
	um método de compactação tal como abaixo:</para>
      <code>
	data (compact=average) LHUR PUNEW
      </code> 
      <para>
	Os quatro métodos de compactação disponíveis são: 
	Média; <quote>average</quote> (usa a média das observações
	de maior frequência),
	Último; <quote>last</quote> (usa a última observação),
	Primeiro; <quote>first</quote> e 
	Soma; <quote>sum</quote>.  Se não tiver sido indicado
	nenhum métod, será usado a Média.
      </para>
      <para>
	If the series to be read are of lower frequency than the
	working dataset, the default is to repeat the values of the
	added data as required, but the <opt>interpolate</opt> option
	can be used to request interpolation using the method of <cite
	key="chowlin71">Chow and Lin (1971)</cite>: the regressors are
	a constant and quadratic trend and an AR(1) disturbance
	process is assumed. Note, however, that this option is
	available only for conversion from quarterly data to monthly
	or annual data to quarterly.
      </para>
      <para>
	In the case of native gretl databases (only), the
	<quote>glob</quote> characters <lit>*</lit> and <lit>?</lit>
	can be used in <repl>varlist</repl> to import series that
	match the given pattern. For example, the following will
	import all series in the database whose names begin with
	<lit>cpi</lit>:
      </para>
      <code>
	data cpi*
      </code>

    </description>

    <gui-access>
      <menu-path>/Ficheiro/Bases de Dados</menu-path>
    </gui-access>

  </command>

  <command name="data-files" section="Programming" 
	   label="Data files" context="gui">
    <description>
      <para>
	This dialog enables you to specify additional files to be
	included with a function package. Including such material
	implies that the package takes the form of a zip file.  If
	gretl is to build the zip file for you, all files referenced
	here must be present in the same directory as the gfn file.
	Sub-directories can be listed as well as regular files; in
	that case it is implied that all of their contents should be
	included in the zip package.
      </para>
      <para>
	There are two main intended uses for this facility. First, you
	can include a data file for use with the package's sample
	script, if none of the data files supplied with the gretl
	distribution are suitable. In this case the data should be in
	gretl native format (<lit>gdt</lit> or binary
	<lit>gdtb</lit>). Second, if your package requires a big
	matrix (for example, holding critical values for a specialized
	test statistic) it may be more convenient to include this as a
	gretl matrix file (<lit>mat</lit>) than to assemble the matrix
	via multiple hansl statements.
      </para>
      <para>
	To access a packaged <lit>gdt</lit> or <lit>gdtb</lit> file
	from a sample script, use the <opt>frompkg</opt> option with
	the <lit>open</lit> command, supplying the name of the
	package as a parameter, as in
      </para>
      <code>
	open almon.gdt --frompkg=almonreg
      </code>
      <para>
	To read a packaged matrix file from within your package code,
	use the built-in string variable <lit>$pkgdir</lit>, as in
      </para>
      <code>
	string mname = sprintf("%s/A.mat", $pkgdir)
	matrix A = mread(mname)
      </code>
      <para>
	(Note that <quote><lit>/</lit></quote> will work OK as
	path separator on MS Windows.)
      </para>
    </description>
  </command>  

  <command name="dataset" section="Dataset" 
    label="Manipular o conjunto de dados" context="cli">

    <usage>
      <arguments>
        <argument>palavra-chave</argument>
	<argument>parâmetros</argument>
      </arguments>
      <examples>
        <example>dataset addobs 24</example>
	<example>dataset insobs 10</example>
        <example>dataset compact 1</example>
        <example>dataset compact 4 last</example>
        <example>dataset expand interp</example>
        <example>dataset transpose</example>
	<example>dataset sortby x1</example>
	<example>dataset resample 500</example>
	<example>dataset renumber x 4</example>
	<example>dataset pad-daily 7</example>
	<example>dataset clear</example>
      </examples>
    </usage>

    <description>
      <para>
	Efectua diversas operações sobre o conjunto de dados como
	um todo, dependendo da <repl>palavra-chave</repl>, que tem 
	que ser <lit>addobs</lit>, <lit>insobs</lit>, <lit>clear</lit>,
	<lit>compact</lit>, <lit>expand</lit>, <lit>transpose</lit>,
	<lit>sortby</lit>, <lit>dsortby</lit>, <lit>resample</lit> ou
	<lit>renumber</lit>. Nota: exceptuando <lit>clear</lit>, estas
	ações não estão disponíveis enquanto o conjunto de dados estiver
	subamostrado por seleção de casos com algum critério Booleano.
      </para>
      <para>
	<lit>addobs</lit>: Tem que ser seguida por um inteiro positivo.
	Acrescenta o número indicado de observações adicionais no
	final do conjunto de dados em uso.  Essencialmente, isto é 
	pretendido para efeitos de predição.  Os valores na maior parte
	das variáveis no intervalo acrescentado, serão marcados como 
	omissos, mas certas variáveis determinísticas são reconhecidas
	e extendidas, nomeadamente, uma tendência linear simples e 
	variáveis periódicas auxiliares.
      </para>
      <para>
	<lit>insobs</lit>: Tem que ser seguida por um inteiro positivo
	que não seja maior que o número de observações atual. Inserte
	uma única observação na posição indicada. Todos os dados
	subsequentes são deslocados uma posição e o conjunto de dados
	fica extendido com mais uma observação. Todas as variáveis exceto
	a constante recebem um valor omisso na nova observação. Esta ação
	não está disponível para conjuntos de dados de painel.
      </para>
      <para>
	<lit>clear</lit>: Não necessita parâmetros.  Limpa o conjunto
	de dados corrente, ficando gretl no seu estado 
	<quote>vazio</quote> inicial.
      </para>      
      <para>
	<lit>compact</lit>: Tem que ser seguida por um inteiro positivo
	representando uma nova frequência, que deve ser inferior à
	frequência atual (por exemplo, um valor 4 quando a frequência
	corrente é 12, indica a compactação de mensal para trimestral).
	Este comando apenas está disponível para séries temporais; ele
	compacta todas as séries temporais do conjunto de dados para a
	nova frequência.  Pode ser dado um segundo parâmetro, nomeadamente
	um de <lit>sum</lit>, <lit>first</lit> ou <lit>last</lit>, para
	especificar, respectivamente, compactação usando a soma dos valores
	de maior frequência, valores de ínicio e de fim de período. Por 
	omissão é feita compactação por média.
      </para>
      <para>
	<lit>expand</lit>: Este comando está apenas disponível
	para séries temporais anuais ou trimestrais: dados anuais
	podem ser expandidos para trimestrais, e dados trimestrais
	para frequência mensal.  Por omissão todas as séries temporais
	no conjunto de dados são preenchidas com repetição de valores
	existentes até atingirem a nova frequência, mas se tiver sido 
	acrescentado o modificador <lit>interp</lit> então as séries
	temporais serão expandidas usando a interpolação de Chow-Lin: 
	os regressores são a constante e a tendência quadrada e é
	assumido um processo de perturbação AR(1).
      </para>
      <para>
	<lit>transpose</lit>: Não necessita parâmetros.
	Transpõe o conjunto de dados actual.  Isto é, cada observação
	(linha) será tratada como uma variável (coluna), e cada variável
	como uma observação.  Este comando pode ser útil se quando os dados
	tenham sido lidos a partir de uma origem externa em que as linhas da
	tabela de dados representam variáveis.
      </para>
      <para>
	<lit>sortby</lit>: É necessário o nome de uma lista ou de uma única 
	série de dados. Se tiver sido dada uma série de dados, as observações
	em todas as variáveis do conjunto de dados são re-ordenadas por ordem
	crescente da série especificada.  Se tiver sido dada uma lista, a 
	ordenação é hierárquica: se as observações estiverem empatadas no que 
	diz respeito à primeira variável chave então é usada a segunda chave 
	para desempatar, e assim sucessivamente até que não haja empates ou 
	se tenham esgotado as chaves. Note que este comando apenas está
	disponível para dados sem data.
      </para>
      <para>
	<lit>dsortby</lit>: Funciona como <lit>sortby</lit> exceto que
	a re-ordenação é por ordem decrescente das séries chave.
      </para>
      <para>
	<lit>resample</lit>: Constrói um novo conjunto de dados
	por amostragem aleatória, com substituição das linhas
	do conjunto de dados corrente. Requer um argumento, 
	designadamente o número de linhas a incluir. Este pode ser
	menor, igual ou maior que o número de observações nos dados
	originais.  O conjunto de dados original pode ser obtido 
	usando o comando <lit>smpl full</lit>.  
      </para>
      <para>
	<lit>renumber</lit>: Requer o nome de uma de uma série seguido
	por um inteiro entre 1 e o número de séries no conjunto de dados
	menos 1. Move a série especificada para a posição indicada dentro
	do conjunto de dados, renumerando adequadamente as restantes séries.
	(A posição 0 está ocupada pela constante, que não pode ser movida.)
      </para>
    </description>

    <gui-access>
      <menu-path>/Dados</menu-path>
    </gui-access>

  </command>

  <command name="datasort" section="Dataset" context="gui"
    label="Ordenar dados">

    <description>
      <para>
	A variável selecionada é usada como a chave de ordenação
	para todo o conjunto de dados.  As observações em todas
	as variáveis são re-ordenadas por ordem crescente dos
	valores da variável chave, ou por valores decrescentes
	caso tenha selecionado a opção <quote>Descrescente</quote>.
      </para>
    </description>
  </command>

  <!--
  <command name="debug" section="Programming" context="cli"
    label="Despiste de erros de programa">

    <usage>
      <arguments>
        <argument>função</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Comando experimental para despiste de erros de programa ('debugger')
	em funções definidas pelo utilizador, disponível em programas de 
	linha-de-comandos, gretlcli, e na consola do ambiente gráfico (GUI).
	O comando <lit>debug</lit> deve ser utilizado antes da chamada da 
	função, mas depois da definição desta. O efeito é de que a execução
	é suspensa quando a função é chamada e é activada uma linha de 
	interação especial.
      </para>
      <para>
	Na linha de interação, você introduz <lit>next</lit> para executar
	o comando seguinte dentro da função, ou <lit>continue</lit> para 
	permitir continuar a execução da função sem impedimentos. Estes
	comandos podem ser abreviados com <lit>n</lit> e <lit>c</lit> 
	respetivamente.  Você também pode interpôr uma instrução nesta
	linha de interação, por exemplo um comando <lit>print</lit> para
	mostrar o valor atual de alguma variável de interesse.
      </para>
    </description>
  </command>  
  -->

  <command name="delete" section="Dataset" 
    label="Apagar variáveis" context="cli">

    <usage>
      <altforms>
        <altform><lit>delete</lit> <repl>lista-de-variáveis</repl></altform>
        <altform><lit>delete</lit> <repl>nome-de-variável</repl></altform>
	<altform><lit>delete --type=</lit><repl>nome-de-tipo</repl></altform>
      </altforms>
      <options>
	<option>
	  <flag>--db</flag>
	  <effect>apaga séries em base de dados</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Este comando é um destrutor geral para variáveis com nome
	(quer sejam séries, escalares, matrizes, texto, ou 'bundles').
	Tem que ser usada com cuidado; não será peguntada nenhuma
	confirmação.
      </para>
      <para>
	No caso de uma série <repl>nome-de-variável</repl> pode tomar a 
	forma de uma lista com nome, o que faz com que todas as séries
	nessa lista sejam apagadas, ou pode tomar a forma de uma lista
	explícita de séries por nome ou número ID.  Note que quando
	você apaga séries, quaisquer séries com números ID superiores
	àquelas que estão na lista a apagar serão renumeradas.
      </para>
      <para>
	Se for dada a opção <opt>--db</opt>, o comando apaga as séries
	listadas não do conjunto de dados atual mas de uma base de dados
	gretl, assumindo que a base de dados está aberta e que o utilizador
	tem permissão de escrita no ficheiro em questão.  Ver também o 
	comando	<cmdref targ="open"/>.
      </para>
      <para>
	Se tiver sido dada a opção <opt>--type</opt> ela terá de ser
        acompanhada por um dos seguintes nomes-de-tipo:
	<lit>matrix</lit> (matriz), <lit>bundle</lit> ('bundle'), <lit>string</lit> (texto),
	<lit>list</lit> (lista), ou <lit>scalar</lit> (escalar). O efeito é o de apagar todas
	as variáveis do tipo dado. Neste caso (somente neste), não deve ser indicado o argumento
	<repl>nome-de-variável</repl>.
      </para>
    </description>

    <gui-access>
      <menu-path>Menu de contexto da janela principal (selecção singular)</menu-path>
    </gui-access>

  </command>

  <command name="density" section="Statistics" context="gui"
    label="Estimação de densidade de núcleo">

    <description>

      <para>O procedimento de estimação de densidade de núcleo 
	cria um conjunto de pontos de referência igualmente
	espaçados, ao longo de um intervalo adequado em relação
	ao intervalo dos dados, e atribui uma densidade a cada
	ponto de referência baseado nas observações reais na
	vizinhança.
      </para>
      <para>A função utilizada para determinar a densidade estimada
	em cada ponto de referência, <math>x</math>, é
      <equation status="display"
	tex="\[f(x)=(1/nh) \sum_{t-1}^{n} k\left((x-x_t)/h\right)\]"
	ascii="f(x) = (1/nh) sum(t=1 to n) k((x - x(t)) / h)"
	graphic="kernel1"/>
	onde <math>n</math> é o número de pontos de dados, 
	<math>h</math> é o parâmetro de <quote>largura de banda</quote>,
	e <math>k</math>() é a função núcleo.
	Quanto maior for o valor da largura de banda, mais alisada será
	a densidade estimada.
      </para>
      <para>
	Você pode escolher o núcleo Gaussiano (a densidade normal padrão)
	ou núcleo de Epanechnikov.  Por omissão, a largura de banda é
	a sugerida pela regra de ouro de 
	<cite key="silverman96">Silverman (1986)</cite>, nomeadamente
	<equation status="display"
	  tex="\[h=0.9 {\rm min}(s, {\rm IQR}/1.349) n^{-1/5}\]"
	  ascii="h = 0.9 min(s, IQR/1.349) n^{-1/5}"
	  graphic="kernel2"/>
	onde <math>s</math> designa o desvio padrão dos dados
	e IQR é a amplitude inter-quartil.  Você pode alargar
	ou encolher a largura de banda por meio do 
	<quote>fator de ajustamento de largura de banda</quote>: 
	a largura de banda final é a obtida pela multiplicação do
	valor de Silverman pelo fator de ajustamento.
      </para>
      <para>
	Para uma boa introdução sobre a estimação de densidade de
	núcleo ver o Capítulo 15 do livro de Davidson e MacKinnon,
	<book>Econometric Theory and Methods</book>.
      </para>

    </description>

  </command>  

  <command name="dfgls" section="Tests" context="gui"
    label="O teste ADF-GLS">

    <description>
      <para>
	O teste ADF-GLS é uma variante do teste de Dickey&ndash;Fuller
	para uma raiz unitária, para o caso onde se assume que a 
	variável a ser testada tem uma média não-nula ou manifesta
	uma tendência linear.  A diferença está no processo de 
	anulação da média ou da têndencia da variável ser feito com
	o procedimento GLS sugerido por Elliott, Rothenberg
	e Stock (1996).  Isto resulta num teste mais potente do que
	o da abordagem padrão de Dickey&ndash;Fuller.
      </para>
      <para>
	Ver também o comando <cmdref targ="adf"/> e a opção <opt>--gls</opt>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Testes de raiz unitária/Teste ADF-GLS</menu-path>
    </gui-access>

  </command>

  <command name="dialog" section="Estimation" context="gui"
    label="Janela de diálogo do modelo">

    <description>
      <para>Para selecionar a variável dependente, escolher uma 
	variável na lista à esquerda e clicar no botão <quote>Escolher</quote>
	que aponta para o espaço da variável dependente.  Se você activar
	a caixa de seleção <quote>Definir por omissão</quote>, a variável
	selecionada ficará pré-selecionada nas próximas vezes que abrir uma
	janela de diálogo do modelo.  Atalho: Fazer clique-duplo numa variável
	à esquerda para a selecionar como variável dependente e também 
	defini-la como por omissão.
      </para>
      <para>Para selecionar as variáveis independentes, escolhê-las na 
	lista à esquerda e clicar no botão <quote>Acrescentar</quote>
	(ou clicar o botão direito do rato).  Você pode escolher um grupo
	não-contínuo de variáveis clicando nelas mantendo a tecla 
	<lit>Ctrl</lit> pressionada.
      </para>
    </description>

  </command>

  <command name="diff" section="Transformations" 
    label="Primeiras diferenças" context="cli">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
    </usage>

    <description>
      <para>
	É obtida a primeira diferença de cada variável na
	<repl>lista-de-variáveis</repl> e o resultado é guardado
	numa nova variável com o prefixo <lit>d_</lit>.  Portanto
	<cmd>diff x y</cmd> cria as duas novas variáveis
      </para>
      <mono>
	d_x = x(t) - x(t-1)
	d_y = y(t) - y(t-1)
      </mono>
    </description>

    <gui-access>
      <menu-path>/Acrescentar/Primeiras diferenças das variáveis selecionadas</menu-path>
    </gui-access>

  </command>

  <command name="difftest" section="Tests" 
    label="Testes não paramétricos para as diferenças" context="cli">

    <usage>
      <arguments>
        <argument>variável1</argument>
	<argument>variável2</argument>
      </arguments>
      <options>
	<option>
	  <flag>--sign</flag>
	  <effect>Teste dos Sinais, por omissão</effect>
	</option>
	<option>
	  <flag>--rank-sum</flag>
	  <effect>Teste ordinal da soma de Wilcoxon</effect>
	</option>
	<option>
	  <flag>--signed-rank</flag>
	  <effect>Teste ordinal dos sinais de Wilcoxon</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Realiza um teste não-paramétrico para a diferença entre duas 
	populações ou grupos, o teste específico depende da opção 
	selecionada.
      </para>
      <para>
	Com a opção <opt>--sign</opt>, é executado o teste dos Sinais.
	Este teste baseia-se no facto de duas amostras <math>x</math> 
	e <math>y</math>, terem sido extraídas aleatóriamente de uma
	mesma distribuição, a probabilidade
	de
	<math>x</math><sub>i</sub> &gt;
	<math>y</math><sub>i</sub>, para cada observação
	<math>i</math>, deve ser igual a 0,5.  A estatística de teste é
	<math>w</math>, o número de observações para as quais 
	<math>x</math><sub>i</sub> &gt;
	<math>y</math><sub>i</sub>. Sob a hipótese nula de que segue uma
	distribuição Binomial com os parâmetros
	(<math>n</math>, 0,5), onde <math>n</math> é o número de observações.
      </para>
      <para>
	Com a opção <opt>--rank-sum</opt>, é executado o teste ordinal da soma
	de Wilcoxon.  Este teste consiste em ordenar as observações de ambas as
	amostras conjuntamente, da menor para a maior, e depois determinar a
	soma das ordens de uma das amostras.  As duas amostras não necessitam ser
	do mesmo tamanho, e se isso acontece então usa-se a de menor dimensão
	no cálculo da soma das ordens.  Sob a hipótese nula de que as amostras
	terem sido extraídas de populações com a mesma mediana, a distribuição de
	probabilidade da soma das ordens pode ser determinada para quaisquer
	tamanhos das amostras; e para amostras consideravelmente grandes existe
	uma forte aproximação a uma distribuição Normal.
      </para>
      <para>
	Com a opção <opt>--signed-rank</opt>, é executado o teste ordinal
	dos sinais de Wilcoxon.  Este destina-se para pares de dados
	associados assim como, por exemplo, os valores das variáveis de 
	uma amostra de indivíduos antes e depois de algum tratamento.
	O teste começa por encontrar as diferenças entre as observações
	emparelhadas,
	<math>x</math><sub>i</sub> &minus;
	<math>y</math><sub>i</sub>, ordenando estas diferenças por valor
	absoluto, e então atribuindo a cada par um posto com sinal, 
	coincidindo o sinal com o sinal da diferença.  De seguida é
	calculado o <math>W</math><sub>+</sub>, que é a soma dos postos
	com sinal positivo.  Tal como o teste ordinal da soma, esta
	estatística tem uma distribuição bem definida, sob a hipótese
	nula de que a diferença mediana é zero, que converge para a 
	distribuição Normal em amostras de tamanho razoável.
      </para>
      <para>
	Para os testes de Wilcoxon, se a opção <opt>--verbose</opt> tiver sido
	dada então será mostrado as ordens.  (Esta opção não tem efeito se
	tiver sido selecionado o teste dos Sinais.)
      </para>
    </description>

  </command>

  <command name="discrete" section="Transformations" 
    label="Marca variáveis como sendo discretas" context="cli">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--reverse</flag>
	  <effect>marca variáveis como sendo contínuas</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Marca cada variável em <repl>lista-de-variáveis</repl> como sendo discreta. Por
	omissão todas as variáveis são tratadas como sendo contínuas; ao
	marcar uma variável como sendo discreta afeta o modo como a variável
	é usada em diagramas de frequência, e também permite-lhe selecionar
	a variável para o comando <cmdref targ="dummify"/>.
      </para>
      <para>
	Se a opção <opt>--reverse</opt> tiver sido dada, é feito o contrário;
	ou seja, as variáveis em <repl>lista-de-variáveis</repl> são marcadas
	como sendo contínuas.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Editar caraterísticas</menu-path>
    </gui-access>

  </command>

  <command name="dpanel" section="Estimation" label="Modelos de painel dinâmico">

    <usage>
      <arguments>
	<argument>p</argument> 
	<argblock separated="true">
	  <argument>variável-dependente</argument>
	  <argument>variáveis-independentes</argument>
	</argblock>
	<argblock optional="true" separated="true">
	  <argument>instrumentos</argument>
	</argblock>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar o modelo estimado</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
	</option>
        <option>
	  <flag>--two-step</flag>
	  <effect>efetuar estimação GMM de dois passos</effect>
        </option>
        <option>
	  <flag>--system</flag>
	  <effect>acrescentar equações por níveis</effect>
        </option>
        <option>
	  <flag>--time-dummies</flag>
	  <effect>acrescentar variáveis auxiliares tempo</effect>
        </option>
        <option>
	  <flag>--dpdstyle</flag>
	  <effect>comportamento semelhante ao do pacote DPD do Ox</effect>
        </option>
        <option>
	  <flag>--asymptotic</flag>
	  <effect>erros padrão assimptóticos não corrigidos</effect>
        </option>
      </options>
      <examples>
        <example>dpanel 2 ; y x1 x2</example>
	<example>dpanel 2 ; y x1 x2 --system</example>
        <example>dpanel {2 3} ; y x1 x2 ; x1</example>
	<example>dpanel 1 ; y x1 x2 ; x1 GMM(x2,2,3)</example>
	<demos>
	  <demo>bbond98.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Efectua a estimação de modelos de dados de painel dinâmico
	(ou seja, modelos de painel que incluem um ou mais desfasamentos
	da variável dependente) usando os métodos GMM-DIF ou GMM-SYS.
      </para>
      <para context="cli">
	O parâmetro <repl>p</repl> representa a ordem de autoregressão
	para a variável dependente.  Na forma mais simples esta é um
	valor escalar, mas este argumento pode ser uma matriz pré-definida,
	para especificar um conjunto (eventualmente descontínuo) de 
	desfasamentos a serem usados.
      </para>
      <para>
	A variável dependente e os regressores devem ser dados na 
	forma de níveis; eles serão automaticamente diferenciados
	(pois este estimador usa diferenciação para anular os efeitos
	individuais).
      </para>
      <para context="cli">
	O último campo (opcional) do comando é para especificar 
	instrumentos.  Se não tiver sido dado instrumentos, é assumido 
	que todas as variáveis independentes são estritamente exógenas.
	Caso você especifique alguns intrumentos, deverá incluir na
	lista variáveis independentes estritamente exógenas.  Para 
	regressores pré-determinados, você pode usar a função 
	<lit>GMM</lit> para incluir a especificação de uma gama de 
	desfasamentos numa forma bloco-diagonal.  Isto está exemplificado
	no terceiro exemplo acima.  O primeiro argumento de <lit>GMM</lit> 
	é o nome da variável em questão, o segundo é o desfasamento mínimo
	a ser usado como instrumento, e o terceiro é o desfasamento máximo.
	A mesma sintaxe pode ser utilizada na função <lit>GMMlevel</lit>
	para especificar instrumentos do tipo GMM para as equações nos 
	níveis.
      </para>
      <para context="gui">
	No que diz respeito à manipulação de instrumentos,
	veja a documentação da versão de sequência de comandos deste
	comando.  Presentemente você não pode especificar no interface
	gráfico (GUI) instrumentos explicítamente: todas as variáveis
	independentes são consideras como sendo estritamente exógenas.
      </para>
      <para>
	Por omissão são apresentados os resultados da estimação a um passo
	(juntamente com erros padrão robustos). Opcionalmente você pode 
	selecionar a estimação a dois passos.  Em ambos os casos são 
	determinados os testes de autocorrelação de ordem 1 e 2, assim como
	o teste de Sargan para a sobre-identificação e o teste de Wald para
	a significância conjunta dos regressores.  Note que neste modelo
	diferenciado a autocorrelação de primeira ordem não é um risco para
	a validade do modelo, mas a autocorrelação de segunda ordem viola
	as assunções estatísticas presentes.
      </para>
      <para context="cli">
	No caso da estimação em dois passos, os erros padrão são obtidos
	por omissão usando a correção de amostra-finita sugerida por <cite
	key="windmeijer05">Windmeijer (2005)</cite>.  Os erros padrão
	assimptóticos associados ao estimador de dois passos, são em geral,
	considerados como um guia pouco fiável para inferência, mas se por
	alguma razão você desejar observá-los você pode usar a opção
	<opt>--asymptotic</opt> para desligar a correção de Windmeijer.
      </para>
      <para context="cli">
	Se tiver sido dada a opção <opt>--time-dummies</opt>, uma
	conjunto de variáveis auxiliares tempo é acrescentado aos
	regressores especificados. O número de auxiliares é menos um
	que o número máximo de períodos usados na estimação, para
	assim se evitar a colinearidade exata com a constante.
	As variáveis auxiliares entram na forma diferenciada, exceto
	se tiver sido dada a opção <lit>--dpdstyle</lit>, entrando 
	nesse caso por níveis.
      </para>
      <para>
	Para mais detalhes e exemplos, ver o <guideref
	targ="chap:dpanel"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Painel/Modelo de painel dinâmico</menu-path>
    </gui-access>

  </command>

  <command name="dummify" section="Transformations" context="gui"
    label="Create sets of dummies">

    <description>
      <para>
	The <quote>dummify</quote> operation is available only
	for discrete-valued series. Its effect is to create a
	set of dummy variables coding for the distinct values
	present in the series.
      </para>
      <para>
	For example suppose one has a series named <lit>race</lit>,
	with values 1 for <quote>white</quote>, 2 for
	<quote>black</quote>, 3 for <quote>hispanic</quote> and 4 for
	<quote>other</quote>. To dummify this series means to create 4
	dummy variables: the first has value 1 for all observations at
	which race = 1, zero otherwise; the second has value 1 for all
	observations at which race = 2, zero otherwise; and so on.
      </para>
      <para>
	In practice it's likely that for a discrete series with
	<math>k</math> categories you will want to create only
	<math>k</math> &minus; 1 dummies, to avoid falling into
	the so-called <quote>dummy variable trap</quote>. Hence you
	have the option of dropping either the lowest or the
	highest value from the coding.
      </para>
    </description>

  </command>

  <command name="dummify" section="Transformations" 
    label="Cria conjuntos de variáveis auxiliares ('dummies')" context="cli">

    <usage>
      <arguments>
        <argument>lista-de-variáveis</argument>
      </arguments>
      <options>
	<option>
	  <flag>--drop-first</flag>
	  <effect>omitir da codificação o menor valor</effect>
	</option>
	<option>
	  <flag>--drop-last</flag>
	  <effect>omitir da codificação o maior valor</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Para cada uma das variáveis adequadas em <repl>lista-de-variáveis</repl>,
	cria um conjunto de variáveis auxiliares codificando para os diferentes 
	valores dessa variável. É adequado para as variáveis que tenham sido 
	explícitamente marcadas como sendo discretas, ou aquelas que tomem uma
	quantidade razoavelmente pequena de valores todos eles 
	<quote>quase redondos</quote> (múltiplos de 0,25).
      </para>
      <para>
	Por omissão é criada uma variável auxiliar para cada valor
	distinto na variável em questão. Por exemplo se uma variável
	discreta <lit>x</lit> tiver 5 valores distintos, serão criadas
	5 variáveis auxiliares e acrescentadas ao conjunto de dados,
	com os nomes, <lit>Dx_1</lit>, <lit>Dx_2</lit> e por aí adiante.
	A primeira variável auxiliar terá o valor 1 para observações onde
	<lit>x</lit> toma o seu valor mais pequeno, 0 caso contrário; a
	variável auxiliar seguinte terá o valor 1 quando <lit>x</lit>
	toma o seu segundo valor mais pequeno, e por aí adiante.  Se uma
	das opções <opt>--drop-first</opt> ou <opt>--drop-last</opt>
	tiver sido acrescentada, então o menor ou o maior valor de cada 
	variável será omitido da codificação (o que pode ser útil para
	evitar a <quote>armadilha das variáveis auxiliares</quote>).
      </para>
      <para>
	Este comando também pode ser introduzido no contexto da 
	especificação de uma regressão.  Por exemplo, a linha seguinte
	especifica um modelo onde <lit>y</lit> é regredido sobre o
	conjunto de variáveis auxiliares codificadas em <lit>x</lit>.
	(Neste contexto, as opções não podem ser passadas a <cmd>dummify</cmd>.)
      </para>
      <code>
	ols y dummify(x)
      </code>
    </description>

    <gui-access>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="duration" section="Estimation" label="Modelos de Durações" 
    context="cli">
    <usage>
      <arguments>
	  <argument>variável-dependente</argument>
	  <argument>variáveis-independentes</argument>
        <argument separated="true" optional="true">variável-censora</argument>
      </arguments>
      <options>
        <option>
          <flag>--exponential</flag>
          <effect>usar a distribuição exponencial</effect>
        </option>
        <option>
          <flag>--loglogistic</flag>
          <effect>usar a distribuição log-logística</effect>
        </option>
        <option>
          <flag>--lognormal</flag>
          <effect>usar a distribuição log-normal</effect>
        </option>
        <option>
          <flag>--medians</flag>
          <effect>os valores ajustados são medianas</effect>
        </option>
        <option>
          <flag>--robust</flag>
          <effect>erros padrão robustos (QML)</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>variável-agrupada</optparm>
	  <effect>ver a explicação em <cmdref targ="logit"/></effect>
        </option>
        <option>
          <flag>--vcv</flag>
          <effect>mostrar a matriz de covariância</effect>
        </option>
        <option>
          <flag>--verbose</flag>
          <effect>mostrar detalhes das iterações</effect>
        </option>
      </options>      
      <examples>
        <example>duration y 0 x1 x2</example>
	<example>duration y 0 x1 x2 ; cens</example>
      </examples>
    </usage>

    <description>
      <para>
	Estima um modelo de duração: a variável dependente (que tem que
	ser positiva) representa a duração de algum tipo de estado num 
	certo assunto, por exemplo a duração de episódios de desemprego
	para uma seção-cruzada de inquiridos.  Por omissão é utilizada a
	distribuição de Weibull, mas é possível usar as distribuições
	exponencial, log-logística e a log-normal.
      </para>
      <para>
	Se alguma das medições de durações estiver censurada ('right-censored')
	(por exemplo, para um certo índividuo um episódio de desemprego não
	chegou ao fim dentro do período de observação) então você pode acrescentar
	como último argumento a <repl>variável-censora</repl>, que é uma série
	na qual valores diferentes de zero indicam caso censurados.
      </para>
      <para>
	Por omissão os valores ajustados obtidos por intermédio do acessor
	<lit>$yhat</lit> são as médias condicionadas das durações, mas se 
	tiver sido dada a opção <lit>--medians</lit>, então <lit>$yhat</lit>
	devolve as medianas condicionadas.
      </para>
      <para>
	Para mais detalhes ver <guideref targ="chap:probit"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Modelos não-lineares/Dados de durações...</menu-path>
    </gui-access>

  </command>  

  <command name="elif" section="Programming" label="Controlo de fluxo" context="cli">
    <description>
      <para>Ver <cmdref targ="if"/>.
      </para>
    </description>
  </command>

  <command name="else" section="Programming" label="Controlo de fluxo" context="cli">
    <description>
      <para>Ver <cmdref targ="if"/>. Note que <cmd>else</cmd>
	requer uma linha para ele mesmo, antes do comando
	condicional seguinte.  Você pode juntar um comentário,
	como em
      </para>
      <code>
	else # Certo, fazer algo diferente
      </code>
      <para>
	Mas não pode juntar a um comando, como em
      </para>
      <code>
	else x = 5 # Errado!
      </code>
    </description>
  </command>

  <command name="ema-filter" section="Transformations" context="gui"
    label="Exponential Moving Average">

    <description>
      <para>
	The formula for the exponential moving average (EMA) employed
	by gretl is that of <cite key="roberts59">Roberts
	(1959)</cite>, namely
      </para>
      <para>
	<math>s</math><sub>t</sub> =
	&agr;<math>y</math><sub>t</sub> +
        (1&minus;&agr;)<math>s</math><sub>t&minus;1</sub>
      </para>
      <para>
	where <math>s</math> is the EMA, <math>y</math> is the
	original series, and &agr; is a constant between 0 and
	1. Larger values of &agr; place more weight on the current
	observation; smaller values produce greater smoothing.
      </para>
      <para>
	The <quote>initial EMA value</quote>, however specified, is
	taken to be the last pre-sample value, meaning that
	calculation of the filter starts with the first observation in
	the current sample range.
      </para>
      <para>
	For a command-line equivalent, see the <fncref targ="movavg"/>
	function.
      </para>
    </description>

  </command>  

  <command name="end" section="Programming" 
    label="Termina um bloco de comandos" context="cli">
    <description>
      <para>
	Termina diversos tipos de bloco de comandos. Por exemplo, 
	<cmd>end system</cmd> termina uma equação <cmdref targ="system"/>.
      </para>
    </description>
  </command>

  <command name="endif" section="Programming" label="Controlo de fluxo" context="cli">
    <description><para>Ver <cmdref targ="if"/>.</para>
    </description>
  </command>

  <command name="endloop" section="Programming" 
    label="Termina um ciclo de comandos" context="cli">
    <description>
      <para>
	Marca o fim de um ciclo de comandos.  Ver <cmdref targ="loop"/>.
      </para>
    </description>
  </command>

  <command name="eqnprint" section="Printing" 
    label="Mostra o modelo como equação" context="cli">

    <usage>
      <options>
        <option>
	  <flag>--complete</flag>
	  <effect>Cria um documento completo</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>nome-de-ficheiro</optparm>
	  <effect>envia a saída para o ficheiro especificado</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Tem que ser invocado a seguir à estimação de um modelo.
	Mostra o modelo estimado na forma de uma equação &latex;.  Se
	o nome-de-ficheiro tiver sido especificado usando a opção 
	<lit>-f</lit> a saída é redirecionada para esse ficheiro,
	caso contrário vai para um ficheiro com o nome no formato
	<filename>equation_N.tex</filename>, onde <lit>N</lit> é o
	número de modelos estimados até ao momento na sessão
	corrente.
	Ver também <cmdref targ="tabprint"/>.
      </para>
      <para>
	Se a opção <opt>--complete</opt> tiver sido dada, o ficheiro &latex; é
	um documento completo, pronto para ser processado; de outro modo ele
	terá que ser incluído num documento.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do Modelo, /LaTeX</menu-path>
    </gui-access>

  </command>

  <command name="equation" section="Estimation" 
    label="Define uma equação dentro de um sistema de equações" context="cli">

    <usage>
      <arguments>
        <argument>variável-dependente</argument>
	  <argument>variáveis-independentes</argument>
      </arguments>
      <examples>
        <example>equation y x1 x2 x3 const</example>
      </examples>
    </usage>

    <description>
      <para>
	Especifica uma equação dentro de um sistema de equações (ver 
	<cmdref targ="system"/>).  A sintaxe para especificar uma 
	equação dentro de um sistema SUR é o mesmo que em, por exemplo, 
	<cmdref targ="ols"/>.  Para uma equação dentro de um sistema
	de Mínimos Quadrados de Três-Fases você tanto pode (a) fornecer
	uma especificação de equação tipo OLS e dar uma lista comum de 
	instrumentos usando o comando <cmd>instr</cmd> (mais uma vez, ver
	 <cmdref targ="system"/>), ou (b) usar a mesma sintaxe de equação
	como para <cmdref targ="tsls"/>.
      </para>
    </description>

  </command>

  <command name="estimate" section="Estimation" 
    label="Estimar um sistema de equações" context="cli">

    <usage>
      <arguments>
        <argument optional="true">nome-do-sistema</argument>
        <argument optional="true">estimador</argument>
      </arguments>
      <options>
	<option>
	  <flag>--iterate</flag>
	  <effect>iterar até à convergência</effect>
	</option>
	<option>
	  <flag>--no-df-corr</flag>
	  <effect>não usar correção de graus de liberdade</effect>
	</option>
	<option>
	  <flag>--geomean</flag>
	  <effect>ver abaixo</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>não mostrar resultados</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das iterações</effect>
	</option>
      </options>
      <examples>
        <example>estimate "Klein Model 1" method=fiml</example>
	<example>estimate Sys1 method=sur</example>
	<example>estimate Sys1 method=sur --iterate</example>
      </examples>
    </usage>

    <description>
      <para>
	Chama a estimação de um sistema de equações, que foi 
	previamente definido usando o comando <cmdref targ="system"/>.
	O nome do sistema deve ser dado em primeiro lugar, dentro de
	aspas caso contenha espaços.  O estimador, que tem que ser um de
	<cmd>ols</cmd>,	<cmd>tsls</cmd>, <cmd>sur</cmd>, <cmd>3sls</cmd>,
	<cmd>fiml</cmd> ou <cmd>liml</cmd>, é precedido pelo texto
	<lit>method=</lit>. Estes argumentos são opcionais se o sistema
	em questão já foi estimado e ocupa a posição do 
	<quote>último modelo</quote>; nesse caso o estimador é o mesmo
	definido anteriormente.
      </para>
      <para>
	Se o sistema em questão tinha aplicado um conjunto de restrições
	(ver o comando <cmdref targ="restrict"/>), a estimação estará
	sujeita às restrições especificadas.
      </para>
      <para>
	Se o método de estimação é <cmd>sur</cmd> ou <cmd>3sls</cmd>
	e a opção <opt>--iterate</opt> tiver sido dada, o estimador
	será iterado.  No caso do SUR, se o procedimento convergir
	os resultados são estimativas de máxima vesrosimilhança.  No
	entanto, a iteração de Mínimos Quadrados de Três-Fases, geralmente não
 	converge para resultados de informação-completa de máxima verosimilhança.
	A opção <opt>--iterate</opt> é ignorada nos outros métodos de estimação.
      </para>
      <para>
	Se tiverem sido escolhidos os estimadores equação-a-equação <cmd>ols</cmd>
	ou <cmd>tsls</cmd>, por omissão é aplicada uma correção dos graus de
	liberdade quando se calcula os erros padrão.  Isto pode ser suprimido
	usando a opção <opt>--no-df-corr</opt>. Esta opção não tem efeito nos outros
	estimadores; de qualquer modo não seria aplicada a correção de graus de 
	liberdade.
      </para>
      <para>
	Por omissão, a equação usada no cálculo dos elementos da
	matriz de covariância das equações cruzadas é
	<equation status="display"
	tex="\[\hat{\sigma}_{i,j}=\frac{\hat{u}_i' \hat{u}_j}{T}\]"
	ascii="sigma(i,j) = u(i)' * u(j) / T"
	graphic="syssigma1"/>
	Se tiver sido dada a opção <opt>--geomean</opt>, a 
	correção de graus de liberdade será aplicada: a equação é
	<equation status="display"
	tex="\[\hat{\sigma}_{i,j}=\frac{\hat{u}_i' \hat{u}_j}{\sqrt{(T-k_i)(T-k_j)}}\]"
	ascii="sigma(i,j) = u(i)' * u(j) / sqrt((T - ki) * (T - kj))"
	graphic="syssigma2"/>
	onde os <math>k</math>s são o número de parâmetros independentes em cada 
	equação.
      </para>
      <para>
	Se tiver sido dada a opção <opt>verbose</opt> e ter sido especificado um
	método iterativo, serão mostrados os detalhes das iterações.
      </para>
    </description>

  </command>

  <command name="eval" section="Utilities">
    <usage>
      <arguments>
        <argument>expression</argument>
      </arguments>
      <examples>
        <example>eval x</example>
	<example>eval inv(X'X)</example>
	<example>eval sqrt($pi)</example>
      </examples>
    </usage>    
    <description>
      <para>
	This command makes gretl act like a glorified calculator.  The
	program evaluates <repl>expression</repl> and prints its
	value. The argument may be the name of a variable, or
	something more complicated. In any case, it should be an
	expression which could stand as the right-hand side of an
	assignment statement.
      </para>
    </description>
  </command>

  <command name="expand" section="Dataset" context="gui"
    label="Expandir dados">

    <description>
      <para>
	Se você pretender acrescentar a um conjunto de dados uma série
	que tenha uma frequência inferior, será necessário 
	<quote>expandir</quote> a nova série.  Por examplo, uma série
	trimestral terá que ser expandida para caber dentro de um conjunto
	de dados mensal.  Adicionalmente, você pode desejar expandir
	a totalidade de um conjunto de dados para uma frequência superior
	(eventualmente, antes de acrescentar uma variável de 
	alta-frequência ao conjunto de dados).
      </para>
      <para>
	Expandir dados deve ser considerada uma opção para 
	<quote>especialistas</quote>; você tem que saber o que está
	a fazer.  Ao combinar séries de diferentes frequências dentro
	de um conjunto de dados, você deve preferencialmente compactar
	os dados de alta-frequência em vez de expandir as séries
	baixa-frequência.
      </para>
      <para>
	Dito isto, gretl oferece duas opções: valores de 
	alta-frequência podem ser interpolados usando o método
	de Chow e Lin (1971), ou os valores de seéries de 
	baixa-frequência podem ser repetidos o número de vezes 
	necessárias.
      </para>
      <para>
	O método de Chow-Lin baseia-se em regressão, usando uma 
	constante e uma tendência quadrática e assumindo um 
	processo autoregressivo de primeira ordem para as 
	perturbações. Este procedimento usa quatro graus de liberdade.
	Quanto à repetição de valores, suponha que temos uma série
	trimestral com o valor 35,5 em 1990:1, o primeiro trimestre de
	1990.  Ao expandir para mensal, o valor 35,5 será atribuído
	às observações de janeiro, fevereiro e março de 1990.  A
	variável expandida será assim inútil para análises apuradas
	de séries temporais, excetuando o caso especial onde você
	sabe que a variável em questão se mantém de fato constante
	ao longo dos subperíodos.
      </para>
    </description>
  </command>

  <command name="export" section="Dataset" context="gui"
    label="Exportar dados">

     <description>
      <para>
	Voce pode exportar dados no formato Valores Separados por 
	Vírgulas (CSV): esse tipo de dados pode ser aberto em
	folhas-de-cálculo e em muitos outros programas.  Se
	você escolher esta opção poderá selecionar outras
	opções sobre o formato específico do ficheiro CSV.
      </para>
      <para>
	Você também tem a possibilidade de exportar dados na forma
	<quote>nativa</quote> de ficheiro-de-dados gretl, ou (se os dados
	forem adequandos) exportar para uma base-de-dados gretl. Ver
	<url>gretl.sourceforge.net/gretl_data.html</url> para uma listagem
	de bases-de-dados gretl.
      </para>
      <para>
	Você também pode exportar os dados num formato adequado
	para usar nos seguintes programas:
      </para>
      <ilist>
	<li>
	  <para>GNU R (<url>www.r-project.org</url>)</para>
	</li>
	<li>
	  <para>GNU octave (<url>www.gnu.org/software/octave</url>)
	  </para>
	</li>
	<li>
	  <para>JMulTi (<url>www.jmulti.de</url>)</para>
	</li>
	<li>
	  <para>PcGive (<url>www.pcgive.com</url>)</para>
	</li>
      </ilist>
      <para>
	Se você quiser exportar dados copiando para a memória em vez
	de escrever para um ficheiro em disco, selecione a série que
	você quer copiar na janela principal, clique com o botão 
	direito e selecione <quote>Copiar para a memória de edição</quote>. 
	(Neste contexto apenas é suportado o formato CSV.) 
      </para>
    </description>
  </command>

  <command name="factorized" section="Graphs" context="gui"
    label="Gráfico com separação fatorizada">

    <description>
      <para>
	Este comando requer a seleção de três variáveis sendo a última
	delas uma variável auxiliar (valores 1 ou 0). A variável Y é
	representada contra a variável X, com os pontos coloridos
	diferentemente de acordo com o valor da terceira.
      </para>
      <para>
	Examplo: Você tem dados de salários e níveis de educação obtidos
	numa amostra de pessoas; você tem também uma variável auxiliar
	com valores 1 para homens e 0 para mulheres (tal como nos dados
	<filename>data7-2</filename> de Ramanathan).  Um 
	<quote>gráfico com separação fatorizada</quote> de <lit>WAGE</lit>
	contra <lit>EDUC</lit> usando a variável auxiliar <lit>GENDER</lit>
	como fator mostrará os pontos de dados para homens numa cor e os das
	mulheres noutra (com uma legenda para as identificar).
      </para>
    </description>

  </command>

  <command name="fcast" section="Prediction" 
    label="Gerar predições">

    <usage>
      <arguments>
        <argument optional="true">observações-iniciais observações-finais</argument>
	<argument optional="true">passos-à-frente</argument>
	<argument optional="true">nome-de-variável</argument>
      </arguments>
      <options>
        <option>
	  <flag>--dynamic</flag>
	  <effect>criar predição dinâmica</effect>
        </option>
        <option>
	  <flag>--static</flag>
	  <effect>criar predição estática</effect>
        </option>
        <option>
	  <flag>--out-of-sample</flag>
	  <effect>gerar predição fora-da-amostra</effect>
        </option>
        <option>
	  <flag>--no-stats</flag>
	  <effect>não mostrar estatísticas de predição</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>não mostrar nada</effect>
        </option>
        <option>
	  <flag>--rolling</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--plot</flag>
	  <optparm optional="true">nome-de-ficheiro</optparm>
	  <effect>ver abaixo</effect>
        </option>
      </options>
      <examples>
        <example>fcast 1997:1 2001:4 f1</example>
	<example>fcast fit2</example>
	<example>fcast 2004:1 2008:3 4 rfcast --rolling</example>
      </examples>
    </usage>

    <description>

      <para context="gui">
	Tem que se seguir a um comando de estimação.  A predições são 
	geradas para o intervalo de observações especificado. Dependendo
	da natureza do modelo, também podem ser gerados erros padrão
	(ver abaixo).
      </para>

      <para context="cli">
	Tem que se seguir a um comando de estimação.  A predições são 
	geradas para um certo intervalo de observações: se tiverem sido
	dados <repl>observações-iniciais</repl> e 
	<repl>observações-finais</repl>, para esse intervalo (se possível); 
	caso contrário se a opção <opt>--out-of-sample</opt> tiver sido dada, 
	para observações a seguir ao intervalo onde o modelo foi estimado;
	caso contrário pelo intervalo de amostragem corrente.  Se uma 
	predição fora-da-amostra for pedida mas não haja observações relevantes,
	será assinalado um erro.  Dependendo da natureza do modelo, poderão ser
	gerados erros padrão; ver abaixo.  Ver também abaixo o efeito especial
	da opção <opt>--rolling</opt>.
      </para>

      <para context="cli">
	Se o último modelo estimado é de uma única equação, então
	o argumento opcional <repl>nome-de-variável</repl> tem o seguinte
	efeito: o valores preditos não são mostrado, mas guardados dentro
	do conjunto de dados no nome fornecido.  Se o último modelo 
	estimado é um sistema de equações, <repl>nome-de-variável</repl> 
	tem um efeito diverente, nomeadamente a seleção de uma variável
	endógena específica para a predição (por omissão são produzidas
	predições para todas as variáveis endógenas).  No caso do sistema,
	ou se não tiver sido dado o <repl>nome-de-variável</repl>, os 
	valores de predição podem ser obtidos usando o acessor 
	<lit>$fcast</lit>, e os erros padrão, se disponíveis, pelo 
	<lit>$fcse</lit>.
      </para>

      <para>
	A escolha entre predições estáticas ou dinâmicas aplica-se
	apenas no caso de modelos dinâmicos, com processamento 
	autoregressivo de erros ou que incluam um ou mais valores
	desfasados da variável dependente como regressores.  As 
	predições estáticas são um passo à frente, baseadas em valores
	concretizados no período anterior, enquanto que as predições
	dinâmicas usam a regra de encadeamento de predição.  Por 
	exemplo, se uma predição de <math>y</math> em 2008
	requer como entrada um valor de <math>y</math> em 2007, uma
	predição estática é impossível sem dados reais para 2007.  Uma
	predição dinâmica para 2008 é possível se uma predição anterior
	poder ser substítuida no <math>y</math> em 2007.
      </para>

      <para>
	Por omissão o normal é produzir uma predição estática para
	alguma parte do intervalo de predição que abrange o 
	intervalo da amostra onde o modelo foi estimado, e uma predição
	dinâmica (se relevante) para fora-da-amostra.  A opção
	 <lit>dynamic</lit> chama uma predição dinâmica a partir da 
	date mais cedo possível, e a opção <opt>static</opt> chama
	uma predição estática até para fora-da-amosta.
      </para>

      <para context="cli">
	A opção <opt>rolling</opt> está atualmente apenas disponível para
	modelos de um única equação estimados por Mínimos Quadrados (OLS).
	Quando esta opção é dada as predições são recursivas.  Isto é, cada 
	predição é gerada a partir de uma estimativa do modelo dado usando
	dados de uma ponto de partida fixo (nomeadamente, a partir do início 
	do intervalo da amostra da estimação original) até à data de predição
	menos <math>k</math>, onde <math>k</math> é o número de passos à frente
	que têm que ser dados no argumento <repl>passos-à-frente</repl>.  As
	predições serão sempre dinâmicas se isso for aplicável.  Note que o
	argumento <repl>passos-à-frente</repl> deve ser dado apenas em conjunto
	com a opção <opt>rolling</opt>.
      </para>

      <para context="cli">
	A opção <opt>--plot</opt> (disponível apenas no caso de 
	estimação de equação única) invoca a produção de um 
	ficheiro de gráfico, que contém a representação
	gráfica da predição.  Quando não é dado o parâmetro
	<repl>nome-de-ficheiro</repl>, gretl escreve os comandos
	gnuplot para um ficheiro com nomes do tipo
	<lit>gpttmp01.plt</lit> na diretoria de trabalho de gretl
	do utilizador (com o número incrementado em gráficos sucessivos).
	Se o <repl>nome-de-ficheiro</repl> é acrescentado, a sua 
	extensão é usada para determinar o tipo de ficheiro a ser
	escrito (<lit>.eps</lit> para EPS, <lit>.pdf</lit> para PDF, ou
	<lit>.png</lit> para PNG; qualquer outra extensão resulta num
	ficheiro de script gnuplot).  Por exemplo,
      </para>
      <code>
	fcast --plot=fc.pdf
      </code>
      <para>
	produzirá um gráfico em formato PDF.  Serão respeitados caminhos
	completos para ficheiros, senão os ficheiros são escritos na
	diretoria de trabalho do gretl.
      </para>
      <para>
	A natureza dos erros padrão de predição (se disponíveis)
	depende da natureza do modelo e da predição.  Para modelos
	lineares estáticos os erros padrão são determinados usando o
	método traçado por <cite key="davidson-mackinnon04">Davidson e MacKinnon
	(2004)</cite>; eles incorporam tanto a incerteza devida aos 
	processos de erro como a incerteza dos parâmetros ( resumidos na
	matriz de covariância das estimativas dos parâmetros).  Para
	modelos dinâmicos, os erros padrão de predição são apenas
	calculados no caso de uma predição dinâmica, e eles não
	incorporam incerteza de parâmetros.  Para modelos não-lineares,
	os erros padrão de predição não estão disponíveis atualmente.
      </para>	

    </description>

    <gui-access>
      <menu-path>Janela de Modelo, /Análise/Predições...</menu-path>
    </gui-access>

  </command>

  <command name="flush" section="Programming" context="cli">

    <description>
     <para>
       This simple command (no arguments, no options) is intended for
       use in time-consuming scripts that may be executed via the
       gretl GUI (it is ignored by the command-line program), to give
       the user a visual indication that things are moving along and
       gretl is not <quote>frozen</quote>.
     </para>
     <para>
       Ordinarily if you launch a script in the GUI no output is shown
       until its execution is completed, but the effect of invoking
       <lit>flush</lit> is as follows:
     </para>
     <ilist>
       <li>
	 <para>
	   On the first invocation, gretl opens a window, displays the
	   output so far, and appends the message
	   <quote>Processing...</quote>.
	 </para>
       </li>
       <li>
	 <para>
	   On subsequent invocations the text shown in the output
	   window is updated, and a new <quote>processing</quote>
	   message is appended.
	 </para>
       </li>
     </ilist>
     <para>
       When execution of the script is completed any remaining output
       is automatically flushed to the text window.
     </para>
     <para>
       Please note, there is no point in using <lit>flush</lit> in
       scripts that take less than (say) 5 seconds to execute. Also
       note that this command should not be used at a point in the
       script where there is no further output to be printed, as the
       <quote>processing</quote> message will then be misleading
       to the user.
     </para>
     <para>
       The following illustrates the intended use of <lit>flush</lit>:
     </para>
     <code>
       set echo off
       scalar n = 10
       loop i=1..n
           # do some time-consuming operation
           loop 100 --quiet
               a = mnormal(200,200)
               b = inv(a)
           endloop
           # print some results
           printf "Iteration %2d done\n", i
           if i &lt; n
               flush
           endif
       endloop
     </code>
    </description>

  </command>

  <command name="foreign" section="Programming" 
    label="Comandos não-nativos" context="cli">

    <usage>
      <syntax><lit>foreign language=</lit><repl>linguagem</repl></syntax>
      <options>
	<option>
	  <flag>--send-data</flag>
	  <effect>pré-carregar o conjunto de dados corrente; ver abaixo</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suprimir a saída do programa estrangeiro</effect>
	</option>
      </options>
    </usage>

    <description>
     <para>
	Este comando inicia um modo especial no qual se aceita os comandos a serem
	executados por outro programa.  Você sai deste modo com <lit>end
	  foreign</lit>; onde neste ponto são executados os comandos acumulados.
      </para>
      <para>
	Presentemente são suportados três programas <quote>estrangeiros</quote>,
 	GNU R (<lit>language=R</lit>), Ox de Jurgen Doornik 
	(<lit>language=Ox</lit>) e GNU Octave (<lit>language=Octave</lit>).
	Os nomes de linguagem são reconhecidos sem considerar capitalização.
      </para>
      <para>
	A opção <opt>--send-data</opt> é válida apenas quando em ligação
	com R e Octave; tem o efeito de tornar o corrente conjunto de dados de
	gretl disponível dentro do programa alvo, usando o nome <lit>gretldata</lit>.
      </para>
      <code>
	list Rlist = x1 x2 x3
	foreign language=R --send-data=Rlist
      </code>
      <para>
	Para detalhes e exemplos ver <guideref targ="chap:gretlR"/>.
      </para>
    </description>

  </command>

  <command name="fractint" section="Statistics" label="Integração fracional">
  
    <usage>
      <arguments>
        <argument>série</argument>
	<argument optional="true">ordem</argument>
      </arguments>
      <options>
        <option>
	  <flag>--gph</flag>
	  <effect>fazer o teste de Geweke e Porter-Hudak</effect>
        </option>
        <option>
	  <flag>--all</flag>
	  <effect>fazer ambos os testes</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>não mostrar resultados</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Testa a integração fracional sobre a série especificada
	(<quote>memória longa</quote>). A hipótese nula é de que a
	ordem de integração da série é zero.  Por omissão é usado o
	estimador local de Whittle <cite key="robinson95" p="true">(Robinson,
	1995)</cite> mas se tiver sido dada a opção <opt>--gph</opt>
	será usado o teste GPH <cite key="GPH83" p="true">(Geweke e
	Porter-Hudak, 1983)</cite>. Se a opção <opt>--all</opt> for 
	dada então serão mostrados os resultados dos dois testes.
      </para>
      <para>
	Para mais detalhes sobre este tipo de testes, ver <cite key="phillips04">Phillips 
	e Shimotsu (2004)</cite>. 
      </para>
      <para>
	Se não tiver sido dado o argumento opcional <repl>ordem</repl>, a ordem para 
	os teste(s) é automaticamente definida como sendo o menor de
	<math>T</math>/2 e <math>T</math><sup>0.6</sup>.
      </para>
      <para>
	Os resultados podem ser obtidos usando os acessores <lit>$test</lit>
	e <lit>$pvalue</lit>. Estes valores baseiam-se no estimador local de
	Whittle exceto quando dada a opção <opt>--gph</opt>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Testes de raiz unitária/Integração fracional</menu-path>
    </gui-access>

  </command>


  <command name="freq" section="Statistics" label="Distribução de frequência">

    <usage>
      <arguments>
        <argument>variável</argument>
      </arguments>
      <options>
        <option>
	  <flag>--nbins</flag>
	  <optparm>n</optparm>
	  <effect>especificar o número de classes</effect>
        </option>
        <option>
	  <flag>--min</flag>
	  <optparm>valor-mínimo</optparm>
	  <effect>especificar o mínimo, ver abaixo</effect>
        </option>
        <option>
	  <flag>--binwidth</flag>
	  <optparm>amplitude</optparm>
	  <effect>especificar a amplitude das classes, ver abaixo</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>não mostrar o gráfico</effect>
        </option>
        <option>
	  <flag>--normal</flag>
	  <effect>testar a distribuição normal</effect>
        </option>
        <option>
	  <flag>--gamma</flag>
	  <effect>testar a distribuição gama</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>não mostrar nada</effect>
        </option>
        <option>
	  <flag>--show-plot</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>nome</optparm>
	  <effect>usar coluna da matriz indicada</effect>
        </option>
      </options>
      <examples>
        <example>freq x</example>
	<example>freq x --normal</example>
	<example>freq x --nbins=5</example>
	<example>freq x --min=0 --binwidth=0.10</example>
      </examples>
    </usage>

    <description context="cli">
      <para>
	Se não forem dadas opções, mostra a distribuição de frequência
	da série <repl>variável</repl> (dada por nome ou por número), com
	o número de classes e respetivo tamanho escolhidos automaticamente.
      </para>
      <para>
	Se tiver sido dada a opção <opt>--matrix</opt>, a <repl>variável</repl>
	(que tem que ser um inteiro) será interpretada com um índice de base 1
	que selecciona a coluna da matriz designada.
      </para>
      <para>
	Para controlar a apresentação da distribuição você pode
	especificar <emphasis>tanto</emphasis> o número de classes
	ou o valor mínimo e ainda a amplitude das classes, tal como
	mostrado nos dois últimos exemplos acima. A opção <opt>--min</opt>
	define o limite inferior da classe mais à esquerda.
      </para>
      <para>
	Se a opção <opt>--normal</opt> tiver sido dada, será calculado
	o teste qui-quadrado para a normalidade de Doornik&ndash;Hansen.
	Se a opção <opt>--gamma</opt> tiver sido dada, o teste de normalidade
	será substituído pelo teste não paramétrico de Locke para a hipótese
	nula de que a variável segue uma distribuição gama; ver <cite
	key="locke76">Locke (1976)</cite>, <cite
	key="shapiro-chen01">Shapiro e Chen (2001)</cite>.  Note que a
	parametrização da distribuição gama utilizadada em gretl é 
	(forma, escala).
      </para>
      <para>
	Em modo interactivo, por omissão é apresentado o gráfico da
	distribuição.  A opção <opt>--quiet</opt> pode ser usada para
	suprimir isto.  Pelo contrário, normalmente não é mostrado o
	gráfico quando se usa a opção <cmd>freq</cmd> dentro de uma
	sequência-de-comandos, mas você pode forçar que seja apresentado
	usando a opção <opt>--show-plot</opt>. (Isto não se aplica quando
	se usa o programa em modo de linha-de-comandos,
	<lit>gretlcli</lit>.)
      </para>
      <para>
	A opção <opt>--silent</opt> suprime toda a saída do programa.
	Isto apenas faz sentido quando em conjunto com uma das opções
	de teste de distribuição: a estatística de teste e o seu valor p
	ficam guardados e podem ser obtidos usando os acessores
	<lit>$test</lit> e <lit>$pvalue</lit>.
      </para>
    </description>

    <description context="gui">
      <para>
	Na janela de diálogo do gráfico de frequência você pode controlar
	as caraterísticas do gráfico em duas maneiras diferentes.
      </para>
      <para>
	Primeiro, você pode escolher o número de classes. Neste caso
	a largura e localização das classes são calculadas automaticamente.
      </para>
      <para>
	Em alternativa, você pode especificar o limite inferior da classe 
	mais à esquerda, e a largura das classes.  Neste caso o número
	de classes é calculado automaticamente.
      </para>
      <para>
	Se você desejar alinhar as classes para números redondos, esta é uma
	maneira possível: comece por especificar o número de classes, e 
	observe o gráfico produzido. Se não fôr do seu agrado, anote a 
	modificação necessária (por exemplo, fazer a classe mais à esquerda
	iniciar em 100 e impor uma largura de classe de 200).
	Então efectue uma segunda passagem onde você especifica o limite
	da classe mais à esquerda e a largura das classes.
      </para>
      <para>
	Este diálogo também permite selecionar a apresentação da curva da
	distribuição teórica dos dados: a normal ou a gama.  Se a opção
	normal for selecionada será determinado o teste para normalidade 
	de Doornik&ndash;Hansen. Se a opção gama é selecionada, gretl
	calcula o teste não paramétrico de Locke para a hipótese nula de que
	variável segue uma distribuição gama.  Note que a
	parametrização da distribuição gama utilizadada em gretl é 
	(forma, escala).
      </para>
    </description>

    <gui-access>
      <menu-path>/Variável/Distribuição de frequência</menu-path>
    </gui-access>

  </command>

  <command name="function" section="Programming" 
    label="Definir uma função" context="cli">

    <usage>
      <arguments>
        <argument>nome-da-função</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Abre um bloco de declarações no qual é definida a função.  Este
	bloco tem que ser finalizado com <lit>end function</lit>.  Para mais
	detalhes ver <guideref targ="chap:functions"/>.
      </para>
    </description>

  </command>  

  <command name="garch" section="Estimation" label="Modelo GARCH">

    <usage>
      <arguments>
        <argument>p</argument>
	<argument>q</argument>
	<argument separated="true">variável-dependente</argument>
	<argument optional="true">variáveis-independentes</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>erros padrão robustos</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das iterações</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
        </option>
        <option>
	  <flag>--nc</flag>
	  <effect>não incluir uma constante</effect>
        </option>
        <option>
	  <flag>--stdresid</flag>
	  <effect>normalizar os resíduos</effect>
        </option>
        <option>
	  <flag>--fcp</flag>
	  <effect>usar o algoritmo Fiorentini, Calzolari, Panattoni</effect>
        </option>
        <option>
	  <flag>--arma-init</flag>
	  <effect>parâmetros iniciais da variância a partir de ARMA</effect>
        </option>
      </options>
      <examples>
        <example>garch 1 1 ; y</example>
	<example>garch 1 1 ; y 0 x1 x2 --robust</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Estima um modelo GARCH (GARCH = Autoregressivo Generalizado
        de Heterocedastidade Condicional, "Generalized Autoregressive
	Conditional Heteroskedasticity"), que pode ser um modelo
	univariado, ou multivariado se especificadas as 
	<repl>variáveis-independentes</repl>, incluindo as variáveis 
	exógenas.  Os valores inteiros <repl>p</repl> e <repl>q</repl>
	(que podem ser dados na forma numérica ou como nomes de variáveis
	escalares pré-existentes) representam as ordens de desfasamento 
	na equação de variância condicional:
	<equation status="display"
	  tex="\[h_t = \alpha_0 + \sum_{i=1}^q \alpha_i \varepsilon^2_{t-i} +
	  \sum_{j=1}^p \beta_j h_{t-j}\]"
	  ascii="h(t) = a(0) + sum(i=1 to q) a(i)*u(t-i)^2 + sum(j=1 to p) b(j)*h(t-j)"
	  graphic="garch_h"/>
      </para>
      <para context="cli">
	Portanto, o parâmetro <repl>p</repl> representa a ordem Generalizada
	(ou <quote>AR</quote>), enquanto <repl>q</repl> representa a ordem
	normal ARCH (ou <quote>MA</quote>).  Se <repl>p</repl> for não-nulo,
	<repl>q</repl> tem também que ser não-nulo senão o modelo fica
	não-identificado.  No entanto, você pode estimar um modelo ARCH normal
	ao definir <repl>q</repl> para um valor positivo e <repl>p</repl>
	para zero.  A soma de <repl>p</repl> e <repl>q</repl> não pode ser
	maior que 5.  Note que é automaticamente incluida uma constante na 
        equação da média, exceto se tiver sido dada a opção <opt>--nc</opt>.
      </para>

      <para context="gui">
	Estima um modelo GARCH (GARCH = Autoregressivo Generalizado
        de Heterocedastidade Condicional, "Generalized Autoregressive
	Conditional Heteroskedasticity"), que pode ser um modelo
	univariado, ou multivariado se selecionadas variáveis 
	independentes, incluindo as variáveis exógenas.  Em baixo é
	mostrada a equação de variância condicional:
	<equation status="display" tex="\[h_t = \alpha_0 + 
	\sum_{i=1}^q \alpha_i \varepsilon^2_{t-i} + \sum_{j=1}^p
	\beta_i h_{t-j}\]" ascii="h(t) = a(0) + sum(i=1 to q) a(i)*u(t-i) +
	sum(j=1 to p) b(j)*h(t-j)" graphic="garch_h"/>
      </para>
      <para context="gui">
	Portanto, o parâmetro <repl>p</repl> representa a ordem Generalizada
	(ou <quote>AR</quote>), enquanto <repl>q</repl> representa a ordem
	normal ARCH (ou <quote>MA</quote>).  Se <repl>p</repl> for não-nulo,
	<repl>q</repl> tem também que ser não-nulo senão o modelo fica
	não-identificado.  No entanto, você pode estimar um modelo ARCH normal
	ao definir <repl>q</repl> para um valor positivo e <repl>p</repl>
	para zero.  A soma de <repl>p</repl> e <repl>q</repl> não pode ser
	maior que 5.
      </para>

      <para>
	Por omissão a estimação de modelos GARCH é feita usando código
	nativo gretl, mas você também tem a possibilidade de usar o 
	algoritmo de <cite key="fiorentini96">Fiorentini, Calzolari e Panattoni
	(1996)</cite>.  O primeiro usa o maximizador BFGS enquanto o 
	segundo usa a matriz de informação para maximizar a
	verosimilhança, com aperfeiçoamento por via da Hessiana.
      </para>

      <para context="cli">
	Para este comando estão disponíveis diferentes estimadores da
	matriz de covariância.  Por omissão, usa-se a Hessiana, ou se
	a opção	<opt>--robust</opt> tiver sido dada, será usada a 
	matriz de covariança QML (White).  Outras possibilidades podem
	ser especificadas usando o comando <cmdref targ="set"/> 
	(por exemplo a matriz de informação, ou o estimador 
	Bollerslev&ndash;Wooldridge ).
      </para>

      <para context="gui">
	Para este comando estão disponíveis diferentes estimadores da
	matriz de covariância.  Por omissão, usa-se a Hessiana, ou se
	a opção	<quote>Erros padrão robustos</quote> tiver sido 
	selecionada, será usada a matriz de covariância QML (White).  
	 Outras possibilidades podem ser especificadas usando o 
	comando <cmdref targ="set"/> (por exemplo a matriz de 
	informação, ou o estimador Bollerslev&ndash;Wooldridge ).
      </para>

      <para context="gui">
	A variância condicional estimada, juntamente com os resíduos e 
	outras estatísticas do modelo, podem ser acedidas e acrescentadas
	ao modelo usando o menu <quote>Gravar</quote> na janela
	onde o modelo é apresentado.  Se a opção 
	<quote>Normalizar os resíduos</quote> estiver selecionada, os
	resíduos são divididos pela raiz quadrada da variância condicional.
      </para>

      <para context="cli">
	Por omissão, as estimativas dos parâmetros da variância são
	inicializados usando a variância do erro incondicional da 
	estimação OLS inicial para a constante, e pequenos valores
	positivos para os coeficientes dos valores anteriores do
	quadrado do erro e da variância do erro.  A opção 
	<opt>--arma-init</opt> faz com que os valores iniciais destes
	parâmetros sejam definidos usando inicialmente um modelo ARMA,
	explorando a relação entre GARCH e ARMA demosntrado no 
	Capítulo 21 do livro de Hamilton, <book>Time Series Analysis</book>.
	 Em alguns casos isto pode melhorar as possibilidades de converência.
      </para>

      <para context="cli">
	Os resíduos GARCH e a variância condicional estimada podem ser
	obtidos como <lit>$uhat</lit> e <lit>$h</lit> respectivamente.  Por
	exemplo, para obter a variância condicional:  
      </para>
      <code context="cli">
	genr ht = $h
      </code>
      <para context="cli">
	Se a opção <opt>--stdresid</opt> tiver sido dada, os valores <lit>$uhat</lit>
	são divididos pela raiz quadrada de <math>h</math><sub>t</sub>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/Série temporal/GARCH</menu-path>
    </gui-access>

  </command>

  <command name="genr" section="Dataset"
    label="Gerar uma nova variável">

    <usage>
      <arguments>
        <argument>nova-variável</argument>
        <argument>= expressão</argument>
      </arguments>
    </usage>

    <description>

      <para>
	NOTE: this command has undergone numerous changes and
	enhancements since the following help text was written, so for
	comprehensive and updated info on this command you'll want to
	refer to <guideref targ="chap:genr"/>. On the other hand, this
	help does not contain anything actually erroneous, so take the
	following as <quote>you have this, plus more</quote>.
      </para>

      <para context="cli">
	No contexto apropriado, o nomes; <lit>series</lit>, <lit>scalar</lit>
	e <lit>matrix</lit> são sinónimos para este comando.
      </para>

      <para context="cli">
	Cria novas variáveis, frequentemente a partir de transformações 
	de variáveis já existentes. Ver também os atalhos, <cmdref targ="diff"/>, <cmdref targ="logs"/>,
	<cmdref targ="lags"/>, <cmdref targ="ldiff"/>, 
	<cmdref targ="sdiff"/> e <cmdref targ="square"/>.  No contexto de
	uma expressão <lit>genr</lit>, as variáveis existentes têm que ser 
	referenciadas por nome e não por número ID.  A expressão deve ser uma 
	combinação bem construída de nomes de variáveis, constantes, operadores
	e funções (descrito adiante).  Note que detalhes adicionais sobre alguns
	aspetos deste comando podem ser encontrados em <guideref targ="chap:genr"/>.
      </para>

      <para context="gui">
	Use esta caixa de txto para definir uma nova variável, seguindo o padrão
	<repl>nome</repl> = <repl>expressão</repl>.  A expressão deve ser uma 
	combinação bem construída de nomes de variáveis, constantes, operadores
	e funções (descrito adiante).  Para garantir que o tipo de variável
	criada é o desejado, você pode anteceder o nome com o tipo, como
	sejam, <lit>scalar</lit>, <lit>series</lit> ou <lit>matrix</lit>.  Por
	exemplo, para criar uma série que tenha um valor constante de 10, você
	pode escrever
      </para>
      <code context="gui">
	series c = 10
      </code>
      <para context="gui">
	(de outro modo <lit>c = 10</lit> resultaria numa variável escalar).
      </para>

      <para context="cli">
	Um comando <lit>genr</lit> pode resultar tanto num escalar como numa
	série.  Por exemplo, a expressão <lit>x2 = x * 2</lit> naturalmente
	resulta numa série se a variável <lit>x</lit> for uma série e num 
	escalar se <lit>x</lit> for um escalar.  As expressões <lit>x = 0</lit>
	e <lit>mx = mean(x)</lit> naturalmente retornam escalares.  Em alguma
	circusntâncias você poderá querer ter um resultado escalar expandido 
	numa série ou num vetor.  Você pode fazer isso usando <lit>series</lit>
	como um <quote>aliás</quote> para o comando <lit>genr</lit>.  Por 
	exemplo, <lit>series x = 0</lit> produz uma série em que todos os 
	valores são 0.  Você também pode usar <lit>scalar</lit> como sendo
	um aliás para <lit>genr</lit>.  Não é possível forçar um resultado do
	tipo vetor para um escalar mas o uso desta palavra reservada indica
	que o resultado <emphasis>deve ser</emphasis> um escalar: se não for
	ocorrerá um erro.
      </para>

      <para context="cli">
	Quando uma expressão resulta numa série, o intervalo que será 
	escrito na variável destino depende do actual intervalo de amostragem.
	É assim possível, definir uma série por troços usando o comando 
	<lit>smpl</lit> conjugado com <lit>genr</lit>.
      </para>

      <para>
	Os <emphasis>operadores aritméticos</emphasis> suportados são, por
	ordem de precedência: <lit>^</lit> (potenciação);
	<lit>*</lit>, <lit>/</lit> e <lit>%</lit> (resto da divisão inteira);
	<lit>+</lit> e <lit>-</lit>. 
      </para>

      <para>
	Os <emphasis>operadores Booleanos</emphasis> são (mais uma vez,
	por ordem de precedência): <lit>!</lit> (negação),
	<lit>&amp;&amp;</lit> (E lógico), <lit>||</lit> (OU lógico),
	<lit>&gt;</lit>, <lit>&lt;</lit>, <lit>=</lit>, <lit>&gt;=</lit>
	(maior ou igual), <lit>&lt;=</lit> (menor ou igual) e
	<lit>!=</lit> (diferente).  Os operadores Booleanos podem ser 
	usados na construção de variáveis auxiliares ('dummy'): por
	exemplo <lit>(x > 10)</lit> retorna 1 se <lit>x</lit> &gt; 10,
	0 caso contrário.
      </para>

      <para>
	As constantes pré-definidas são <lit>pi</lit> e <lit>NA</lit>.  Esta última
	representa um valor omisso: você pode inicializar uma variável como tendo
	um valor omisso com <lit>scalar x = NA</lit>.
      </para>

      <para>
	O comando <lit>genr</lit> suporta uma larga gama de funções matemáticas
	e estatísticas, incluindo, para além das usuais, várias que são
	especialmente dedicadas à econometria.  Adicionalmente oferece
	acesso a numerosas variáveis internas que são definidas no 
	decorrer das regressões, testes de hipóteses e outros.
	<refnote xref="false"> 
	  Para uma lista de acessores, escrever 
	  <quote>help functions</quote>. 
	</refnote> 
	<refnote xref="true">
	  Para uma lista de funções e acessores, ver 
	  <gfr targ="chap:funcref"/>. 
	</refnote>
      </para>

      <para>
	Para além dos operadores e funções mencionados acima, existem alguns
	usos especiais de <cmd>genr</cmd>:
      </para>

      <ilist>
	<li>
	  <para>
	    <cmd>genr time</cmd> cria uma variável de tendência temporal (1,2,3,&hellip;)
	    com o nome <cmd>time</cmd>. <cmd>genr index</cmd> faz a mesma coisa exceto
	    em que o nome da variável é <lit>index</lit>.
	  </para>
	</li>
	<li>
	  <para>
	    <cmd>genr dummy</cmd> cria variáveis auxiliares ('dummy')
            até à periodicidade dos dados.  No caso de dados trimestrais
	    (periodicidade 4), o programa cria <lit>dq1</lit> = 1 para
	    o primeiro trimestre 0 nos outros timestres, <lit>dq2</lit> = 1 
	    para o segundo trimestre e 0 para os outros trimestres, e por aí
	    adiante.  No caso de dados mensais as variáveis auxiliares têm os
	    nomes <lit>dm1</lit>, <lit>dm2</lit>, e por aí adiante. No caso de
	    outras frequências os nomes são <lit>dummy_1</lit>, 
	    <lit>dummy_2</lit>, etc.
	  </para>
	</li>
	<li>
	  <para>
	    <cmd>genr unitdum</cmd> e <cmd>genr timedum</cmd> criam 
	    conjuntos de variáveis auxiliares especiais para usar com
	    dados de painel. O primeiro codifica para as seções-cruzadas
	    e o segundo para os períodos temporais das observações.
	  </para>
	</li>
      </ilist>

      <para>
	<emphasis>Nota</emphasis>: No programa de linha-de-comandos, 
	os comandos <cmd>genr</cmd> que obtenham dados de modelo
	referem-se sempre ao modelo que foi estimado mais recentemente.
	Isto também é válido para o programa em ambiente gráfico (GUI),
	ao se usar <cmd>genr</cmd> na <quote>consola gretl</quote> ou
	ao introduzir uma expressão usando <quote>Definir nova variável</quote>
	no menu Acrescentar na janela principal.  No entanto, no GUI, você
	tem a possibilidade de obter dados a partir de qualquer modelo que
	esteja disponível numa janela (independentemente se é ou não o modelo
	mais recente).  Isso é feito no menu <quote>Gravar</quote> na 
	janela do modelo.
      </para>

      <para>
	A variável especial <lit>obs</lit> serve como um índice das
	observações.  Por exemplo <lit>genr dum = (obs=15)</lit> irá gerar
	uma variável auxiliar que tem valor 1 para a observação 15 e 0
	para as outras.  Você também pode usar esta variável para escolher
	certas observações por data ou nome.  Por exemplo, 
	<lit>genr d = (obs&gt;1986:4)</lit>, 
	<lit>genr d = (obs&gt;"2008/04/01")</lit>, ou
	<lit>genr d = (obs="CA")</lit>.  Se se usarem datas diárias ou 
	etiquetas neste contexto, elas devem ser indicadas dentro de aspas.
	Datas trimestrais ou mensais (com um dois-pontos) podem ser usadas
	sem aspas.  Note que no caso de dados de séries temporais anuais, o
	ano não se distingue sintáticamente de um simples inteiro; como tal,
	se você quiser comparar observações com <lit>obs</lit> por ano, você
	tem que usar a função <lit>obsnum</lit> para converter o ano para
	um valor de índice iniciado em 1, tal como em <lit>genr d = (obs&gt;obsnum(1986))</lit>.
      </para>

      <para>
	Valores escalares podem ser extraídos de uma série no contexto de uma
	expressão <lit>genr</lit>, usando a sintaxe 
	<repl>varname</repl><lit>[</lit><repl>obs</repl><lit>]</lit>.  O valor
	<repl>obs</repl> pode ser dado por núnmero ou data. Exemplos:
	<lit>x[5]</lit>, <lit>CPI[1996:01]</lit>.  Para dados diários, deve se
	usar a forma <repl>YYYY/MM/DD</repl>, por exemplo, <lit>ibm[1970/01/23]</lit>.
      </para>

      <para>
	Uma observação individual numa série pode ser modificado usando 
	<lit>genr</lit>.  Para fazer isto, uma observação válida numérica ou de data,
	tem que ser acrescentada dentro de parentesis rectos, ao nome da variável no
	lado esquerdo da expressão.  Por exemplo, <lit>genr x[3] = 30</lit>
	ou <lit>genr x[1950:04] = 303.7</lit>.
      </para>

      <table id="tab-genr" title="Exemplos de uso do comando genr"
	lhead="Expressão" rhead="Comentário" lwidth="100pt" rwidth="300pt" 
	style="rpara">
	<row>
	  <cell><lit>y = x1^3</lit></cell>
	  <cell><lit>x1</lit> ao cubo</cell>
	</row>          
	<row>
	  <cell><lit>y = ln((x1+x2)/x3)</lit></cell>
	  <cell></cell>
	</row>
	<row>
	  <cell><lit>z = x&gt;y</lit></cell>
	  <cell><lit>z(t)</lit> = 1 if <lit>x(t) &gt; y(t)</lit>,
	    caso contrário 0</cell>
	</row> 
	<row>
	  <cell><lit>y = x(-2)</lit></cell>
	  <cell><lit>x</lit> desfasado 2 períodos</cell>
	</row>     
	<row>
	  <cell><lit>y = x(+2)</lit></cell>
	  <cell><lit>x</lit> adiantado 2 períodos</cell>
	</row>
	<row>
	  <cell><lit>y = diff(x)</lit></cell>
	  <cell><lit>y(t) = x(t) - x(t-1)</lit></cell>
	</row>
	<row>
	  <cell><lit>y = ldiff(x)</lit></cell>
	  <cell><lit>y(t) = log x(t) - log x(t-1)</lit>, o
	    rácio de crescimento instantâneo de <lit>x</lit></cell>
	</row>
	<row>
	  <cell><lit>y = sort(x)</lit></cell>
	  <cell>ordena <lit>x</lit> por ordem crescente e guarda em
	    <lit>y</lit></cell>
	</row>
	<row>
	  <cell><lit>y = dsort(x)</lit></cell>
	  <cell>ordena <lit>x</lit> por ordem decrescente</cell>
	</row>
	<row>
	  <cell><lit>y = int(x)</lit></cell>
	  <cell>guarda a parte inteira de <lit>x</lit> em 
	    <lit>y</lit></cell>
	</row>
	<row>
	  <cell><lit>y = abs(x)</lit></cell>
	  <cell>guarda os valores absolutos de <lit>x</lit></cell>
	</row>
	<row>
	  <cell><lit>y = sum(x)</lit></cell>
	  <cell>soma os valores de <lit>x</lit> excluíndo entradas <lit>NA</lit>
	    de valores omissos</cell>
	</row>
	<row>
	  <cell><lit>y = cum(x)</lit></cell>
	  <cell>acumulado: 
		<equation status="inline"
		  tex="$y_t = \sum_{\tau=1}^t x_{\tau}$"
		  ascii="y(t) = a soma de s=1 a s=t de x(s)"
		  graphic="cumulate"/>
	  </cell>
	</row>
	<row>
	  <cell><lit>aa = $ess</lit></cell>
	  <cell>define <lit>aa</lit> igual ao Erro da Soma de Quadrados
	    da última regressão</cell>
	</row>
	<row>
	  <cell><lit>x = $coeff(sqft)</lit></cell>
	  <cell>obtém o coeficiente estimado da variável 
	    <lit>sqft</lit> da última regressão</cell>
	</row>
	<row>
	  <cell><lit>rho4 = $rho(4)</lit></cell>
	  <cell>obtém o coeficiente autoregressivo de quarta-ordem
	    do último modelo (assume um modelo <lit>ar</lit>)</cell>
	</row>
	<row>
	  <cell><lit>cvx1x2 = $vcv(x1, x2)</lit></cell>
	  <cell>obtém a covariância estimada dos coeficientes das
	     variáveis <lit>x1</lit> e <lit>x2</lit> do último modelo</cell>
	</row>
	<row>
	  <cell><lit>foo = uniform()</lit></cell>
	  <cell>variável pseudo-aleatória uniforme no intervalo
	     0&ndash;1</cell>
	</row>
	<row>
	  <cell><lit>bar = 3 * normal()</lit></cell>
	  <cell>variável pseudo-aleatória normal, &mu; = 0, &sigma; =
	    3</cell>
	</row>
	<row>
	  <cell><lit>samp = ok(x)</lit></cell>
	  <cell>= 1 para observações onde <lit>x</lit> não está
	    ausente.</cell>
	</row>
      </table>

    </description>

    <gui-access>
      <menu-path>/Acrescentar/Definir nova variável</menu-path>
      <other-access>Menu de contexto da janela principal</other-access>
    </gui-access>

  </command>

  <command name="genrand" section="Programming" context="gui"
    label="Gerador de variáveis aleatórias">

    <description>
      <para>
	Neste diálogo você tem que fornecer um nome para a variável
	a ser criada, mais alguma informação adicional depedendo da
	distribuição.
      </para>

      <ilist>
	<li>
	  <para>
	    Uniforme: os limites inferior e superior para a distribuição.
	  </para>
	</li>
	<li>
	  <para>
	    Normal: a média e o desvio padrão (positivo).
	  </para>
	</li>
	<li>
	  <para>
	    Qui-quadrado e t de Student: os graus de liberdade, que têm que ser
	    positivos.
	  </para>
	</li>
	<li>
	  <para>
	    F: numerador, denonimador e o grau de liberdade.
	  </para>
	</li>
	<li>
	  <para>
	    gama: parâmetros de forma e de escala (ambos positivos).
	  </para>
	</li>
	<li>
	  <para>
	    Binomial: a probabilidade de <quote>sucesso</quote> e o
	    inteiro positivo para o número de experiências.
	  </para>
	</li>
	<li>
	  <para>
	    Poisson: a média positiva (que também é igual à variância).
	  </para>
	</li>
      </ilist>

      <para>
	Se você quiser gerar sequências repetíveis de números pseudo-aleatórios,
	você pode definir a semente no menu Ferramentas.
      </para>

    </description>
  </command>

  <command name="genseed" section="Programming" context="gui"
    label="Definindo a semente dos números aleatórios">

    <description>
      <para>
	A "semente" controla o ponto inicial da sequência dos
	números pseudo-aleatórios gerados num dada sessão gretl.  Por
	omissão a semente é definida usando a hora do sistema, quando
	o programa é iniciado.  Isto garante que você obtém uma sequência
	diferente de número aleatórios de cada vez que iniciar o programa.
	Se você quiser obter sequências repetíveis, você precisa de
	definir manualmente a semente (tomando nota do valor utilizado).
      </para>
      <para>
	Note que sempre que você clicar "OK" nesta janela de diálogo,
	o gerador é reiniciado usando a semente fornecida.  Portanto,
	por exemplo, se você (a) definir a semente para (digamos) 147;
	(b) gerar uma série com distribuição normal; (c) voltar a esta
	janela de diálogo e clicar "OK" outra vez com a semente ainda a
	147; e depois (d) gerar uma segunda série com a distribuição
	normal, as duas séries serão iguais.
      </para>
    </description>
  </command>

  <command name="gmm" section="Estimation" label="Estimação GMM">

    <usage>
      <options>
	<option>
	  <flag>--two-step</flag>
	  <effect>estimação em duas fases</effect>
	</option>
	<option>
	  <flag>--iterate</flag>
	  <effect>GMM iterado</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar detalhes das iterações</effect>
	</option>
	<option>
	  <flag>--lbfgs</flag>
	  <effect>usar L-BFGS-B em vez do normal BFGS</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Faz estimação usando o Método dos Momentos Generalizado, 
	'Generalized Method of Moments' (GMM) com o algoritmo 
	BFGS (Broyden, Fletcher, Goldfarb, Shanno). Você tem que
	especificar um ou mais comandos para a atualização das quantidades
	relevantes (tipicamente os resíduos GMM), um ou mais conjuntos
	das condições, uma matriz inicial dos pesos, e uma listagem dos
	parâmetros a serem estimados, tudo entre os marcadores 
	<lit>gmm</lit> e <lit>end gmm</lit>. Quaisquer opções devem ser
	acrescentadas à linha <lit>end gmm</lit> .
      </para>
      <para>
	Por favor veja mais detalhes sobre este comando em <guideref targ="chap:gmm"/>.
	Aqui apenas ilustramos com um exemplo simples.
      </para>
      <code>
	gmm e = y - X*b
	  orthog e ; W
	  weights V
	  params b
	end gmm
      </code>
      <para>
	No exemplo acima nós assumimos que <lit>y</lit> e <lit>X</lit>
	são matrizes, <lit>b</lit> é um vetor de tamanho apropriado dos
	valores dos parâmetros, <lit>W</lit> é a matriz dos instrumentos,
	e <lit>V</lit> é uma matriz adequada de pesos.  A declaração
      </para>
      <code>
	orthog e ; W
      </code>
      <para>
	indica que o vetor dos resíduos <lit>e</lit> é em princípio
	ortognal a cada um dos instromentos que compõem as colunas de
	<lit>W</lit>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Modelo/GMM</menu-path>
    </gui-access>

  </command>

  <command name="gnuplot" section="Graphs" 
    label="Criar um gráfico gnuplot" context="cli">

    <usage>
      <arguments>
        <argument>variáveis-y</argument>
        <argument>variável-x</argument>
	<argument optional="true">variável-auxiliar</argument>
      </arguments>
      <options>
        <option>
	  <flag>--with-lines</flag>
	  <optparm optional="true">especificação-de-variáveis</optparm>
	  <effect>usar linhas, e não pontos</effect>
        </option>
        <option>
	  <flag>--with-lp</flag>
	  <optparm optional="true">especificação-de-variáveis</optparm>
	  <effect>usar linhas e pontos</effect>
        </option>
        <option>
	  <flag>--with-impulses</flag>
	  <optparm optional="true">especificação-de-variáveis</optparm>
	  <effect>usar linhas verticais</effect>
        </option>
        <option>
	  <flag>--time-series</flag>
	  <effect>gráfico temporal</effect>
        </option>
        <option>
	  <flag>--suppress-fitted</flag>
	  <effect>não mostrar a linha ajustada</effect>
        </option>
        <option>
	  <flag>--single-yaxis</flag>
	  <effect>forçar o uso de apenas um eixo y</effect>
        </option>
        <option>
	  <flag>--linear-fit</flag>
	  <effect>mostrar o ajustamento por mínimos quadrados</effect>
        </option>
        <option>
	  <flag>--inverse-fit</flag>
	  <effect>mostrar o ajustamento inverso</effect>
        </option>
        <option>
	  <flag>--quadratic-fit</flag>
	  <effect>mostrar o ajustamento quadrático</effect>
        </option>
	<option>
	  <flag>--cubic-fit</flag>
	  <effect>mostrar o ajustamento cúbico</effect>
        </option>
        <option>
	  <flag>--loess-fit</flag>
	  <effect>mostrar o ajustamento loess</effect>
        </option>
        <option>
	  <flag>--semilog-fit</flag>
	  <effect>mostrar o ajustamento semilog</effect>
        </option>
        <option>
	  <flag>--dummy</flag>
	  <effect>ver abaixo</effect>
        </option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>nome</optparm>
	  <effect>representar as colunas da matriz indicada</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>nome-de-ficheiro</optparm>
	  <effect>enviar a saída para o ficheiro especificado</effect>
        </option>
        <option>
	  <flag>--input</flag>
	  <optparm>nome-de-ficheiro</optparm>
	  <effect>obter entrada a partir do ficheiro especificado</effect>
        </option>
      </options>
      <examples>
        <example>gnuplot y1 y2 x</example>
        <example>gnuplot x --time-series --with-lines</example>
	<example>gnuplot wages educ gender --dummy</example>
	<example>gnuplot y x --fit=quadratic</example>
	<example>gnuplot y1 y2 x --with-lines=y2</example>
      </examples>
    </usage>

    <description>
      <para>
	As variáveis na lista <repl>variáveis-y</repl> são representadas
	contra a <repl>variável-x</repl>.  Para um gráfico de série 
	temporal você pode indicar <lit>tempo</lit> como sendo a 
	<repl>variável-x</repl> ou usar a opção <opt>--time-series</opt>.
      </para>
      <para>
	Por omissão os dados são representados como pontos; isto pode ser
	alterado com o uso de uma das opções <opt>--with-lines</opt>, 
	<opt>--with-lp</opt> ou	<opt>--with-impulses</opt>. Se for representada
	mais que uma variável no eixo dos <math>y</math>, o efeito destas opções
	pode ficar confinada a uma subconjunto de variáveis usando o parâmetro
	<repl>especificação-de-variáveis</repl>. Isto deve tomar a forma de uma
	lista separada por vírgulas dos nomes ou números das variáveis a serem
	representadas por linhas ou impulsos respetivamente. O último exemplo 
	mostrado acima, mostra como fazer um gráfico de <lit>y1</lit> e 
	<lit>y2</lit> contra <lit>x</lit>, de modo a que <lit>y2</lit> é representada por
	uma linha, mas <lit>y1</lit> é por pontos.
      </para>
      <para>
	Se a opção <opt>--dummy</opt> tiver sido indicada, terão que ser
	dadas exatamente três variáveis: uma única variável <math>y</math>,
	uma variável <math>x</math>, e uma variável 
	<repl>variável-auxiliar</repl>, uma variável discreta.  O efeito é
	o de representar as <repl>variáveis-y</repl> contra <repl>variável-x</repl>
	com os pontos mostrados com diferentes cores dependendo do valor da
	<repl>variável-auxiliar</repl> na respectiva observação.
      </para>

      <para context="cli">
	Geralmente, as <repl>variáveis-y</repl> e <repl>variável-x</repl> 
	referem-se a séries no conjunto de dados corrente (tanto referenciadas
	por nome como por número ID).  Mas se o nome de uma matriz for indicado
	com a opção <opt>--matrix</opt> estes argumentos (que têm que ser dados
	como valores numéricos) indicam indices de colunas (iniciados em 1) para
	a matriz fornecida. Assim, por exemplo, se você quiser um gráfico X-Y
	da coluna 2 da matriz <lit>M</lit> contra a coluna 1, você deve usar:
      </para>
      <code context="cli">
	gnuplot 2 1 --matrix=M
      </code>
      <subhead>Mostrar a linha de melhor ajuste</subhead>
      <para>
	Em modo interativo o gráfico é apresentado imediatamente. Em modo de 
	sequência de comandos o comportamento por omissão é o de criar um 
	ficheiro de script gnuplot na directoria de trabalho do utilizador,
	com um nome seguindo o padrão <filename>gpttmpN.plt</filename>, 
	iniciando com N = <lit>01</lit>. Os gráficos podem ser depois gerados
	usando o programa <program>gnuplot</program> (em MS Windows,
	 <program>wgnuplot</program>).  Este comportamento pode ser modificado
	com o uso da opção <opt>--output=</opt><repl>nome-de-ficheiro</repl>.
	Esta opção controla o nome do ficheiro usado, e ao mesmo tempo permite-lhe
	especificar um formato de saída de acordo com a extensão de três letras
	no nome do ficheiro, sendo: <lit>.eps</lit> resultante na produção de um
	ficheiro 'Encapsulated PostScript' (EPS); <lit>.pdf</lit> produz PDF; 
	<lit>.png</lit> produz no formato PNG, <lit>.emf</lit> em formato EMF
	('Enhanced MetaFile'), <lit>.fig</lit> no formato Xfig, e <lit>.svg</lit>
	para o formato SVG ('Scalable Vector Graphics'). Se usado o nome de ficheiro
	<quote><lit>display</lit></quote> o gráfico é apresentado no écran tal como
	em modo interativo. Se o nome de ficheiro tiver outra qualquer extensão que
	não as mencionadas, será escrito um ficheiro de script gnuplot.
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>linear</lit>: show the OLS fit regardless of its
	    level of statistical significance.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>none</lit>: don't show any fitted line.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>inverse</lit>, <lit>quadratic</lit>,
	    <lit>cubic</lit>, <lit>semilog</lit> or <lit>linlog</lit>:
	    show a fitted line based on a regression of the specified
	    type. By <lit>semilog</lit>, we mean a regression of log
	    <math>y</math> on <math>x</math>; the fitted line
	    represents the conditional expectation of <math>y</math>,
	    obtained by exponentiation. By <lit>linlog</lit> we mean a
	    regression of <math>y</math> on the log of <math>x</math>.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>loess</lit>: show the fit from a robust locally
	    weighted regression (also is sometimes known as
	    <quote>lowess</quote>).
	  </para>
	</li>	
      </ilist>
      <subhead>Representando uma banda</subhead>
      <para>
	As várias opções de <quote>ajustamento</quote> são aplicáveis
	apenas nos caso de gráficos de dispersão bivariados e nos de uma
	única série-temporal. O comportamento por omissão para um gráfico
	de dispersão é o de mostrar a linha de ajustamento de mínimos
	quadrados se e só se o coeficiente do declive fôr significativo
	num nível de 10 porcento. Se a opção <opt>--suppress</opt>
	tiver sido dada, não será mostrada a linha ajustada. O comportamento
	por omissão para um gráfico de série-temporal é o de não mostrar a 
	linha de ajustamento.
	Se a opção <opt>--linear</opt> fôr dada, a linha mínimos quadrados
	será mostrada independentemente de ser significativa ou não.  As outras
	opções de ajustamento (<opt>--inverse</opt>, <opt>--quadratic</opt>, 
	<opt>--cubic</opt>, <opt>--loess</opt> e <opt>--semilog</opt>) produzem 
	respetivamente um ajustamento inverso (regressão de <math>y</math> sobre
	 1/<math>x</math>), um ajustamento quadrático, um ajustamento cúbico, um
	 ajustamento loess e um ajustamento semilog. Loess (também por vezes chamado
	<quote>lowess</quote>) é uma regressão robusta com pesos locais. Por semilog, 
	nós designamos uma regressão do logaritmo de <math>y</math> sobre <math>x</math>
	(ou tempo); a linha ajustada representa o <math>y</math> esperado condicionalmente,
	obtido por exponenciação.
      </para>
      <code>
	        gnuplot y --time-series --band=y,se_y,1.96 --with-lines
      </code>
      <para>
	Uma outra opção está disponível para este comando:
	a seguir às especificações das variáveis a serem representadas
	e das opções (caso hajam), você pode acrescentar comandos 
	gnuplot para controlar a aparência do gráfico (por exemplo,
	para definir o título e/ou as escalas dos eixos).  Estes comandos
	devem ser colocados dentro de chavetas, e cada comando gnuplot
	tem que ser terminado com um ponto-e-vírgula.  Um '\' pode ser
	usado para continuar uma conjunto de comandos gnuplot por mais que
	uma linha.
	Aqui está um exemplo da sintaxe:
      </para>

      <para>
	<lit>{ set title 'O Meu Título'; set yrange [0:1000]; }</lit>
      </para>
      <code>
	eval readfile("@gretldir/data/gnuplot/gpcolors.txt")
      </code>
      <subhead>Controlando o resultado</subhead>
      <para>??????
      </para>
      <subhead>Especificando unha fonte</subhead>
      <para>
	Podes utilizar a opção <opt>font</opt> para especificar uma fonte
	concreta para o gráfico. O parámetro <repl>espfonte</repl> deve
	ter a forma do nome duma fonte, seguida opcionalmente por um
	número que indique o tamanho em pontos, separado do nome por uma
	vírgula ou espaço, tudo dentro de aspas, como em
      </para>
      <code>
	--font="serif,12"
      </code>
      <para>
	Tem em conta que as fontes disponíveis para Gnuplot variam dependendo
	da plataforma, e se estás escrevendo uma instrução de gráfico que
	pretendes que seja portável, é melhor restringir o nome da fonte
	às genéricas <lit>sans</lit> ou <lit>serif</lit>.
      </para>
      <subhead>Agregando instruções Gnuplot</subhead>
      <para>
	Depois de escrita uma instrução Gnuplot, ela pode ser seguida
	da especificação de variáveis a desenhar e do indicador de opção
	(caso exista), podes agregar instruções literais de Gnuplot
	para controlar a aparência do gráfico (por exemplo, definindo
	o título da gráfico e/ou intervalos dos eixos). Estas instruções devem
	de estar dentro de chavetas, e deves terminar cada instrução
	Gnuplot com ponto e vírgula. Podes utilizar uma barra invertida para
	continuar um conjunto de instruções Gnuplot ao longo de mais do que uma
	linha. Aqui está um exemplo da sintaxe:
      </para>
      <code>
	{ set title 'O meu Título'; set yrange [0:1000]; }
      </code>

    </description>

    <gui-access>
      <menu-path>/Ver/Gráfico das variáveis</menu-path>
      <other-access>Menu de contexto na janela principal, botão de gráfico na barra de ferramentas</other-access>
    </gui-access>

  </command>

  <command name="graphing" section="Graphs" context="gui"
    label="Representação Gráfica">

    <description>

      <para>Gretl chama um programa separado, designadamente gnuplot,
	para gerar gráficos.  Gnuplot é um programa de gráficos
	completo com muitas de opções.  Gretl dá-lhe acesso direto,
	por intermédio de um interface gráfico, a um subconjunto destas
	opções e tenta escolher valores adequados para si;  também
	permite-lhe controlar completamente os detalhes dos gráficos
	se assim o desejar.</para>

      <para>Com um gráfico apresentado, você pode clicar na janela do
	gráfico para aceder a um menu de contexto com as seguintes opções:
      </para>

      <ilist>
	<li><para>Gravar como 'postscript' (EPS): grava o gráfico no formato
	    'postscript' encapsulado (EPS)</para>
	</li>
	<li><para>Gravar como PNG: grava o gráfico no formato 
	    'Portable Network Graphics'</para>
	</li>
	<li><para>Gravar como PDF: grava o gráfico no formato 
	    'Portable Document Format'</para>
	</li>
	<li><para>Gravar como metaficheiro do Windows (EMF): grava o gráfico no formato 
	    'Enhanced Metafile Format', a cores ou monocromático</para>
	</li>
	<li><para>Guardar para a sessão como ícone: o gráfico aparecerá
	    na forma de ícone quando você selecionar <quote>Ver/Por Ícones</quote> do
            menu da janela principal</para>
	</li>
	<li><para>Ampliar: deixa você selecionar uma área dentro do gráfico
	para melhor inspeção. Depois pode substituir o gráfico inicial ou restaurar
	a vista normal.</para>
	</li>
	<li><para>Imprimir: (apenas no ambiente gráfico Gnome e no MS Windows)
	    permite-lhe imprimir o gráfico diretamente</para>
	</li>
	<li><para>Copia para a memória de edição: (apenas no MS Windows) permite-lhe
	    colar o gráfico em aplicações Windows como seja o MS Word</para>
	</li>
	<li><para>Editar: abre uma janela de controlo do gráfico que 
	    lhe permite ajustar os vários aspetos da sua aparência
	    </para>
	</li>
	<li><para>Fechar: fecha a janela do gráfico</para>
	</li>
      </ilist>

      <para>
	Se você conhece algo sobre o gnuplot e deseja obter melhor controlo
	sobre a aparência sobre o gráfico do que a que é disponibilizada
	no controlador gráfico (<quote>Editar</quote> option), você tem
	outras opção:
      </para>

      <ilist>
	<li>
	  <para>
	    Assim que o gráfico é guardado para a sessão como ícone, você pode clicar com
	    o botão direito sobre o ícone para aceder a um menu de contexto.  Uma das
	    opções é <quote>Editar comandos do gráfico</quote>, o que abre uma janela
            de edição com os comandos gnuplot utilizados nesse gráfico. Você pode editar
	    esses comandos e gravá-los para uso futuro ou enviar para o gnuplot (com o ícone
	    de execução na barra de ferramentas da janela de edição).
	  </para>
	</li>
	<li>
	  <para>
	    Another way to save the plot commands (or to save
	    the displayed plot in formats other than EPS or PNG) is to
	    use <quote>Edit</quote> item on a graph's pop-up menu to
	    invoke the graphical controller, then click on the
	    <quote>Output to file</quote> tab in the controller.  You
	    are then presented with a drop-down menu of formats in
	    which to save the graph.
	  </para>
	</li>
      </ilist>

      <para>
	Para saber mais sobre gnuplot, ver http://www.gnuplot.info
      </para>

    </description>

  </command>

  <command name="graphpg" section="Graphs" label="Página dos gráficos de Gretl">

    <usage>
      <altforms>
        <altform><lit>graphpg add</lit></altform>
	<altform><lit>graphpg fontscale </lit><repl>value</repl></altform>
	<altform><lit>graphpg show</lit></altform>
	<altform><lit>graphpg free</lit></altform>
	<altform><lit>graphpg --output=</lit><repl>filename</repl></altform>
      </altforms>
    </usage>

    <description>

      <para>
	A <quote>página dos gráficos</quote> de sessão apenas funcionará se você
	tiver instalado o sistema de produção de texto &latex;, e puder gerar e 
	visionar documentos PDF ou PostScript.
      </para>
      <para>
	Na janela de sessão por ícones, você pode arrastar até oito gráficos
	para dentro de um ícone de página de gráficos.  Quando você fizer
	duplo-clique na página de gráficos (ou com o botão direito e selecionar 
	<quote>Mostrar</quote>), será produzida uma página com os gráficos
	selecionados e apresentada no visionador adequado.  A partir deste você
	poderá imprimir a página.
      </para>
      <para>
	Para limpar a página de gráficos, clicar com o botão direito no seu ícone e
	selecionar <quote>Limpar</quote>.
      </para>
      <para>
	Note que em sistemas diferentes do MS Windows, você pode ter que
	ajustar as definições dos programas usados para visionar documentos
	PDF ou PostScript. Isso encontra-se dentro do separador <quote>Programas</quote>
	na janela de diálogo das Preferências do gretl (a partir do menu Ferramentas da
	janela principal).
      </para>
      <para>
	Também é possível trabalhar com a página de gráficos a partir
	se sequência de comandos, ou usando a consola (dentro do programa em
	ambiente gráfico). São suportados os seguintes comandos e opções:
      </para>
      <para>
	Para acrescentar um gráfico à página de gráficos, dê o comando
	<lit>graphpg add</lit> depois de o ter gravado como um gráfico com
	nome, tal como
      </para>
      <code>
	grf1 &lt;- gnuplot Y X
	graphpg add
      </code>
       <para>
	Para ver a página de gráficos: <lit>graphpg show</lit>.
      </para>
      <para>
	Para limpar a página de gráficos: <lit>graphpg free</lit>.
      </para>
      <para>
	Para ajustar a escala da fonte usada na página de gráficos, use
	<lit>graphpg fontscale</lit> <repl>escala</repl>, onde
	<repl>escala</repl> é um multiplicador (com o valor 1,0 por omissão).
	Assim, para tornar a o fonte 50 porcento maior que a por inicial você
	pode
      </para>
      <code>
	graphpg fontscale 1.5
      </code>
      <para>
	Para chamar a impressão da página de gráficos para um ficheiro, use a opção
	<opt>output=</opt> mais um nome de ficheiro; o nome do ficheiro deverá ter 
	o sufixo <quote><lit>.pdf</lit></quote>,
	<quote><lit>.ps</lit></quote> ou
	<quote><lit>.eps</lit></quote>. Por exemplo:
      </para>
      <code>
	graphpg --output="meu_ficheiro.pdf"
      </code>
      <para>
	Neste contexto, por omissão o resultado usa linhas coloridas; para
	usar padrões ponto/traço em vez de cores, você pode acrescentar a opção
	<opt>monochrome</opt>.
      </para>

    </description>

  </command>

  <command name="3-D" section="Graphs" context="gui"
    label="Gráficos tridimensionais">

    <description>
      <para>Caso você tenha instalado o gnuplot 3.8 ou superior, pode 
	pode aproveitar a funcionalidade de manipulação de gráficos
	3-D com o rato (rodá-los e expandir ou encolher os eixos).</para>

      <para>Ao criar um gráfico 3-D, note que o eixo Z será mostrado
	como sendo o eixo vertical.  Como tal, se você tiver alguma
	variável dependente que pense poder ser influenciada por duas
	variáveis independentes, você deve colocar a variável dependente
	no eixo Z, e as variáveis independentes nos eixos X e Y.</para>  

      <para>Contrariamente à maior parte dos gráficos de gretl, os gráficos
	3-D são controlados por gnuplot e não pelo gretl.  O menu de edição
	de gráficos do gretl não estará disponível.</para>

    </description>
  </command>

  <command name="gui-funcs" section="Programming" 
	   label="Funções especiais" context="gui">
    <description>
      <para>
	Este diálogo permite-lhe especificar quais as funções de um
	pacote, deverão ter certos papéis especiais.
	Note que uma certa função pode ter apenas um dos seguintes
	papéis, e para se qualificarem como candidatas para um destes
	papéis as funções tem que satisfazer certos critérios.
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>bundle-print</lit>: imprime saídas baseadas no
            conteúdo de um "bundle" produzido pelo seu pacote.
	    Critério: esta função tem que ter como o seu primeiro
            parâmetro um ponteiro-"bundle". Se estiver presente
            um segundo parâmetro ele tem que tomar a forma de
            seletor inteiro que tenha um valor por omissão.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>bundle-plot</lit>: produz um ou mais gráficos usando
            um "bundle" produzido pelo seu pacote. Critério: o mesmo que
	    para <lit>bundle-print</lit>.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>bundle-test</lit>: efetua algum tipo de teste estatístico
            usando um "bundle" produzido pelo seu pacote. Critério: o mesmo que
	    para <lit>bundle-print</lit>.
	  </para>
	</li>	
	<li>
	  <para>
	    <lit>gui-main</lit>: o interface que deve ser apresentado
            aos usuários por omissão quando usado em modo gráfico (GUI).
            Isto é útil apenas quando um pacote tem mais que um interface
            público.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>gui-precheck</lit>: função guardião que retorna
	    0 se a funcionalidade do seu pacote é aplicável no
            contexto corrente, e não-zero caso contrário. Isto destina-se
            a ser utilizado com pacotes que operam num modelo de algum modo,
            para descartar tipos de modelos que não são suportados pelo pacote.
	  </para>
	</li>	
      </ilist>
      <para>
	In addition certain functions may be marked as
	<quote>no-print</quote>. Usually, when a function is invoked
	via the GUI program, gretl opens a window to display its text
	output. By checking this box you are telling gretl not to do
	this, since no text output should be expected.
      </para>
      <para>
	Finally, the <lit>gui-main</lit> function (if any) can be
	marked as <quote>menu-only</quote>. This tells gretl that the
	function in question is specifically designed to be called
	from the GUI menu to which it is attached, and should not be
	presented to users otherwise.
      </para>
    </description>
  </command>

  <command name="gui-htest" section="Tests" context="gui"
    label="Calculador de Estatísticas de Teste">

    <description>
      <para>
	O calculador de estatísticas de teste do gretl determina estatísticas
	de teste e valores p para diversos testes de hipóteses para uma ou duas
        populações.  As entradas necessárias tomam a forma de estatísticas amostrais
        derivadas de uma ou duas amostras, dependendo do teste escolhido.  Estas
        estatísticas podem ser indicadas como valores numéricos.  Em alternativa,
        se você tiver um conjunto de dados aberto, você pode usar gretl para calcular
        estatísticas amostrais para uma ou mais variáveis selecionadas (no caso de
        médias e variâncias, mas não no caso de proporções).
      </para>

      <para>
	Se você quiser basear o seu teste numa variável do conjunto de dados,
        primeiro ative esta opção selecionando a caixa intitulada "Usar variável do
        conjunto de dados".  De seguida o seletor de variáveis fica ativo e você pode
        selecionar a variável.  Quando seleciona uma variável a partir da lista,
        as estatísticas relevantes são automaticamente colocadas nas caixas abaixo.
      </para>

      <para>
	Adicionalmente, além da simples seleção de uma variável, você tem a 
        opção de especificar uma restrição para a variável selecionada (isto é,
        definir uma sub-amostra).  Por exemplo, suponhamos que você tem
	informação de salários na variável "salario" e que também tem uma
	variável auxiliar chamada "sexo" que é igual a 1 para masculino e 0
        para feminino (ou vice versa). Então, no teste para as diferenças de 
        duas médias, você pode selecionar "salario" em ambos os campos, mas
        acrescentar ao campo do topo "(sexo=0)" e ao de baixo "(sexo=1)".
	Isto resultaria num teste para as diferenças entre rendimentos dos
        homens e rendimentos das mulheres.  Note que quando você usa uma
        restrição desta forma, você tem que pressionar a tecla "Enter" para
        obter o cálculo das estatísticas amostrais.  
      </para>

      <para>
	A restrição de sub-amostragem pode ser colocada dentro de parêntesis após
        a variável selecionada, e em geral a restrição toma a forma "var2
	op val", onde var2 é o nome de uma variável no conjunto de dados corrente,
        val é um valor numérico, e op é um operador de comparação escolhido dentre
        =, !=, &lt;, &gt;, &lt;= ou &gt;= (respetivamente igualdade, desigualdade, menor
	que, maior que, menor ou igual, e maior ou igual).  Os espaços separando o operador
        são opcionais.
      </para>

    </description>
  </command>

  <command name="gui-htest-np" section="Tests" context="gui"
    label="Testes Não-paramétricos">

    <description>
      <para>
	No separador <quote>Teste da Diferença</quote> você pode efetuar
        o teste não-paramétrico para a diferença entre duas populações ou
        grupos, o teste específico depende da opção selecionada.
      </para>
      <para>
	Teste dos sinais: Este teste é baseado no fato de que se duas amostras,
	<math>x</math> e <math>y</math>, são obtidas aleatoriamente a partir da
        mesma distribuição, a probabilidade de
	<math>x</math><sub>i</sub> &gt;
	<math>y</math><sub>i</sub>, para cada observação
	<math>i</math>, deve ser igual a 0,5.  A estatística de teste é
	<math>w</math>, o número de observações em que
	<math>x</math><sub>i</sub> &gt;
	<math>y</math><sub>i</sub>. Sob a hipótese nula isto segue uma
        distribuição Binomial com parâmetros
	(<math>n</math>; 0,5), onde <math>n</math> é o número de
        observações.
      </para>
      <para>
	Teste ordinal da soma de Wilcoxon: É executado o teste ordinal da 
        soma de Wilcoxon.  Este teste inicia por ordenar as observações da
        amostra conjunta do menor para o maior, depois obtém a soma das
        ordens para uma das amostras.  As duas amostras não têm que ser da
        mesma dimensão, e se elas forem diferentes será usada a de menor
        tamanho para calcular a soma das ordens.  Sob a hipótese nula de que
        as amostras foram obtidas a partir de populações com igual mediana,
        a distribuição de probabilidade da soma de ordens pode ser calculada
        para quaisquer dimensões de amostras; e para amostras suficientemente
        grandes existe uma aproximação à Normal.
      </para>
      <para>
	Teste ordinal dos sinais de Wilcoxon: É executado o teste ordinal dos 
        sinais de Wilcoxon. Isto é designado pelo emparelhamento de dados de
        modo a que, por exemplo, os valores de uma variável de uma amostra de
        indivíduos antes e depois de algum tratamento.   O teste inicia por
        encontrar as diferenças entre as observações emparelhadas, 
        <math>x</math><sub>i</sub> &minus; <math>y</math><sub>i</sub>, ordenando
        estas diferenças por valor absoluto, depois atribuindo a cada par uma
        ordem com sinal, este sinal concorda com o sinal da diferença.
	Em seguida é calculado <math>W</math><sub>+</sub>, a soma das ordens com
        sinal positivo.  Tal como no teste ordinal da soma, esta estatística tem 
        uma distribuição bem-definida sob a hipótese nula de que a diferença de
        medianas é zero, o que converge para a Normal em amostras de tamanho 
        razoável.
      </para>
      <para>
	No separador <quote>Teste de Aleatoriedade</quote> você pode efetuar
        um teste para a aleatoriedade de uma dada variável, baseada no número
        de sequências consecutivas de valores positivos ou negativos.  Se você
        selecionar a opção <quote>Usar a primeira diferença</quote>, a variável
        é diferenciada antes da análise e como tal as sequências são interpretadas
        como sendo sequências crescentes ou decrescentes de valores da variável
        original.  A estatística de teste é baseada na aproximação normal à
        distribuição do número de sequências segundo a nulidade de aleatoriedade.
      </para>

    </description>
  </command>   
 
  <command name="hausman" section="Tests" label="Diagnósticos de Painel">

    <description>
      <para>
	Este teste apenas está disponível após e estimar um modelo de
        mínimos quadrados (OLS) usando dados de painel (ver também
        <cmd>setobs</cmd>).  Ele testa o modelo de amostragem simples
        ("pooled") contra as alternativas principais, os modelos de efeitos
        fixos e efeitos aleatórios.
      </para>

      <para>
	O modelo de efeitos fixos permite variar a interseção da regressão
        ao longo das unidades de seção cruzada.  Uma estatística 
        teste-<math>F</math> é apresentada segundo a hipótese nula de que
        as interseções não diferem.  O modelo de efeitos aleatórios
        decompõe a variância dos resíduos em duas partes, uma parte 
        específica à unidade de seção cruzada e outra específica para
        a observação em particular. (Este estimador pode ser calculado
        apenas se o número de unidades de seção cruzada nos dados exceder o
        número de parâmetros a serem estimados.) A estatística de teste 
        Breusch&ndash;Pagan LM, testa a hipótese nula de que o estimador
        mínimos quadrados de amostragem ("pooled") é adequado em oposição ao
        da alternativa de efeitos aleatórios.
      </para>

      <para>
	O modelo mínimos quadrados de amostragem ("pooled") pode ser
        rejeitado contra ambas as alternativas, efeitos fixos e efeitos
        aleatórios.  Desde que o erro específico por unidade ou por grupo seja não
        correlacionado com as variáveis independentes, o estimador de 
        efeitos aleatórios é mais eficiente do que o estimador de efeitos
        fixos; caso contrário o estimador de efeitos aleatórios é 
        inconsistente e o estimador de efeitos fixos será preferido.  A
        hipótese nula para o teste de Hausman é de que o erro específico de grupo
        não é tão correlacionado (e como tal o modelo de efeitos aleatórios é
        preferível).  Um valor p baixo para este teste conta contra o modelo de
        efeitos aleatórios e a favor do modelo de efeitos fixos.
      </para>
    </description>

    <gui-access>
      <menu-path>Janela do modelo, /Testes/Diagnósticos de Painel</menu-path>
    </gui-access>

  </command>

  <command name="hccme" section="Estimation" context="gui"
    label="Erros padrão robustos">

    <description>
      <para>
	Você tem disponível várias variantes de cálculo para erros
        padrão que são robustos na presença de heteroscedasticidade
	(e, no caso do estimador HAC, autocorrelação).
      </para>
      <para>
	HC0 produz os <quote>erros padrão de White</quote> originais;
	HC1, HC2, HC3 e HC3a são variações subsequentes que em geral
        são reconhecidas de produzirem melhores (mais fiáveis) resultados.
	Para detalhes dos estimadores, ver <cite
	key="mackinnon-white85">MacKinnon and White (Journal of
	Econometrics, 1985)</cite> ou <cite
	key="davidson-mackinnon04">Davidson and MacKinnon, Econometric
	Theory and Methods (Oxford, 2004)</cite>.  As etiquetas aqui usadas
        são as que Davidson e MacKinnon usaram.  A variante
	<quote>HC3a</quote> é o canivete ("jackknife"), como descrita em MacKinnon
	e White; HC3 é uma boa aproximação ao canivete.
      </para>
      <para>
	Se você usar o estimador HAC para OLS em dados de séries temporais,
        você pode afinar o comprimento de desfasamento usando o comando 
       <cmd>set</cmd>.  Por favor, para mais detalhes, ver o manual do
        arquivo de ajuda das sequências de comandos.
      </para>
      <para>
	Ao se estimar um modelo OLS usando dados de painel, o estimador
	robusto por omissão da matriz de covariância é o dado por Arellano. 
        A alternativa são os Erros Padrão Corrigidos de Painel de 
        Beck e Katz (PCSE).  Este tem em conta a heteroscedasticidade mas
        não a autocorrelação.  
      </para>
      <para>
	Para modelos GARCH são fornecidos dois estimadores robustos para a 
        matriz de covariância: o estimador de Verosimilhança Quasi-Maximum
        (QML), e estimador de Bollerslev-Wooldridge (BW).
      </para>
      <para>
	By default gretl uses the Student <math>t</math> distribution
	when calculating p-values based on robust standard errors in
	the context of least squares estimators. The option labeled
	<quote>Use the normal distribution for robust p-values</quote>
	can be used to change this behavior.
      </para>
    </description>

  </command>

  <command name="heckit" section="Estimation" context="cli"
    label="Modelo de seleção Heckman">

    <usage>
      <arguments>
        <argument>variável-dependente</argument>
        <argument>variáveis-independentes</argument>
	<argument separated="true">equação de seleção</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>suprimir a escrita de resultados</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <effect>erros padrão QML</effect>
        </option>
        <option>
	  <flag>--two-step</flag>
	  <effect>efetuar estimação de dois passos</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>mostrar a matriz de covariância</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>mostrar saídas adicionais</effect>
        </option>
      </options>      
      <examples>
        <example>heckit y 0 x1 x2 ; ys 0 x3 x4</example>
	<demos>
	  <demo>heckit.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Seleção do modelo de tipo Heckman.  Na especificação, a lista
        antes do ponto-e-vírgula representa a equação do resultado.  A 
        variável dependente na equação de seleção (<lit>ys</lit> no 
        exemplo acima) tem que ser uma variável binária.
      </para>
      <para>
	Por omissão, os parâmetros são estimados por máxima verosimilhança.
        A matriz de covariância dos parâmetros é calculada usando a 
        inversão negativa da Hessiana. Se for desejável a estimação,
        de dois passos, use a opção <opt>--two-step</opt>. Neste caso,
        a matriz de covariância dos parâmetros da equação resultante é
        adequadamente ajustada de acordo com <cite key="heckman79">Heckman (1979)</cite>.
      </para>
      <para>
	Repare que na estimação de Máxima Verosimilhança (ML) é usada uma
        aproximação numérica da Hessiana; isto pode levar a inexatidões na
        matriz de covariância estimada se a escala das variáveis explanatórias
        for para alguns dos coeficientes estimados muito pequena em 
        valor absoluto. Este problema pode ser abordado em versões futuras;
        por agora, como solução temporária, pode-se re-escalar a variável 
        ou variáveis explanatórias que estão a causar problemas.
      </para>
    </description>

    <gui-access>
      <menu-path>/Modelo/Variável dependente limitada/Heckit...</menu-path>
    </gui-access>

  </command>

  <command name="help" section="Utilities"
    label="Help on commands" context="cli">

    <usage>
      <altforms>
        <altform><lit>help</lit></altform>
	<altform><lit>help functions</lit></altform>
        <altform><lit>help</lit> <repl>command</repl></altform>
        <altform><lit>help</lit> <repl>function</repl></altform>
      </altforms>
      <options>
	<option>
	  <flag>--func</flag>
	  <effect>select functions help</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	If no arguments are given, prints a list of available commands. If the
	single argument <lit quote="true">functions</lit> is given, prints a
	list of available functions (see <cmdref targ="genr"/>).
      </para>
      <para>
	<lit>help</lit> <repl>command</repl> describes <repl>command</repl>
	(&eg; <lit>help smpl</lit>).  <lit>help</lit> <repl>function</repl>
	describes <repl>function</repl> (&eg; <lit>help ldet</lit>).
	Some functions have the same names as related commands (&eg;
	<lit>diff</lit>): in that case the default is to print help
	for the command, but you can get help on the function by
	using the <opt>func</opt> option.
      </para>
    </description>

    <gui-access>
      <menu-path>/Help</menu-path>
    </gui-access>

  </command>

  <command name="hfplot" section="Graphs"
    label="Create a MIDAS plot" context="cli">

    <usage>
      <arguments>
        <argument>hflist</argument>
	<argument optional="true" separated="true">lflist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--with-lines</flag>
	  <effect>plot with lines</effect>
	</option>
	<option>
	  <flag>--time-series</flag>
	  <effect>put time on x-axis</effect>
	</option>
	<option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Provides a means of plotting a high-frequency series, possibly
	along with one or more series observed at the base frequency
	of the dataset.  The first argument should be a <cmdref
	targ="MIDAS_list"/>; the optional additional
	<repl>lflist</repl> terms, following a semicolon, should be
	regular (<quote>low-frequency</quote>) series.
      </para>
      <para>
	For details on the effect of the <opt>output</opt> option,
	please see the <cmdref targ="gnuplot"/> command.
      </para>
    </description>

  </command>

  <command name="hsk" section="Estimation"
    label="Heteroskedasticity-corrected estimates">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--no-squares</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print anything</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	This command is applicable where heteroskedasticity is present in the
	form of an unknown function of the regressors which can be
	approximated by a quadratic relationship.  In that context it offers
	the possibility of consistent standard errors and more efficient
	parameter estimates as compared with OLS.
      </para>
      <para>
	The procedure involves (a) OLS estimation of the model of interest,
	followed by (b) an auxiliary regression to generate an estimate of the
	error variance, then finally (c) weighted least squares, using as
	weight the reciprocal of the estimated variance.
      </para>
      <para context="cli">
	In the auxiliary regression (b) we regress the log of the
	squared residuals from the first OLS on the original
	regressors and their squares (by default), or just on the
	original regressors (if the <opt>no-squares</opt> option is
	given).  The log transformation is performed to ensure that the
	estimated variances are all non-negative.  Call the fitted
	values from this regression <math>u</math><sup>*</sup>.  The
	weight series for the final WLS is then formed as
	1/exp(<math>u</math><sup>*</sup>).
      </para>
      <para context="gui">
	In the auxiliary regression (b) we regress the log of the
	squared residuals from the first OLS on the original
	regressors and their squares (by default), or just on the
	original regressors (if the <quote>include squares</quote> box
	is cleared).  The log transformation is performed to ensure
	that the estimated variances are all non-negative.  Call the
	fitted values from this regression <math>u</math><sup>*</sup>.
	The weight series for the final WLS is then formed as
	1/exp(<math>u</math><sup>*</sup>).
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Other linear models/Heteroskedasticity corrected</menu-path>
    </gui-access>

  </command>

  <command name="hurst" section="Statistics"
    label="Hurst exponent">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--plot</flag>
	  <optparm>mode-or-filename</optparm>
	  <effect>see below</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Calculates the Hurst exponent (a measure of persistence or
	long memory) for a time-series variable having at least 128
	observations. The result (together with its standard error)
	can be retrieved via the <fncref targ="$result"/> accessor.
      </para>
      <para>
	The Hurst exponent is discussed by <cite
	key="mandelbrot83">Mandelbrot (1983)</cite>.  In theoretical
	terms it is the exponent, <math>H</math>, in the relationship
	<equation status="display"
		  tex="\[\mathrm{RS}(x) = an^H\]"
		  ascii="RS(x) = an^H" graphic="hurst"/> where RS is the
	<quote>rescaled range</quote> of the variable <math>x</math>
	in samples of size <math>n</math> and <math>a</math> is a
	constant. The rescaled range is the range (maximum minus
	minimum) of the cumulated value or partial sum of
	<math>x</math> over the sample period (after subtraction of
	the sample mean), divided by the sample standard deviation.
      </para>
      <para>
	As a reference point, if <math>x</math> is white noise
	(zero mean, zero persistence) then the range of its cumulated
	<quote>wandering</quote> (which forms a random walk), scaled
	by the standard deviation, grows as the square root of the
	sample size, giving an expected Hurst exponent of 0.5.  Values
	of the exponent significantly in excess of 0.5 indicate
	persistence, and values less than 0.5 indicate
	anti-persistence (negative autocorrelation).  In principle the
	exponent is bounded by 0 and 1, although in finite samples it
	is possible to get an estimated exponent greater than 1.
      </para>
      <para>
	In gretl, the exponent is estimated using binary sub-sampling:
	we start with the entire data range, then the two halves of
	the range, then the four quarters, and so on.  For sample
	sizes smaller than the data range, the RS value is the mean
	across the available samples.  The exponent is then estimated
	as the slope coefficient in a regression of the log of RS on
	the log of sample size.
      </para>
      <para>
	By default, if the program is not in batch mode a plot of the
	rescaled range is shown.  This can be adjusted via the
	<opt>plot</opt> option. The acceptable parameters to this
	option are <lit>none</lit> (to suppress the plot);
	<lit>display</lit> (to display a plot even when in batch
	mode); or a file name. The effect of providing a file name is
	as described for the <opt>output</opt> option of the <cmdref
	targ="gnuplot"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Hurst exponent</menu-path>
    </gui-access>

  </command>

  <command name="if" section="Programming" label="Flow control" context="cli">

    <description>
      <para>Flow control for command execution.  Three sorts of
	construction are supported, as follows.
      </para>
      <code>
	# simple form
	if condition
	    commands
	endif

	# two branches
	if condition
	    commands1
	else
	    commands2
	endif

	# three or more branches
	if condition1
	    commands1
	elif condition2
	    commands2
	else
	    commands3
	endif
      </code>

      <para>
	<repl quote="true">condition</repl> must be a Boolean expression, for
	the syntax of which see <cmdref targ="genr"/>.  More than one
	<cmd>elif</cmd> block may be included.  In addition, <lit>if</lit>
	&hellip; <lit>endif</lit> blocks may be nested.
      </para>
    </description>

  </command>

  <command name="include" section="Programming"
    label="Include function definitions" context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
      <options>
	<option>
	  <flag>--force</flag>
	  <effect>force re-reading from file</effect>
        </option>
      </options>
      <examples>
        <example>include myfile.inp</example>
        <example>include sols.gfn</example>
      </examples>
    </usage>

    <description>
      <para>
	Intended for use in a command script, primarily for including
	definitions of functions.  <repl>filename</repl> should have
	the extension <lit>inp</lit> (a plain-text script) or
	<lit>gfn</lit> (a gretl function package). The commands in
	<repl>filename</repl> are executed then control is returned to
	the main script.
      </para>
      <para>
	The <opt>force</opt> option is specific to <lit>gfn</lit>
	files: its effect is to force gretl to re-read the function
	package from file even if it is already loaded into memory.
	(Plain <lit>inp</lit> files are always read and processed in
	response to this command.)
      </para>
      <para>
	See also <cmdref targ="run"/>.
      </para>
    </description>

  </command>

  <command name="info" section="Dataset"
    label="Information on data set" context="cli">

    <description>
      <para>
	Prints out any supplementary information stored with the
	current datafile.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Dataset info</menu-path>
      <other-access>Data browser windows</other-access>
    </gui-access>

  </command>

  <command name="intreg" section="Estimation" label="Interval regression model">

    <usage>
      <arguments>
        <argument>minvar</argument>
        <argument>maxvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--opg</flag>
	  <effect>see below</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
      </options>
      <examples>
	<example>intreg lo hi const x1 x2</example>
	<demos>
	  <demo>wtp.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Estimates an interval regression model.  This model arises when the
	dependent variable is imperfectly observed for some (possibly all)
	observations.  In other words, the data generating process is assumed
	to be
	<equation status="display"
	tex="\[y^*_t = x_t \beta+\epsilon_t\]" ascii="y* = x b + u"/> but we
	only observe
	<equation status="inline" tex="\[m_t \le
	y_t \le M_t\]" ascii="m &lt;= y* &lt;= M"/> (the interval may be left-
	or right-unbounded). Note that for some observations <math>m</math>
	may equal <math>M</math>.  The variables <repl>minvar</repl> and
	<repl>maxvar</repl> must contain <lit>NA</lit>s for left- and
	right-unbounded observations, respectively.
      </para>

      <para context="gui">
	In the model specification dialog, <repl>minvar</repl> and
	<repl>maxvar</repl> are indentified as the Lower bound variable and
	the Upper bound variable respectively.
      </para>

      <para>
	The model is estimated by maximum likelihood, assuming normality of
	the disturbance term.
      </para>

      <para context="cli">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <opt>robust</opt> flag is given,
	then QML or Huber&ndash;White standard errors are calculated
	instead. In this case the estimated covariance matrix is a
	<quote>sandwich</quote> of the inverse of the estimated Hessian
	and the outer product of the gradient. Alternatively, the
	<opt>opg</opt> option can be given, in which case standard
	errors are based on the outer product of the gradient alone.
      </para>
      <para context="gui">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the "Robust standard errors" box is
	checked, then QML or Huber&ndash;White standard errors are
	calculated instead. In this case the estimated covariance matrix
	is a <quote>sandwich</quote> of the inverse of the estimated
	Hessian and the outer product of the gradient.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Interval regression</menu-path>
    </gui-access>

  </command>

  <command name="irfboot" section="Graphs" context="gui"
    label="Impulse response plots">

    <description>
      <para>
	If you select the bootstrap option when plotting impulse
	responses, gretl computes a confidence interval for the
	responses using the bootstrap method.  The residuals from the
	original VAR (or VECM) are resampled with replacement; an
	artificial dataset is constructed based on the original
	parameter estimates and the resampled residuals; the system is
	re-estimated and the impulse responses are re-evaluated.  By
	default this is repeated 1999 times and the &alpha;/2 and 1
	&minus; &alpha;/2 quantiles for the responses are found and
	plotted along with the point estimates. This option is not
	currently available for restricted VECMs.
      </para>
      <para>
	This dialog also supports reordering of the variables for the
	Cholesky decomposition of the cross-equation covariance matrix.
	The default is given by the order in which the variables are
	entered into the model specification, but the up and down arrows
	can be used to promote or demote a selected variable.
      </para>
      <para>
	Regarding the scale of the impulse responses: the
	<quote>shock</quote> is sized at one standard deviation of the
	estimated innovations in the source variable, and the
	responses are given in whatever is the <quote>natural</quote>
	unit of the target variable.
      </para>
	  <para>
	  (See the SVAR addon for other shock identification approaches
	  and more bootstrap options.)
	  </para>
    </description>

  </command>

  <command name="johansen" section="Tests" label="Johansen cointegration test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>ylist</argument>
	<argblock optional="true" separated="true">
	  <argument>xlist</argument>
	</argblock>
	<argblock optional="true" separated="true">
	  <argument>rxlist</argument>
	</argblock>
      </arguments>
      <options>
        <option>
	  <flag>--nc</flag>
	  <effect>no constant</effect>
        </option>
        <option>
	  <flag>--rc</flag>
	  <effect>restricted constant</effect>
        </option>
        <option>
	  <flag>--uc</flag>
	  <effect>unrestricted constant</effect>
        </option>
        <option>
	  <flag>--crt</flag>
	  <effect>constant and restricted trend</effect>
        </option>
        <option>
	  <flag>--ct</flag>
	  <effect>constant and unrestricted trend</effect>
        </option>
        <option>
	  <flag>--seasonals</flag>
	  <effect>include centered seasonal dummies</effect>
        </option>
        <option>
	  <flag>--asy</flag>
	  <effect>record asymptotic p-values</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>print just the tests</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>print details of auxiliary regressions</effect>
        </option>
      </options>
      <examples>
        <example>johansen 2 y x</example>
	<example>johansen 4 y x1 x2 --verbose</example>
	<example>johansen 3 y x1 x2 --rc</example>
	<demos>
	  <demo>hamilton.inp</demo>
	  <demo>denmark.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Carries out the Johansen test for cointegration among the
	variables in <repl>ylist</repl> for the given lag order.  For
	details of this test see <guideref targ="chap:vecm"/> or <cite
	key="hamilton94">Hamilton (1994)</cite>, Chapter 20. P-values
	are computed via Doornik's gamma approximation <cite
	key="doornik98" p="true">(Doornik, 1998)</cite>. Two sets of
	p-values are shown for the trace test, straight asymptotic
	values and values adjusted for the sample size. By default the
	<fncref targ="$pvalue"/> accessor gets the adjusted variant,
	but the <opt>asy</opt> flag may be used to record the
	asymptotic values instead.
      </para>

      <para context="gui">
	Carries out the Johansen test for cointegration among the
	listed variables for the selected lag order.  For details of
	this test see, for example, Hamilton, <book>Time Series
	Analysis</book> (1994), Chapter 20.  P-values are computed via
	Doornik's (1998) gamma approximation. Two sets of
	p-values are shown for the trace test, straight asymptotic
	values and values adjusted for the sample size.
      </para>

      <para context="cli">
	The inclusion of deterministic terms in the model is controlled by
	the option flags.  The default if no option is specified is to
	include an <quote>unrestricted constant</quote>, which allows for
	the presence of a non-zero intercept in the cointegrating
	relations as well as a trend in the levels of the endogenous
	variables.  In the literature stemming from the work of Johansen
	(see for example his 1995 book) this is often referred to as
	<quote>case 3</quote>.  The first four options given above, which
	are mutually exclusive, produce cases 1, 2, 4 and 5 respectively.
	The meaning of these cases and the criteria for selecting a case
	are explained in <guideref targ="chap:vecm"/>.
      </para>

      <para context="gui">
	The inclusion of deterministic terms in the model is controlled by
	the drop-down option list.  The default is to include an
	<quote>unrestricted constant</quote>, which allows for the
	presence of a non-zero intercept in the cointegrating relations as
	well as a trend in the levels of the endogenous variables.  In the
	literature stemming from the work of Johansen (see for example his
	1995 book) this is often referred to as <quote>case 3</quote>. The
	other four options produce cases 1, 2, 4 and 5 respectively. The
	meaning of these cases and the criteria for selecting a case are
	explained in <guideref targ="chap:vecm"/>.
      </para>

      <para context="cli">
	The optional lists <repl>xlist</repl> and <repl>rxlist</repl>
	allow you to control for specified exogenous variables: these
	enter the system either unrestrictedly (<repl>xlist</repl>) or
	restricted to the cointegration space (<repl>rxlist</repl>). These
	lists are separated from <repl>ylist</repl> and from each other by
	semicolons.
      </para>

      <para context="gui">
	You may control for exogenous variables by adding them to the
	lower list box.  By default these enter the model in unrestricted
	form (indicated by a <lit>U</lit> next to the name of the
	variable).  If you want a certain exogenous variable to be
	restricted to the cointegrating space, right-click on it and
	select <quote>Restricted</quote> from the pop-up menu.  The symbol
	next to the variable will change to R.
      </para>

      <para context="cli">
	The <opt>seasonals</opt> option, which may be combined with any of the
	other options, specifies the inclusion of a set of centered seasonal
	dummy variables.  This option is available only for quarterly or monthly
	data.
      </para>

      <para context="gui">
	If the data are quarterly or monthly, a check box is shown that allows
	you to include a set of centered seasonal dummy variables.  In all
	cases, an additional check box (<quote>Show details</quote>) allows
	for the printing of the auxiliary regressions that form the starting
	point of the Johansen maximum likelihood estimation procedure.
      </para>

      <para context="notex">
	The following table is offered as a guide to the
	interpretation of the results shown for the test, for the
	3-variable case.  <lit>H0</lit> denotes the null hypothesis,
	<lit>H1</lit> the alternative hypothesis, and <lit>c</lit> the
	number of cointegrating relations.
      </para>
      <mono context="notex">
         Rank     Trace test         Lmax test
                  H0     H1          H0     H1
         ---------------------------------------
          0      c = 0  c = 3       c = 0  c = 1
          1      c = 1  c = 3       c = 1  c = 2
          2      c = 2  c = 3       c = 2  c = 3
         ---------------------------------------
      </mono>
      <para context="tex">
	The following table is offered as a guide to the
	interpretation of the results shown for the test, for the
	3-variable case.  $H_0$ denotes the null hypothesis,
	$H_1$ the alternative hypothesis, and $c$ the
	number of cointegrating relations.

	\begin{center}
	\begin{tabular}{cllll}
	&amp; \multicolumn{2}{c}{Trace test} &amp;
	   \multicolumn{2}{c}{$\lambda$-max test} \\
	Rank &amp;  \multicolumn{1}{c}{$H_0$} &amp;
	       \multicolumn{1}{c}{$H_1$} &amp;
	       \multicolumn{1}{c}{$H_0$} &amp;
	       \multicolumn{1}{c}{$H_1$} \\ [4pt]
 	0 &amp; $c$ = 0 &amp; $c$ = 3 &amp; $c$ = 0 &amp; $c$ = 1 \\
	1 &amp; $c$ = 1 &amp; $c$ = 3 &amp; $c$ = 1 &amp; $c$ = 2 \\
	2 &amp; $c$ = 2 &amp; $c$ = 3 &amp; $c$ = 2 &amp; $c$ = 3
	\end{tabular}
	\end{center}
      </para>

      <para>
	See also the <cmdref targ="vecm"/> command, and <cmdref
	targ="coint"/> if you want the Engle&ndash;Granger
	cointegration test.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Multivariate time series</menu-path>
    </gui-access>

  </command>

  <command name="join" section="Dataset" label="Manage data sources"
	   context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
	<argument>varname</argument>
      </arguments>
      <options>
	<option>
	  <flag>--data</flag>
	  <optparm>column-name</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--filter</flag>
	  <optparm>expression</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--ikey</flag>
	  <optparm>inner-key</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--okey</flag>
	  <optparm>outer-key</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--aggr</flag>
	  <optparm>method</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--tkey</flag>
	  <optparm>column-name,format-string</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>report on progress</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	This command imports one or more data series from the source
	<repl>filename</repl> (which must be either a delimited text
	data file or a <quote>native</quote> gretl data file) under
	the name <repl>varname</repl>. For details please see
	<guideref targ="chap:join"/>; here we just give a brief
	summary of the available options.  See also <cmdref
	targ="append"/> for simpler joining operations.
      </para>
      <para>
	The <opt>data</opt> option can be used to specify the column
	heading of the data in the source file, if this differs from
	the name by which the data should be known in gretl.
      </para>
      <para>
	The <opt>filter</opt> option can be used to specify a
	criterion for filtering the source data (that is, selecting a
	subset of observations).
      </para>
      <para>
	The <opt>ikey</opt> and <opt>okey</opt> options can be used to
	specify a mapping between observations in the current dataset
	and observations in the source data (for example, individuals
	can be matched against the household to which they belong).
      </para>
      <para>
	The <opt>aggr</opt> option is used when the mapping between
	observations in the current dataset and the source is not
	one-to-one.
      </para>
      <para>
	The <opt>tkey</opt> option is applicable only when the current
	dataset has a time-series structure. It can be used to specify
	the name of a column containing dates to be matched to the
	dataset and/or the format in which dates are represented in
	that column.
      </para>
      <subhead>Importing more than one series at once</subhead>
      <para>
	The <cmd>join</cmd> command can handle the importation of
	several series at once. This happens when (a) the
	<repl>varname</repl> argument is a space-separated list of
	names rather than a single name, or (b) when it points to an
	array of strings: the elements of this array should be the
	names of the series to import.
      </para>
      <para>
	This methods has some limitations, however: the
	<opt>data</opt> option is not available. When importing
	multiple series you are obliged to accept their
	<quote>outer</quote> names. The other options apply uniformly
	to all the series imported via a given command.
      </para>
    </description>

  </command>

  <command name="join" section="Dataset" label="Append data with controls"
	   context="gui">
    <description>
    <para>
      This dialog gives you access to some, but not all, of the
      functionality of the <lit>join</lit> command. For full details
      see <guideref targ="chap:join"/>.
    </para>
    <para>
      On the left you should see a listing of series in the current
      dataset. You can select a series here and use the arrow buttons
      to specify it as one or other of the (optional) <quote>inner
      keys</quote>. Keys work to match rows between the current
      dataset and the file from which you are importing data.
    </para>
    <para>
      On the right should be listed the series in the data file you
      selected. The arrow buttons can be used to select from that list
      the name of the series to import, and (if required) the names of
      series that correspond to the <quote>inner</quote> keys. (By
      default the inner and outer keys are presumed to have the same
      name.) Also in this list-box you will see a <quote>dummy</quote>
      entry, <lit>$obsmajor</lit>. This cannot be imported, but it
      may be used as a key; see <fncref targ="$obsmajor"/>.
    </para>
    <para>
      In the middle panel of the dialog you can specify additional
      parameters for the <quote>join</quote> operation:
    </para>
    <ilist>
      <li>
	<para>
	  A name under which the imported series should be known.  (By
	  default this is the same as the <quote>import</quote> name).
	</para>
      </li>
      <li>
	<para>
	  A filter expression. This will be evaluated for each row
	  in the outer dataset, and only rows for which the
	  expression yields a non-zero value will be imported.
	</para>
      </li>
      <li>
	<para>
	  An aggregation method. This is required only if matching by
	  keys selects more than one outer value per inner
	  observation.
	</para>
      </li>
    </ilist>
    <subhead>Time-series data</subhead>
    <para>
      If the current dataset is time series it's likely that the data
      to be joined are also time series. In that case gretl may be
      able to figure out the joining without any help from
      user-specified keys. This is signaled by the place-holder
      string <quote>auto-detect</quote> in the inner-key entry boxes.
      While there's no guarantee that this will do precisely what you
      want it is probably worth trying before you resort to a more
      complicated approach.
    </para>
    </description>
  </command>

  <command name="kalman" section="Utilities"
	   label="State space modeling" context="gui">
    <description>
      <para>
	This graphical interface offers a small sample of the
	functionality available via scripting in gretl's state space
	apparatus.
      </para>
      <para>
	If you're interested in what you see here, please take a look
	at <guideref targ="chap:kalman"/> (titled <quote>State Space
	Modeling</quote>). There you'll find details on how to handle
	time-varying matrices, disturbances that are correlated across
	the observation and state transition equations, and much
	more. You will also find several downloadable example scripts.
	The examples illustrate, among other things, how to hook up
	gretl's Kalman filter to its likelihood maximizer.
      </para>
    </description>
  </command>

  <command name="kpss" section="Tests" label="KPSS stationarity test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--trend</flag>
	  <effect>include a trend</effect>
	</option>
	<option>
	  <flag>--seasonals</flag>
	  <effect>include seasonal dummies</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print regression results</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
	</option>
	<option>
	  <flag>--difference</flag>
	  <effect>use first difference of variable</effect>
	</option>
      </options>
      <examples>
	<example>kpss 8 y</example>
        <example>kpss 4 x1 --trend</example>
      </examples>
    </usage>

    <description>

      <para context="gui">
	Computes the KPSS test (Kwiatkowski, Phillips, Schmidt and Shin,
	Journal of Econometrics, 1992) for stationarity of the given
	variable (or its first difference, if the differencing option is
	selected).  The null hypothesis is that the variable in question
	is stationary, either around a level or, if the <quote>include a
	trend</quote> box is checked, around a deterministic linear trend.
      </para>

      <para context="cli">
	For use of this command with panel data please see the final
	section in this entry.
      </para>

      <para context="cli">
	Computes the KPSS test <cite key="KPSS92" p="true">(Kwiatkowski
	et al, Journal of Econometrics, 1992)</cite> for stationarity,
	for each of the specified variables (or their first difference,
	if the <opt>difference</opt> option is selected). The null
	hypothesis is that the variable in question is stationary,
	either around a level or, if the <opt>trend</opt> option is
	given, around a deterministic linear trend.
      </para>

      <para context="gui">
	The selected lag order determines the size of the window used
	for Bartlett smoothing.  If the <quote>show regression
	  results</quote> box is checked the results of the auxiliary
	regression are printed, along with the estimated variance of
	the random walk component of the variable.
      </para>

      <para context="cli">
	The <repl>order</repl> argument determines the size of the
	window used for Bartlett smoothing. If a negative value is
	given this is taken as a signal to use an automatic window
	size of 4(<math>T</math>/100)<sup>0.25</sup>, where
	<math>T</math> is the sample size.
      </para>

      <para context="cli">
	If the <opt>verbose</opt> option is chosen the results of
	the auxiliary regression are printed, along with the estimated
	variance of the random walk component of the variable.
      </para>

      <para>
	The critical values shown for the test statistic are based on
	response surfaces estimated in the manner set out by <cite
	key="sephton95">Sephton (Economics Letters, 1995)</cite>,
	which are more accurate for small samples than the values
	given in the original KPSS article. When the test statistic
	lies between the 10 percent and 1 percent critical values a
	p-value is shown; this is obtained by linear interpolation and
	should not be taken too literally.  See the <fncref
	targ="kpsscrit"/> function for a means of obtaining these
	critical values programmatically.
      </para>

      <subhead context="cli">Panel data</subhead>

      <para context="cli">
	When the <lit>kpss</lit> command is used with panel data, to
	produce a panel unit root test, the applicable options and the
	results shown are somewhat different.  While you may give a list
	of variables for testing in the regular time-series case, with
	panel data only one variable may be tested per command. And the
	<opt>verbose</opt> option has a different meaning: it produces a
	brief account of the test for each individual time series (the
	default being to show only the overall result).
      </para>
      <para context="cli">
	When possible, the overall test (null hypothesis: the series in
	question is stationary for all the panel units) is calculated
	using the method of <cite key="choi01">Choi (Journal of
	International Money and Finance, 2001)</cite>. This is not
	always straightforward, the difficulty being that while the
	Choi test is based on the p-values of the tests on the
	individual series, we do not currently have a means of
	calculating p-values for the KPSS test statistic; we must
	rely on a few critical values.
      </para>
      <para context="cli">
	If the test statistic for a given series falls between the 10
	percent and 1 percent critical values, we are able to interpolate
	a p-value. But if the test falls short of the 10 percent value, or
	exceeds the 1 percent value, we cannot interpolate and can at best
	place a bound on the global Choi test. If the individual test
	statistic falls short of the 10 percent value for some units but
	exceeds the 1 percent value for others, we cannot even compute
	a bound for the global test.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Unit root tests/KPSS test</menu-path>
    </gui-access>

  </command>

  <command name="labels" section="Dataset"
    label="Labels for variables" context="cli">

    <usage>
      <altforms>
	<altform><lit>labels [</lit> <repl>varlist</repl> <lit>]</lit></altform>
	<altform><lit>labels --to-file=</lit><repl>filename</repl></altform>
	<altform><lit>labels --from-file=</lit><repl>filename</repl></altform>
	<altform><lit>labels --delete</lit></altform>
      </altforms>
	<examples>
	<demos>
	  <demo>oprobit.inp</demo>
	</demos>
    </examples>
    </usage>

    <description>
      <para>
	In the first form, prints out the informative labels (if
	present) for the series in <repl>varlist</repl>, or for all
	series in the dataset if <repl>varlist</repl> is not
	specified.
      </para>
      <para>
	With the option <opt>to-file</opt>, writes to the named file
	the labels for all series in the dataset, one per line. If no
	labels are present an error is flagged; if some series have
	labels and others do not, a blank line is printed for series
	with no label. The output file will be written in the currently
	set <cmdref targ="workdir"/>, unless the <repl>filename</repl>
	string contains a full path specification.
      </para>
      <para>
	With the option <opt>from-file</opt>, reads the specified file
	(which should be plain text) and assigns labels to the series in
	the dataset, reading one label per line and taking blank lines
	to indicate blank labels.
      </para>
      <para>
	The <opt>delete</opt> option does what you'd expect: it
	removes all the series labels from the dataset.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Variable labels</menu-path>
    </gui-access>

  </command>

  <command name="lad" section="Estimation"
    label="Least Absolute Deviation estimation">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--no-vcv</flag>
	  <effect>don't compute covariance matrix</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print anything</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Calculates a regression that minimizes the sum of the absolute
	deviations of the observed from the fitted values of the
	dependent variable.  Coefficient estimates are derived using
	the Barrodale&ndash;Roberts simplex algorithm; a warning is
	printed if the solution is not unique.
      </para>
      <para>
	Standard errors are derived using the bootstrap procedure with
	500 drawings. The covariance matrix for the parameter
	estimates, printed when the <opt>vcv</opt> flag is given, is
	based on the same bootstrap. Since this is quite an expensive
	operation, the <opt>no-vcv</opt> option is provided for the
	case where the covariance matrix is not required; when this
	option is given standard errors will not be available.
      </para>
      <para>
	Note that this method can be slow when the sample is large or
	there are many regressors; in that case it may be preferable
	to use the <cmdref targ="quantreg"/> command. Given a
	dependent variable <lit>y</lit> and a list of regressors
	<lit>X</lit>, the following commands are basically equivalent,
	except that the quantreg method uses the faster
	Frisch&ndash;Newton algorithm and provides analytical rather
	than bootstrapped standard errors.
      </para>
      <code>
	lad y const X
	quantreg 0.5 y const X
      </code>
    </description>

    <gui-access>
      <menu-path>/Model/Robust estimation/Least Absolute Deviation</menu-path>
    </gui-access>

  </command>

  <command name="lags" section="Transformations" label="Create lags"
	   context="cli">

    <usage>
      <arguments>
        <argument optional="true" separated="true">order</argument>
	<argument>laglist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--bylag</flag>
	  <effect>order terms by lag</effect>
	</option>
      </options>
      <examples>
	<example>lags x y</example>
	<example>lags 12 ; x y</example>
	<example>lags 4 ; x1 x2 x3 --bylag</example>
	<demos>
	  <demo>sw_ch12.inp</demo>
	  <demo>sw_ch14.inp</demo>
	</demos>
	</examples>
    </usage>

    <description>
      <para>
	Creates new series which are lagged values of each of the
	series in <repl>varlist</repl>.  By default the number of lags
	created equals the periodicity of the data. For example, if
	the periodicity is 4 (quarterly), the command <cmd>lags
	x</cmd> creates
      </para>
      <mono>
	x_1 = x(t-1)
	x_2 = x(t-2)
	x_3 = x(t-3)
	x_4 = x(t-4)
      </mono>
      <para>
	The number of lags created can be controlled by the optional
	first parameter (which, if present, must be followed by a
	semicolon).
      </para>
      <para>
	The <opt>bylag</opt> option is meaningful only if
	<repl>varlist</repl> contains more than one series and the
	maximum lag order is greater than 1. By default the lagged
	terms are added to the dataset by variable: first all lags of
	the first series, then all lags of the second series, and so
	on. But if <opt>bylag</opt> is given, the ordering is by lags:
	first lag 1 of all the listed series, then lag 2 of all the
	list series, and so on.
      </para>
      <para>
        This facility is also available as a function: see <fncref
	targ="lags"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Lags of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="lags-dialog" section="Estimation" context="gui"
    label="Lag selection box">

    <description>
      <para>
	In this dialog you can select the lag order for the independent
	variables in a time-series model, and in some cases for the dependent
	variable also.  (But note that the common lag order for vector models
	such as VARs and VECMs is handled separately, via a selection spinner in
	the main model dialog box.)
      </para>
      <para>
	The spinners on the left let you select a range of consecutive lags for
	any given variable. To specify non-consecutive lags, click the check box
	next to the entry field titled <quote>specific lags</quote>.  This
	activates the entry box, into which you can type a list of lags,
	separated by spaces.
      </para>
      <para>
	The row marked <quote>default</quote> offers a quick way to set a common
	lag specification for all the independent variables: values set in that
	row are copied to all the others (apart from the dependent variable, if
	present).
      </para>
      <para>
	The dependent variable is treated specially: the minimum lag must be
	zero, which places the current value of the variable on the left-hand
	side of the model.  Any higher lags appear with the independent
	variables on the right-hand side of the model.
      </para>
      <para>
	Values selected in this dialog are remembered for the duration of your
	session with a given dataset.
      </para>

    </description>

  </command>

  <command name="ldiff" section="Transformations"
    label="Log-differences" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The first difference of the natural log of each series in
	<repl>varlist</repl> is obtained and the result stored in a
	new series with the prefix <lit>ld_</lit>.  Thus <cmd>ldiff
	  x y</cmd> creates the new variables
      </para>
      <mono>
	ld_x = log(x) - log(x(-1))
	ld_y = log(y) - log(y(-1))
      </mono>
    </description>

    <gui-access>
      <menu-path>/Add/Log differences of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="leverage" section="Tests" label="Influential observations">

    <usage>
      <options>
        <option>
	  <flag>--save</flag>
	  <effect>save the resulting series</effect>
	</option>
        <option>
	  <flag>--overwrite</flag>
	  <effect>OK to overwrite existing series</effect>
	</option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
	</option>
	<option>
	  <flag>--plot</flag>
	  <optparm>mode-or-filename</optparm>
	  <effect>see below</effect>
        </option>
      </options>
	<examples>
	<demos>
	  <demo>leverage.inp</demo>
	</demos>
    </examples>
    </usage>

    <description>
      <para>
	Must follow an <cmd>ols</cmd> command. Calculates the leverage
	(<math>h</math>, which must lie in the range 0 to 1) for each
	data point in the sample on which the previous model was
	estimated.  Displays the residual (<math>u</math>) for each
	observation along with its leverage and a measure of its
	influence on the estimates, <math>uh</math>/(1 &minus;
	<math>h</math>).  <quote>Leverage points</quote> for which the
	value of <math>h</math> exceeds 2<math>k</math>/<math>n</math>
	(where <math>k</math> is the number of parameters being
	estimated and <math>n</math> is the sample size) are flagged
	with an asterisk.  For details on the concepts of leverage and
	influence see <cite key="davidson-mackinnon93">Davidson and
	MacKinnon (1993)</cite>, Chapter 2.
      </para>
      <para context="tex">
	DFFITS values are also computed: these are Studentized
	residuals (residuals divided by their standard errors)
	multiplied by $\sqrt{h/(1 - h)}$. They give a measure of the
	difference in fit for observation <math>i</math> depending on
	whether or not that observation is included in the sample for
	estimation. For more on this point see chapter 12 of Maddala's
	<cite key="maddala92">Introduction to Econometrics</cite> or
	<cite key="belsley-etal80">Belsley, Kuh and Welsch
	(1980)</cite>.  For more on Studentized residuals see the
	section headed <emphasis>Accessor matrix</emphasis> below.
      </para>
      <para context="notex">
	DFFITS values are also computed: these are Studentized
	residuals (residuals divided by their standard errors)
	multiplied by the square root of <math>h</math>(1 &minus;
	<math>h</math>). They give a measure of the difference in fit
	for observation <math>i</math> depending on whether or not
	that observation is included in the sample for estimation.
	For more on this point see chapter 12 of Maddala's <cite
	key="maddala92">Introduction to Econometrics</cite> or <cite
	key="belsley-etal80">Belsley, Kuh and Welsch
	(1980)</cite>. For more on Studentized residuals see the
	section headed <emphasis>Accessor matrix</emphasis> below.
      </para>
      <para context="cli">
	If the <opt>save</opt> flag is given with this command, the
	leverage, influence and DFFITS values are added to the current
	data set; in this context the <opt>quiet</opt> flag may be
	used to suppress the printing of results.  The default names
	of the saved series are, respectively, <lit>lever</lit>,
	<lit>influ</lit> and <lit>dffits</lit>. If series of these
	names already exist, what happens depends on whether the
	<opt>overwrite</opt> option is given. If so, the existing
	series are overwritten; if not, the names will be adjusted to
	ensure uniqueness. In the latter case the newly generated
	series will always be the highest-numbered three series in the
	dataset.
      </para>
      <para context="gui">
	The "+" icon at the top of the leverage test window brings up
	a dialog box that allows you to save one or more of the test
	variables to the current data set.
      </para>
      <para context="tex">
	After execution, the <fncref targ="$test"/> accessor returns the
	cross-validation criterion, which is defined as
        \[
	\sum_{i=1}^n (y_i - \hat{y}_{-i})^2
        \]
        where $\hat{y}_{-i}$ is the forecast error for the $i$-th
        observation, after it has been excluded from the sample. The
        criterion is, hence, the sum of the squared forecasting errors
        where all $n$ observations but the $i$-th one are used to
        predict it (the so-called <emphasis>leave-one-out</emphasis>
        estimator).  For a broader discussion of the cross-validation
        criterion, see Davidson and MacKinnon's <book>Econometric
        Theory and Methods</book>, pages 685--686, and the references
        therein.
      </para>
      <para context="notex">
	After execution, the <fncref targ="$test"/> accessor returns the
	cross-validation criterion, which is defined as the sum of
	squared deviations of the dependent variable from its forecast
	value, the forecast for each observation being based on a
	sample from which that observation is excluded.  (This is
	known as the <emphasis>leave-one-out</emphasis> estimator).
	For a broader discussion of the cross-validation criterion,
	see Davidson and MacKinnon's <book>Econometric Theory and
	Methods</book>, pages 685&ndash;686, and the references therein.
      </para>
      <para context="cli">
	By default, if this command is invoked interactively a
	plot of the leverage and influence values is shown.  This can
	be adjusted via the <opt>plot</opt> option. The acceptable
	parameters to this option are <lit>none</lit> (to suppress the
	plot); <lit>display</lit> (to display a plot even when in
	script mode); or a file name. The effect of providing a file
	name is as described for the <opt>output</opt> option of the
	<cmdref targ="gnuplot"/> command.
      </para>
      <subhead context="cli">Accessor matrix</subhead>
      <para context="cli">
	Besides the <opt>save</opt> option noted above, results from
	this command can be retrieved in the form of a three-column
	matrix via the <fncref targ="$result"/> accessor. The first
	two columns of this matrix contain leverage and influence
	values (as with <opt>save</opt>) but the third column holds
	Studentized residuals rather than DFFITS values. These are
	<quote>externally Studentized</quote> or
	<quote>jackknifed</quote> residuals&mdash;that is, the
	standard error in the divisor for observation <math>i</math>
	uses the residual mean square with that observation
	omitted. Such a residual can be interpreted as a
	<math>t</math> statistic for the hypothesis that a 0/1 dummy
	variable coding specifically for observation <math>i</math>
	would have a true coefficient of zero. For further discussion
	of Studentized residuals see <cite
	key="chatterjee-hadi86">Chatterjee and Hadi (1986)</cite>.
      </para>
      <para>
	DFFITS values may be obtained from the <lit>$result</lit>
	matrix as follows:
      </para>
      <code>
	R = $result
	dffits = R[,3] .* sqrt(R[,1] ./ (1-R[,1]))
      </code>
      <para>
	Or using series:
      </para>
      <code>
	series h = $result[,1]  # leverage
	series sr = $result[,3] # Studentized residual
	series dffits = sr * sqrt(h/(1-h))
      </code>
    </description>

    <gui-access>
      <menu-path>Model window, /Analysis/Influential observations</menu-path>
    </gui-access>

  </command>

  <command name="levinlin" section="Tests" label="Levin-Lin-Chu test">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--nc</flag>
	  <effect>test without a constant</effect>
	</option>
	<option>
	  <flag>--ct</flag>
	  <effect>with constant and trend</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print per-unit results</effect>
	</option>
      </options>
      <examples>
	<example>levinlin 0 y</example>
        <example>levinlin 2 y --ct</example>
        <example>levinlin {2,2,3,3,4,4} y</example>
      </examples>
    </usage>

    <description>
      <para>
	Carries out the panel unit-root test described by <cite
	key="LLC2002">Levin, Lin and Chu (2002)</cite>. The null
	hypothesis is that all of the individual time series exhibit a
	unit root, and the alternative is that none of the series has a
	unit root. (That is, a common AR(1) coefficient is assumed,
	although in other respects the statistical properties of the
	series are allowed to vary across individuals.)
      </para>
      <para context="cli">
	By default the test ADF regressions include a constant;
	to suppress the constant use the <opt>nc</opt> option, or
	to add a linear trend use the <opt>ct</opt> option.
	(See the <cmdref targ="adf"/> command for explanation of
	ADF regressions.)
      </para>
      <para context="cli">
	The (non-negative) <repl>order</repl> for the test (governing
	the number of lags of the dependent variable to include in the
	ADF regressions) may be given in either of two forms. If a
	scalar value is given, this is applied to all the individuals
	in the panel.  The alternative is to provide a matrix
	containing a specific lag order for each individual; this must
	be a vector with as many elements as there are individuals in
	the current sample range. Such a matrix can be specified by
	name, or constructed using braces as illustrated in the
	last example above.
      </para>
      <para context="cli">
	When the <opt>verbose</opt> option is given, the following
	results are printed for each unit in the panel:
	<lit>delta</lit>, the coefficient on the lagged level in each
	ADF regression; <lit>s2e</lit>, the estimated variance of the
	innovations; and <lit>s2y</lit>, the estimated long-run
	variance of the differenced series.
      </para>
      <para>
	Note that panel unit-root tests can also be conducted using
	the <cmdref targ="adf"/> and <cmdref targ="kpss"/> commands.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Unit root tests/Levin-Lin-Chu test</menu-path>
    </gui-access>

  </command>

  <command name="loess" section="Estimation" label="Loess" context="gui">
    <description>
      <para>
	Performs locally-weighted polynomial regression and produces a
	series containing predicted values of the dependent variable for
	each non-missing value of the independent variable. The method is
	as described by <cite key="cleveland79">William Cleveland
	(1979)</cite>.
      </para>
      <para>
	The controls allow you to specify the order of the polynomial
	in the independent variable and the proportion of the data
	points to be used in each local regression (the
	bandwidth). Higher values of the bandwidth produce a smoother
	outcome.
      </para>
      <para>
	If the robust weights box is checked the local regression
	procedure is iterated twice, with the weights being modified
	based on the residuals from the previous iteration so as to
	give less influence to outliers.
      </para>
     </description>
  </command>

  <command name="logistic" section="Estimation" label="Logistic regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--ymax</flag>
	  <optparm>value</optparm>
	  <effect>specify maximum of dependent variable</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--fixed-effects</flag>
	  <effect>see below</effect>
	</option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print anything</effect>
        </option>
      </options>
      <examples>
        <example>logistic y const x</example>
        <example>logistic y const x --ymax=50</example>
      </examples>
    </usage>

    <description>
      <para>
	Logistic regression: carries out an OLS regression using the
	logistic transformation of the dependent variable,
	<equation status="display"
	  tex="\[\log\left(\frac{y}{y^*-y}\right)\]"
	  ascii="log(y/(y* - y))"
	  graphic="logistic1"/>
	In the case of panel data the specification may include
	individual fixed effects.
      </para>
      <para>
	The dependent variable must be strictly positive.  If all its
	values lie between 0 and 1, the default is to use a
	<math>y</math><sup>*</sup> value (the asymptotic maximum of
	the dependent variable) of 1; if its values lie between 0 and
	100, the default <math>y</math><sup>*</sup> is 100.
      </para>
      <para context="cli">
	If you wish to set a different maximum, use the
	<opt>ymax</opt> option. Note that the supplied value must be
	greater than all of the observed values of the dependent
	variable.
      </para>
      <para context="gui">
	You may specify a different maximum <math>y</math> value.
	Note that the supplied value must be greater than all of the
	observed values of the dependent variable.
      </para>
      <para>
	The fitted values and residuals from the regression are
	automatically adjusted using the inverse of the logistic
	transformation:
	<equation status="display"
	  tex="\[y \approx E\left(\frac{y^*}{1+e^{-x}}\right)\]"
	  ascii="y =~ E(y* / (1 + exp(-x)))"
	  graphic="logistic2"/> where <math>x</math> represents
	either a fitted value or a residual from the OLS regression
	using the logistic dependent variable.  The reported values
	are therefore comparable with the original dependent
	variable. The need for approximation arises because the
	inverse transformation is nonlinear and therefore does
	not conserve expectation.
      </para>
      <para>
	The <opt>fixed-effects</opt> option is applicable only if the
	dataset takes the form of a panel. In that case we subtract
	the group means from the logistic transform of the dependent
	variable and estimation proceeds as usual for fixed effects.
      </para>
      <para>
	Note that if the dependent variable is binary, you should
	use the <cmdref targ="logit"/> command instead.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Logistic</menu-path>
      <menu-path>/Model/Panel/FE logistic</menu-path>
    </gui-access>

  </command>

  <command name="logit" section="Estimation"
    label="Logit regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
	</option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>clustered standard errors</effect>
        </option>
	<option>
	  <flag>--multinomial</flag>
	  <effect>estimate multinomial logit</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
	<option>
	  <flag>--p-values</flag>
	  <effect>show p-values instead of slopes</effect>
	</option>
	<option>
	  <flag>--estrella</flag>
	  <effect>select pseudo-R-squared variant</effect>
	</option>
      </options>
	<examples>
	<demos>
	  <demo>keane.inp</demo>
	  <demo>oprobit.inp</demo>
	</demos>
    </examples>
    </usage>

    <description>
      <para>
	If the dependent variable is a binary variable (all values are
	0 or 1) maximum likelihood estimates of the coefficients on
	<repl>indepvars</repl> are obtained via the
	Newton&ndash;Raphson method. As the model is nonlinear the
	slopes depend on the values of the independent variables.  By
	default the slopes with respect to each of the independent
	variables are calculated (at the means of those variables) and
	these slopes replace the usual p-values in the regression
	output.  This behavior can be suppressed by giving the
	<opt>p-values</opt> option. The chi-square statistic tests the
	null hypothesis that all coefficients are zero apart from the
	constant.
      </para>
      <para context="cli">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <opt>robust</opt> flag is
	given, then QML or Huber&ndash;White standard errors are
	calculated instead. In this case the estimated covariance
	matrix is a <quote>sandwich</quote> of the inverse of the
	estimated Hessian and the outer product of the gradient; see
	chapter 10 of <cite key="davidson-mackinnon04">Davidson and
	MacKinnon (2004)</cite>.  But if the <opt>cluster</opt> option
	is given, then <quote>cluster-robust</quote> standard errors
	are produced; see <guideref targ="chap:robust_vcv"/> for
	details.
      </para>
      <para context="cli">
	By default the pseudo-R-squared statistic suggested by <cite
	key="mcfadden74">McFadden (1974)</cite> is shown, but in the
	binary case if the <opt>estrella</opt> option is given, the
	variant recommended by <cite key="estrella98">Estrella
	(1998)</cite> is shown instead. This variant arguably mimics
	more closely the properties of the regular
	<math>R</math><sup>2</sup> in the context of least-squares
	estimation.
      </para>
      <para context="cli">
	If the dependent variable is binary, logit coefficients
	represent the log of the odds ratio (the ratio of the
	probability of <math>y</math> = 1 to that of <math>y</math> =
	0). In this case the <lit>$model</lit> bundle available after
	estimation includes an the extra element named
	<lit>oddsratios</lit>, a matrix with four columns holding the
	exponentiated coefficient (odds ratio) plus standard error
	computed via the delta method and 95 percent confidence
	interval, for each regressor. Note, however, that the
	confidence interval is calculated as the exponential of that
	for the original coefficient.
      </para>
      <para context="gui">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <quote>Robust standard
	errors</quote> box is checked, then QML or Huber&ndash;White
	standard errors are calculated instead. In this case the
	estimated covariance matrix is a <quote>sandwich</quote> of
	the inverse of the estimated Hessian and the outer product of
	the gradient.  See chapter 10 of Davidson and MacKinnon for
	details.
      </para>
      <para>
	If the dependent variable is not binary but is discrete, then
	by default it is interpreted as an ordinal response, and
	Ordered Logit estimates are obtained.  However, if the
	<opt>multinomial</opt> option is given, the dependent variable
	is interpreted as an unordered response, and Multinomial Logit
	estimates are produced. (In either case, if the variable
	selected as dependent is not discrete an error is flagged.)
	The accessor <lit>$allprobs</lit> is available after
	estimation, to get a matrix containing the estimated
	probabilities of the outcomes at each observation
	(observations in rows, outcomes in columns).
      </para>
      <para>
	If you want to use logit for analysis of proportions (where
	the dependent variable is the proportion of cases having a
	certain characteristic, at each observation, rather than a 1
	or 0 variable indicating whether the characteristic is present
	or not) you should not use the <cmd>logit</cmd> command, but
	rather construct the logit variable, as in
      </para>
      <code>
	series lgt_p = log(p/(1 - p))
      </code>
      <para>and use this as the dependent variable in an OLS regression.
      See chapter 12 of <cite key="ramanathan02">Ramanathan (2002)</cite>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Logit</menu-path>
    </gui-access>

  </command>

  <command name="logs" section="Transformations"
    label="Create logs" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The natural log of each of the series in <repl>varlist</repl>
	is obtained and the result stored in a new series with the
	prefix <lit>l_</lit> (<quote>el</quote> underscore).  For example,
	<cmd>logs x y</cmd> creates the new variables <lit>l_x</lit> =
	ln(<lit>x</lit>) and <lit>l_y</lit> = ln(<lit>y</lit>).
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Logs of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="loop" section="Programming"
    label="Start a command loop" context="cli">

    <usage>
      <arguments>
        <argument>control</argument>
      </arguments>
      <options>
	<option>
	  <flag>--progressive</flag>
	  <effect>enable special forms of certain commands</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>echo commands and show confirmatory messages</effect>
	</option>
        <option>
          <flag>--decr</flag>
          <effect>see below</effect>
        </option>
      </options>
      <examples>
        <example>loop 1000</example>
        <example>loop i=1..10</example>
        <example>loop while essdiff &gt; .00001</example>
        <example>loop for (r=-.99; r&lt;=.99; r+=.01)</example>
	<example>loop foreach i xlist</example>
	<demos>
	  <demo>armaloop.inp</demo>
	  <demo>keane.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	This command opens a special mode in which the program
	accepts commands to be executed repeatedly.  You exit the mode
	of entering loop commands with <cmd>endloop</cmd>: at this
	point the stacked commands are executed.
      </para>
      <para>
	The parameter <repl quote="true">control</repl> may take any
	of five forms, as shown in the examples: an integer number of
	times to repeat the commands within the loop; a range of
	integer values for an index variable;
	<quote><lit>while</lit></quote> plus a boolean condition;
	<quote><lit>for</lit></quote> plus three expressions in
	parentheses, separated by semicolons (which emulates the
	<lit>for</lit> statement in the C programming language); or
	<quote><lit>foreach</lit></quote> plus an index variable and a
	list.
      </para>
      <para>
        The <opt>decr</opt> option is specific to the <quote>range of
        integer values</quote> form of loop. By default the index is
        incremented by 1 at each iteration, and if the starting value
        is less than the ending value the loop will not run at all.
        When <opt>decr</opt> is given the index is decremented by 1
        at each iteration.
      </para>
      <para>
	See <guideref targ="chap:looping"/> for full details and
	examples.  The effect of the <opt>progressive</opt> option
	(which is designed for use in Monte Carlo simulations) is
	explained there. Not all gretl commands may be used within
	a loop; the commands available in this context are also
	set out there.
      </para>
      <para>
	By default, execution of commands proceeds more quietly within
	loops than in other contexts. If you want more feedback on
	what's going on in a loop, give the <opt>verbose</opt> option.
      </para>
    </description>

  </command>

  <command name="mahal" section="Statistics" label="Mahalanobis distances">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print anything</effect>
        </option>
	<option>
	  <flag>--save</flag>
	  <effect>add distances to the dataset</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Computes the Mahalanobis distances between the series in
	<repl>varlist</repl>.  The Mahalanobis distance is the
	distance between two points in a <math>k</math>-dimensional
	space, scaled by the statistical variation in each dimension
	of the space.  For example, if <math>p</math> and
	<math>q</math> are two observations on a set of <math>k</math>
	variables with covariance matrix <math>C</math>, then the
	Mahalanobis distance between the observations is given by
	<equation status="display"
        tex="\[\sqrt{(p-q)^{\prime}C^{-1}(p-q)}\]"
        ascii="sqrt((p - q)' * C-inverse * (p - q))"
          graphic="mahal"/>
	where (<math>p</math> &minus; <math>q</math>) is a
	<math>k</math>-vector. This reduces to Euclidean
	distance if the covariance matrix is the identity
	matrix.
      </para>
      <para>
	The space for which distances are computed is defined by
	the selected variables.  For each observation in the current
	sample range, the distance is computed between the observation
	and the centroid of the selected variables.  This distance is
	the multidimensional counterpart of a standard
	<math>z</math>-score, and can be used to judge whether a
	given observation <quote>belongs</quote> with a group of other
	observations.
      </para>
      <para context="cli">
	If the <opt>vcv</opt> option is given, the
	covariance matrix and its inverse are printed.  If the
	<opt>save</opt> option is given, the distances are saved to
	the dataset under the name <lit>mdist</lit> (or
	<lit>mdist1</lit>, <lit>mdist2</lit> and so on if there is
	already a variable of that name).
      </para>
      <para context="gui">
	If the number of variables selected is 4 or
	less, the covariance matrix and its inverse are printed.
	Clicking the "+" button at the top of the window displaying
	the distances give you the option of adding the distances to
	the dataset as a new variable.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Mahalanobis distances</menu-path>
    </gui-access>

  </command>

  <command name="mailer" section="Utilities" context="gui"
    label="Sending a file by email">

    <description>
      <para>
	You can send a dataset or script as an email attachment from
	within gretl using SMTP (Simple Mail Transfer Protocol). We
	explain below how to understand the two tabs (<quote>Mail
	setup</quote> and <quote>Message</quote>) in the mail dialog
	box. We also comment on the password requirement.
      </para>
      <subhead>Mail setup tab</subhead>
      <ilist>
	<li>
	  <para>
	    SMTP server: the server by which your mail should be
	    sent. Those who use gmail should hopefully find that the
	    default setting, <lit>smtps://smtp.gmail.com:465</lit>,
	    works; others will have to enter the information
	    themselves. Note that the server string must start with
	    either <lit>smtp://</lit> or <lit>smtps://</lit>.
	    Following the name of the server it may be necessary to
	    add a colon followed by a port number, as in the
	    <quote><lit>:465</lit></quote> shown above. Other possible
	    values for the SMTP port number are 25 and 587.
	  </para>
	</li>
	<li>
	  <para>
	    Mail username: the username that identifies you to
	    your mail server. This is likely the same as your email
	    address.
	  </para>
	</li>
	<li>
	  <para>
	    Mail password: most likely your SMTP server wants to see a
	    password, but if not you can click on <quote>A password is
	    not required</quote>.
	  </para>
	</li>
      </ilist>
      <para>
	Your server and username information is remembered from one
	gretl session to the next so you should only have to enter it
	once. As for the password, to store it or not is your
	choice; see the <emphasis>Email password</emphasis> section
	below for more details.
      </para>
      <para>
	Clicking <lit>OK</lit> in the Mail setup tab moves you to the
	message tab (unless required information is missing).
      </para>
      <subhead>Message tab</subhead>
      <ilist>
       <li>
	 <para>
	   To: the email address of the recipient. In the first
	   instance you will have to type this yourself: gretl does
	   not have access to your address book. But once you have
	   entered an address it is remembered and can be selected
	   subsequently via a drop-down list. Up to ten addresses can
	   be stored in this way.
	 </para>
       </li>
       <li>
	 <para>
	   From: your own email address. This is remembered from one
	   gretl session to the next.
	 </para>
       </li>
       <li>
	 <para>
	   Subject: this is filled out automatically but you can edit
	   it if you wish.
	 </para>
       </li>
       <li>
	 <para>
	   Note: also filled out automatically, but can be edited.
	 </para>
       </li>
      </ilist>
      <para>
	Clicking <lit>OK</lit> in the Message tab sends the message
	(unless some required information is missing).
      </para>
      <subhead>Email password</subhead>
      <para>
	If a password is required to send mail, you can choose to have
	gretl store it. In that case it is saved in obfuscated form in
	a directory that is private to you, on your own computer (not
	on any internet server). However, we cannot guarantee its
	security.
      </para>
      <para>
	If you're a gmail user and you employ 2-step authentication,
	please note: your regular gmail password will not work in
	this context. You will need an <quote>App password</quote>
	for use with gretl, see
	<url>https://support.google.com/accounts/answer/185833</url>.
      </para>
    </description>

  </command>

  <command name="makepkg" section="Programming" context="cli"
    label="Make function package">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
      <options>
        <option>
	  <flag>--index</flag>
	  <effect>write auxiliary index file</effect>
        </option>
        <option>
	  <flag>--translations</flag>
	  <effect>write auxiliary strings file</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>operate quietly</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Supports creation of a gretl function package via the command
	line. The mode of operation of this command depends on the
	extension of <repl>filename</repl>, which must be either
	<lit>.gfn</lit> or <lit>.zip</lit>.
      </para>
      <subhead>Gfn mode</subhead>
      <para>
	Writes a gfn file. It is assumed that a package specification
	file, with the same basename as <repl>filename</repl> but with
	the extension <lit>.spec</lit>, is accessible, along with any
	auxiliary files that it references. It is also assumed that
	all the functions to be packaged have been read into memory.
      </para>
      <subhead>Zip mode</subhead>
      <para>
	Writes a zip package file (gfn plus other materials).  If a
	gfn file of the same basename as <repl>filename</repl> is
	found, gretl checks for corresponding <lit>inp</lit> and
	<lit>spec</lit> files: if these are both found and at least
	one of them is newer than the gfn file then the gfn is
	rebuilt, otherwise the existing gfn is used. If no such file
	is found, gretl first attempts to build the gfn.
      </para>
      <subhead>Gfn options</subhead>
      <para>
	The option flags support the writing of auxiliary files,
	intended for use with gretl <quote>addons</quote>. The index
	file is a short XML document containing basic information
	about the package; it has the same basename as the package and
	the extension <lit>.xml</lit>. The translations file contains
	strings from the package that may be suitable for translation,
	in C format; for package <lit>foo</lit> this file is named
	<lit>foo-i18n.c</lit>. These files are not produced if the
	command is operating in zip mode and a pre-existing gfn
	file is used.
      </para>
      <para>
	For details on all of this, see the gretl <mnu
	targ="Pkgbook">Function Package Guide</mnu>.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Function packages/New package</menu-path>
    </gui-access>

  </command>

  <command name="maps" section="Utilities" label="Drawing maps" context="gui">
    <description>
      <para>
	This dialog allows you to create a map, based on previously
	loaded geographical data (in the form of a GeoJSON file or
	ESRI shapefile). The options are as follows.
      </para>
      <ilist>
	<li>
	  <para>
	    series to plot: this is the series (if any) by which
	    regions of the map should be colored, also known as the
	    <quote>payload</quote>. In the absence of a payload only
	    the map outlines will be shown. If the current dataset
	    contains one or more series that look like possible
	    candidates (on a simple heuristic) they will be shown in
	    the drop-down selector.
	  </para>
	</li>
	<li>
	  <para>
	    palette: this enables selection of the set of colors for
	    representing the payload, and so is relevant only if a
	    payload is selected.
	  </para>
	</li>
	<li>
	  <para>
	    log scale: in case a payload is selected, should it be
	    represented on a log scale? By default, no.
	  </para>
	</li>
	<li>
	  <para>
	    border: should a rectangular border be drawn around the
	    map? By default, yes, but you can remove it.
	  </para>
	</li>
	<li>
	  <para>
	    feature border width: gives you control over the width of
	    the border or outline drawn around the distinct
	    <quote>features</quote> in the map (for example,
	    countries, states or counties). If your plot includes a
	    payload the width can be run down to zero to eliminate
	    such borders.
	  </para>
	</li>
	<li>
	  <para>
	    height: this controls the size of the map image. Given the
	    height, the width is calculated based on the longitude
	    range of the map.
	  </para>
	</li>
      </ilist>
      <para>
	Please note, you can gain a <i>lot</i> more control over the
	details of the map by calling the <fncref targ="geoplot"/>
	function.
      </para>
    </description>
  </command>

  <command name="markers" section="Dataset" label="Observation markers" context="cli">

    <usage>
      <altforms>
	<altform><lit>markers --to-file=</lit><repl>filename</repl></altform>
	<altform><lit>markers --from-file=</lit><repl>filename</repl></altform>
	<altform><lit>markers --to-array=</lit><repl>name</repl></altform>
	<altform><lit>markers --from-array=</lit><repl>name</repl></altform>
	<altform><lit>markers --from-series=</lit><repl>name</repl></altform>
	<altform><lit>markers --delete</lit></altform>
      </altforms>
    </usage>

    <description>
      <para>
	The options <opt>to-file</opt> and <opt>to-array</opt> provide
	means of saving the observation marker strings from the
	current dataset, either to a named file or a named array.  If
	no such strings are present an error is flagged. In the file
	case output will be written in the current <cmdref
	targ="workdir"/> unless the <repl>filename</repl> string
	contains a full path specification. The markers are written
	one per line. In the array case, if <repl>name</repl> is
	the identifier of an existing array of strings it will be
	overwritten, otherwise a new array will be created.
      </para>
      <para>
	With the option <opt>from-file</opt>, reads the specified
	file (which should be UTF-8 text) and assigns observation
	markers to the rows in the dataset, reading one marker per
	line. In general there should be at least as many markers in
	the file as observations in the dataset, but if the dataset is
	a panel it is also acceptable if the number of markers in the
	file matches the number of cross-sectional units (in which
	case the markers are repeated for each time period.) The
	<opt>from-array</opt> option works similarly, reading from
	a named array of strings.
      </para>
      <para>
	The option <opt>from-series</opt> offers a convenient way of
	creating observation markers by copying from a string-valued
	series. An error is flagged if the specified series does not
	have string values.
      </para>
      <para>
	The <opt>delete</opt> option does what you'd expect: it
	removes the observation marker strings from the dataset.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Observation markers</menu-path>
    </gui-access>

  </command>

  <command name="meantest" section="Tests" label="Difference of means">

    <usage>
      <arguments>
        <argument>series1</argument>
        <argument>series2</argument>
      </arguments>
      <options>
        <option>
	  <flag>--unequal-vars</flag>
	  <effect>assume variances are unequal</effect>
        </option>
      </options>
    </usage>

    <description>
      <para context="cli">
	Calculates the <math>t</math> statistic for the null
	hypothesis that the population means are equal for the
	variables <repl>series1</repl> and <repl>series2</repl>, and
	shows its p-value.
      </para>
      <para>
	By default the test statistic is calculated on the assumption
	that the variances are equal for the two variables. With the
	<opt>unequal-vars</opt> option the variances are assumed to
	be different; in this case the degrees of freedom for the test
	statistic are approximated as per <cite
	key="satter46">Satterthwaite (1946)</cite>.
      </para>
      <para context="gui">
	Calculates the t-test for the null hypothesis that the
	population means are equal for two selected series, and shows
	its p-value.  The command may be called with or without the
	assumption that the variances are equal for the two variables.
	In the latter case the degrees of freedom for the test are
	approximated as per <cite key="satter46">Satterthwaite
	(1946)</cite>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/Test statistic calculator</menu-path>
    </gui-access>

  </command>

  <command name="MIDAS_list" section="Dataset" label="MIDAS list"
	   context="gui">
    <description>
      <para>
	A MIDAS list (MIDAS = Mixed Data Sampling) is a named list (of
	series) whose members jointly represent a time-series variable
	which is observed at a higher frequency than that of the
	<quote>host</quote> dataset. For example, such a list might
	represent a monthly series in the context of a quarterly or
	annual dataset, or a daily series in the context of a monthly
	dataset.
      </para>
      <para>
	Such a list must have <math>m</math> members, where
	<math>m</math> is the number of high-frequency periods
	per dataset period: each series holds the values for a
	given sub-period. In the monthly/quarterly case, this
	means the list has three members: one holds values for
	the first month of the quarter; another holds values for the
	second month; and another, values for the third month.
      </para>
      <para>
	Moreover, these list members must be arranged in a particular
	order, namely <emphasis>most recent
	first</emphasis>. Continuing the quarterly/monthly example,
	the order must be month 3, month 2, month 1. This may seem
	<quote>backwards</quote>, but it's the order that is required
	for creating lists of lags, which are the stock in trade of
	MIDAS modeling.
      </para>
      <para>
	For guidance on how to create a dataset that supports MIDAS
	lists, please see
      </para>
      <para>
	<url>http://gretl.sourceforge.net/midas/midas_gretl.pdf</url>
      </para>
    </description>
  </command>

  <command name="MIDAS_parm" section="Estimation"
	   label="MIDAS hyper-parameters" context="gui">
    <description>
      <para>
	In this dialog you are asked to select the type of
	parameterization for a set of high-frequency terms, as well as
	the range of lags of these terms.  The supported types of
	parameterization are:
      </para>
      <ilist>
	<li>
	  <para>
	    U-MIDAS or <quote>unrestricted MIDAS</quote>: each lag has
	    its own coefficient.
	  </para>
	</li>
	<li>
	  <para>
	    Normalized exponential Almon: this requires at least one
	    parameter and commonly uses two.
	  </para>
	</li>
	<li>
	  <para>
	    Normalized beta with a zero last lag; requires exactly two
	    parameters.
	  </para>
	</li>
	<li>
	  <para>
	    Normalized beta with non-zero last lag; requires exactly
	    three parameters.
	  </para>
	</li>
	<li>
	  <para>
	    Almon polynomial; requires at least one parameter.
	  </para>
	</li>
	<li>
	  <para>
	    Normalized beta, one parameter: this is a variant of the
	    normalized beta with a zero last lag, in which the first
	    parameter is fixed at 1.0. The second parameter is
	    estimated subject to the restriction that it be at least
	    1.0.
	  </para>
	</li>
      </ilist>
    </description>
  </command>

  <command name="midasreg" section="Estimation" label="MIDAS regression">
    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
	<argument separated="true">MIDAS-terms</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
        <option>
	  <flag>--levenberg</flag>
	  <effect>see below</effect>
        </option>
      </options>
      <examples>
        <example>midasreg y 0 y(-1) ; mds(X, 1, 9, 1, theta)</example>
	<example>midasreg y 0 y(-1) ; mds(X, 1, 9, 0)</example>
	<example>midasreg y 0 y(-1) ; mdsl(XL, 2, theta)</example>
	<demos>
	  <demo>gdp_midas.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Carries out least-squares estimation (either NLS or OLS,
	depending on the specification) of a MIDAS (Mixed Data
	Sampling) model. Such models include one or more independent
	variables that are observed at a higher frequency than the
	dependent variable; for a good brief introduction see <cite
	key="armesto10">Armesto, Engemann and Owyang (2010)</cite>.
      </para>
      <para context="cli">
	The variables in <repl>indepvars</repl> should be of the same
	frequency as the dependent variable. This list should usually
	include <lit>const</lit> or <lit>0</lit> (intercept) and
	typically includes one or more lags of the dependent variable.
	The high-frequency terms are given after a semicolon; each one
	takes the form of a number of comma-separated arguments within
	parentheses, prefixed by either <lit>mds</lit> or
	<lit>mdsl</lit>.
      </para>
      <para context="gui">
	The variables under <repl>Regressors</repl> are of the same
	frequency as the dependent variable, and are selected from the
	upper list on the left. MIDAS models typically include one or
	more lags of the dependent variable; that is controlled by the
	<quote>AR order</quote> spin button, which defaults to 1 lag.
	To add MIDAS (high frequency) terms, select from the lower
	left-hand list and use the lower green arrow (or right-click).
      </para>
      <para context="gui">
	On adding a MIDAS term, a dialog pops up to let you select
	the range of lags, the parameterization type, and (for
	types that do not have a fixed number of parameters) the
	number of hyper-parameters. You can bring this dialog up
	again to revise a specification by right-clicking on a
	MIDAS term on the right.
      </para>
      <para context="cli">
	<lit>mds</lit>: this variant generally requires 5 arguments,
	as follows: the name of a <cmdref targ="MIDAS_list"/>, two
	integers giving the minimum and maximum high-frequency lags,
	an integer between 0 and 4 (or string, see below) specifying
	the type of parameterization to use, and the name of a vector
	holding initial values of the parameters. The example below
	calls for lags 3 to 11 of the high-frequency series
	represented by the list <lit>X</lit>, using parameterization
	type 1 (exponential Almon, see below) with initializer
	<lit>theta</lit>.
      </para>
      <code context="cli">
	mds(X, 3, 11, 1, theta)
      </code>
      <para context="cli">
	<lit>mdsl</lit>: generally requires 3 arguments: the name of a
	list of MIDAS lags, an integer (or string, see below) to
	specify the type of parameterization and the name of an
	initialization vector. In this case the minimum and maximum
	lags are implicit in the initial list argument. In the example
	below <lit>Xlags</lit> should be a list which already holds
	all the required lags; such a list can be constructed using
	the <fncref targ="hflags"/> function.
      </para>
      <code context="cli">
	mdsl(XLags, 1, theta)
      </code>
      <para context="cli">
	The supported types of parameterization are shown below; in
	the context of <lit>mds</lit> and <lit>mdsl</lit>
	specifications these may be given in the form of numeric codes
	or the double-quoted strings shown after the numbers.
      </para>
      <para context="cli">
	0 or <lit>"umidas"</lit>: unrestricted MIDAS or U-MIDAS (each
	lag has its own coefficient)
      </para>
      <para context="cli">
	1 or <lit>"nealmon"</lit>: normalized exponential Almon;
	requires at least one parameter, commonly uses two
      </para>
      <para context="cli">
	2 or <lit>"beta0"</lit>: normalized beta with a zero last lag;
	requires exactly two parameters
      </para>
      <para context="cli">
	3 or <lit>"betan"</lit>: normalized beta with non-zero last
	lag; requires exactly three parameters
      </para>
      <para context="cli">
	4 or <lit>"almonp"</lit>: (non-normalized) Almon polynomial;
	requires at least one parameter
      </para>
      <para context="cli">
	5 or <lit>"beta1"</lit>: as <lit>beta0</lit>, but with the
	first parameter fixed at 1, leaving a single free parameter.
      </para>
      <para context="cli">
	When the parameterization is U-MIDAS, the final initializer
	argument is not required. In other cases you can request an
	automatic initialization by substituting one or other of these
	two forms for the name of an initial parameter vector:
      </para>
      <ilist context="cli">
	<li>
	  <para>
	    The keyword <lit>null</lit>: this is accepted if the
	    parameterization has a fixed number of terms (the beta
	    cases, with 2 or 3 parameters). It's also accepted for the
	    exponential Almon case, implying the default of 2
	    parameters.
	  </para>
	</li>
	<li>
	  <para>
	    An integer value giving the required number of parameters.
	  </para>
	</li>
      </ilist>
      <para context="cli">
	The estimation method used by this command depends on the
	specification of the high-frequency terms. In the case of
	U-MIDAS the method is OLS, otherwise it is nonlinear least
	squares (NLS). When the normalized exponential Almon or
	normalized beta parameterization is specified, the default
	NLS method is a combination of constrained BFGS and OLS, but
	the <opt>levenberg</opt> option can be given to force use
	of the Levenberg&ndash;Marquardt algorithm.
      </para>
      <para context="gui">
	The estimation method used by this command depends on the
	specification of the high-frequency terms. In the case of
	U-MIDAS the method is OLS, otherwise it is nonlinear least
	squares (NLS). When the normalized exponential Almon or
	normalized beta parameterization is specified, the NLS method
	is a combination of constrained BFGS and OLS, unless you check
	the box labeled <quote>Prefer NLS via
	Levenberg-Marquardt</quote>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Univariate time series/MIDAS</menu-path>
    </gui-access>
  </command>

  <command name="missing" section="Dataset" context="gui"
    label="Missing data values">

    <description>
      <para>
	Set a numerical value that will be interpreted as
	<quote>missing</quote> or <quote>not available</quote>, either for
	a particular data series (under the Variable menu) or globally for
	the entire data set (under the Data menu).
      </para>
      <para>
	Gretl has its own internal coding for missing values, but
	sometimes imported data may employ a different code.  For
	example, if a particular series is coded such that a value of
	-1 indicates <quote>not applicable</quote>, you can select
	<quote>Set missing value code</quote> under the Variable menu
	and type in the value <quote>-1</quote> (without the quotes).
	Gretl will then read the -1s as missing observations.
      </para>
    </description>
  </command>

  <command name="menu-attach" section="Programming"
	   label="Menu attachment" context="gui">
    <description>
      <para>
	This dialog enables you to specify a menu attachment for a
	function package. To do this you must complete the following
	three fields in the dialog box.
      </para>
      <subhead>1. Label</subhead>
      <para>
	This requires a short label string, which will appear as
	the menu entry for the package.
      </para>
      <subhead>2. Window</subhead>
      <para>
	Select <quote>model window</quote> for a function package that
	does something with a gretl model, and should appear in the
	menu bar in a gretl model window. Otherwise, select
	<quote>main window</quote>.
      </para>
      <subhead>3. Menu tree</subhead>
      <para>
	Select the position within the menu tree (for either the
	main window or the model window, as chosen above) where the
	entry for the package should appear.
      </para>
      <subhead>Optional elements</subhead>
      <para>
	In addition you can use the <quote>GUI help text</quote>
	button to add or edit GUI-specific help text, to be shown when
	the package is called from a menu. And if the package is
	intended to be called from a model window you can specify a
	certain type of model (identified by its gretl command-word)
	as a requirement.
      </para>
    </description>
  </command>

  <command name="mle" section="Estimation"
    label="Maximum likelihood estimation">

    <usage>
      <arguments>
        <argument>log-likelihood function</argument>
	<argument optional="true">derivatives</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't show estimated model</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--hessian</flag>
	  <effect>base covariance matrix on the Hessian</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <optparm optional="true">hac</optparm>
	  <effect>QML or HAC covariance matrix</effect>
	</option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>cluster-robust covariance matrix</effect>
        </option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
	<option>
	  <flag>--no-gradient-check</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--auxiliary</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--lbfgs</flag>
	  <effect>use L-BFGS-B instead of regular BFGS</effect>
	</option>
      </options>
      <examples>
	<demos>
	  <demo>weibull.inp</demo>
	  <demo>biprobit_via_ghk.inp</demo>
	  <demo>frontier.inp</demo>
	  <demo>keane.inp</demo>
	</demos>
      </examples>
    </usage>

    <description context="gui">
      <para>
	Performs Maximum Likelihood (ML) estimation using either the
	BFGS (Broyden, Fletcher, Goldfarb, Shanno) algorithm or
	Newton's method.  You must specify the log-likelihood
	function; it is recommended that you also supply expressions
	for the derivatives of this function with respect to each of
	the parameters if possible.
      </para>
      <para>
	This help text assumes use of the default BFGS maximizer. For
	information on using Newton's method please see
	<guideref targ="chap:mle"/>.
      </para>
      <para>
	Simple example: Suppose we have a series <lit>X</lit> with values 0
	or 1 and we wish to obtain the maximum likelihood estimate of the
	probability, <lit>p</lit>, that <lit>X</lit> = 1.  (In this simple case
	we can guess in advance that the ML estimate of <lit>p</lit> will simply
	equal the proportion of Xs equal to 1 in the sample.)
      </para>
      <para>
	The parameter <lit>p</lit> must first be added to the dataset and
	given an initial value.  This can be done using the genr command or via
	menu choices.  Appropriate <quote>genr</quote> lines may be typed into
	the MLE specification window prior to the specification of the
	log-likelihood function.
      </para>
      <para>
	In the MLE window we type the following lines:
      </para>
      <code>
	loglik = X*log(p) + (1-X)*log(1-p)
	deriv p = X/p - (1-X)/(1-p)
      </code>
      <para>
	The first line specifies the log-likelihood function, and the
	next line supplies the derivative of that function with
	respect to the parameter p.  If no "deriv" lines are given, a
	numerical approximation to the derivatives is computed.
      </para>
      <para>
	If the parameter p was not previously declared we could
	preface the above lines with something like the following:
      </para>
      <code>
	scalar p = 0.5
      </code>
      <para>
	By default, standard errors are based on the Outer Product of the
	Gradient.  If the robust standard errors box is checked, a QML
	estimator is used (namely, a sandwich of the negative inverse of
	the Hessian and the covariance matrix of the gradient).  The
	Hessian is approximated numerically.
      </para>
      <para>
	For a much more in-depth description of <cmd>mle</cmd>, please
	refer to <guideref targ="chap:mle"/>.
      </para>
    </description>

    <description context="cli">
      <para>
	Performs Maximum Likelihood (ML) estimation using either the
	BFGS (Broyden, Fletcher, Goldfarb, Shanno) algorithm or
	Newton's method. The user must specify the log-likelihood
	function.  The parameters of this function must be declared
	and given starting values prior to estimation.  Optionally,
	the user may specify the derivatives of the log-likelihood
	function with respect to each of the parameters; if analytical
	derivatives are not supplied, a numerical approximation is
	computed.
      </para>
      <para>
	This help text assumes use of the default BFGS maximizer. For
	information on using Newton's method please see
	<guideref targ="chap:mle"/>.
      </para>
      <para>
	Simple example: Suppose we have a series <lit>X</lit> with
	values 0 or 1 and we wish to obtain the maximum likelihood
	estimate of the probability, <lit>p</lit>, that <lit>X</lit> = 1.
	(In this simple case we can guess in advance that the ML estimate
	of <lit>p</lit> will simply equal the proportion of Xs equal to 1
	in the sample.)
      </para>
      <para>
	The parameter <lit>p</lit> must first be added to the dataset
	and given an initial value.  For example,
        <lit>scalar p = 0.5</lit>.
      </para>
      <para>
	We then construct the MLE command block:
      </para>
      <code>
	mle loglik = X*log(p) + (1-X)*log(1-p)
	  deriv p = X/p - (1-X)/(1-p)
	end mle
      </code>
      <para>
	The first line above specifies the log-likelihood function. It
	starts with the keyword <lit>mle</lit>, then a dependent
	variable is specified and an expression for the log-likelihood
	is given (using the same syntax as in the <cmd>genr</cmd>
	command).  The next line (which is optional) starts with the
	keyword <lit>deriv</lit> and supplies the derivative of the
	log-likelihood function with respect to the parameter
	<lit>p</lit>. If no derivatives are given, you should include
	a statement using the keyword <lit>params</lit> which
	identifies the free parameters: these are listed on one line,
	separated by spaces and can be either scalars, or vectors, or
	any combination of the two.  For example, the above could be
	changed to:
      </para>
      <code>
	mle loglik = X*log(p) + (1-X)*log(1-p)
	  params p
	end mle
      </code>
      <para>
	in which case numerical derivatives would be used.
      </para>
      <para>
	Note that any option flags should be appended to the ending line
	of the MLE block. For example:
      </para>
      <code>
	mle loglik = X*log(p) + (1-X)*log(1-p)
	  params p
	end mle --quiet
      </code>
      <subhead>Covariance matrix and standard errors</subhead>
      <para>
	If the log-likelihood function returns a series or vector
	giving per-observation values then estimated standard errors
	are by default based on the Outer Product of the Gradient
	(OPG), while if the <opt>hessian</opt> option is given they
	are instead based on the negative inverse of the Hessian,
	which is approximated numerically.  If the <opt>robust</opt>
	option is given, a QML estimator is used (namely, a sandwich
	of the negative inverse of the Hessian and the OPG). If the
	<lit>hac</lit> parameter is added to this option the OPG is
	augmented in the manner of <cite key="newey-west87">Newey and
	West</cite> to allow for serial correlation of the gradient.
	(This only makes sense with time-series data.)  However, if
	the log-likelihood function just returns a scalar value, the
	OPG is not available (and so neither is the QML estimator),
	and standard errors are of necessity computed using the
	numerical Hessian.
      </para>
      <para>
	In the event that you just want the primary parameter
	estimates you can give the <opt>auxiliary</opt> option, which
	suppresses computation of the covariance matrix and standard
	errors; this will save some CPU cycles and memory usage.
      </para>
      <subhead>Checking analytical derivatives</subhead>
      <para>
	If you supply analytical derivatives, by default gretl runs a
	numerical check on their plausibility.  Occasionally this may
	produce false positives, instances where correct derivatives
	appear to be wrong and estimation is refused. To counter this,
	or to achieve a little extra speed, you can give the option
	<opt>no-gradient-check</opt>.  Obviously, you should do this
	only if you are confident that the gradient you have specified
	is right.
      </para>
      <subhead>Parameter names</subhead>
      <para>
	In estimating a nonlinear model it is often convenient to name
	the parameters tersely. In printing the results, however, it
	may be desirable to use more informative labels. This can be
	achieved via the additional keyword <lit>param_names</lit>
	within the command block. For a model with <math>k</math>
	parameters the argument following this keyword should be
	a double-quoted string literal holding <math>k</math>
	space-separated names, the name of a string variable that
	holds <math>k</math> such names, or the name of an array of
	<math>k</math> strings.
      </para>
      <para>
	For an in-depth description of <cmd>mle</cmd> please refer to
	<guideref targ="chap:mle"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Maximum likelihood</menu-path>
    </gui-access>

  </command>

  <command name="modeltab" section="Utilities" label="The model table">

    <usage>
      <altforms>
        <altform><lit>modeltab add</lit></altform>
	<altform><lit>modeltab show</lit></altform>
	<altform><lit>modeltab free</lit></altform>
	<altform><lit>modeltab --output=</lit><repl>filename</repl></altform>
      </altforms>
    </usage>

    <description context="gui">
      <para>
	In econometric research it is common to estimate several
	models with a common dependent variable&mdash;the models
	differing in respect of which independent variables are
	included, or perhaps in respect of the estimator used.  In
	this situation it is convenient to present the regression
	results in the form of a table, where each column contains the
	results (coefficient estimates and standard errors) for a
	given model, and each row contains the estimates for a given
	variable across the models.
      </para>

      <para>
	Gretl provides a means of constructing such a table (and
	copying it in plain text, &latex; or Rich Text Format).  Here is
	how to do it:
      </para>

      <nlist>
	<li>
	  <para>
	    Estimate a model which you wish to include in the table,
	    and in the model display window, under the File menu,
	    select <quote>Save to session as icon</quote> or
	    <quote>Save as icon and close</quote>.
	  </para>
	</li>
	<li>
	  <para>
	    Repeat step 1 for the other models to be included in the
	    table (up to a total of six models).
	  </para>
	</li>
	<li>
	  <para>
	    When you are done estimating the models, open the icon
	    view of your gretl session (by selecting <quote>icon
	    view</quote> under the View menu in the main gretl window,
	    or by clicking the <quote>session icon view</quote> icon
	    on the gretl toolbar).
	  </para>
	</li>
	<li>
	  <para>
	    In session icon view, there is an icon labeled
	    <quote>Model table</quote>. Decide which model you wish to
	    appear in the left-most column of the model table and add
	    it to the table, either by dragging its icon onto the
	    Model table icon, or by right-clicking on the model icon
	    and selecting <quote>Add to model table</quote> from the
	    pop-up menu.
	  </para>
	</li>
	<li>
	  <para>
	    Repeat step 4 for the other models you wish to include in
	    the table.  The second model selected will appear in the
	    second column from the left, and so on.
	  </para>
	</li>
	<li>
	  <para>
	    When you are finished composing the model table, display
	    it by double-clicking on its icon. Via the Copy button in
	    the window which appears, you have the option of copying
	    the table to the clipboard in various formats.
	  </para>
	</li>
	<li>
	  <para>
	    If the ordering of the models in the table is not what you
	    wanted, right-click on the model table icon and select
	    <quote>Clear table</quote>.  Then go back to step 4 above
	    and try again.
	  </para>
	</li>
      </nlist>
    </description>

    <description context="cli">
      <para>
	Manipulates the gretl <quote>model table</quote>. See
	<guideref targ="chap:modes"/> for details. The sub-commands
	have the following effects: <cmd>add</cmd> adds the last model
	estimated to the model table, if possible; <cmd>show</cmd>
	displays the model table in a window; and <cmd>free</cmd>
	clears the table.
      </para>
      <para>
	To call for printing of the model table, use the flag
	<opt>output=</opt> plus a filename. If the filename has the
	suffix <quote><lit>.tex</lit></quote>, the output will be in
	&tex; format; if the suffix is <quote><lit>.rtf</lit></quote>
	the output will be RTF; otherwise it will be plain text.
	In the case of &tex; output the default is to produce a
	<quote>fragment</quote>, suitable for inclusion in a
	document; if you want a stand-alone document instead,
	use the <opt>complete</opt> option, for example
      </para>
      <code>
	modeltab --output="myfile.tex" --complete
      </code>
    </description>

    <gui-access>
      <menu-path>Session icon window, Model table icon</menu-path>
    </gui-access>

  </command>

  <command name="modprint" section="Printing"
    label="Print a user-defined model" context="cli">

    <usage>
      <arguments>
        <argument>coeffmat</argument>
        <argument>names</argument>
	<argument optional="true">addstats</argument>
      </arguments>
      <options>
	<option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Prints the coefficient table and optional additional
	statistics for a model estimated <quote>by
	hand</quote>. Mainly useful for user-written functions.
      </para>
      <para>
	The argument <repl>coeffmat</repl> should be a <math>k</math>
	by 2 matrix containing <math>k</math> coefficients and
	<math>k</math> associated standard errors. The
	<repl>names</repl> argument should supply at least
	<math>k</math> names for labeling the coefficients; it can
	take the form of a string literal (in double quotes) or string
	variable, in which case the names should be separated by
	commas or spaces, or it may be given as a named array of
	strings.
      </para>
      <para>
	The optional argument <repl>addstats</repl> is a vector
	containing <math>p</math> additional statistics to be printed
	under the coefficient table.  If this argument is given, then
	<repl>names</repl> should contain <math>k + p</math> names,
	the additional <math>p</math> names to be associated with
	the extra statistics.
      </para>
      <para>
	If <repl>addstats</repl> is not provided and the
	<repl>coeffmat</repl> matrix has row names attached, then
	the <repl>names</repl> argument can be omitted.
      </para>
      <para>
	To put the output into a file, use the flag <opt>output=</opt>
	plus a filename. If the filename has the suffix
	<quote><lit>.tex</lit></quote>, the output will be in &tex;
	format; if the suffix is <quote><lit>.rtf</lit></quote> the
	output will be RTF; otherwise it will be plain text.  In the
	case of &tex; output the default is to produce a
	<quote>fragment</quote>, suitable for inclusion in a document;
	if you want a stand-alone document instead, use the
	<opt>complete</opt> option.
      </para>
      <para>
	The output file will be written in the currently set <cmdref
	targ="workdir"/>, unless the <repl>filename</repl> string
	contains a full path specification.
      </para>
    </description>

  </command>

  <command name="modtest" section="Tests" label="Model tests"
    context="cli">

    <usage>
      <arguments>
        <argument optional="true">order</argument>
      </arguments>
      <options>
        <option>
	  <flag>--normality</flag>
	  <effect>normality of residual</effect>
        </option>
        <option>
	  <flag>--logs</flag>
	  <effect>nonlinearity, logs</effect>
        </option>
        <option>
	  <flag>--squares</flag>
	  <effect>nonlinearity, squares</effect>
        </option>
        <option>
	  <flag>--autocorr</flag>
	  <effect>serial correlation</effect>
        </option>
        <option>
	  <flag>--arch</flag>
	  <effect>ARCH</effect>
        </option>
        <option>
	  <flag>--white</flag>
	  <effect>heteroskedasticity, White's test</effect>
        </option>
        <option>
	  <flag>--white-nocross</flag>
	  <effect>White's test, squares only</effect>
        </option>
        <option>
	  <flag>--breusch-pagan</flag>
	  <effect>heteroskedasticity, Breusch&ndash;Pagan</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust variance estimate for Breusch&ndash;Pagan</effect>
        </option>
        <option>
	  <flag>--panel</flag>
	  <effect>heteroskedasticity, groupwise</effect>
        </option>
        <option>
	  <flag>--comfac</flag>
	  <effect>common factor restriction, AR1 models only</effect>
        </option>
        <option>
	  <flag>--xdepend</flag>
	  <effect>cross-sectional dependence, panel data only</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print details</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
        </option>
      </options>
	  <examples>
	<demos>
	  <demo>credscore.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Must immediately follow an estimation command. The discussion
	below applies to usage of the command following estimation of
	a single-equation model; see <guideref targ="chap:var"/> for
	an account of how <cmd>modtest</cmd> operates after estimation
	of a VAR.
      </para>
      <para>
	Depending on the option given, this command carries out one of
	the following: the Doornik&ndash;Hansen test for the normality
	of the error term; a Lagrange Multiplier test for nonlinearity
	(logs or squares); White's test (with or without
	cross-products) or the Breusch&ndash;Pagan test (<cite
	key="breusch-pagan79">Breusch and Pagan, 1979</cite>) for
	heteroskedasticity; the LMF test for serial correlation <cite
	key="kiviet86" p="true">(Kiviet, 1986)</cite>; a test for ARCH
	(Autoregressive Conditional Heteroskedasticity; see also the
	<cmd>arch</cmd> command); a test of the common factor
	restriction implied by AR(1) estimation; or a test for
	cross-sectional dependence in panel-data models.  With the
	exception of the normality, common factor and cross-sectional
	dependence tests most of the options are only available for
	models estimated via OLS, but see below for details regarding
	two-stage least squares.
      </para>
      <para>
	The optional <lit>order</lit> argument is relevant only in case
	the <opt>autocorr</opt> or <opt>arch</opt> options are
	selected.  The default is to run these tests using a lag order
	equal to the periodicity of the data, but this can be adjusted by
	supplying a specific lag order.
      </para>
      <para>
	The <opt>robust</opt> option applies only when the
	Breusch&ndash;Pagan test is selected; its effect is to use the
	robust variance estimator proposed by <cite
	key="koenker81">Koenker (1981)</cite>, making the test less
	sensitive to the assumption of normality.
      </para>
      <para>
	The <opt>panel</opt> option is available only when the model
	is estimated on panel data: in this case a test for groupwise
	heteroskedasticity is performed (that is, for a differing
	error variance across the cross-sectional units).
      </para>
      <para>
	The <opt>comfac</opt> option is available only when the model is
	estimated via an AR(1) method such as Hildreth&ndash;Lu.  The
	auxiliary regression takes the form of a relatively unrestricted
	dynamic model, which is used to test the common factor restriction
	implicit in the AR(1) specification.
      </para>
      <para>
	The <opt>xdepend</opt> option is available only for models
	estimated on panel data. The test statistic is that developed
	by <cite key="pesaran04">Pesaran (2004)</cite>. The null
	hypothesis is that the error term is independently distributed
	across the cross-sectional units or individuals.
      </para>
      <para>
	By default, the program prints the auxiliary regression on
	which the test statistic is based, where applicable.  This may
	be suppressed by using the <opt>quiet</opt> flag (minimal
	printed output) or the <opt>silent</opt> flag (no printed
	output).  The test statistic and its p-value may be retrieved
	using the accessors <fncref targ="$test"/> and <fncref
	targ="$pvalue"/> respectively.
      </para>
      <para>
	When a model has been estimated by two-stage least squares (see
	<cmdref targ="tsls"/>), the LM principle breaks down and gretl
	offers some equivalents: the <flag>--autocorr</flag> option
	computes Godfrey's test for autocorrelation <cite key="godfrey94"
	p="true">(Godfrey, 1994)</cite> while the <flag>--white</flag>
	option yields the HET1 heteroskedasticity test <cite
	key="pesaran99" p="true">(Pesaran and Taylor, 1999)</cite>.
      </para>
      <para>
	For additional diagnostic tests on models, see
	<cmdref targ="chow"/>, <cmdref targ="cusum"/>, <cmdref
	targ="reset"/> and <cmdref targ="qlrtest"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests</menu-path>
    </gui-access>

  </command>

  <command name="mpi" section="Programming"
	   label="Message Passing Interface">
    <usage>
      <arguments>
	<argument>see below</argument>
      </arguments>
    </usage>
    <description>
      <para>
	The <lit>mpi</lit> command starts a block of statements (which
	must be ended with <lit>end mpi</lit>) to be executed using
	MPI (Message Passing Interface) parallelization. See
	<doc>gretl-mpi.pdf</doc> for a full account of this facility.
      </para>
    </description>
  </command>

  <command name="mpols" section="Estimation" label="Multiple-precision OLS">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
	<option>
	  <flag>--simple-print</flag>
	  <effect>do not print auxiliary statistics</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Computes OLS estimates for the specified model using multiple
	precision floating-point arithmetic, with the help of the
	Gnu Multiple Precision (GMP) library.  By default 256 bits of
	precision are used for the calculations, but this can be increased
	via the environment variable <lit>GRETL_MP_BITS</lit>.  For
	example, when using the bash shell one could issue the following
	command, before starting gretl, to set a precision of 1024 bits.
      </para>
      <code>
	export GRETL_MP_BITS=1024
      </code>

      <para context="cli">
	A rather arcane option is available for this command (primarily
	for testing purposes): if the <repl>indepvars</repl> list is
	followed by a semicolon and a further list of numbers, those
	numbers are taken as powers of <repl>x</repl> to be added to the
	regression, where <repl>x</repl> is the last variable in
	<repl>indepvars</repl>.  These additional terms are computed and
	stored in multiple precision.  In the following example
	<lit>y</lit> is regressed on <lit>x</lit> and the second, third
	and fourth powers of <lit>x</lit>:
      </para>
      <code context="cli">
	mpols y 0 x ; 2 3 4
      </code>
    </description>

    <gui-access>
      <menu-path>/Model/Other linear models/High precision OLS</menu-path>
    </gui-access>

  </command>

  <command name="nadarwat" section="Estimation" label="Nadaraya-Watson"
	   context="gui">
    <description>
      <para>
	Computes the Nadaraya&ndash;Watson nonparametric estimator of
	the conditional mean of the dependent variable,
	<math>m(x)</math>, for each non-missing value of the
	independent variable.
      </para>
      <para>
	The kernel function employed by this estimator is given by
	<math>K = exp(-x</math><sup>2</sup><math> / 2h)</math> for
	<math>|x| &lt; T</math>, and zero otherwise. <math>T</math> is
	a trimming parameter, by default equal to 4<math>h</math>.
      </para>
      <para>
	The bandwidth <math>h</math>, which is usually a small number,
	controls the smoothness of <math>m(x)</math> (higher values
	producing a smoother series); by default this is a
	data-determined value proportional to
	<math>n</math><sup>-0.2</sup>, where <math>n</math> is the
	sample size.
      </para>
      <para>
	If the <quote>leave-one-out</quote> box is checked, a variant
	of the estimator is employed in which the <math>i</math>-th
	observation is not used in evaluating
	<math>m(x</math><sub>i</sub><math>)</math>. This makes the
	algorithm more robust numerically and its usage is generally
	advised when the estimator is computed for inference purposes.
      </para>
      <para>
	For more detail on nonparametric estimation see <guideref
	targ="chap:nonparam"/>.
      </para>
     </description>
  </command>

  <command name="negbin" section="Estimation"
    label="Negative Binomial regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
	<argument separated="true" optional="true">offset</argument>
      </arguments>
      <options>
	<option>
	  <flag>--model1</flag>
	  <effect>use NegBin 1 model</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <effect>QML covariance matrix</effect>
	</option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
	<option>
	  <flag>--opg</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
      </options>
	 <examples>
	<demos>
	  <demo>camtriv.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Estimates a Negative Binomial model.  The dependent variable is taken
	to represent a count of the occurrence of events of some sort, and
	must have only non-negative integer values. By default the model
	NegBin 2 is used, in which the conditional variance of the count is
	given by &mu;(1 + &alpha;&mu;), where &mu; denotes the conditional
	mean.  But if the <opt>model1</opt> option is given the conditional
	variance is &mu;(1 + &alpha;).
      </para>
      <para>
	The optional <lit>offset</lit> series works in the same way as for the
	<cmdref targ="poisson"/> command.  The Poisson model is a restricted
	form of the Negative Binomial in which &alpha; = 0 by construction.
      </para>
      <para>
	By default, standard errors are computed using a numerical
	approximation to the Hessian at convergence.  But if the
	<opt>opg</opt> option is given the covariance matrix is based on
	the Outer Product of the Gradient (OPG), or if the
	<opt>robust</opt> option is given QML standard errors are
	calculated, using a <quote>sandwich</quote> of the inverse of the
	Hessian and the OPG.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Count data</menu-path>
    </gui-access>
  </command>

  <command name="nls" section="Estimation"
    label="Nonlinear Least Squares">

    <usage>
      <arguments>
        <argument>function</argument>
        <argument optional="true">derivatives</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't show estimated model</effect>
	</option>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
	<option>
	  <flag>--no-gradient-check</flag>
	  <effect>see below</effect>
	</option>
      </options>
      <examples>
	<demos>
	  <demo>wg_nls.inp</demo>
	  <demo>ects_nls.inp</demo>
	</demos>
      </examples>
    </usage>

    <description context="gui">
      <para>
	Performs Nonlinear Least Squares (NLS) estimation using a
	modified version of the Levenberg&ndash;Marquardt
	algorithm. You must supply a function specification; it is
	recommended but not required that you also supply expressions
	for the derivatives of this function with respect to each of
	the parameters if possible.  If you do not supply derivatives
	you should instead give a list of the parameters to be
	estimated (separated by spaces or commas), preceded by the
	keyword <lit>params</lit>; these can be either scalars, or
	vectors, or any combination of the two.
      </para>
      <para>
	Example: Suppose we have a data set with variables
	<math>C</math> and <math>Y</math> (&eg;
	<lit>greene11_3.gdt</lit>) and we wish to estimate a nonlinear
	consumption function of the form
	<equation status="display"
	  tex="\[C = \alpha + \beta Y^{\gamma}\]"
	  ascii="C = alpha + beta * Y^gamma"
	  graphic="greene_Cfunc"/>
      </para>
      <para>
	The parameters alpha, beta and gamma must first be added to
	the dataset and given initial values.  Appropriate lines may
	be typed into the NLS specification window prior to the
	function specification.
      </para>
      <para>
	In the NLS window we type the following lines:
      </para>
      <code>
	C = alpha + beta * Y^gamma
	deriv alpha = 1
	deriv beta = Y^gamma
	deriv gamma = beta * Y^gamma * log(Y)
      </code>
      <para>
	The first line specifies the regression function, and the next
	three lines supply the derivatives of that function with respect
	to each of the parameters in turn. If the "deriv" lines are not
	given, a numerical approximation to the Jacobian is computed.
      </para>
      <para>
	If the parameters alpha, beta and gamma were not previously
	declared we could preface the above lines with something like the
	following:
      </para>
      <code>
	scalar alpha = 1
	scalar beta = 1
	scalar gamma = 1
      </code>
      <para>For further details on NLS estimation please see
	<guideref targ="chap:nls"/>.
      </para>
    </description>

    <description context="cli">
      <para>
	Performs Nonlinear Least Squares (NLS) estimation using a
	modified version of the Levenberg&ndash;Marquardt algorithm.
	You must supply a function specification.  The parameters of
	this function must be declared and given starting values prior
	to estimation.  Optionally, you may specify the derivatives of
	the regression function with respect to each of the
	parameters.  If you do not supply derivatives you should
	instead give a list of the parameters to be estimated
	(separated by spaces or commas), preceded by the keyword
	<lit>params</lit>.  In the latter case a numerical
	approximation to the Jacobian is computed.
      </para>
      <para>
	It is easiest to show what is required by example.  The
	following is a complete script to estimate the nonlinear
	consumption function set out in William Greene's
	<book>Econometric Analysis</book> (Chapter 11 of the 4th
	edition, or Chapter 9 of the 5th).  The numbers to the left of
	the lines are for reference and are not part of the commands.
	Note that any option flags, such as <opt>vcv</opt> for
	printing the covariance matrix of the parameter estimates,
	should be appended to the final command, <lit>end nls</lit>.
      </para>
      <code>
	1   open greene11_3.gdt
	2   ols C 0 Y
	3   scalar a = $coeff(0)
	4   scalar b = $coeff(Y)
	5   scalar g = 1.0
	6   nls C = a + b * Y^g
	7    deriv a = 1
	8    deriv b = Y^g
	9    deriv g = b * Y^g * log(Y)
	10  end nls --vcv
      </code>
      <para>
	It is often convenient to initialize the parameters by
	reference to a related linear model; that is accomplished here
	on lines 2 to 5.  The parameters alpha, beta and gamma could
	be set to any initial values (not necessarily based on a model
	estimated with OLS), although convergence of the NLS procedure
	is not guaranteed for an arbitrary starting point.
      </para>
      <para>
	The actual NLS commands occupy lines 6 to 10. On line 6 the
	<cmd>nls</cmd> command is given: a dependent variable is
	specified, followed by an equals sign, followed by a function
	specification.  The syntax for the expression on the right is
	the same as that for the <cmd>genr</cmd> command.  The next
	three lines specify the derivatives of the regression function
	with respect to each of the parameters in turn.  Each line
	begins with the keyword <cmd>deriv</cmd>, gives the name of a
	parameter, an equals sign, and an expression whereby the
	derivative can be calculated. As an alternative to supplying
	analytical derivatives, you could substitute the following for
	lines 7 to 9:
      </para>
      <code>
	params a b g
      </code>
      <para>
	Line 10, <cmd>end nls</cmd>, completes the command and calls for
	estimation. Any options should be appended to this line.
      </para>
      <para>
	If you supply analytical derivatives, by default gretl runs a
	numerical check on their plausibility.  Occasionally this may
	produce false positives, instances where correct derivatives
	appear to be wrong and estimation is refused. To counter this,
	or to achieve a little extra speed, you can give the option
	<opt>no-gradient-check</opt>.  Obviously, you should do this
	only if you are confident that the gradient you have specified
	is right.
      </para>
      <subhead>Parameter names</subhead>
      <para>
	In estimating a nonlinear model it is often convenient to name
	the parameters tersely. In printing the results, however, it
	may be desirable to use more informative labels. This can be
	achieved via the additional keyword <lit>param_names</lit>
	within the command block. For a model with <math>k</math>
	parameters the argument following this keyword should be a
	double-quoted string literal holding <math>k</math>
	space-separated names, the name of a string variable that
	holds <math>k</math> such names, or the name of an array of
	<math>k</math> strings.
      </para>
      <para>
	For further details on NLS estimation please see
	<guideref targ="chap:nls"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Nonlinear Least Squares</menu-path>
    </gui-access>

  </command>

  <command name="normtest" section="Tests" label="Normality test">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--dhansen</flag>
	  <effect>Doornik&ndash;Hansen test, the default</effect>
        </option>
	<option>
	  <flag>--swilk</flag>
	  <effect>Shapiro&ndash;Wilk test</effect>
        </option>
	<option>
	  <flag>--lillie</flag>
	  <effect>Lilliefors test</effect>
        </option>
	<option>
	  <flag>--jbera</flag>
	  <effect>Jarque&ndash;Bera test</effect>
        </option>
	<option>
	  <flag>--all</flag>
	  <effect>do all tests</effect>
        </option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printed output</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Carries out a test for normality for the given
	<repl>series</repl>.  The specific test is controlled by the
	option flags (but if no flag is given, the Doornik&ndash;Hansen
	test is performed).  Note: the Doornik&ndash;Hansen and
	Shapiro&ndash;Wilk tests are recommended over the others, on
	account of their superior small-sample properties.
      </para>
      <para>
	The test statistic and its p-value may be retrieved using the
	accessors <fncref targ="$test"/> and <fncref targ="$pvalue"/>.
	Please note that if the <opt>all</opt> option is given, the
	result recorded is that from the Doornik&ndash;Hansen test.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Normality test</menu-path>
    </gui-access>

  </command>

  <command name="nulldata" section="Dataset"
    label="Creating a blank dataset">

    <usage>
      <arguments>
        <argument>series_length</argument>
      </arguments>
      <options>
	<option>
	  <flag>--preserve</flag>
	  <effect>preserve variables other than series</effect>
        </option>
      </options>
      <examples>
        <example>nulldata 500</example>
      </examples>
    </usage>

    <description>
      <para>
	Establishes a <quote>blank</quote> data set, containing only a
	constant and an index variable, with periodicity 1 and the
	specified number of observations. This may be used for
	simulation purposes: functions such as <cmd>uniform()</cmd>
	and <cmd>normal()</cmd> will generate artificial series from
	scratch to fill out the data set. This command may be useful
	in conjunction with <cmd>loop</cmd>.  See also the
	<quote>seed</quote> option to the <cmdref targ="set"/>
	command.
      </para>
      <para>
	By default, this command cleans out all data in gretl's
	current workspace: not only series but also matrices, scalars,
	strings, etc.  If you give the <opt>preserve</opt> option,
	however, any currently defined variables other than series are
	retained.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/New data set</menu-path>
    </gui-access>

  </command>

  <command name="ols" section="Estimation" label="Ordinary Least Squares">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>clustered standard errors</effect>
        </option>
        <option>
	  <flag>--jackknife</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--simple-print</flag>
	  <effect>do not print auxiliary statistics</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
        <option>
	  <flag>--anova</flag>
	  <effect>print an ANOVA table</effect>
        </option>
        <option>
	  <flag>--no-df-corr</flag>
	  <effect>suppress degrees of freedom correction</effect>
        </option>
        <option>
	  <flag>--print-final</flag>
	  <effect>see below</effect>
        </option>
      </options>
      <examples>
        <example>ols 1 0 2 4 6 7</example>
	<example>ols y 0 x1 x2 x3 --vcv</example>
	<example>ols y 0 x1 x2 x3 --quiet</example>
      </examples>
    </usage>

    <description>
      <para context="gui">
        Computes ordinary least squares (OLS) estimates for the
	specified model.
      </para>

      <para context="cli">
        Computes ordinary least squares (OLS) estimates with
	<repl>depvar</repl> as the dependent variable and
	<repl>indepvars</repl> as the list of independent variables.
	Variables may be specified by name or number; use the number
	zero for a constant term.
      </para>

      <para>Besides coefficient estimates and standard errors, the
	program also prints p-values for <math>t</math>
	(two-tailed) and <math>F</math>-statistics.  A p-value
	below 0.01 indicates statistical significance at the 1 percent
	level and is marked with <lit>***</lit>. <lit>**</lit>
	indicates significance between 1 and 5 percent and
	<lit>*</lit> indicates significance between the 5 and 10
	percent levels. Model selection statistics (the Akaike
	Information Criterion or AIC and Schwarz's Bayesian Information
	Criterion) are also printed.  The formula used for the AIC is
	that given by <cite key="akaike74">Akaike (1974)</cite>, namely
	minus two times the maximized log-likelihood plus two times the
	number of parameters estimated.</para>

      <para context="cli">If the option <opt>no-df-corr</opt> is
	given, the usual degrees of freedom correction is not applied
	when calculating the estimated error variance (and hence also
	the standard errors of the parameter estimates).</para>

      <para context="cli">
	The option <opt>print-final</opt> is applicable only in the
	context of a <cmdref targ="loop"/>.  It arranges for the
	regression to be run silently on all but the final iteration
	of the loop. See <guideref targ="chap:looping"/> for details.
      </para>

      <para context="cli">
	Various internal variables may be retrieved following
	estimation. For example
      </para>
      <code context="cli">
	series uh = $uhat
      </code>
      <para context="cli">
	saves the residuals under the name <lit>uh</lit>.  See the
	<quote>accessors</quote> section of the gretl function
	reference for details.
      </para>

      <para context="cli">
	The specific formula (<quote>HC</quote> version) used for
	generating robust standard errors when the <opt>robust</opt>
	option is given can be adjusted via the <cmdref targ="set"/>
	command.  The <opt>jackknife</opt> option has the effect of
	selecting an <lit>hc_version</lit> of <lit>3a</lit>. The
	<opt>cluster</opt> overrides the selection of HC version, and
	produces robust standard errors by grouping the observations
	by the distinct values of <repl>clustvar</repl>; see <guideref
	targ="chap:robust_vcv"/> for details.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Ordinary Least Squares</menu-path>
      <other-access>Beta-hat button on toolbar</other-access>
    </gui-access>

  </command>

  <command name="omit" section="Tests" label="Omit variables">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
	<option>
	  <flag>--test-only</flag>
	  <effect>don't replace the current model</effect>
	</option>
	<option>
	  <flag>--chi-square</flag>
	  <effect>give chi-square form of Wald test</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>print only the basic test result</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
	</option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix for reduced model</effect>
	</option>
	<option>
	  <flag>--auto</flag>
	  <optparm optional="true">alpha</optparm>
	  <effect>sequential elimination, see below</effect>
	</option>
      </options>
      <examples>
        <example>omit 5 7 9</example>
        <example>omit seasonals --quiet</example>
        <example>omit --auto</example>
        <example>omit --auto=0.05</example>
	 <demos>
	  <demo>restrict.inp</demo>
	  <demo>sw_ch12.inp</demo>
	  <demo>sw_ch14.inp</demo>
	 </demos>
     </examples>
    </usage>

    <description context="gui">
      <para>
	The primary form of this command re-estimates the given model
	after omitting the specified variables. Besides the usual
	model output, it prints a test for the joint significance of
	the omitted variables. The null hypothesis is that the true
	coefficients on all the omitted variables equal zero.
      </para>
      <para>
	If the <quote>Wald test</quote> option is selected, the joint
	significance of the specified variables is assessed via a Wald
	test on the covariance matrix of the given model, and no
	re-estimation is performed.
      </para>
      <para>
	If the <quote>Sequential elimination</quote> option is
	selected (note: this option will not always be displayed) it
	works as follows: at each step the variable with the highest
	p-value is omitted, until all remaining variables have a
	p-value no greater than some cutoff.  The default cutoff is 10
	percent (two-sided); this can be adjusted via a spin button.
	By default this process operates on all variables in the model
	(apart from the constant); if you want to confine it to a
	subset of the variables, check the box labeled <quote>Test
	only selected variables</quote> and make a selection. As in
	the primary case, the model is re-estimated after omitting
	variables unless no variables remain.
      </para>
    </description>

    <description context="cli">
      <para>
	This command must follow an estimation command.  In its
	primary form, it calculates a Wald test for the joint
	significance of the variables in <repl>varlist</repl>, which
	should be a subset (though not necessarily a proper subset) of
	the independent variables in the model last estimated. The
	results of the test may be retrieved using the accessors
	<fncref targ="$test"/> and <fncref targ="$pvalue"/>.
      </para>
      <para>
	Unless the restriction removes all the original regressors, by
	default the restricted model is estimated and it replaces the
	original as the <quote>current model</quote> for the purposes
	of, for example, retrieving the residuals as <lit>$uhat</lit>
	or doing further tests. This behavior may be suppressed via
	the <opt>test-only</opt> option.
      </para>
      <para>
	By default the <math>F</math>-form of the Wald test is
	recorded; the <opt>chi-square</opt> option may be used to
	record the chi-square form instead.
      </para>
      <para>
	If the restricted model is both estimated and printed, the
	<opt>vcv</opt> option has the effect of printing its
	covariance matrix, otherwise this option is ignored.
      </para>
      <para>
	Alternatively, if the <opt>auto</opt> flag is given,
	sequential elimination is performed: at each step the variable
	with the highest p-value is omitted, until all remaining
	variables have a p-value no greater than some cutoff.  The
	default cutoff is 10 percent (two-sided); this can be adjusted
	by appending <quote><lit>=</lit></quote> and a value between 0
	and 1 (with no spaces), as in the fourth example above.  If
	<repl>varlist</repl> is given this process is confined to the
	listed variables, otherwise all regressors aside from the
	constant are treated as candidates for omission. Note that the
	<opt>auto</opt> and <opt>test-only</opt> options cannot be
	combined.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Omit variables</menu-path>
    </gui-access>

  </command>

  <command name="online" section="Dataset" context="gui"
    label="Access online databases">

    <description>
      <para>
	Gretl is able to access databases at Wake Forest University
	(your computer must be connected to the internet for this to
	work).
      </para>
      <para>
	Under the <quote>File, Browse databases</quote> menu,
	select the item <quote>on database server</quote>. A window
	should appear, showing a listing of the gretl databases
	available at Wake Forest. (Depending on your location and the
	speed of your internet connection, this may take a few
	seconds.)  Along with the name of the database and a short
	description, there will appear a <quote>Local status</quote>
	entry: this shows whether you have the database installed
	locally (on the hard drive of your computer) and if so,
	whether or not it is up to date with the version on the
	server.
      </para>
      <para>
	If you have a given database installed locally, and it is
	up to date, there is no advantage in accessing it via the
	server.  But for a database that is not already installed and
	up to date, you may wish to get a listing of the data series:
	click on <quote>Get series listing</quote>.  This brings up a
	further window, from which you can display the values of a
	chosen data series, graph those values, or import them into
	gretl's workspace.  These tasks can be accomplished using the
	<quote>Series</quote> menu, or via the popup menu that appears
	when you click the right mouse button on a given series.  You
	can also search the listing for a variable of interest (the
	<quote>Find</quote> menu item).
      </para>
      <para>
	If you want faster access to the data, or wish to access
	the database offline, then select the line showing the
	database you want, in the initial database window, and press
	the <quote>Install</quote> button.  This will download the
	database in compressed format, then uncompress it and install
	it on your hard drive. Thereafter you should be able to find
	it under the <quote>File, Browse databases, gretl
	  native</quote> menu.
      </para>
    </description>
  </command>

  <command name="open" section="Dataset"
    label="Open a data file" context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print list of series</effect>
	</option>
	<option>
	  <flag>--preserve</flag>
	  <effect>preserve variables other than series</effect>
	</option>
	<option>
	  <flag>--select</flag>
	  <optparm>selection</optparm>
	  <effect>read only the specified series, see below</effect>
	</option>
	<option>
	  <flag>--frompkg</flag>
	  <optparm>pkgname</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--all-cols</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--www</flag>
	  <effect>use a database on the gretl server</effect>
	</option>
	<option>
	  <flag>--odbc</flag>
	  <effect>use an ODBC database</effect>
	</option>
	<optnote>See below for additional specialized options</optnote>
      </options>
      <examples>
        <example>open data4-1</example>
        <example>open voter.dta</example>
	<example>open fedbog.bin --www</example>
	<example>open dbnomics</example>
      </examples>
    </usage>

    <description>
      <para>
	Opens a data file or database&mdash;see <guideref
	targ="chap:datafiles"/> for an explanation of this
	distinction. The effect is somewhat different in the two
	cases. When a <emphasis>data file</emphasis> is opened, its
	content is read into gretl's workspace, replacing the
	current dataset (if any). To add data to the current dataset
	instead of replacing, see <cmdref targ="append"/> or (for
	greater flexibility) <cmdref targ="join"/>. When a
	<emphasis>database</emphasis> is opened this does not
	immediately load any data; rather, it sets the source for
	subsequent invocations of the <cmdref targ="data"/> command,
	which is used to import selected series. For specifics
	regarding databases see the section headed <quote>Opening a
	database</quote> below.
      </para>
      <para>
	If <repl>filename</repl> is not given as a full path, gretl
	will search some relevant paths to try to find the file, with
	<cmdref targ="workdir"/> as a first choice. If no filename
	suffix is given (as in the first example above), gretl assumes
	a native datafile with suffix <lit>.gdt</lit>.  Based on the
	name of the file and various heuristics, gretl will try to
	detect the format of the data file (native, plain text, CSV,
	MS Excel, Stata, SPSS, etc.).
      </para>
      <para>
	If the <opt>frompkg</opt> option is used, gretl will look for
	the specified data file in the subdirectory associated with
	the function package specified by <repl>pkgname</repl>.
      </para>
      <para>
	If the <repl>filename</repl> argument takes the form of a URI
	starting with <lit>http://</lit> or <lit>https://</lit>, then
	gretl will attempt to download the indicated data file before
	opening it.
      </para>
      <para>
	By default, opening a new data file clears the current gretl
	session, which includes deletion of all named variables,
	including matrices, scalars and strings.  If you wish to keep
	your currently defined variables (other than series, which are
	necessarily cleared out), use the <opt>preserve</opt> option.
      </para>
      <subhead context="cli">Spreadsheet files</subhead>
      <para>
	When opening a data file in a spreadsheet format (Gnumeric,
	Open Document or MS Excel), you may give up to three
	additional parameters following the filename.  First, you can
	select a particular worksheet within the file.  This is done
	either by giving its (1-based) number, using the syntax, &eg;,
	<opt>sheet=2</opt>, or, if you know the name of the sheet, by
	giving the name in double quotes, as in
	<opt>sheet="MacroData"</opt>. The default is to read the first
	worksheet. You can also specify a column and/or row offset
	into the worksheet via, &eg;,
      </para>
      <code>
	--coloffset=3 --rowoffset=2
      </code>
      <para>
	which would cause gretl to ignore the first 3 columns and the
	first 2 rows.  The default is an offset of 0 in both
	dimensions, that is, to start reading at the top-left cell.
      </para>
      <subhead context="cli">Delimited text files</subhead>
      <para>
	With plain text files, gretl generally expects to find the
	data columns delimited in some standard manner (generally via
	comma, tab, space or semicolon). By default gretl looks for
	observation labels or dates in the first column if its heading
	is empty or is a suggestive string such as
	<quote>year</quote>, <quote>date</quote> or
	<quote>obs</quote>. You can prevent gretl from treating the
	first column specially by giving the <opt>all-cols</opt>
	option.
      </para>
      <subhead context="cli">Fixed format text</subhead>
      <para>
	A <quote>fixed format</quote> text data file is one without
	column delimiters, but in which the data are laid out
	according to a known set of specifications such as
	<quote>variable <math>k</math> occupies 8 columns starting at
	column 24</quote>.  To read such files, you should append a
	string <opt>fixed-cols=</opt><repl>colspec</repl>, where
	<repl>colspec</repl> is composed of comma-separated integers.
	These integers are interpreted as a set of pairs.  The first
	element of each pair denotes a starting column, measured in
	bytes from the beginning of the line with 1 indicating the
	first byte; and the second element indicates how many bytes
	should be read for the given field.  So, for example, if you
	say
      </para>
      <code>
	open fixed.txt --fixed-cols=1,6,20,3
      </code>
      <para>
	then for variable 1 gretl will read 6 bytes starting at column
	1; and for variable 2, 3 bytes starting at column 20.  Lines
	that are blank, or that begin with <lit>#</lit>, are ignored,
	but otherwise the column-reading template is applied, and if
	anything other than a valid numerical value is found an error
	is flagged.  If the data are read successfully, the variables
	will be named <lit>v1</lit>, <lit>v2</lit>, etc.  It's up to
	the user to provide meaningful names and/or descriptions using
	the commands <cmdref targ="rename"/> and/or <cmdref
	targ="setinfo"/>.
      </para>
      <subhead context="gui">String-valued series</subhead>
      <para>
	By default, when you import a file that contains string-valued
	series, a text box will open showing you the contents of
	<lit>string_table.txt</lit>, a file which contains the
	mapping between strings and their numeric coding. You can
	suppress this behavior via the <opt>quiet</opt> option.
      </para>
      <subhead context="cli">Loading selected series</subhead>
      <para>
	Use of <lit>open</lit> with a data file argument (as opposed
	to the database case, see below) generally implies loading all
	series from the specified file. However, in the case of native
	gretl files (<lit>gdt</lit> and <lit>gdtb</lit>) only, it is
	possible to specify by name a subset of series to load. This
	is done via the <opt>select</opt> option, which requires
	an accompanying argument in one of three forms: the name of a
	single series; a list of names, separated by spaces and
	enclosed in double quotes; or the name of an array of strings.
	Examples:
      </para>
      <code>
	# single series
	open somefile.gdt --select=x1
	# more than one series
	open somefile.gdt --select="x1 x5 x27"
	# alternative method
	strings Sel = defarray("x1", "x5", "x27")
	open somefile.gdt --select=Sel
	</code>
       <subhead context="cli">Opening a database</subhead>
      <para>
	As mentioned above, the <lit>open</lit> command can be used to
	open a database file for subsequent reading via the <cmdref
	targ="data"/> command. Supported file-types are native gretl
	databases, RATS 4.0 and PcGive.
      </para>
      <para>
	Besides reading a file of one of these types on the local
	machine, three further cases are supported. First, if the
	<opt>www</opt> option is given, gretl will try to access a
	native gretl database of the given name on the gretl
	server&mdash;for instance the Federal Reserve interest rates
	database <lit>fedbog.bin</lit> in the third example shown
	above. Second, the command <quote><lit>open
	dbnomics</lit></quote> can be used to set DB.NOMICS as the
	source for database reads; on this see <mnu
	targ="gretlDBN">dbnomics for gretl</mnu>.  Third, if the
	<opt>odbc</opt> option is given gretl will try to access an
	ODBC database. This option is explained at length in <guideref
	targ="chap:odbc"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Open data</menu-path>
      <other-access>Drag a data file onto gretl's main window</other-access>
    </gui-access>

  </command>

  <command name="orthdev" section="Transformations"
    label="Orthogonal deviations" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Applicable with panel data only. A series of forward orthogonal
	deviations is obtained for each variable in <repl>varlist</repl> and
	stored in a new variable with the prefix <lit>o_</lit>. Thus
	<cmd>orthdev x y</cmd> creates the new variables <lit>o_x</lit> and
	<lit>o_y</lit>.
      </para>
      <para>
	The values are stored one step ahead of their true temporal location
	(that is, <lit>o_x</lit> at observation <math>t</math> holds the
	deviation that, strictly speaking, belongs at <math>t</math> &minus;
	1).  This is for compatibility with first differences: one loses the
	first observation in each time series, not the last.
      </para>
    </description>

  </command>

  <command name="outfile" section="Printing"
    label="Direct printing to file" context="cli">

    <usage>
      <altforms>
	<altform><lit>outfile</lit> <repl>filename</repl></altform>
	<altform><lit>outfile</lit> <lit>--buffer=</lit><repl>strvar</repl></altform>
	<altform><lit>outfile</lit> <lit>--tempfile=</lit><repl>strvar</repl></altform>
      </altforms>
      <options>
        <option>
	  <flag>--append</flag>
	  <effect>append to file, first variant only</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--buffer</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--tempfile</flag>
	  <effect>see below</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	The <lit>outfile</lit> command starts a block in which any
	printed output is diverted to a file or buffer (or just
	discarded, if you wish). Such a block is terminated by the
	command <quote><lit>end outfile</lit></quote>, after which
	output reverts to the default stream.
      </para>

      <subhead>Diversion to a named file</subhead>
      <para>
	The first variant shown above sends output to a file named by
	the <repl>filename</repl> argument. By default a new file is
	created (or an existing one is overwritten). The output file
	will be written in the currently set <cmdref targ="workdir"/>,
	unless the <repl>filename</repl> string contains a full path
	specification to the contrary. If you wish to append output to
	an existing file instead, use the <opt>append</opt> flag.
      </para>
      <para>
	Some special variations on this theme are available. If you
	give the keyword <lit>null</lit> in place of a real filename
	the effect is to suppress all printed output until redirection
	is ended. If either of the keywords <lit>stdout</lit> or
	<lit>stderr</lit> are given in place of a regular filename the
	effect is to redirect output to standard output or standard
	error output respectively.
      </para>
      <para>
	A simple example follows, where the output from a particular
	regression is written to a named file.
      </para>
      <code>
	open data4-10
	outfile regress.txt
	  ols ENROLL 0 CATHOL INCOME COLLEGE
	end outfile
      </code>

      <subhead>Diversion to a string buffer</subhead>
      <para>
	The <opt>buffer</opt> option is used to store output in a
	string variable. The required parameter for this option must
	be the name of an existing string variable, whose content will
	be over-written.  We show below the example given above,
	revised to write to a string. In this case printing
	<lit>model_out</lit> will display the redirected output.
      </para>
      <code>
	open data4-10
	string model_out = ""
	outfile --buffer=model_out
	  ols ENROLL 0 CATHOL INCOME COLLEGE
	end outfile
	print model_out
      </code>

      <subhead>Diversion to a temporary file</subhead>
      <para>
	The <opt>tempfile</opt> option is used to direct output to a
	temporary file, with an automatically constructed name that is
	guaranteed to be unique, in the user's <quote>dot</quote>
	directory. As in the redirection to buffer case, the option
	parameter should be the name of a string variable: in this
	case its content is over-written with the name of the
	temporary file.  Please note: files written to the dot
	directory are cleaned up on exit from the program, so don't
	use this form if you want the output to be preserved after
	your gretl session.
      </para>
      <para>
	We repeat the simple example from above, with a couple of
	extra lines to illustrate the points that <repl>strvar</repl>
	tells you where the output went, and you can retrieve it
	using the <fncref targ="readfile"/> function.
      </para>
      <code>
	open data4-10
	string mytemp
	outfile --tempfile=mytemp
	  ols ENROLL 0 CATHOL INCOME COLLEGE
	end outfile
	printf "Output went to %s\n", mytemp
	printf "The output was:\n%s\n", readfile(mytemp)
	# clean up if the file is no longer needed
	remove(mytemp)
      </code>
      <para>
	In some cases you may wish to exercise some control over the
	name of the temporary file. You can do this by supplying a
	string variable which contains six consecutive <lit>X</lit>s,
	as in
      </para>
      <code>
	string mytemp = "tmpXXXXXX.csv"
	outfile --tempfile=mytemp
	...
      </code>
      <para>
	In this case <lit>XXXXXX</lit> will be replaced by random
	characters that ensure uniqueness of the filename, but the
	<quote><lit>.csv</lit></quote> suffix will be preserved. As in
	the simpler case above, the file is automatically written into
	the user's <quote>dot</quote> directory and the content of the
	string variable passed via the option flag is modified to hold
	the full path to the temporary file.
      </para>

      <subhead>Quietness</subhead>
      <para>
	The effect of the <opt>quiet</opt> option is to turn off the
	echoing of commands and the printing of auxiliary messages
	while output is redirected. It is equivalent to doing
      </para>
      <code>
	set echo off
	set messages off
      </code>
      <para>
	except that when redirection is ended the original values of
	the <lit>echo</lit> and <lit>messages</lit> variables are
	restored. This option is available in all cases.
      </para>

      <subhead>Levels of redirection</subhead>
       <para>
	In general only one file can be opened in this way at any
	given time, so calls to this command cannot be
	nested. However, use of this command is permitted inside
	user-defined functions (provided the output file is also
	closed from inside the same function) such that output can be
	temporarily diverted and then given back to an original output
	file, in case <lit>outfile</lit> is currently in use by the
	caller. For example, the code
      </para>
      <code>
	function void f (string s)
	    outfile inner.txt
	      print s
	    end outfile
	end function

	outfile outer.txt --quiet
	  print "Outside"
	  f("Inside")
	  print "Outside again"
	end outfile
      </code>
      <para>
	will produce a file called <quote>outer.txt</quote> containing
	the two lines
      </para>
      <code>
	Outside
	Outside again
      </code>
      <para>
	and a file called <quote>inner.txt</quote> containing the line
      </para>
      <code>
	Inside
      </code>
    </description>

  </command>

  <command name="packages" section="Utilities" label="Function packages" context="gui">
    <description>
      <para>
	Gretl's functionality can be extended by the use of function
	packages. These come in two sorts: official
	<quote>Addons</quote> and contributed packages. Jointly, they
	cover many estimators and utilities not available as built-in
	commands or functions.
      </para>
      <para>
	The official Addons are included in the gretl installers for
	Windows and Mac. On Linux if they are not preinstalled then
	they are downloaded on demand (for example, if you select the
	menu item /Model/Univariate time series/GARCH variants, gretl will
	download the <lit>gig</lit> (GARCH in gretl) Addon. You can
	check that your Addons are up to date via <mnu
	targ="SFAddons">Check for addons</mnu> in the Help menu.
      </para>
      <para>
	You can browse the contributed packages installed on your
	computer via the menu item <mnu targ="LocalGfn">On local
	machine</mnu>, and if you are online you can access a listing
	of available packages via the item <mnu targ="RemoteGfn">On
	server</mnu>. Both items are found under /File/Function
	packages.
      </para>
      <para>
	Many packages offer to attach themselves to GUI menus. You can
	inspect these attachments via the <mnu
	targ="Registry">function package registry</mnu> (access via
	the Preferences button in the browser for installed packages).
      </para>
      <para>
	For full details on installing and working with function
	packages, see <mnu targ="Pkgbook">the Function package
	guide</mnu> (under the Help menu). This guide also contains
	details on writing function packages. A nice, short take on
	package writing can be found at
	<url>http://gretl.sf.net/gfnguide/gfn_for_dummies.html</url>.
      </para>
      <para>
	To deal with packages via the gretl command line, see the
	<cmdref targ="pkg"/> command.
      </para>
    </description>
  </command>

  <command name="panel" section="Estimation" label="Panel models">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
        <option>
	  <flag>--fixed-effects</flag>
	  <effect>estimate with group fixed effects</effect>
        </option>
        <option>
	  <flag>--random-effects</flag>
	  <effect>random effects or GLS model</effect>
        </option>
        <option>
	  <flag>--nerlove</flag>
	  <effect>use the Nerlove transformation</effect>
        </option>
        <option>
	  <flag>--pooled</flag>
	  <effect>estimate via pooled OLS</effect>
        </option>
        <option>
	  <flag>--between</flag>
	  <effect>estimate the between-groups model</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors; see below</effect>
        </option>
        <option>
	  <flag>--time-dummies</flag>
	  <effect>include time dummy variables</effect>
        </option>
        <option>
	  <flag>--unit-weights</flag>
	  <effect>weighted least squares</effect>
        </option>
        <option>
	  <flag>--iterate</flag>
	  <effect>iterative estimation</effect>
        </option>
        <option>
	  <flag>--matrix-diff</flag>
	  <effect>compute Hausman test via matrix difference</effect>
        </option>
	<option>
	  <flag>--unbalanced</flag>
	  <optparm>method</optparm>
	  <effect>random effects only, see below</effect>
	</option>
        <option>
	  <flag>--quiet</flag>
	  <effect>less verbose output</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>more verbose output</effect>
        </option>
      </options>
	<examples>
	<demos>
	  <demo>penngrow.inp</demo>
	</demos>
    </examples>
    </usage>

    <description>
      <para>
	Estimates a panel model.  By default the fixed effects estimator is
	used; this is implemented by subtracting the group or unit means from
	the original data.
      </para>
      <para context="cli">
	If the <opt>random-effects</opt> flag is given, random
	effects estimates are computed, by default using the method of
	<cite key="swamy72">Swamy and Arora (1972)</cite>. In this
	case (only) the option <opt>matrix-diff</opt> forces use of
	the matrix-difference method (as opposed to the regression
	method) for carrying out the Hausman test for the consistency
	of the random effects estimator. Also specific to the random
	effects estimator is the <opt>nerlove</opt> flag, which
	selects the method of <cite key="nerlove71">Nerlove
	(1971)</cite> as opposed to Swamy and Arora.
      </para>
      <para context="cli">
	Alternatively, if the <opt>unit-weights</opt> flag is given, the
	model is estimated via weighted least squares, with the weights
	based on the residual variance for the respective cross-sectional
	units in the sample.  In this case (only) the <opt>iterate</opt>
	flag may be added to produce iterative estimates: if the
	iteration converges, the resulting estimates are Maximum
	Likelihood.
      </para>
      <para context="cli">
	As a further alternative, if the <opt>between</opt> flag is
	given, the between-groups model is estimated (that is, an OLS
	regression using the group means).
      </para>
      <para context="cli">
	The default means of calculating robust standard errors in
	panel-data models is the Arellano HAC estimator, but
	Beck&ndash;Katz <quote>Panel Corrected Standard Errors</quote>
	can be selected via the command <lit>set pcse on</lit>. When
	the robust option is specified the joint <math>F</math> test
	on the fixed effects is performed using the robust method of
	<cite key="welch51">Welch (1951)</cite>.
      </para>
      <para context="gui">
	If the "Random effects" button is checked, random effects
	(GLS) estimates are computed. By default the method of Swamy
	and Arora is used for the GLS transformation, but the Nerlove
	method is available as an option (via the drop-down
	selector). A further option is <quote>Swamy-Arora /
	Baltagi-Chang</quote>: in the case of an unbalanced panel
	this invokes a modification of the Swamy-Arora method
	devised by <cite key="baltagi-chang94">Baltagi and Chang
	(1994)</cite>, otherwise it's just equivalent to regular
	Swamy-Arora.
      </para>
      <para context="cli">
	The <opt>unbalanced</opt> option is available only for random
	effects models: it can be used to choose an ANOVA method for
	use with an unbalanced panel. By default gretl uses the
	Swamy&ndash;Arora method as for balanced panels, except that
	the harmonic mean of the individual time-series lengths is
	used in place of a common <math>T</math>. Under this option
	you can specify either <lit>bc</lit>, to use the method of
	<cite key="baltagi-chang94">Baltagi and Chang (1994)</cite>,
	or <lit>stata</lit>, to emulate the <lit>sa</lit> option to
	the <lit>xtreg</lit> command in Stata.
      </para>
      <para>
	For more details on panel estimation, please see <guideref
	  targ="chap:panel"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Panel</menu-path>
    </gui-access>

  </command>

  <command name="panel-between" section="Estimation" context="gui"
    label="Between groups model">

    <description>
      <para>
	This dialog allows you to enter a specification for the
	<quote>between model</quote> in the context of panel data.  This
	regression uses the group-means of the data, thereby ignoring the
	variation within the groups.  This model is rarely of great
	interest in its own right, but may be useful for purposes of
	comparison (for example, with the fixed effects model).
      </para>
    </description>

  </command>

  <command name="panel-mode" section="Dataset" context="gui"
    label="Panel data organization">

    <description>
      <para>
	This dialog offers up to three options with regard to defining a
	data set as a panel.  The first two options require that the data
	set is already organized in a panel format (although this may not
	yet be recognized by gretl).  The third option requires that the
	data set contains variables that represent the panel structure.
      </para>
      <para>
	<emphasis>Stacked time series</emphasis>: Let there be <repl>N</repl>
	cross-sectional units in the data set, and let <repl>T</repl> = the
	number of time-series observations per unit.  By selecting this option
	you are telling gretl that the data set is currently composed of
	<repl>N</repl> consecutive blocks of <repl>T</repl> time-series
	observations, one for each cross-sectional unit.  The next step will
	be to specify the value of <repl>N</repl>.
      </para>
      <para>
	<emphasis>Stacked cross sections</emphasis>: You are telling gretl
	that the data set is currently composed of <repl>T</repl> consecutive
	blocks of <repl>N</repl> cross-sectional observations, one for each
	time period. The next step, again, will be to specify the value of
	<repl>N</repl>.
      </para>
      <para>
	If the total number of observations in the current dataset is
	prime, the above options are not available.
      </para>
      <para>
	<emphasis>Use index variables</emphasis>: You are saying that the data
	set is currently organized any old way (it doesn't matter how), but
	that it contains two variables that index the cross-sectional units
	and the time periods respectively.  The next step will be to select
	those two variables.  Panel index variables must have nothing but
	non-negative integer values, with no missing values.  If there are no
	such variables in the dataset this option is not available.
      </para>
    </description>

  </command>

  <command name="panel-wls" section="Estimation" context="gui"
    label="Groupwise weighted least squares">

    <description>
      <para>
	Groupwise weighted least squares for panel data.  Computes
	weighted least squares (WLS) estimates, with the weights based on
	the estimated error variances for the respective cross-sectional
	units in the sample.
      </para>
      <para>
	If the iteration option is selected, the procedure is iterated: at
	each round the residuals are re-computed using the current WLS
	parameter estimates, which gives rise to a new set of estimates of
	the error variances, and a hence a new set of weights. Iteration
	stops when the maximum difference in the parameter estimates from
	one round to the next falls below 0.0001 or the number of
	iterations reaches 20.  If the iteration converges, the resulting
	estimates are Maximum Likelihood.
      </para>
    </description>

  </command>

  <command name="panplot" section="Graphs"
   label="plot a panel series" context="cli">

    <usage>
      <arguments>
        <argument>plotvar</argument>
      </arguments>
      <options>
        <option>
	  <flag>--means</flag>
	  <effect>time series, group means</effect>
        </option>
        <option>
	  <flag>--overlay</flag>
	  <effect>plot per group, overlaid, N &lt;= 130</effect>
        </option>
        <option>
	  <flag>--sequence</flag>
	  <effect>plot per group, in sequence, N &lt;= 130</effect>
        </option>
        <option>
	  <flag>--grid</flag>
	  <effect>plot per group, in grid, N &lt;= 16</effect>
        </option>
        <option>
	  <flag>--stack</flag>
	  <effect>plot per group, stacked, N &lt;= 6</effect>
        </option>
        <option>
	  <flag>--boxplots</flag>
	  <effect>boxplot per group, in sequence, N &lt;= 150</effect>
        </option>
        <option>
	  <flag>--boxplot</flag>
	  <effect>single boxplot, all groups</effect>
        </option>
	<option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>
      </options>
      <examples>
        <example>panplot x --overlay</example>
        <example>panplot x --means --output=display</example>
      </examples>
    </usage>

    <description>
      <para>
	Graphing command specific to panel data: the series
	<repl>plotvar</repl> is plotted in a mode specified by one or
	other of the options.
      </para>
      <para>
	Apart from the <opt>means</opt> and <opt>boxplot</opt> options
	the plot explicitly represents variation in both the
	time-series and cross-sectional dimensions.  Such plots are
	limited in respect of the number of groups (also known as
	individuals or units) in the current sample range of the
	panel. For example, the <opt>overlay</opt> option, which shows
	a time series for each group in a single plot, is available
	only when the number of groups, <math>N</math>, is 130 or
	less. (Otherwise the graphic becomes too dense to be
	informative.) If a panel is too large to permit the desired
	plot specification one can select a reduced range of groups or
	units temporarily, as in
      </para>
      <code>
	smpl 1 100 --unit
	panplot x --overlay
	smpl full
      </code>
      <para>
	The <opt>output=</opt><repl>filename</repl> option can be
	used to control the form and destination of the output; see
	the <cmdref targ="gnuplot"/> command for details.
      </para>

    </description>

    <gui-access>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="panspec" section="Tests" label="Panel specification">
    <usage>
      <options>
	<option>
	  <flag>--nerlove</flag>
	  <effect>use Nerlove method for random effects</effect>
	</option>
	<option>
	  <flag>--matrix_diff</flag>
	  <effect>use matrix-difference method for Hausman test</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>Suppress printed output</effect>
	</option>
      </options>
    </usage>
    <description>
      <para>
	This command is available only after estimating a panel-data
	model via OLS. It tests the simple pooled specification
	against the most common alternatives, fixed effects and random
	effects.
      </para>
      <para>
	The fixed effects specification allows the intercept of the
	regression to vary across the cross-sectional units.  A Wald
	<math>F</math>-test is reported for the null hypotheses that
	the intercepts do not differ. The random effects specification
	decomposes the residual variance into two parts, one part
	specific to the cross-sectional unit and the other specific to
	the particular observation.  (This estimator can be computed
	only if the number of cross-sectional units in the data set
	exceeds the number of parameters to be estimated.) The
	Breusch&ndash;Pagan LM statistic tests the null hypothesis
	that pooled OLS is adequate against the random effects
	alternative.
      </para>
      <para>
	Pooled OLS may be rejected against both of the alternatives.
	Provided the unit- or group-specific error is uncorrelated
	with the independent variables, the random effects estimator
	is more efficient than fixed effects; otherwise the random
	effects estimator is inconsistent and fixed effects are to be
	preferred. The null hypothesis for the Hausman test is that
	the group-specific error is <emphasis>not</emphasis> so
	correlated (and therefore the random effects estimator is
	preferable).  A low p-value for this test counts against
	random effects and in favor of fixed effects.
      </para>
      <para>
	The first two options for this command pertain to random
	effects estimation.  By default the method of Swamy and Arora
	is used, and the Hausman test statistic is calculated using
	the regression method. The options enable the use of Nerlove's
	alternative variance estimator, and/or the matrix-difference
	approach to the Hausman statistic.
      </para>
      <para>
	On successful completion the accessors <fncref targ="$test"/>
	and <fncref targ="$pvalue"/> retrieve 3-vectors holding test
	statistics and p-values for the three tests noted above:
	poolability (Wald), poolability (Breusch&ndash;Pagan), and
	Hausman. If you just want the results in this form you can
	give the <opt>quiet</opt> option to skip printed output.
      </para>
      <para>
	Note that after estimating the random effects specification
	via the <cmd>panel</cmd> command, the Hausman test is
	automatically carried out and the results can be retrieved via
	the <fncref targ="$hausman"/> accessor.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Panel specification</menu-path>
    </gui-access>

  </command>

  <command name="pca" section="Statistics"
    label="Principal Components Analysis">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--covariance</flag>
	  <effect>use the covariance matrix</effect>
        </option>
        <option>
	  <flag>--save</flag>
	  <optparm optional="true">n</optparm>
	  <effect>save major components</effect>
        </option>
        <option>
	  <flag>--save-all</flag>
	  <effect>save all components</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
      </options>
    </usage>

    <description context="gui">
      <para>
	Principal Components Analysis.  Prints the eigenvalues of the
	correlation matrix (or the covariance matrix if the option box is
	checked) for the variables in <repl>varlist</repl>, along with the
	proportion of the joint variance accounted for by each component.
	Also prints the corresponding eigenvectors (or <quote>component
	  loadings</quote>).
      </para>
      <para>
	In the window displaying the results, you have the option of
	saving the principal components to the dataset as series.
      </para>
    </description>

    <description context="cli">
      <para>
	Principal Components Analysis. Unless the <opt>quiet</opt>
	option is given, prints the eigenvalues of the correlation
	matrix (or the covariance matrix if the
	<opt>covariance</opt> option is given) for the variables
	in <repl>varlist</repl>, along with the proportion of the
	joint variance accounted for by each component. Also prints
	the corresponding eigenvectors or <quote>component
	loadings</quote>.
      </para>
      <para>
	If you give the <opt>save-all</opt> option then all
	components are saved to the dataset as series, with names
	<lit>PC1</lit>, <lit>PC2</lit> and so on. These artificial
	variables are formed as the sum of (component loading) times
	(standardized <math>X</math><sub>i</sub>), where
	<math>X</math><sub>i</sub> denotes the <math>i</math>th
	variable in <repl>varlist</repl>.
      </para>
      <para>
	If you give the <opt>save</opt> option without a parameter
	value, components with eigenvalues greater than the mean
	(which means greater than 1.0 if the analysis is based on the
	correlation matrix) are saved to the dataset as described
	above. If you provide a value for <repl>n</repl> with this
	option then the most important <repl>n</repl> components are
	saved.
      </para>
      <para>
	See also the <fncref targ="princomp"/> function.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Principal components</menu-path>
    </gui-access>

  </command>

  <command name="pergm" section="Statistics" label="Periodogram">

    <usage>
      <arguments>
        <argument>series</argument>
        <argument optional="true">bandwidth</argument>
      </arguments>
      <options>
        <option>
	  <flag>--bartlett</flag>
	  <effect>use Bartlett lag window</effect>
        </option>
        <option>
	  <flag>--log</flag>
	  <effect>use log scale</effect>
        </option>
        <option>
	  <flag>--radians</flag>
	  <effect>show frequency in radians</effect>
        </option>
        <option>
	  <flag>--degrees</flag>
	  <effect>show frequency in degrees</effect>
        </option>
	<option>
	  <flag>--plot</flag>
	  <optparm>mode-or-filename</optparm>
	  <effect>see below</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Computes and displays the spectrum of the specified series.
	By default the sample periodogram is given, but optionally a
	Bartlett lag window is used in estimating the spectrum (see,
	for example, Greene's <book>Econometric Analysis</book> for a
	discussion of this).  The default width of the Bartlett window
	is twice the square root of the sample size but this can be
	set manually using the <repl>bandwidth</repl> parameter, up to
	a maximum of half the sample size.
      </para>
      <para>
	If the <opt>log</opt> option is given the spectrum is
	represented on a logarithmic scale.
      </para>
      <para>
	The (mutually exclusive) options <opt>radians</opt> and
	<opt>degrees</opt> influence the appearance of the frequency
	axis when the periodogram is graphed. By default the frequency
	is scaled by the number of periods in the sample, but these
	options cause the axis to be labeled from 0 to &pi; radians
	or from 0 to 180&deg;, respectively.
      </para>
      <para>
	By default, if the program is not in batch mode a plot of the
	periodogram is shown.  This can be adjusted via the
	<opt>plot</opt> option. The acceptable parameters to this
	option are <lit>none</lit> (to suppress the plot);
	<lit>display</lit> (to display a plot even when in batch
	mode); or a file name. The effect of providing a file name is
	as described for the <opt>output</opt> option of the <cmdref
	targ="gnuplot"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Periodogram</menu-path>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>
  </command>

  <command name="pkg" section="Utilities" context="cli">
    <usage>
      <arguments>
	<argument>action</argument>
        <argument>pkgname</argument>
      </arguments>
      <options>
        <option>
	  <flag>--local</flag>
	  <effect>install from local file</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>see below</effect>
        </option>
      </options>
     <examples>
        <example>pkg install armax</example>
	<example>pkg install /path/to/myfile.gfn --local</example>
	<example>pkg query ghosts</example>
	<example>pkg unload armax</example>
     </examples>
    </usage>

    <description>
      <para>
	This command provides a means of installing, unloading,
	querying or deleting gretl function packages. The
	<repl>action</repl> argument must be one of
	<lit>install</lit>, <lit>query</lit>, <lit>unload</lit>,
	<lit>remove</lit> or <lit>index</lit>.
      </para>
      <para>
	<lit>install</lit>: In the most basic form, with no option
	flag and the <repl>pkgname</repl> argument given as the
	<quote>plain</quote> name of a gretl function package (as in
	the first example above), the effect is to download the
	specified package from the gretl server (unless
	<repl>pkgname</repl> starts with <lit>http://</lit>) and
	install it on the local machine. In this case it is not
	necessary to supply a filename extension. If the
	<opt>local</opt> option is given, however,
	<repl>pkgname</repl> should be the path to an uninstalled
	package file on the local machine, with the correct extension
	(<lit>.gfn</lit> or <lit>.zip</lit>). In this case the effect
	is to copy the file into place (<lit>gfn</lit>), or unzip it
	into place (<lit>zip</lit>), <quote>into place</quote> meaning
	where the <cmdref targ="include"/> command will find it.
      </para>
      <para>
	<lit>query</lit>: The default effect is to print basic
	information about the specified package (author, version,
	etc.). But if the <opt>quiet</opt> option is appended nothing
	is printed; the package information is instead stored in the
	form of a gretl bundle, which can be accessed via <fncref
	targ="$result"/>. If no information can be found this bundle
	will be empty.
      </para>
      <para>
	<lit>unload</lit>: <repl>pkgname</repl> should be given in
	plain form, without path or suffix as in the last example
	above.  The effect is to unload the package in question from
	gretl's memory, if it is currently loaded, and also to remove
	it from the GUI menu to which it is attached, if any.
      </para>
      <para>
	<lit>remove</lit>: performs the actions noted for
	<lit>unload</lit> and in addition deletes the file(s)
	associated with the package from disk.
      </para>
      <para>
	<lit>index</lit>: is a special case in which
	<repl>pkgname</repl> must be replaced by the keyword
	<quote><lit>addons</lit></quote>: the effect is to update the
	index of the standard packages known as addons. Such updating
	is performed automatically from time to time but in some cases
	a manual update may be useful. In this case the
	<opt>verbose</opt> flag produces a printout of where gretl has
	searched and what it has found. To be clear, here's the way to
	get full indexing output:
      </para>
      <code>
	pkg index addons --verbose
      </code>
    </description>

    <gui-access>
      <menu-path>/File/Function packages/On server</menu-path>
    </gui-access>
  </command>

  <command name="pkg-depends" section="Programming"
	   label="Dependencies" context="gui">
    <description>
      <para>
	By <quote>dependencies</quote> we mean function packages
	upon which your package depends for some of its
	functionality, and which must therefore be installed and
	loaded in order for your package to work correctly. You
	may specify up to four such packages in this dialog.
      </para>
      <para>
	For example, suppose you make use of one or more functions
	from the <lit>extra</lit> package, which contains various
	utilities for hansl scripting. Then you would enter
      </para>
      <code>
	extra
      </code>
      <para>
	in one of the available slots, the first slot if there are no
	other dependencies. Note that you should give the plain name
	of the package, without a full path or the <lit>.gfn</lit> or
	<lit>.zip</lit> suffix.
      </para>
      <para>
	Optionally, the first dependency can be marked as a
	<quote>provider</quote>. The effect of this is to give your
	package access to any private functions in the specified
	package. This should not usually be required but can be useful
	when two packages are designed to work closely together.
      </para>
    </description>
  </command>

  <command name="plot" section="Graphs" context="cli">
    <usage>
      <arguments>
        <argument optional="true">data</argument>
      </arguments>
      <options>
        <option>
	  <flag>--with-lines</flag>
	  <optparm optional="true">varspec</optparm>
	  <effect>use lines, not points</effect>
        </option>
        <option>
	  <flag>--with-lp</flag>
	  <optparm optional="true">varspec</optparm>
	  <effect>use lines and points</effect>
        </option>
        <option>
	  <flag>--with-impulses</flag>
	  <optparm optional="true">varspec</optparm>
	  <effect>use vertical lines</effect>
        </option>
        <option>
	  <flag>--with-steps</flag>
	  <optparm optional="true">varspec</optparm>
	  <effect>use horizontal and vertical line segments</effect>
        </option>
        <option>
	  <flag>--time-series</flag>
	  <effect>plot against time</effect>
        </option>
        <option>
	  <flag>--single-yaxis</flag>
	  <effect>force use of just one y-axis</effect>
        </option>
        <option>
	  <flag>--ylogscale</flag>
	  <optparm optional="true">base</optparm>
	  <effect>use log scale for vertical axis</effect>
        </option>
        <option>
	  <flag>--dummy</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--fit</flag>
	  <optparm>fitspec</optparm>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--band</flag>
	  <optparm>bandspec</optparm>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--band-style</flag>
	  <optparm>style</optparm>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>
      </options>
	<examples>
	<demos>
	  <demo>nile.inp</demo>
	</demos>
    </examples>
    </usage>

    <description>
      <para>
	The <lit>plot</lit> block provides an alternative to the
	<cmdref targ="gnuplot"/> command which may be more convenient
	when you are producing an elaborate plot (with several options
	and/or gnuplot commands to be inserted into the plot file).
	In addition to the following explanation, please also refer
	to <guideref targ="chap:graphs"/> for some further examples.
      </para>
      <para>
	A <lit>plot</lit> block starts with the command-word
	<lit>plot</lit>. This is commonly followed by a
	<repl>data</repl> argument, which specifies data to be
	plotted: this should be the name of a list, a matrix, or a
	single series. If no input data are specified the block must
	contain at least one directive to plot a formula instead; such
	directives may be given via <lit>literal</lit> or
	<lit>printf</lit> lines (see below).
      </para>
      <para>
	If a list or matrix is given, the last element (list) or
	column (matrix) is assumed to be the <math>x</math>-axis
	variable and the other(s) the <math>y</math>-axis variable(s),
	unless the <opt>time-series</opt> option is given in which
	case all the specified data go on the <math>y</math> axis.
      </para>
      <para>
	The option of supplying a single series name is restricted to
	time-series data, in which case it is assumed that a
	time-series plot is wanted; otherwise an error is flagged.
      </para>
      <para>
	The starting line may be prefixed with
	the <quote><repl>savename</repl> <lit>&lt;-</lit></quote>
	apparatus to save a plot as an icon in the GUI program.
	The block ends with <lit>end plot</lit>.
      </para>
      <para>
	Inside the block you have zero or more lines of these types,
	identified by an initial keyword:
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>option</lit>: specify a single option.
	  </para>
	</li>
	<li>
	  <para>
	  <lit>options</lit>: specify multiple options on a single
	  line, separated by spaces.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>literal</lit>: a command to be passed to gnuplot
	    literally.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>printf</lit>: a printf statement whose result will be
	    passed to gnuplot literally.
	  </para>
	</li>
      </ilist>
      <para>
	Note that when you specify an option using the
	<lit>option</lit> or <lit>options</lit> keywords, it is not
	necessary to supply the customary double-dash before the
	option specifier. For details on the effects of the various
	options please see <cmdref targ="gnuplot"/> (but see
	below for some specifics on using the <opt>band</opt>
	option in the <lit>plot</lit> context).
      </para>
      <para>
	The intended use of the <lit>plot</lit> block is best
	illustrated by example:
      </para>
      <code>
	string title = "My title"
	string xname = "My x-variable"
	plot plotmat
	    options with-lines fit=none
	    literal set linetype 3 lc rgb "#0000ff"
	    literal set nokey
	    printf "set title \"%s\"", title
	    printf "set xlabel \"%s\"", xname
	end plot --output=display
      </code>
      <para>
	This example assumes that <lit>plotmat</lit> is the name of a
	matrix with at least 2 columns (or a list with at least two
	members).  Note that it is considered good practice to place
	the <opt>output</opt> option (only) on the last line of the
	block; other options should be placed within the block.
      </para>
      <subhead>Plotting a band with matrix data</subhead>
      <para>
	The <opt>band</opt> and <opt>band-style</opt> options mostly
	work as described in the help for <cmdref targ="gnuplot"/>,
	with the following exception: when the data to be plotted are
	given in the form of a matrix, the first parameter to
	<opt>band</opt> must be given as the name of a matrix with two
	columns (holding, respectively, the center and the width of
	the band). This parameter takes the place of the two values
	(series names or ID numbers, or matrix columns) required by
	the <lit>gnuplot</lit> version of this option. An illustration
	follows:
      </para>
      <code>
	scalar n = 100
	matrix x = seq(1,n)'
	matrix y = x + filter(mnormal(n,1), 1, {1.8, -0.9})
	matrix B = y ~ muniform(n,1)
	plot y
	    options time-series with-lines
	    options band=B,10 band-style=fill
	end plot --output=display
      </code>
      <subhead>Plotting without data</subhead>
      <para>
	The following example shows a simple case of specifying a
	plot without a data source.
      </para>
      <code>
	plot
	    literal set title 'CRRA utility'
	    literal set xlabel 'c'
	    literal set ylabel 'u(c)'
	    literal set xrange[1:3]
	    literal set key top left
	    literal crra(x,s) = (x**(1-s) - 1)/(1-s)
	    printf "plot crra(x, 0) t 'sigma=0', \\"
	    printf " log(x) t 'sigma=1', \\"
	    printf " crra(x,3) t 'sigma=3"
	end plot --output=display
      </code>
    </description>

  </command>

  <command name="polyweights" section="Transformations" context="gui"
    label="Polynomial trend fitting">

    <description>
      <para>
	In fitting a polynomial trend to a time series it may be
	desirable to give extra weight to the observations at the
	start and end of the sample. (Points in the middle of the
	sample range have neighbours on both sides that are likely to
	be pulling the fit in the same general direction.)
      </para>
      <para>
	The weighting schemes offered here (quadratic, cosine-bell and
	steps) can be used to this effect. If you select one of these
	schemes two additional settings must be chosen: first, what
	maximum weight should be used (the minimum, baseline weight is
	1.0)?  Second, what central fraction of the sample should be
	given a uniform (minimal) weighting?
      </para>
      <para>
	Suppose, for example, you select a maximum weight of 3.0 and a
	central fraction of 0.4. This means that the middle 40 percent
	of the data get a weight of 1.0. If the <quote>steps</quote>
	shape is selected the first and last 30 percent of the
	observations get a weight of 3.0; otherwise, for the first 30
	percent of observations the weights decline gradually from 3.0
	to 1.0; and for the last 30 percent the weights increase from
	1.0 to 3.0.
      </para>
    </description>

  </command>

  <command name="poisson" section="Estimation"
    label="Poisson estimation">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
        <argument separated="true" optional="true">offset</argument>
      </arguments>
      <options>
        <option>
          <flag>--robust</flag>
          <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
        <option>
          <flag>--vcv</flag>
          <effect>print covariance matrix</effect>
        </option>
        <option>
          <flag>--verbose</flag>
          <effect>print details of iterations</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
      </options>
      <examples>
        <example>poisson y 0 x1 x2</example>
	<example>poisson y 0 x1 x2 ; S</example>
	<demos>
	  <demo>camtriv.inp</demo>
	  <demo>greene19_3.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	Estimates a poisson regression.  The dependent variable is taken to
	represent the occurrence of events of some sort, and must take on only
	non-negative integer values.
      </para>
      <para>
	If a discrete random variable <math>Y</math> follows
	the Poisson distribution, then
        <equation status="display"
          tex="\[\mathrm{Pr}(Y = y) = \frac{e^{-v} v^y}{y!}\]"
          ascii="Pr(Y = y) = exp(-v) * v^y / y!"
          graphic="poisson1"/>
	for <math>y</math> = 0, 1,
      2,&hellip;.  The mean and variance of the distribution are both
      equal to <math>v</math>.  In the Poisson regression model,
      the parameter <math>v</math> is represented as a function
      of one or more independent variables.  The most common version
      (and the only one supported by gretl) has
        <equation status="display"
          tex="\[v = \mathrm{exp}(\beta_0+\beta_1 x_1+\beta_2 x_2 + \cdots)\]"
          ascii="v = exp(b0 + b1*x1 + b2*x2 + ...)"
          graphic="poisson2"/>
	or in other words the log of
      <math>v</math> is a linear function of the independent
      variables.
      </para>
      <para>
	Optionally, you may add an <quote>offset</quote> variable to the
	specification.  This is a scale variable, the log of which is added to
	the linear regression function (implicitly, with a coefficient of
	1.0).  This makes sense if you expect the number of occurrences of the
	event in question to be proportional, other things equal, to some
	known factor.  For example, the number of traffic accidents might be
	supposed to be proportional to traffic volume, other things equal, and
	in that case traffic volume could be specified as an
	<quote>offset</quote> in a Poisson model of the accident rate. The
	offset variable must be strictly positive.
      </para>
      <para>
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <opt>robust</opt> flag is given,
	then QML or Huber&ndash;White standard errors are calculated
	instead. In this case the estimated covariance matrix is a
	<quote>sandwich</quote> of the inverse of the estimated Hessian
	and the outer product of the gradient.
      </para>
      <para>
	See also <cmdref targ="negbin"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Count data</menu-path>
    </gui-access>

  </command>

  <command name="print" section="Printing"
    label="Print data or strings" context="cli">

    <usage>
      <altforms>
        <altform><lit>print</lit> <repl>varlist</repl></altform>
	<altform><lit>print</lit></altform>
	<altform><lit>print</lit> <repl>object-names</repl></altform>
        <altform><lit>print</lit> <repl>string-literal</repl></altform>
      </altforms>
      <options>
	<option>
	  <flag>--byobs</flag>
	  <effect>by observations</effect>
	</option>
	<option>
	  <flag>--no-dates</flag>
	  <effect>use simple observation numbers</effect>
	</option>
	<option>
	  <flag>--range</flag>
	  <optparm>start:stop</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--midas</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--tree</flag>
	  <effect>specific to bundles; see below</effect>
	</option>
      </options>
      <examples>
	<example>print x1 x2 --byobs</example>
	<example>print my_matrix</example>
	<example>print "This is a string"</example>
	<example>print my_array --range=3:6</example>
	<example>print hflist --midas</example>
      </examples>
    </usage>

    <description>
      <para>
	Please note that <lit>print</lit> is a rather
	<quote>basic</quote> command (primarily intended for printing
	the values of series); see <cmdref targ="printf"/> and <cmdref
	targ="eval"/> for more advanced, and less restrictive,
	alternatives.
      </para>
      <para>
	In the first variant shown above (also see the first example),
	<repl>varlist</repl> should be a list of series (either a
	named list or a list specified via the names or ID numbers of
	series, separated by spaces). In that case this command prints
	the values of the listed series. By default the data are
	printed <quote>by variable</quote>, but if the
	<opt>byobs</opt> flag is added they are printed by
	observation. When printing by observation, the default is to
	show the date (with time-series data) or the observation
	marker string (if any) at the start of each line. The
	<opt>no-dates</opt> option suppresses the printing of dates or
	markers; a simple observation number is shown instead. See
	the final paragraph of this entry for the effect of the
	<opt>midas</opt> option (which applies only to a named
	list of series).
      </para>
      <para>
	If no argument is given (the second variant shown above) then
	the action is similar to the first case except that
	<emphasis>all</emphasis> series in the current dataset are
	printed. The supported options are as decribed above.
      </para>
      <para>
	The third variant (with the <repl>object-names</repl>
	argument; see the second example) expects a space-separated
	list of names of primary gretl objects other than series
	(scalars, matrices, strings, bundles, arrays). The value(s) of
	these objects are displayed. In the case of bundles, their
	members are sorted by type and alphabetically.
      </para>
      <para>
	In the fourth form (third example),
	<repl>string-literal</repl> should be a string enclosed in
	double-quotes (and there should be nothing else following on
	the command line). The string in question is printed, followed
	by a newline character.
      </para>
      <para>
	The <opt>range</opt> option can be used to control the amount
	of information printed. The <repl>start</repl> and
	<repl>stop</repl> (integer) values refer to observations for
	series and lists, rows for matrices, elements for arrays, and
	lines of text for strings. In all cases the minimum
	<repl>start</repl> value is 1 and the maximum
	<repl>stop</repl> value is the <quote>row-wise size</quote> of
	the object in question. Negative values for these indices are
	taken to indicate a count back from the end. The indices may
	be given in numeric form or as the names of predefined scalar
	variables. If <repl>start</repl> is omitted that is taken as
	an implicit 1 and if <repl>stop</repl> is omitted that means
	go all the way to the end. Note that with series and lists the
	indices are relative to the current sample range.
      </para>
      <para>
	The <opt>tree</opt> option is specific to the printing of a
	gretl bundle: the effect is that if the specified bundle
	contains further bundles, or arrays of bundles, their contents
	are listed. Otherwise only the top-level members of the
	bundle are listed.
      </para>
      <para>
	The <opt>midas</opt> option is specific to the printing of a
	list of series, and moreover it is specific to datasets that
	contain one or more high-frequency series, each represented by
	a <cmdref targ="MIDAS_list"/>. If one such list is given as
	argument and this option is appended, the series is printed by
	observation at its <quote>native</quote> frequency.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Display values</menu-path>
    </gui-access>

  </command>

  <command name="printf" section="Printing"
    label="Formatted printing" context="cli">

    <usage>
      <arguments>
        <argument>format</argument>
	<argpunct>, </argpunct>
        <argument>args</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Prints scalar values, series, matrices, or strings under the
	control of a format string (providing a subset of the
	<lit>printf</lit> function in the C programming language).
	Recognized numeric formats are <lit>%e</lit>, <lit>%E</lit>,
	<lit>%f</lit>, <lit>%g</lit>, <lit>%G</lit>, <lit>%d</lit> and
	<lit>%x</lit>, in each case with the various modifiers
	available in C.  Examples: the format <lit>%.10g</lit> prints
	a value to 10 significant figures; <lit>%12.6f</lit> prints a
	value to 6 decimal places, with a width of 12
	characters. Note, however, that in gretl the format
	<lit>%g</lit> is a good default choice for all numerical
	values; you don't need to get too complicated.  The format
	<lit>%s</lit> should be used for strings.
      </para>
      <para>
	The format string itself must be enclosed in double quotes.
	The values to be printed must follow the format string,
	separated by commas.  These values should take the form of
	either (a) the names of variables, (b) expressions that
	yield some sort of printable result, or (c) the special
	functions <lit>varname()</lit> or <lit>date()</lit>.  The
	following example prints the values of two variables plus that
	of a calculated expression:
      </para>
      <code>
	ols 1 0 2 3
	scalar b = $coeff[2]
	scalar se_b = $stderr[2]
	printf "b = %.8g, standard error %.8g, t = %.4f\n",
          b, se_b, b/se_b
      </code>
      <para>
	The next lines illustrate the use of the varname and date
	functions, which respectively print the name of a variable,
	given its ID number, and a date string, given a 1-based
	observation number.
      </para>
      <code>
	printf "The name of variable %d is %s\n", i, varname(i)
	printf "The date of observation %d is %s\n", j, date(j)
      </code>
      <para>
	If a matrix argument is given in association with a numeric
	format, the entire matrix is printed using the specified
	format for each element.  The same applies to series, except
	that the range of values printed is governed by the current
	sample setting.
      </para>
      <para>
	The maximum length of a format string is 127 characters.  The
	escape sequences <lit>\n</lit> (newline), <lit>\r</lit>
	(carriage return), <lit>\t</lit> (tab), <lit>\v</lit>
	(vertical tab) and <lit>\\</lit> (literal backslash) are
	recognized.  To print a literal percent sign, use
	<lit>%%</lit>.
      </para>
      <para>
	As in C, numerical values that form part of the format (width
	and or precision) may be given directly as numbers, as in
	<lit>%10.4f</lit>, or they may be given as variables.  In the
	latter case, one puts asterisks into the format string and
	supplies corresponding arguments in order.  For example,
      </para>
      <code>
	scalar width = 12
	scalar precision = 6
	printf "x = %*.*f\n", width, precision, x
      </code>
    </description>

  </command>

  <command name="probit" section="Estimation"
    label="Probit model">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
	</option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
	<option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
	</option>
	<option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
	</option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
	<option>
	  <flag>--p-values</flag>
	  <effect>show p-values instead of slopes</effect>
	</option>
	<option>
	  <flag>--estrella</flag>
	  <effect>select pseudo-R-squared variant</effect>
	</option>
	<option>
	  <flag>--random-effects</flag>
	  <effect>estimates a random effects panel probit model</effect>
	</option>
	<option>
	  <flag>--quadpoints</flag>
	  <optparm>k</optparm>
	  <effect>number of quadrature points for RE estimation</effect>
	</option>
      </options>
      <examples>
	<demos>
	  <demo>ooballot.inp</demo>
	  <demo>oprobit.inp</demo>
	  <demo>reprobit.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	If the dependent variable is a binary variable (all values are
	0 or 1) maximum likelihood estimates of the coefficients on
	<repl>indepvars</repl> are obtained via the
	Newton&ndash;Raphson method. As the model is nonlinear the
	slopes depend on the values of the independent variables.  By
	default the slopes with respect to each of the independent
	variables are calculated (at the means of those variables) and
	these slopes replace the usual p-values in the regression
	output.  This behavior can be suppressed by giving the
	<opt>p-values</opt> option. The chi-square statistic tests the
	null hypothesis that all coefficients are zero apart from the
	constant.
      </para>
      <para context="cli">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <opt>robust</opt> flag is
	given, then QML or Huber&ndash;White standard errors are
	calculated instead. In this case the estimated covariance
	matrix is a <quote>sandwich</quote> of the inverse of the
	estimated Hessian and the outer product of the gradient. See
	chapter 10 of Davidson and MacKinnon for details.
      </para>
      <para context="cli">
	By default the pseudo-R-squared statistic suggested by <cite
	key="mcfadden74">McFadden (1974)</cite> is shown, but in the
	binary case if the <opt>estrella</opt> option is given, the
	variant recommended by <cite key="estrella98">Estrella
	(1998)</cite> is shown instead. This variant arguably mimics
	more closely the properties of the regular
	<math>R</math><sup>2</sup> in the context of least-squares
	estimation.
      </para>
      <para context="gui">
	By default, standard errors are computed using the negative
	inverse of the Hessian.  If the <quote>Robust standard
	errors</quote> box is checked, then QML or Huber&ndash;White
	standard errors are calculated instead. In this case the
	estimated covariance matrix is a <quote>sandwich</quote> of
	the inverse of the estimated Hessian and the outer product of
	the gradient.  See chapter 10 of Davidson and MacKinnon for
	details.
      </para>
      <para>
	If the dependent variable is not binary but is discrete, then
	Ordered Probit estimates are obtained.  (If the variable
	selected as dependent is not discrete, an error is flagged.)
      </para>
      <subhead>Probit for panel data</subhead>
      <para>
	With the <opt>random-effects</opt> option, the error
	term is assumed to be composed of two normally distributed
	components: one time-invariant term that is specific to the
	cross-sectional unit or <quote>individual</quote> (and is
	known as the individual effect); and one term that is specific
	to the particular observation.
      </para>
      <para>
	Evaluation of the likelihood for this model involves the use
	of Gauss-Hermite quadrature for approximating the value of
	expectations of functions of normal variates. The number of
	quadrature points used can be chosen through the
	<opt>quadpoints</opt> option (the default is 32). Using more
	points will increase the accuracy of the results, but at the
	cost of longer compute time; with many quadrature points and a
	large dataset estimation may be quite time consuming.
      </para>
      <para>
	Besides the usual parameter estimates (and associated
	statistics) relating to the included regressors, certain
	additional information is presented on estimation of this
	sort of model:
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>lnsigma2</lit>: the maximum likelihood estimate of
	    the log of the variance of the individual effect;
	  </para>
	</li>
	<li>
	  <para>
	    <lit>sigma_u</lit>: the estimated standard deviation of
	    the individual effect; and
	  </para>
	</li>
	<li>
	  <para>
	    <lit>rho</lit>: the estimated share of the individual
	    effect in the composite error variance (also known as the
	    intra-class correlation).
	  </para>
	</li>
      </ilist>
      <para>
	The Likelihood Ratio test of the null hypothesis that
	<lit>rho</lit> equals zero provides a means of assessing
	whether the random effects specification is needed. If
	the null is not rejected that suggests that a simple
	pooled probit specification is adequate.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Probit</menu-path>
    </gui-access>

  </command>

  <command name="pvalue" section="Statistics" label="Compute p-values" context="cli">

    <usage>
      <arguments>
        <argument>dist</argument>
        <argument optional="true">params</argument>
	<argument>xval</argument>
      </arguments>
      <examples>
        <example>pvalue z zscore</example>
	<example>pvalue t 25 3.0</example>
	<example>pvalue X 3 5.6</example>
	<example>pvalue F 4 58 fval</example>
	<example>pvalue G shape scale x</example>
	<example>pvalue B bprob 10 6</example>
	<example>pvalue P lambda x</example>
	<example>pvalue W shape scale x</example>
    <demos>
	  <demo>mrw.inp</demo>
	  <demo>restrict.inp</demo>
	</demos>
	</examples>
    </usage>

    <description>
      <para>
	Computes the area to the right of <repl>xval</repl> in the
	specified distribution (<lit>z</lit> for Gaussian, <lit>t</lit>
	for Student's <math>t</math>, <lit>X</lit> for chi-square,
	<lit>F</lit> for <math>F</math>, <lit>G</lit> for gamma,
	<lit>B</lit> for binomial, <lit>P</lit> for Poisson,
	<lit>exp</lit> for Exponential, <lit>W</lit> for Weibull).
      </para>
      <para>
	Depending on the distribution, the following information must
	be given, before the <repl>xval</repl>: for the <math>t</math>
	and chi-square distributions, the degrees of freedom; for
	<math>F</math>, the numerator and denominator degrees of
	freedom; for gamma, the shape and scale parameters; for the
	binomial distribution, the <quote>success</quote> probability
	and the number of trials; for the Poisson distribution, the
	parameter &lgr; (which is both the mean and the variance); for
	the Exponential, a scale parameter; and for the Weibull, shape
	and scale parameters. As shown in the examples above, the
	numerical parameters may be given in numeric form or as the
	names of variables.
      </para>
      <para>
	The parameters for the gamma distribution are sometimes given as
	mean and variance rather than shape and scale. The mean is the
	product of the shape and the scale; the variance is the product of
	the shape and the square of the scale.  So the scale may be found
	as the variance divided by the mean, and the shape as the mean
	divided by the scale.
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/P-value finder</menu-path>
    </gui-access>

  </command>

  <command name="qlrtest" section="Tests" label="Quandt likelihood ratio test">

    <usage>
      <options>
	<option>
	  <flag>--limit-to</flag>
	  <optparm>list</optparm>
	  <effect>limit test to subset of regressors</effect>
	</option>
	<option>
	  <flag>--plot</flag>
	  <optparm>mode-or-filename</optparm>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printed output</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	For a model estimated on time-series data via OLS, performs the
	Quandt likelihood ratio (QLR) test for a structural break at an
	unknown point in time, with 15 percent trimming at the beginning
	and end of the sample period.
      </para>
      <para>
	For each potential break point within the central 70 percent
	of the observations, a Chow test is performed. See <cmdref
	targ="chow"/> for details; as with the regular Chow test, this
	is a robust Wald test if the original model was estimated with
	the <opt>robust</opt> option, an F-test otherwise. The QLR
	statistic is then the maximum of the individual test
	statistics.
      </para>
      <para>
	An asymptotic p-value is obtained using the method of <cite
	key="hansen97">Bruce Hansen (1997)</cite>.
      </para>
      <para>
	Besides the standard hypothesis test accessors <fncref
	targ="$test"/> and <fncref targ="$pvalue"/>, <fncref
	targ="$qlrbreak"/> can be used to retrieve the index of the
	observation at which the test statistic is maximized.
      </para>
      <para context="cli">
	The <opt>limit-to</opt> option can be used to limit the set of
	interactions with the split dummy variable in the Chow tests
	to a subset of the original regressors. The parameter for this
	option must be a named list, all of whose members are among
	the original regressors. The list should not include the
	constant.
      </para>
      <para>
	When this command is run interactively (only), a plot of the
	Chow test statistic is displayed by default.  This can be
	adjusted via the <opt>plot</opt> option. The acceptable
	parameters to this option are <lit>none</lit> (to suppress the
	plot); <lit>display</lit> (to display a plot even when not in
	interactive mode); or a file name. The effect of providing a
	file name is as described for the <opt>output</opt> option of
	the <cmdref targ="gnuplot"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/QLR test</menu-path>
    </gui-access>

  </command>

  <command name="qqplot" section="Graphs" label="Q-Q plot">

    <usage>
      <altforms>
	<altform><lit>qqplot</lit> <repl>y</repl></altform>
	<altform><lit>qqplot</lit> <repl>y</repl> <repl>x</repl></altform>
      </altforms>
      <options>
	<option>
	  <flag>--z-scores</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--raw</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send plot to specified file</effect>
        </option>
      </options>
    </usage>

    <description>
      <para context="gui">
	With just one series selected, displays a plot of the empirical
	quantiles of the given series against the quantiles of the normal
	distribution. The series must include at least 20 valid observations
	in the current sample range. By default the empirical quantiles are
	plotted against quantiles of the normal distribution having the same
	mean and variance as the sample data, but two alternatives are
	available: the data may be standardized (converted to z-scores) before
	plotting, or the <quote>raw</quote> empirical quantiles may be plotted
	against the quantiles of the standard normal distribution.
      </para>
      <para context="cli">
	Given just one series argument, displays a plot of the empirical
	quantiles of the selected series (given by name or ID number)
	against the quantiles of the normal distribution. The series must
	include at least 20 valid observations in the current sample
	range. By default the empirical quantiles are plotted against
	quantiles of the normal distribution having the same mean and
	variance as the sample data, but two alternatives are available:
	if the <opt>z-scores</opt> option is given the data are
	standardized, while if the <opt>raw</opt> option is given the
	<quote>raw</quote> empirical quantiles are plotted against the
	quantiles of the standard normal distribution.
      </para>
      <para context="cli">
	The option <opt>output</opt> has the effect of sending the
	output to the specified file; use <quote>display</quote> to
	force output to the screen. See the <cmdref targ="gnuplot"/>
	command for more detail on this option.
      </para>
      <para>
	Given two series arguments, <repl>y</repl> and <repl>x</repl>,
	displays a plot of the empirical quantiles of <repl>y</repl> against
	those of <repl>x</repl>. The data values are not standardized.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Normal Q-Q plot</menu-path>
      <menu-path>/View/Graph specified vars/Q-Q plot</menu-path>
    </gui-access>

  </command>

  <command name="quantreg" section="Estimation"
    label="Quantile regression">

    <usage>
      <arguments>
	<argument>tau</argument>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--intervals</flag>
	  <optparm optional="true">level</optparm>
	  <effect>compute confidence intervals</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
      </options>
      <examples>
	<example>quantreg 0.25 y 0 xlist</example>
	<example>quantreg 0.5 y 0 xlist --intervals</example>
	<example>quantreg 0.5 y 0 xlist --intervals=.95</example>
	<example>quantreg tauvec y 0 xlist --robust</example>
	<demos>
	  <demo>mrw_qr.inp</demo>
	</demos>
      </examples>
    </usage>

    <description context="gui">
      <para>
	Quantile regression.  By default standard errors are computed
	according to the asymptotic formula given by <cite
	key="koenker-bassett78">Koenker and Bassett (1978)</cite>, but
	if the <quote>robust</quote> box is checked we use the
	heteroskedasticity-robust variant from <cite
	key="koenker-zhao94">Koenker and Zhao (1994)</cite>.
      </para>
      <para>
	If the <quote>Compute confidence intervals</quote> option is
	checked gretl will calculate confidence intervals for the
	coefficients, in place of standard errors. The
	<quote>robust</quote> check-box still has an effect: if it is
	not checked, the intervals are computed on the assumption of
	IID errors; with it, gretl uses the robust estimator developed
	by <cite key="koenker-machado99">Koenker and Machado
	(1999)</cite>.  Note that these intervals are not just
	<quote>plus or minus so many standard errors</quote>; in
	general, they are asymmetrical about the point estimates of
	the coefficients.
      </para>
      <para>
	You may give a list of quantiles (see the drop-down list for
	some pre-defined possibilities).  In that case gretl will
	calculate quantile estimates and either standard errors or
	confidence intervals for each of the specified values.
      </para>
      <para>
	To Follow up on the references given above, please see
	<guideref targ="chap:quantreg"/>.
      </para>
    </description>

    <description context="cli">
      <para>
	Quantile regression.  The first argument, <repl>tau</repl>, is the
	conditional quantile for which estimates are wanted.  It may be given
	either as a numerical value or as the name of a pre-defined scalar
	variable; the value must be in the range 0.01 to 0.99. (Alternatively,
	a vector of values may be given for <repl>tau</repl>; see below for
	details.) The second and subsequent arguments compose a regression
	list on the same pattern as <cmdref targ="ols"/>.
      </para>
      <para>
	Without the <opt>intervals</opt> option, standard errors are
	printed for the quantile estimates.  By default, these are
	computed according to the asymptotic formula given by <cite
	key="koenker-bassett78">Koenker and Bassett (1978)</cite>, but if
	the <opt>robust</opt> option is given, standard errors that are
	robust with respect to heteroskedasticity are calculated using the
	method of <cite key="koenker-zhao94">Koenker and Zhao
	(1994)</cite>.
      </para>
      <para>
	When the <opt>intervals</opt> option is chosen, confidence
	intervals are given for the parameter estimates instead of
	standard errors.  These intervals are computed using the rank
	inversion method, and in general they are asymmetrical about the
	point estimates.  The specifics of the calculation are inflected
	by the <opt>robust</opt> option: without this, the intervals are
	computed on the assumption of IID errors <cite key="koenker94"
	p="true">(Koenker, 1994)</cite>; with it, they use the robust
	estimator developed by <cite key="koenker-machado99">Koenker and
	Machado (1999)</cite>.
      </para>
      <para>
	By default, 90 percent confidence intervals are produced.  You can
	change this by appending a confidence level (expressed as a decimal
	fraction) to the intervals option, as in <opt>intervals=0.95</opt>.
      </para>
      <para>
	Vector-valued <repl>tau</repl>:  instead of supplying a scalar, you
	may give the name of a pre-defined matrix.  In this case estimates are
	computed for all the given <repl>tau</repl> values and the results are
	printed in a special format, showing the sequence of quantile
	estimates for each regressor in turn.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Robust estimation/Quantile regression</menu-path>
    </gui-access>

  </command>

  <command name="quit" section="Utilities"
    label="Exit the program" context="cli">

    <description>
      <para>
	Exits from gretl's current modality.
      </para>
      <ilist>
	<li>
	  <para>
	    When called from a script, execution of the script is
	    terminated.  If the context is gretlcli in batch mode,
	    gretlcli itself exits, otherwise the program reverts to
	    interactive mode.
	  </para>
	</li>
	<li>
	  <para>
	    When called from the GUI console, the console window is
	    closed.
	  </para>
	</li>
	<li>
	  <para>
	    When called from gretlcli in interactive mode the program
	    exits.
	  </para>
	</li>
      </ilist>
      <para>
	Note that this command cannot be called within functions or
	loops.
      </para>
      <para>
	In no case does the <lit>quit</lit> command cause the gretl
	GUI program to exit. That is done via the <lit>Quit</lit> item
	under the <lit>File</lit> menu, or <lit>Ctrl+Q</lit>, or by
	clicking the close control on the title-bar of the main gretl
	window.
      </para>
    </description>
  </command>

  <command name="regls" section="Estimation"
	   label="Regularized least squares" context="gui">
    <description>
      <para>
	This dialog gives you access to the <lit>regls</lit> function
	for regularized least squares.
      </para>
      <para>
	Three estimators are supported: LASSO, which limits the sum of
	absolute values of the coefficients; Ridge, which limits the
	sum of squared coefficients; and Elastic net, which is a
	combination of the first two governed by the hyperparameter
	&alpha;. (&alpha; = 1 gives LASSO, &alpha; = 0 gives Ridge,
	intermediate values give a weighted combination.)
      </para>
      <para>
	The term &lgr; specifies the degree to which the sum of
	absolute or squared coefficients is penalized. You can
	either give a single &lgr;-fraction (from 0 = no penalty
	to 1 = maximal penalty) or a number of &lgr; values. In
	the second case the penalty values are scaled automatically.
      </para>
      <para>
	If you select multiple &lgr; values the option of cross
	validation becomes available. In that case you can select the
	number of <quote>folds</quote> for the training data (default
	10) and whether these folds should be random or contiguous
	subsets of the observations. You may also choose to show
	a plot of average Mean Squared Error (MSE) against the
	&lgr;-fraction.
      </para>
      <para>
	The <quote>Advanced</quote> button opens a further dialog that
	provides somewhat greater control over the regularization
	procedure. But note that you can exercise finer control by
	calling the <lit>regls</lit> function via scripting; see
	<doc>regls.pdf</doc> for details.
      </para>
    </description>
  </command>

  <command name="regls-advanced" section="Estimation"
	   label="Further regls options" context="gui">
    <description>
      <para>
	Selections made via this dialog are applied only where
	relevant, based on the choices made in the main regls dialog.
	They are remembered for the duration of the gretl session.
      </para>
      <ilist>
	<li>
	  <para>
	    Choice of algorithm: You may select the Cyclical
	    Coordinate Descent (CCD) method for either LASSO or Ridge
	    regression, as opposed to the default methods, namely the
	    Alternating Direction Method of Multipliers (ADMM) for
	    LASSO and Singular Value Decomposition (SVD) for Ridge.
	  </para>
	</li>
	<li>
	  <para>
	    The default criterion for <quote>best</quote> performance
	    on cross validation is minimization of MSE, but you can
	    select the alternative <quote>one standard error</quote>
	    rule. This alternative, which favors parsimony, selects
	    the largest penalty factor whose MSE is within one
	    standard error of the minimum.
	  </para>
	</li>
	<li>
	  <para>
	    Seed for random folds: If you wish to obtain replicable
	    results when cross validating with random folds, it is
	    necessary to specify a seed for the random number
	    generator.
	  </para>
	</li>
	<li>
	  <para>
	    By default regls will use MPI (if available) in an effort
	    to speed up cross validation. The MPI checkbox (shown only
	    if gretl is compiled with MPI support) lets you turn this
	    off, which may yield a speed-up for smaller datasets.
	  </para>
	</li>
	<li>
	  <para>
	    The option of showing execution time may be helpful if you
	    wish to compare the performance of different algorithms,
	    or to check whether MPI is worthwhile. The timing shown in
	    the output is for execution of the underlying C code.
	  </para>
	</li>
      </ilist>
    </description>
  </command>

  <command name="rename" section="Dataset"
    label="Rename variables" context="cli">

    <usage>
      <arguments>
	<argument>series</argument>
	<argument>newname</argument>
      </arguments>
      <options>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress printed output</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Changes the name of <repl>series</repl> (identified by name or ID
	number) to <repl>newname</repl>.  The new name must be of 31
	characters maximum, must start with a letter, and must be composed of
	only letters, digits, and the underscore character. In
	addition, it must not be the name of an existing object of any
	kind.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Edit attributes</menu-path>
      <other-access>Main window pop-up menu (single selection)</other-access>
    </gui-access>

  </command>

  <command name="reprobit" section="Estimation" label="Random effects probit"
	   context="gui">

    <description>
      <para>
	The random effects probit estimator provides a means of
	estimating a (binary) probit model for panel data. The error
	term is assumed to be composed of two normally distributed
	components: one time-invariant term that is specific to the
	cross-sectional unit or <quote>individual</quote> (and is
	known as the individual effect); and one term that is specific
	to the particular observation.
      </para>
      <para>
	Evaluation of the likelihood for this model involves the use
	of Gauss-Hermite quadrature for approximating the value of
	expectations of functions of normal variates. In this dialog
	you can select the number of quadrature points used. Using
	more points will increase the accuracy of the results, but at
	the cost of longer compute time; with many quadrature points
	and a large dataset estimation may be quite time consuming.
      </para>
      <para>
	Besides the usual parameter estimates (and associated
	statistics) relating to the included regressors, certain
	additional information is presented on estimation of this
	sort of model:
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>lnsigma2</lit>: the maximum likelihood estimate of
	    the log of the variance of the individual effect;
	  </para>
	</li>
	<li>
	  <para>
	    <lit>sigma_u</lit>: the estimated standard deviation of
	    the individual effect; and
	  </para>
	</li>
	<li>
	  <para>
	    <lit>rho</lit>: the estimated share of the individual
	    effect in the composite error variance (also known as the
	    intra-class correlation).
	  </para>
	</li>
      </ilist>
      <para>
	The Likelihood Ratio test of the null hypothesis that
	<lit>rho</lit> equals zero provides a means of assessing
	whether the random effects specification is needed. If
	the null is not rejected that suggests that a simple
	pooled probit specification is adequate.
      </para>
      <para>
	In scripting mode, the random effects probit model is
	estimated using the <lit>probit</lit> command with
	the <opt>random-effects</opt> option.
      </para>
    </description>
  </command>

  <command name="reset" section="Tests" label="Ramsey's RESET">

    <usage>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print the auxiliary regression</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
	</option>
	<option>
	  <flag>--squares-only</flag>
	  <effect>compute the test using only the squares</effect>
	</option>
	<option>
	  <flag>--cubes-only</flag>
	  <effect>compute the test using only the cubes</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Must follow the estimation of a model via OLS. Carries out
	Ramsey's RESET test for model specification (nonlinearity) by
	adding the squares and/or the cubes of the fitted values to the
	regression and calculating the <math>F</math> statistic for
	the null hypothesis that the coefficients on the added terms
	are zero.
      </para>
      <para context="cli">
	Both the squares and the cubes are added unless one of the
	options <opt>squares-only</opt> or <opt>cubes-only</opt> is
	given.
      </para>
      <para context="cli">
	The <opt>silent</opt> option may be used if one plans to
	make use of the <fncref targ="$test"/> and/or <fncref
	targ="$pvalue"/> accessors to grab the results of the test.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Ramsey's RESET</menu-path>
    </gui-access>

  </command>

  <command name="restrict" section="Tests" context="cli"
    label="Testing restrictions">

    <usage>
      <options>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't print restricted estimates</effect>
	</option>
	<option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
	</option>
	<option>
	  <flag>--wald</flag>
	  <effect>system estimators only &ndash; see below</effect>
	</option>
	<option>
	  <flag>--bootstrap</flag>
	  <effect>bootstrap the test if possible</effect>
	</option>
	<option>
	  <flag>--full</flag>
	  <effect>OLS and VECMs only, see below</effect>
	</option>
      </options>
    <examples>
	<demos>
	  <demo>hamilton.inp</demo>
	   <demo>restrict.inp</demo>
	</demos>
    </examples>
    </usage>

    <description>
      <para>
	Imposes a set of (usually linear) restrictions on either (a)
	the model last estimated or (b) a system of equations
	previously defined and named. In all cases the set of
	restrictions should be started with the keyword
	<quote>restrict</quote> and terminated with <quote>end
	restrict</quote>.
      </para>
      <para>
	In the single equation case the restrictions are always
	implicitly to be applied to the last model, and they are
	evaluated as soon as the <lit>restrict</lit> block is closed.
      </para>
      <para>
	In the case of a system of equations (defined via the <cmdref
	targ="system"/> command), the initial <quote>restrict</quote>
	may be followed by the name of a previously defined system of
	equations. If this is omitted and the last model was a system
	then the restrictions are applied to the last model. By
	default the restrictions are evaluated when the system is next
	estimated, using the <cmdref targ="estimate"/> command. But if
	the <opt>wald</opt> option is given the restriction is
	tested right away, via a Wald chi-square test on the
	covariance matrix. Note that this option will produce an error
	if a system has been defined but not yet estimated.
      </para>
      <para>
	Depending on the context, the restrictions to be tested may be
	expressed in various ways.  The simplest form is as follows:
	each restriction is given as an equation, with a linear
	combination of parameters on the left and a scalar value to
	the right of the equals sign (either a numerical constant or
	the name of a scalar variable).
      </para>
      <para>
	In the single-equation case, parameters may be referenced in
	the form <lit>b[</lit><repl>i</repl><lit>]</lit>, where
	<repl>i</repl> represents the position in the list of
	regressors (starting at 1), or
	<lit>b[</lit><repl>varname</repl><lit>]</lit>, where
	<repl>varname</repl> is the name of the regressor in
	question. In the system case, parameters are referenced using
	<lit>b</lit> plus two numbers in square brackets. The leading
	number represents the position of the equation within the
	system and the second number indicates position in the list of
	regressors.  For example <lit>b[2,1]</lit> denotes the first
	parameter in the second equation, and <lit>b[3,2]</lit> the
	second parameter in the third equation.  The <lit>b</lit>
	terms in the equation representing a restriction may be
	prefixed with a numeric multiplier, for example
	<lit>3.5*b[4]</lit>.
      </para>
      <para>
	Here is an example of a set of restrictions for a previously estimated
	model:
      </para>
      <code>
	restrict
	 b[1] = 0
	 b[2] - b[3] = 0
	 b[4] + 2*b[5] = 1
	end restrict
      </code>
      <para>
	And here is an example of a set of restrictions to be applied to a
	named system.  (If the name of the system does not contain spaces, the
	surrounding quotes are not required.)
      </para>
      <code>
	restrict "System 1"
	 b[1,1] = 0
	 b[1,2] - b[2,2] = 0
	 b[3,4] + 2*b[3,5] = 1
	end restrict
      </code>
      <para>
	In the single-equation case the restrictions are by default evaluated
	via a Wald test, using the covariance matrix of the model in question.
	If the original model was estimated via OLS then the restricted
	coefficient estimates are printed; to suppress this, append the
	<opt>quiet</opt> option flag to the initial <lit>restrict</lit>
	command.  As an alternative to the Wald test, for models estimated via
	OLS or WLS only, you can give the <opt>bootstrap</opt> option to
	perform a bootstrapped test of the restriction.
      </para>
      <para>
	In the system case, the test statistic depends on the estimator
	chosen: a Likelihood Ratio test if the system is estimated using a
	Maximum Likelihood method, or an asymptotic <math>F</math>-test
	otherwise.
      </para>
      <para>
	There are three alternatives to the method of expressing restrictions
	described above.  First, a set of <math>g</math> linear restrictions
	on a <math>k</math>-vector of parameters, &bgr;, may be written
	compactly as <math>R</math>&bgr; &minus; <math>q</math> = 0, where
	<math>R</math> is an <by r="g" c="k"/> matrix and <math>q</math> is a
	<math>g</math>-vector.  You can specify a restriction by giving the
	names of pre-defined, conformable matrices to be used as
	<math>R</math> and <math>q</math>, as in
      </para>
      <code>
	restrict
	  R = Rmat
	  q = qvec
	end restrict
      </code>
      <para>
	Second, in a variant that may be useful when
	<lit>restrict</lit> is used within a function, you can
	construct the set of restriction statements in the form of an
	array of strings.  You then use the <lit>inject</lit> keyword
	with the name of the array. Here's a simple example:
      </para>
      <code>
	strings SR = array(2)
	RS[1] = "b[1,2] = 0"
	RS[2] = "b[2,1] = 0"
	restrict
	  inject RS
	end restrict
      </code>
      <para>
	In actual usage of this method one would likely use <fncref
	targ="sprintf"/> to construct the strings, based on input to
	a function.
      </para>
      <para>
	Lastly, if you wish to test a nonlinear restriction (this is
	currently available for single-equation models only) you should give
	the restriction as the name of a function, preceded by
	<quote><lit>rfunc = </lit></quote>, as in
      </para>
      <code>
	restrict
	  rfunc = myfunction
	end restrict
      </code>
      <para>
	The constraint function should take a single <lit>const matrix</lit>
	argument; this will be automatically filled out with the parameter
	vector.  And it should return a vector which is zero under the null
	hypothesis, non-zero otherwise.  The length of the vector is the
	number of restrictions. This function is used as a
	<quote>callback</quote> by gretl's numerical Jacobian routine, which
	calculates a Wald test statistic via the delta method.
      </para>
      <para>
	Here is a simple example of a function suitable for testing one
	nonlinear restriction, namely that two pairs of parameter values have
	a common ratio.
      </para>
      <code>
	function matrix restr (const matrix b)
	  matrix v = b[1]/b[2] - b[4]/b[5]
	  return v
	end function
      </code>
      <para>
	On successful completion of the <lit>restrict</lit> command
	the accessors <fncref targ="$test"/> and <fncref
	targ="$pvalue"/> give the test statistic and its p-value.
      </para>
      <para>
	When testing restrictions on a single-equation model estimated
	via OLS, or on a VECM, the <opt>full</opt> option can be
	used to set the restricted estimates as the <quote>last
	model</quote> for the purposes of further testing or the use
	of accessors such as <lit>$coeff</lit> and <lit>$vcv</lit>.
	Note that some special considerations apply in the case of
	testing restrictions on Vector Error Correction Models. Please
	see <guideref targ="chap:vecm"/> for details.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /Tests/Linear restrictions</menu-path>
    </gui-access>

  </command>

  <command name="restrict-model" section="Tests" context="gui"
    label="Restrictions on a model">

    <description>
      <para>
	Each restriction in the set should be expressed as an equation, with a
	linear combination of parameters on the left and a numeric value to
	the right of the equals sign. Parameters may be referenced in the form
	<lit>b[</lit><repl>i</repl><lit>]</lit>, where <repl>i</repl>
	represents the position in the list of regressors (starting at 1), or
	<lit>b[</lit><repl>varname</repl><lit>]</lit>, where
	<repl>varname</repl> is the name of the regressor in question.
      </para>
      <para>
	The <lit>b</lit> terms in the equation representing a restriction may
	be prefixed with a numeric multiplier, using <lit>*</lit> to represent
	multiplication, for example <lit>3.5*b[4]</lit>.
      </para>
      <para>
	Here is an example of a set of restrictions:
      </para>
      <code>
	b[1] = 0
	b[2] - b[3] = 0
	b[4] + 2*b[5] = 1
      </code>
    </description>

  </command>

  <command name="restrict-system" section="Tests" context="gui"
    label="Restrictions on a system of equations">

    <description>
      <para>
	Each restriction in the set should be expressed as an equation, with a
	linear combination of parameters on the left and a numeric value to
	the right of the equals sign.  Parameters are referenced using
	<lit>b</lit> plus two numbers in square brackets. The leading number
	represents the position of the equation within the system and the
	second number indicates position in the list of regressors, starting
	at 1 in both cases.  For example <lit>b[2,1]</lit> denotes the first
	parameter in the second equation, and <lit>b[3,2]</lit> the second
	parameter in the third equation.
      </para>
      <para>
	The <lit>b</lit> terms in the equation representing a restriction may
	be prefixed with a numeric multiplier, using <lit>*</lit> to represent
	multiplication, for example <lit>3.5*b[1,4]</lit>.
      </para>
      <para>Here is an example of a set of restrictions:
      </para>
      <code>
	b[1,1] = 0
	b[1,2] - b[2,2] = 0
	b[3,4] + 2*b[3,5] = 1
      </code>
    </description>

  </command>

  <command name="restrict-vecm" section="Tests" context="gui"
    label="Restrictions on a VECM">

    <description>
      <para>
	Use this command to place linear restrictions on the cointegrating
	relations (beta) and/or adjustment coefficients (alpha) in a vector
	error-correction model (VECM).
      </para>
      <para>
	Each restriction should be expressed as an equation, with a linear
	combination of parameters to the left of the equals sign and a
	numerical value on the right.  Restrictions on beta may be
	non-homogeneous (non-zero on the right), but alpha restrictions must
	be homogeneous (zero on the right).
      </para>
      <para>
	If the VECM is of rank 1, the elements of beta are referenced in the
	form <lit>b[</lit><repl>i</repl><lit>]</lit>, where <repl>i</repl>
	represents position in the cointegrating vector, starting at 1. For
	example, <lit>b[2]</lit> denotes the second element in beta. If the
	rank is greater than 1, use <lit>b</lit> plus two numbers in square
	brackets.  For example, <lit>b[2,1]</lit> denotes the first element in
	the second cointegrating vector.
      </para>
      <para>
	To reference elements of alpha, use <lit>a</lit> instead of
	<lit>b</lit>.
      </para>
      <para>
	The parameter identifiers in the equation representing a restriction
	may be prefixed with a numeric multiplier, using <lit>*</lit> to
	represent multiplication, for example <lit>3.5*b[4]</lit>.
      </para>
      <para>Here is an example of a set of restrictions on a VECM of rank 1.
      </para>
      <code>
	b[1] + b[2] = 0
	b[1] + b[3] = 0
      </code>
      <para>
	See also <guideref targ="chap:vecm"/>.
      </para>
    </description>

  </command>

  <command name="rmplot" section="Graphs" label="Range-mean plot">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--trim</flag>
	  <effect>see below</effect>
	</option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printed output</effect>
	</option>
	<option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>see below</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Range&ndash;mean plot: this command creates a simple graph to
	help in deciding whether a time series, <math>y</math>(t), has
	constant variance or not.  We take the full sample t=1,...,T
	and divide it into small subsamples of arbitrary size
	<math>k</math>. The first subsample is formed by
	<math>y</math>(1),...,<math>y</math>(k), the second is
	<math>y</math>(k+1), ..., <math>y</math>(2k), and so on.  For
	each subsample we calculate the sample mean and range (=
	maximum minus minimum), and we construct a graph with the
	means on the horizontal axis and the ranges on the
	vertical. So each subsample is represented by a point in this
	plane.  If the variance of the series is constant we would
	expect the subsample range to be independent of the subsample
	mean; if we see the points approximate an upward-sloping line
	this suggests the variance of the series is increasing in its
	mean; and if the points approximate a downward sloping line
	this suggests the variance is decreasing in the mean.
      </para>
      <para>
	Besides the graph, gretl displays the means and ranges for
	each subsample, along with the slope coefficient for an OLS
	regression of the range on the mean and the p-value for the
	null hypothesis that this slope is zero.  If the slope
	coefficient is significant at the 10 percent significance
	level then the fitted line from the regression of range on
	mean is shown on the graph.  The <math>t</math>-statistic for
	the null, and the corresponding p-value, are recorded and may
	be retrieved using the accessors <fncref targ="$test"/> and
	<fncref targ="$pvalue"/> respectively.
      </para>
      <para context="cli">
	If the <opt>trim</opt> option is given, the minimum and maximum
	values in each sub-sample are discarded before calculating the
	mean and range. This makes it less likely that outliers will
	distort the analysis.
      </para>
      <para context="cli">
	If the <opt>quiet</opt> option is given, no graph is shown and no
	output is printed; only the <math>t</math>-statistic and p-value
	are recorded. Otherwise the form of the plot can be controlled
	via the <opt>output</opt> option; this works as described in
	connection with the <cmdref targ="gnuplot"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/Variable/Range-mean graph</menu-path>
    </gui-access>

  </command>

  <command name="run" section="Programming"
    label="Execute a script" context="cli">

    <usage>
      <arguments>
        <argument>filename</argument>
      </arguments>
    </usage>

    <description>
      <para>
	Executes the commands in <repl>filename</repl> then returns
	control to the interactive prompt.  This command is intended
	for use with the command-line program
	<program>gretlcli</program>, or at the <quote>gretl
	console</quote> in the GUI program.
      </para>
      <para>
	See also <cmdref targ="include"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>Run icon in script window</menu-path>
    </gui-access>

  </command>

  <command name="runs" section="Tests" label="Runs test">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--difference</flag>
	  <effect>use first difference of variable</effect>
	</option>
	<option>
	  <flag>--equal</flag>
	  <effect>positive and negative values are equiprobable</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Carries out the nonparametric <quote>runs</quote> test for
	randomness of the specified <repl>series</repl>, where runs are
	defined as sequences of consecutive positive or negative values.
	If you want to test for randomness of deviations from the median,
	for a variable named <lit>x1</lit> with a non-zero median, you can
	do the following:
      </para>
      <code>
	series signx1 = x1 - median(x1)
	runs signx1
      </code>
      <para>
	If the <opt>difference</opt> option is given, the variable is
	differenced prior to the analysis, hence the runs are interpreted
	as sequences of consecutive increases or decreases in the
	value of the variable.
      </para>
      <para>
	If the <opt>equal</opt> option is given, the null hypothesis
	incorporates the assumption that positive and negative values
	are equiprobable, otherwise the test statistic is invariant
	with respect to the <quote>fairness</quote> of the process
	generating the sequence, and the test focuses on independence
	alone.
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/Nonparametric tests</menu-path>
    </gui-access>

  </command>

  <command name="sampling" section="Dataset" context="gui"
    label="Setting the sample">

    <description>
      <para>
	The Sample menu offers several ways of selecting a sub-sample from the
	current dataset.
      </para>
      <para>
	If you choose <quote>Sample/Restrict based on criterion...</quote> you
	need to supply a Boolean (logical) expression, of the same sort that
	you would use to define a dummy variable.  For example the expression
	<quote>sqft &gt; 1400</quote> will select only cases for which the
	variable sqft has a value greater than 1400. Conditions may be
	concatenated using the logical operators <quote>&amp;&amp;</quote>
	(AND) and <quote>||</quote> (OR), and may be negated using
	<quote>!</quote> (NOT). If the dataset already contains dummy
	variables, you are also given the option of selecting one of these to
	define the sample (observations with a value of 1 for the selected
	dummy will be included, and others excluded).
      </para>
      <para>
	The menu item <quote>Sample/Drop all obs with missing values</quote>
	redefines the sample to exclude all observations for which values of
	one or more variables are missing (leaving only complete cases).
      </para>
      <para>
	To select observations for which a particular variable has no missing
	values, use <quote>Restrict based on criterion...</quote> and
	supply the Boolean condition <quote>!missing(varname)</quote> (replace
	<quote>varname</quote> with the name of the variable you want to use).
      </para>
      <para>
	If the observations are labeled, you can exclude particular
	observations using, for example, <lit>obs!="France"</lit> as the
	Boolean criterion.  The observation name must be enclosed in double
	quotes.
      </para>
      <para>
	One point should be noted about defining a sample based on a
	dummy variable, a Boolean expression, or on the missing values
	criterion: Any <quote>structural</quote> information in the
	data header file (regarding the time series or panel nature of
	the data) is lost.  You may reimpose structure via the
	<quote>Dataset structure</quote> item under the Data menu.
      </para>
      <para>
	Please see <guideref targ="chap:sampling"/> for further details.
      </para>
    </description>
  </command>

  <command name="save-labels" section="Utilities"
    label="Save or remove series labels" context="gui">
    <description>
      <para>
	If you choose Export here, gretl will write a file containing
	the descriptive labels of any series in the current dataset
	that have such labels. This is a plain text file with one
	line per variable. The line will be empty for variables that
	have no descriptive label.
      </para>
      <para>
	If you choose Remove, the descriptive labels will be removed
	for all series that have such labels. This would be
	appropriate only if the current labels have somehow been
	added in error.
      </para>
    </description>
  </command>

  <command name="add-labels" section="Utilities"
    label="Add series labels" context="gui">
    <description>
      <para>
	If you choose Yes here, you are offered a file-open dialog box to
	select a plain text file containing descriptive labels for the
	series in the current dataset. The file should contain one label
	per line; a blank line means no label. Gretl will attempt to read
	as many labels as there are series in the dataset, excluding the
	constant.
      </para>
    </description>
  </command>

  <command name="save-script" section="Utilities"
    label="Save commands?" context="gui">
    <description>
      <para>
	If you choose Yes here, gretl will write a file containing a
	record of the commands you executed in the current session.  Most
	commands that you execute via <quote>point and click</quote> have
	a <quote>script</quote> counterpart, and it is these script
	commands that will be saved.  You could take the file as the basis
	for writing a gretl command script.
      </para>
      <para>
	If you don't care to be prompted to save a record of commands
	on exit, uncheck the tick box in the save commands dialog.
      </para>
    </description>
  </command>

  <command name="save-session" section="Utilities"
    label="Save this gretl session?" context="gui">
    <description>
      <para>
	If you choose Yes here, gretl will write a file containing a
	<quote>snapshot</quote> of the current session, including a copy of
	the working dataset along with any models, graphs or other objects
	that you have saved <quote>as icons</quote>.  You can re-open this
	file later to recreate the state of gretl as of the time you quit the
	session (see the <quote>File/Session files</quote> menu).
      </para>
      <para>
	If you mostly work with gretl using command scripts (which we
	recommend for <quote>serious</quote> econometric work) you
	probably don't need to save the session, but you should be
	sure to save any changes to your script that you wish to keep.
	You may also want to save any changes to your dataset, unless
	these are of a sort that can easily be recreated by running
	a script.
      </para>
      <para>
	If you work with scripts and don't care to be prompted to
	save your session on exit, uncheck the tick box in the
	save session dialog.
      </para>
    </description>
  </command>

  <command name="scatters" section="Graphs"
    label="Multiple pairwise graphs">

    <usage>
      <arguments>
        <argument>yvar</argument>
        <argument separated="true">xvars</argument>
	<argument alternate="true">yvars ; xvar</argument>
      </arguments>
      <options>
	<option>
	  <flag>--with-lines</flag>
	  <effect>create line graphs</effect>
	</option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>name</optparm>
	  <effect>plot columns of named matrix</effect>
        </option>
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>
      </options>
      <examples>
        <example>scatters 1 ; 2 3 4 5</example>
        <example>scatters 1 2 3 4 5 6 ; 7</example>
	<example>scatters y1 y2 y3 ; x --with-lines</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Generates pairwise graphs of <repl>yvar</repl> against all the
	variables in <repl>xvars</repl>, or of all the variables in
	<repl>yvars</repl> against <repl>xvar</repl>.  The first
	example above puts variable 1 on the <math>y</math>-axis
	and draws four graphs, the first having variable 2 on the
	<math>x</math>-axis, the second variable 3 on the
	<math>x</math>-axis, and so on.  The second example
	plots each of variables 1 through 6 against variable 7 on the
	<math>x</math>-axis. Scanning a set of such plots can be
	a useful step in exploratory data analysis.  The maximum
	number of plots is 16; any extra variable in the list will be
	ignored.
      </para>
      <para context="cli">
	By default the graphs are scatterplots, but if you give the
	<opt>with-lines</opt> flag they will be line graphs.
      </para>
      <para context="cli">
	For details on usage of the <opt>output</opt> option, please see
	the <cmdref targ="gnuplot"/> command.
      </para>
      <para context="cli">
	If a named matrix is specified as the data source the
	<repl>x</repl> and <repl>y</repl> lists should be given as
	1-based column numbers; or alternatively, if no such numbers
	are given, all the columns are plotted against time or an
	index variable.
      </para>
      <para context="gui">
	Generates pairwise graphs of the selected <quote>Y-axis
	  variable</quote> against each of the selected <quote>X-axis
	  variables</quote> in turn.  (Or you can select several variables
	for the Y-axis and one for the X-axis.)  Scanning a set of such
	plots can be a useful step in exploratory data analysis.  The
	maximum number of plots is 16; any extra variables will be
	ignored.
      </para>
      <para>
	If the dataset is time-series, then the second sub-list can be
	omitted, in which case it will implicitly be taken as "time",
	so you can plot multiple time series in separated sub-graphs.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Multiple graphs</menu-path>
    </gui-access>

  </command>

  <command name="script-editor" section="Utilities" context="gui"
    label="Script editor preferences">

    <description>
      <para>
	Note that some of these preferences apply only when editing
	native gretl scripts (Smart Tab and Enter, Script editor uses
	tabs); some also apply when editing or viewing any script
	(Show line numbers, Highlighting style).
      </para>
      <para>
	<emphasis>Smart Tab and Enter</emphasis>: If
	this is turned on, then when you press the <lit>Tab</lit> key
	at the start of a line in a hansl script, instead of just
	entering a tab stop the program will try to adjust the
	indentation level of the line consistently with other lines
	that have been entered. Similarly, when you press the
	<lit>Enter</lit> key the program will try to ensure that the
	indentation of the completed line is correct.
      </para>
      <para>
	<emphasis>Show line numbers</emphasis>: Display line numbers
	in the left margin of the script editor or viewer.
      </para>
      <para>
	<emphasis>Script editor uses tabs</emphasis>: Affects program
	behavior when you are editing more than one script at a
	time. If this is checked then each script is shown in a
	<quote>tab</quote> in a notebook-style window; otherwise each
	script has its own window.
      </para>
      <para>
	<emphasis>Enable auto-completion</emphasis>: If this option is
	available and the box is checked, you will be offered possible
	completions for the word you are typing, either as you type or
	in response to the Tab key. To select a completion, use the
	up/down arrow keys and the Tab key; or just keep on typing to
	dismiss the suggested completions.
      </para>
      <para>
	<emphasis>Enable auto-brackets</emphasis>: If this is checked,
	then when you type a left parenthesis, bracket or brace at the
	end of a line the matching right-hand delimiter will be added
	automatically, and the cursor moved between the two
	delimiters.
      </para>
      <para>
	<emphasis>Number of spaces per Tab</emphasis>: How wide do you
	want a tab-stop or indentation level to be? An integer value
	from 2 to 8.
      </para>
      <para>
	<emphasis>Highlighting style</emphasis>: Provides a drop-down
	list of syntax highlighting styles. Some of these are
	dark-on-light and some light-on-dark: experiment and find what
	you like.
      </para>
    </description>

  </command>

  <command name="sdiff" section="Transformations"
    label="Seasonal differencing" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
    </usage>

    <description>
      <para>
	The seasonal difference of each variable in <repl>varlist</repl> is
	obtained and the result stored in a new variable with the prefix
	<lit>sd_</lit>.  This command is available only for seasonal time
	series.
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Seasonal differences of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="set" section="Programming"
    label="Set program parameters" context="cli">

    <usage>
      <altforms>
	<altform><lit>set</lit> <repl>variable</repl> <repl>value</repl></altform>
	<altform><lit>set --to-file=</lit><repl>filename</repl></altform>
	<altform><lit>set --from-file=</lit><repl>filename</repl></altform>
	<altform><lit>set stopwatch</lit></altform>
	<altform><lit>set</lit></altform>
      </altforms>
      <examples>
        <example>set svd on</example>
        <example>set csv_delim tab</example>
	<example>set horizon 10</example>
	<example>set --to-file=mysettings.inp</example>
      </examples>
    </usage>

    <description>
      <para>
	The most common use of this command is the first variant shown
	above, where it is used to set the value of a selected program
	parameter.  This is discussed in detail below.  The other uses
	are: with <opt>to-file</opt>, to write a script file
	containing all the current parameter settings; with
	<opt>from-file</opt> to read a script file containing
	parameter settings and apply them to the current session; with
	<lit>stopwatch</lit> to zero the gretl
	<quote>stopwatch</quote> which can be used to measure CPU time
	(see the entry for the <fncref targ="$stopwatch"/> accessor);
	or, if the word <lit>set</lit> is given alone, to print the
	current settings.
      </para>
      <para>
	Values set via this comand remain in force for the duration of
	the gretl session unless they are changed by a further call to
	<cmd>set</cmd>. The parameters that can be set in this way are
	enumerated below. Note that the settings of
	<lit>hc_version</lit>, <lit>hac_lag</lit> and
	<lit>hac_kernel</lit> are used when the <opt>robust</opt>
	option is given to an estimation command.
      </para>
      <para>
	The available settings are grouped under the following categories:
	program interaction and behavior, numerical methods, random number
	generation, robust estimation, filtering, time series
	estimation, and interaction with GNU R.
      </para>

      <subhead>Program interaction and behavior</subhead>

      <para>
	These settings are used for controlling various aspects of the way
	gretl interacts with the user.
      </para>
      <ilist>
	<li>
	  <para><lit>workdir</lit>: <repl>path</repl>.  Sets the
	  default directory for writing and reading files, whenever
	  full paths are not specified.
	  </para>
	</li>
	<li>
	  <para><lit>use_cwd</lit>: <lit>on</lit> or <lit>off</lit>
	  (the default). Governs the setting of <lit>workdir</lit> at
	  start-up: if it's <lit>on</lit>, the working directory is
	  inherited from the shell, otherwise it is set to whatever
	  was selected in the previous gretl session.
	  </para>
	</li>
	<li>
	  <para><lit>echo</lit>: <lit>off</lit> or <lit>on</lit> (the
	  default). Suppress or resume the echoing of commands in gretl's
	  output.
	  </para>
	</li>
	<li>
	  <para><lit>messages</lit>: <lit>off</lit> or <lit>on</lit> (the
	  default). Suppress or resume the printing of non-error messages
	  associated with various commands, for example when a new variable is
	  generated or when the sample range is changed.
         </para>
	</li>
	<li>
	  <para><lit>verbose</lit>: <lit>off</lit>, <lit>on</lit> (the
	  default) or <lit>comments</lit>. Acts as a <quote>master
	  switch</quote> for <lit>echo</lit> and <lit>messages</lit>
	  (see above), turning them both off or on simultaneously.
	  The <lit>comments</lit> argument turns off echo and messages
	  but preserves printing of comments in a script.
         </para>
	</li>
	<li>
	  <para><lit>warnings</lit>: <lit>off</lit> or <lit>on</lit>
	  (the default). Suppress or resume the printing of warning
	  messages when numerical problems arise, for example a
	  computation produces non-finite values or the convergence of
	  an optimizer is questionable.
         </para>
	</li>
	<li>
	  <para><lit>csv_delim</lit>: either <lit>comma</lit> (the default),
	    <lit>space</lit>, <lit>tab</lit> or <lit>semicolon</lit>.  Sets
	    the column delimiter used when saving data to file in CSV format.
	  </para>
	</li>
	<li>
	  <para><lit>csv_write_na</lit>: the string used to represent
	  missing values when writing data to file in CSV format.
	  Maximum 7 characters; the default is <lit>NA</lit>.
	  </para>
	</li>
	<li>
	  <para><lit>csv_read_na</lit>: the string taken to represent
	  missing values (NAs) when reading data in CSV
	  format. Maximum 7 characters. The default depends on whether
	  a data column is found to contain numerical data (mostly) or
	  string values. For numerical data the following are taken as
	  indicating NAs: an empty cell, or any of the strings
	  <lit>NA</lit>, <lit>N.A.</lit>, <lit>na</lit>,
	  <lit>n.a.</lit>, <lit>N/A</lit>, <lit>#N/A</lit>,
	  <lit>NaN</lit>, <lit>.NaN</lit>, <lit>.</lit>,
	  <lit>..</lit>, <lit>-999</lit>, and <lit>-9999</lit>.  For
	  string-valued data only a blank cell, or a cell containing
	  an empty string, is counted as NA. These defaults can be
	  reimposed by giving <lit>default</lit> as the value for
	  <lit>csv_read_na</lit>. To specify that only empty cells
	  are read as NAs, give a value of <lit>""</lit>. Note that
	  empty cells are always read as NAs regardless of the setting
	  of this variable.
	  </para>
	</li>
	<li>
	  <para><lit>csv_digits</lit>: a positive integer specifying
	  the number of significant digits to use when writing data in
	  CSV format. By default up to 15 digits are used depending on
	  the precision of the original data. Note that CSV output
	  employs the C library's <lit>fprintf</lit> function with
	  <quote><lit>%g</lit></quote> conversion, which means that
	  trailing zeros are dropped.
	  </para>
	</li>
	<li>
	  <para><lit>display_digits</lit>: an integer from 3
	  to 6, specifying the number of significant digits to use
	  when displaying regression coefficients and standard errors
	  (the default being 6). This setting can also be used to
	  limit the number of digits shown by the <cmdref
	  targ="summary"/> command; in this case the default (and also
	  the maximum) is 5, or 4 when the <opt>simple</opt> option
	  is given.
	  </para>
	</li>
	<li>
	  <para><lit>mwrite_g</lit>: <lit>on</lit> or <lit>off</lit>
	  (the default). When writing a matrix to file as text, gretl
	  by default uses scientific notation with 18-digit precision,
	  hence ensuring that the stored values are a faithful
	  representation of the numbers in memory. When writing
	  primary data with no more than 6 digits of precision it may
	  be preferable to use <lit>%g</lit> format for a more compact
	  and human-readable file; you can make this switch via
	  <lit>set mwrite_g on</lit>.
	  </para>
	</li>
	<li>
	  <para><lit>force_decpoint</lit>: <lit>on</lit> or <lit>off</lit>
	  (the default).  Force gretl to use the decimal point
	  character, in a locale where another character (most likely
	  the comma) is the standard decimal separator.
	  </para>
	</li>
	<li>
	  <para><lit>loop_maxiter</lit>: one non-negative integer
	  value (default 100000).  Sets the maximum number of
	  iterations that a <lit>while</lit> loop is allowed before
	  halting (see <cmdref targ="loop"/>). Note that this setting
	  only affects the <lit>while</lit> variant; its purpose is to
	  guard against inadvertently infinite loops. Setting this
	  value to 0 has the effect of disabling the limit; use with
	  caution.
	  </para>
	</li>
	<li>
	  <para><lit>max_verbose</lit>: <lit>off</lit> (the default),
	  <lit>on</lit> or <lit>full</lit>. Controls the verbosity of
	  commands and functions that use numerical optimization
	  methods.  The <lit>on</lit> choice applies only to functions
	  (such as <fncref targ="BFGSmax"/> and <fncref
	  targ="NRmax"/>) which work silently by default; the effect
	  is to print basic iteration information. The <lit>full</lit>
	  setting can be used to trigger more detailed output,
	  including parameter values and their respective gradient for
	  the objective function at each iteration. This choice
	  applies both to functions of the above-mentioned sort and to
	  commands that rely on numerical optimization such as <cmdref
	  targ="arima"/>, <cmdref targ="probit"/> and <cmdref
	  targ="mle"/>. In the case of commands the effect is to make
	  their <opt>verbose</opt> option produce more detail. See
	  also <guideref targ="chap:numerical"/>.
	</para>
	</li>
	<li>
	  <para><lit>debug</lit>: <lit>1</lit>, <lit>2</lit> or <lit>0</lit>
	  (the default).  This is for use with user-defined functions.
	  Setting <lit>debug</lit> to 1 is equivalent to turning
	  <lit>messages</lit> on within all such functions; setting this
	  variable to <lit>2</lit> has the additional effect of turning on
	  <lit>max_verbose</lit> within all functions.
	  </para>
	</li>
	<li>
	  <para><lit>shell_ok</lit>: <lit>on</lit> or <lit>off</lit>
	  (the default). Enable launching external programs from
	  gretl via the system shell. This is disabled by default for
	  security reasons, and can only be enabled via the graphical
	  user interface (Tools/Preferences/General). However, once
	  set to on, this setting will remain active for future
	  sessions until explicitly disabled.
	  </para>
	</li>
	<li>
	  <para><lit>bfgs_verbskip</lit>: one integer. This setting
	  affects the behavior of the <opt>verbose</opt> option to
	  those commands that use BFGS as an optimization algorithm and
	  is used to compact output. if <lit>bfgs_verbskip</lit> is
	  set to, say, 3, then the <opt>verbose</opt> switch will
	  only print iterations 3, 6, 9 and so on.
	  </para>
	</li>
	<li>
	  <para><lit>skip_missing</lit>: <lit>on</lit> (the default)
	  or <lit>off</lit>. Controls gretl's behavior when
	  contructing a matrix from data series: the default is to
	  skip data rows that contain one or more missing values but
	  if <lit>skip_missing</lit> is set <lit>off</lit> missing
	  values are converted to NaNs.
	  </para>
	</li>
	<li>
	  <para><lit>matrix_mask</lit>: the name of a series, or the
	  keyword <lit>null</lit>. Offers greater control than
	  <lit>skip_missing</lit> when constructing matrices from
	  series: the data rows selected for matrices are those
	  with non-zero (and non-missing) values in the specified
	  series. The selected mask remains in force until it is
	  replaced, or removed via the <lit>null</lit> keyword.
	  </para>
	</li>
	<li>
	  <para><lit>quantile_type</lit>: must be one of <lit>Q6</lit>
	  (the default), <lit>Q7</lit> or <lit>Q8</lit>. Selects the
	  specific method used by the <fncref targ="quantile"/>
	  function. For details see <cite key="hyndman96">Hyndman and
	  Fan (1996)</cite> or the Wikipedia entry at
	  <url>https://en.wikipedia.org/wiki/Quantile</url>.
	  </para>
	</li>
	<li>
	  <para><lit>huge</lit>: a large positive number (by default,
	  1.0E100). This setting controls the value returned by the
	  accessor <fncref targ="$huge"/>.
	  </para>
	</li>
	<li>
	  <para><lit>assert</lit>: <lit>off</lit> (the default),
	  <lit>warn</lit> or <lit>stop</lit>. Controls the
	  consequences of failure (return value of 0) from the
	  <fncref targ="assert"/> function.
	  </para>
	</li>
	<li>
	  <para><lit>datacols</lit>: an integer from 1 to 15, with
	  default value 5. Sets the maximum number of series shown
	  side-by-side when data are displayed by observation.
	  </para>
	</li>
	<li>
	  <para><lit>plot_collection</lit>: <lit>on</lit>,
	  <lit>auto</lit> or <lit>off</lit>. This setting affects the
	  way plots are displayed during interactive use. If it's
	  <lit>on</lit>, plots of the same pixel size are gathered in
	  a <quote>plot collection</quote>, that is a single output
	  window in which you can browse through the various plots
	  going back and forth. With the <lit>off</lit> setting,
	  instead, a different window for each plot will be generated,
	  as in older gretl versions. Finally, the <lit>auto</lit>
	  setting has the effect of enabling the plot collection mode
	  only for graphs that are generated within 1.25 seconds from
	  one another (for example, as a result of executing plotting
	  commands in a loop).
	  </para>
	</li>
      </ilist>

      <subhead>Numerical methods</subhead>

      <para>
	These settings are used for controlling the numerical
	algorithms that gretl uses for estimation.
      </para>
      <ilist>
	<li>
	  <para><lit>optimizer</lit>: either <lit>auto</lit> (the
	  default), <lit>BFGS</lit> or <lit>newton</lit>. Sets the
	  optimization algorithm used for various ML estimators, in
	  cases where both BFGS and Newton&ndash;Raphson are
	  applicable. The default is to use Newton&ndash;Raphson
	  where an analytical Hessian is available, otherwise BFGS.
	  </para>
	</li>
	<li>
	  <para><lit>bhhh_maxiter</lit>: one integer, the maximum number of
	    iterations for gretl's internal BHHH routine, which is used in
	    the <cmd>arma</cmd> command for conditional ML estimation. If
	    convergence is not achieved after <lit>bhhh_maxiter</lit>, the
	    program returns an error. The default is set at 500.
	  </para>
	</li>
	<li>
	  <para><lit>bhhh_toler</lit>: one floating point value, or the
	    string <lit>default</lit>.  This is used in gretl's internal
	    BHHH routine to check if convergence has occurred. The
	    algorithm stops iterating as soon as the increment in the
	    log-likelihood between iterations is smaller than
	    <lit>bhhh_toler</lit>.  The default value is 1.0E&minus;06;
	    this value may be re-established by typing <lit>default</lit>
	    in place of a numeric value.
	  </para>
	</li>
	<li>
	  <para><lit>bfgs_maxiter</lit>: one integer, the maximum number of
	    iterations for gretl's BFGS routine, which is used for
	    <cmd>mle</cmd>, <cmd>gmm</cmd> and several specific
	    estimators. If convergence is not achieved in the specified
	    number of iterations, the program returns an error. The
	    default value depends on the context, but is typically
	    of the order of 500.
	  </para>
	</li>
	<li>
	  <para><lit>bfgs_toler</lit>: one floating point value, or the
	    string <lit>default</lit>.  This is used in gretl's BFGS
	    routine to check if convergence has occurred. The algorithm
	    stops as soon as the relative improvement in the objective
	    function between iterations is smaller than
	    <lit>bfgs_toler</lit>.  The default value is the machine
	    precision to the power 3/4; this value may be re-established
	    by typing <lit>default</lit> in place of a numeric value.
	  </para>
	</li>
	<li>
	  <para><lit>bfgs_maxgrad</lit>: one floating point value. This is
	    used in gretl's BFGS routine to check if the norm of the gradient
	    is reasonably close to zero when the <lit>bfgs_toler</lit>
	    criterion is met.  A warning is printed if the norm of the
	    gradient exceeds 1; an error is flagged if the norm exceeds
	    <lit>bfgs_maxgrad</lit>. At present the default is the
	    permissive value of 5.0.
	  </para>
	</li>
	<li>
	  <para><lit>bfgs_richardson</lit>: <lit>on</lit> or
	  <lit>off</lit> (the default). Use Richardson extrapolation
	  when computing numerical derivatives in the context of BFGS
	  maximization.
	  </para>
	</li>
	<li>
	  <para><lit>initvals</lit>: the name of a predefined
	  matrix. Allows manual setting of the initial parameter
	  vector for certain estimation commands that involve
	  numerical optimization: <lit>arma</lit>, <lit>garch</lit>,
	  <lit>logit</lit> and <lit>probit</lit>, <lit>tobit</lit> and
	  <lit>intreg</lit>, <lit>biprobit</lit>, <lit>duration</lit>,
	  <lit>poisson</lit>, <lit>negbin</lit>, and also when
	  imposing certain sorts of restriction associated with
	  VECMs. Unlike other settings, <lit>initvals</lit> is not
	  persistent: it resets to the default initializer after its
	  first use. For details in connection with ARMA estimation
	  see <guideref targ="chap:timeseries"/>.
	  </para>
	</li>
	<li>
	  <para><lit>lbfgs</lit>: <lit>on</lit> or <lit>off</lit> (the
	    default). Use the limited-memory version of BFGS (L-BFGS-B)
	    instead of the ordinary algorithm. This may be advantageous when
	    the function to be maximized is not globally concave.
	  </para>
	</li>
	<li>
	  <para><lit>lbfgs_mem</lit>: an integer value in the range 3 to 20
	    (with a default value of 8).  This determines the number of
	    corrections used in the limited memory matrix when L-BFGS-B
	    is employed.
	  </para>
	</li>
	<li>
	<para>
	  <lit>nls_toler</lit>: a floating-point value. Sets the
	  tolerance used in judging whether or not convergence has
	  occurred in nonlinear least squares estimation using the
	  <cmdref targ="nls"/> command.  The default value is the
	  machine precision to the power 3/4; this value may be
	  re-established by typing <lit>default</lit> in place of a
	  numeric value.
	</para>
	</li>
	<li>
	  <para>
	    <lit>svd</lit>: <lit>on</lit> or <lit>off</lit> (the default). Use
	    SVD rather than Cholesky or QR decomposition in least squares
	    calculations.  This option applies to the <lit>mols</lit> function
	    as well as various internal calculations, but not to the regular
	    <cmdref targ="ols"/> command.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>force_qr</lit>: <lit>on</lit> or <lit>off</lit> (the
	    default). This applies to the <cmdref targ="ols"/>
	    command. By default this command computes OLS estimates
	    using Cholesky decomposition (the fastest method), with a
	    fallback to QR if the data seem too ill-conditioned. You
	    can use <lit>force_qr</lit> to skip the Cholesky step; in
	    <quote>doubtful</quote> cases this may ensure greater
	    accuracy.
	  </para>
	</li>
	<li>
	  <para><lit>fcp</lit>: <lit>on</lit> or <lit>off</lit> (the
	    default). Use the algorithm of Fiorentini, Calzolari and
	    Panattoni rather than native gretl code when computing
	    GARCH estimates.</para>
	</li>
	<li>
	  <para><lit>gmm_maxiter</lit>: one integer, the maximum number of
	    iterations for gretl's <cmdref targ="gmm"/> command when in iterated
	    mode (as opposed to one- or two-step).  The default value is
	    250.
	  </para>
	</li>
	<li>
	  <para><lit>nadarwat_trim</lit>: one integer, the trim
	  parameter used in the  <fncref targ="nadarwat"/> function.
	  </para>
	</li>
	<li>
	  <para><lit>fdjac_quality</lit>: one integer (0, 1 or 2), the
	  algorithm used by the <fncref targ="fdjac"/> function; the
	  default is 0.
	  </para>
	</li>
	<li>
	  <para><lit>gmp_bits</lit>: one integer, which should be an
	  integral power of 2 (default and minimum value 256, maximum
	  8192). Controls the number of bits used to represent a
	  floating point number when GMP (the GNU Multiple Precision
	  Arithmetic Library) is called, primarily via the
	  <lit>mpols</lit> command. Larger values give greater
	  precision at the cost of longer compute time. This setting
	  can also be controlled by the environment variable
	  <lit>GRETL_MP_BITS</lit>.
	  </para>
	</li>
      </ilist>

      <subhead>Random number generation</subhead>

      <ilist>
	<li>
	  <para><lit>seed</lit>: an unsigned integer or the keyword
	  <lit>auto</lit>.  Sets the seed for the pseudo-random number
	  generator.  By default this is set from the system time; if
	  you want to generate repeatable sequences of random numbers
	  you must set the seed manually. To reset the seed to a
	  time-based automatic value, use <lit>auto</lit>.
	  </para>
	</li>
      </ilist>

      <subhead>Robust estimation</subhead>

      <ilist>
	<li>
	  <para><lit>bootrep</lit>: an integer. Sets the number of
	  replications for the <cmdref targ="restrict"/> command with
	  the <opt>bootstrap</opt> option.</para>
	</li>
	<li>
	  <para><lit>garch_vcv</lit>: <lit>unset</lit>,
	    <lit>hessian</lit>, <lit>im</lit> (information matrix) ,
	    <lit>op</lit> (outer product matrix), <lit>qml</lit> (QML
	    estimator), <lit>bw</lit> (Bollerslev&ndash;Wooldridge). Specifies
	    the variant that will be used for estimating the coefficient
	    covariance matrix, for GARCH models.  If <lit>unset</lit> is given
	    (the default) then the Hessian is used unless the
	    <quote>robust</quote> option is given for the garch command, in
	    which case QML is used.
	  </para>
	</li>
	<li>
	  <para><lit>arma_vcv</lit>: <lit>hessian</lit> (the default) or
	    <lit>op</lit> (outer product matrix). Specifies the variant
	    to be used when computing the covariance matrix for ARIMA
	    models.
	  </para>
	</li>
	<li>
	  <para><lit>force_hc</lit>: <lit>off</lit> (the default) or
	  <lit>on</lit>.  By default, with time-series data and when
	  the <opt>robust</opt> option is given with <lit>ols</lit>,
	  the HAC estimator is used.  If you set <lit>force_hc</lit>
	  to <quote>on</quote>, this forces calculation of the regular
	  Heteroskedasticity Consistent Covariance Matrix (HCCM),
	  which does not take autocorrelation into account. Note that
	  VARs are treated as a special case: when the <opt>robust</opt>
	  option is given the default method is regular HCCM, but the
	  <opt>robust-hac</opt> flag can be used to force the use of a
	  HAC estimator.
	  </para>
	</li>
	<li>
	  <para><lit>robust_z</lit>: <lit>off</lit> (the default) or
	  <lit>on</lit>. This controls the distribution used when
	  calculating p-values based on robust standard errors in
	  the context of least-squares estimators. By default gretl
	  uses the Student <math>t</math> distribution but if
	  <lit>robust_z</lit> is turned on the normal distribution
	  is used.
	  </para>
	</li>
	<li>
	  <para><lit>hac_lag</lit>: <lit>nw1</lit> (the default),
	    <lit>nw2</lit>, <lit>nw3</lit> or an integer.  Sets the
	    maximum lag value or bandwidth, <math>p</math>, used when
	    calculating HAC (Heteroskedasticity and Autocorrelation
	    Consistent) standard errors using the Newey-West approach, for
	    time series data.  <lit>nw1</lit> and <lit>nw2</lit> represent
	    two variant automatic calculations based on the sample size,
	    <math>T</math>: for nw1,
	    <equation status="inline"
	      tex="$p = 0.75 \times T^{1/3}$"
	      ascii="p = 0.75 * T^(1/3)"
	      graphic="nw1"/>, and for nw2,
	    <equation status="inline"
	      tex="$p = 4 \times (T/100)^{2/9}$"
	      ascii="p = 4 * (T/100)^(2/9)"
	      graphic="nw2"/>. <lit>nw3</lit> calls for data-based
	    bandwidth selection.  See also <lit>qs_bandwidth</lit> and
	    <lit>hac_prewhiten</lit> below.
	  </para>
	</li>
	<li>
	  <para><lit>hac_kernel</lit>: <lit>bartlett</lit> (the default),
	    <lit>parzen</lit>, or <lit>qs</lit> (Quadratic Spectral). Sets
	    the kernel, or pattern of weights, used when calculating HAC
	    standard errors.
	  </para>
	</li>
	<li>
	  <para><lit>hac_prewhiten</lit>: <lit>on</lit> or <lit>off</lit>
	    (the default). Use Andrews-Monahan prewhitening and
	    re-coloring when computing HAC standard errors.  This also
	    implies use of data-based bandwidth selection.
	  </para>
	</li>
	<li>
	  <para><lit>hac_missvals</lit>: <lit>es</lit> (the default),
	  <lit>am</lit> or <lit>off</lit>. Sets the policy regarding
	  calculation of HAC standard errors when the estimation
	  sample includes incomplete observations: <lit>es</lit>
	  invokes the Equal Spacing method of <cite
	  key="datta-du12">Datta and Du (2012)</cite>; <lit>am</lit>
	  selects the Amplitude Modulation method of <cite
	  key="parzen63">Parzen (1963)</cite>; and <lit>off</lit>
	  causes gretl to refuse such estimation. See <guideref
	  targ="chap:robust_vcv"/> for details.
	  </para>
	</li>	
	<li>
	  <para>
	    <lit>hc_version</lit>: 0 (the default), 1, 2, 3 or 3a. Sets the
	    variant used when calculating Heteroskedasticity Consistent
	    standard errors with cross-sectional data.  The first four options
	    correspond to the HC0, HC1, HC2 and HC3 discussed by
	    Davidson and MacKinnon in <book>Econometric Theory and
	      Methods</book>, chapter 5.  HC0 produces what are usually called
	    <quote>White's standard errors</quote>.  Variant 3a is
	    the MacKinnon&ndash;White <quote>jackknife</quote> procedure.
	  </para>
	</li>
	<li>
	  <para><lit>pcse</lit>: <lit>off</lit> (the default) or
	    <lit>on</lit>.  By default, when estimating a model using
	    pooled OLS on panel data with the <opt>robust</opt> option,
	    the Arellano estimator is used for the covariance matrix.  If
	    you set <lit>pcse</lit> to <quote>on</quote>, this forces use
	    of the Beck and Katz Panel Corrected Standard Errors (which do
	    not take autocorrelation into account).
	  </para>
	</li>
	<li>
	  <para><lit>qs_bandwidth</lit>: Bandwidth for HAC estimation in
	    the case where the Quadratic Spectral kernel is selected.
	    (Unlike the Bartlett and Parzen kernels, the QS bandwidth need
	    not be an integer.)
	  </para>
	</li>
      </ilist>

      <subhead>Time series</subhead>

      <ilist>
	<li>
	  <para>
	    <lit>horizon</lit>: one integer (the default is based on the
	    frequency of the data).  Sets the horizon for impulse responses
	    and forecast variance decompositions in the context of vector
	    autoregressions.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>vecm_norm</lit>: <lit>phillips</lit> (the default),
	    <lit>diag</lit>, <lit>first</lit> or <lit>none</lit>. Used in the
	    context of VECM estimation via the <cmdref targ="vecm"/> command
	    for identifying the cointegration
	    vectors. See the <guideref targ="chap:vecm"/> for
	    details.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>boot_iters</lit>: one integer, <math>B</math>. Sets
	    the number of bootstrap iterations used when computing
	    impulse response functions with confidence intervals. The
	    default is 1999. It is recommended that <math>B</math> + 1
	    is evenly divisible by 100&alpha;/2, so for example with
	    &alpha; = 0.1 <math>B</math> + 1 should be a multiple of
	    5.  The minimum acceptable value is 499.
	  </para>
	</li>
      </ilist>

      <subhead>Interaction with R</subhead>

      <ilist>
	<li>
	  <para><lit>R_lib</lit>: <lit>on </lit>(the default) or
	    <lit>off</lit>.  When sending instructions to be executed by R,
	    use the R shared library by preference to the R executable, if the
	    library is available.
	  </para>
	</li>
	<li>
	  <para><lit>R_functions</lit>: <lit>off</lit> (the default) or
	    <lit>on</lit>. Recognize functions defined in R as if they were
	    native functions (the namespace prefix
	    <quote><lit>R.</lit></quote> is required). See <guideref
	      targ="chap:gretlR"/> for details on this and the
	    previous item.
	  </para>
	</li>
      </ilist>

      <subhead>Miscellaneous</subhead>

      <ilist>
	<li>
	  <para><lit>mpi_use_smt</lit>: <lit>on</lit> or
	  <lit>off</lit> (the default). This switch affects the
	  default number of processes launched in an <lit>mpi</lit>
	  block within a script. If the switch is <lit>off</lit> the
	  default number of processes equals the number of physical
	  cores on the local machine; if it's <lit>on</lit> the
	  default is the maximum number of threads, which will be
	  twice the number of physical cores if the cores support SMT
	  (Simultaneous MultiThreading, also known as
	  Hyper-Threading). This applies only if the user has not
	  specified a number of processes, either directly or
	  indirectly (by specifying a <lit>hosts</lit> file for use
	  with MPI).
	  </para>
	</li>
	<li>
	  <para>
	    <lit>graph_theme</lit>: a string, one of
	    <lit>altpoints</lit>, <lit>classic</lit>, <lit>dark2</lit>
	    (the current default), <lit>ethan</lit>,
	    <lit>iwanthue</lit> or <lit>sober</lit>.  This sets the
	    <quote>theme</quote> used for graphs produced by
	    gretl. The <lit>classic</lit> option reverts to the single
	    theme that was in force prior to version 2020c of gretl.
	  </para>
	</li>
      </ilist>

    </description>
  </command>

  <command name="setinfo" section="Dataset" label="Edit attributes of variable">

    <usage>
      <arguments>
        <argument>series</argument>
      </arguments>
      <options>
	<option>
	  <flag>--description</flag>
	  <optparm>string</optparm>
	  <effect>set description</effect>
	</option>
	<option>
	  <flag>--graph-name</flag>
	  <optparm>string</optparm>
	  <effect>set graph name</effect>
	</option>
	<option>
	  <flag>--discrete</flag>
	  <effect>mark series as discrete</effect>
	</option>
	<option>
	  <flag>--continuous</flag>
	  <effect>mark series as continuous</effect>
	</option>
	<option>
	  <flag>--coded</flag>
	  <effect>mark as an encoding</effect>
	</option>
	<option>
	  <flag>--numeric</flag>
	  <effect>mark as not an encoding</effect>
	</option>
	<option>
	  <flag>--midas</flag>
	  <effect>mark as component of high-frequency data</effect>
	</option>
      </options>
      <examples>
        <example>setinfo x1 --description="Description of x1"</example>
        <example>setinfo y --graph-name="Some string"</example>
	<example>setinfo z --discrete</example>
      </examples>
    </usage>

    <description context="cli">
      <para>
	If the options <opt>description</opt> or <opt>graph-name</opt>
	are invoked the argument must be a single series, otherwise it
	may be a list of series in which case it operates on all
	members of the list. This command sets up to four attributes
	as follows.
      </para>
      <para>
	If the <opt>description</opt> flag is given followed by a
	string in double quotes, that string is used to set the
	variable's descriptive label. This label is shown in response
	to the <cmdref targ="labels"/> command, and is also shown in
	the main window of the GUI program.
      </para>
      <para>
	If the <opt>graph-name</opt> flag is given followed by a
	quoted string, that string will be used in place of the
	variable's name in graphs.
      </para>
      <para>
	If one or other of the <opt>discrete</opt> or
	<opt>continuous</opt> option flags is given, the variable's
	numerical character is set accordingly.  The default is to
	treat all series as continuous; setting a series as discrete
	affects the way the variable is handled in other commands and
	functions, such as for example <cmdref targ="freq"/> or
	<fncref targ="dummify"/> .
      </para>
      <para>
	If one or other of the <opt>coded</opt> or <opt>numeric</opt>
	option flags is given, the status of the given series is set
	accordingly.  The default is to treat all numerical values as
	meaningful as such, at least in an ordinal sense; setting a
	series as <lit>coded</lit> means that the numerical values are
	an arbitrary encoding of qualitative characteristics.
      </para>
      <para>
	The <opt>midas</opt> option sets a flag indicating that a
	given series holds data of a higher frequency than the base
	frequency of the dataset; for example, the dataset is
	quarterly and the series holds values for month 1, 2 or 3
	of each quarter. (MIDAS = Mixed Data Sampling.)
      </para>
    </description>

    <description context="gui">

      <para>
	In this dialog box you can:</para>

      <para>* Rename a (series) variable.</para>

      <para>* Add or edit a description of the variable: this appears
	next to the variable name in the gretl main window.</para>

      <para>* Add or edit the "display name" for the variable (if the
	variable is a series, not a scalar).  This string (maximum 19
	characters) is shown in place of the variable name when the
	variable is displayed in a graph.  Thus for instance you can
	associate a more comprehensible string such as "T-bill rate" with
	a cryptically named variable such as "tb3".</para>

      <para>* (For time-series data) set the compaction method for the
	variable.  This method will be used if you decide to reduce the
	frequency of the dataset, or if you update the variable by
	importing from a database where the variable is at a higher
	frequency than in the working dataset.
      </para>

      <para>* Mark a variable as discrete (for series with integer values
	only).  This affects the way the variable is handled when you ask
	for a frequency plot.
      </para>

    </description>

    <gui-access>
      <menu-path>/Variable/Edit attributes</menu-path>
      <other-access>Main window pop-up menu</other-access>
    </gui-access>

  </command>

  <command name="setmiss" section="Dataset"
    label="Missing value code">

    <usage>
      <arguments>
        <argument>value</argument>
        <argument optional="true">varlist</argument>
      </arguments>
      <examples>
        <example>setmiss -1</example>
        <example>setmiss 100 x2</example>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Get the program to interpret some specific numerical data
	value (the first parameter to the command) as a code for
	<quote>missing</quote>, in the case of imported data.  If this
	value is the only parameter, as in the first example above,
	the interpretation will be applied to all series in the data
	set.  If <repl quote="true">value</repl> is followed by a list
	of variables, by name or number, the interpretation is
	confined to the specified variable(s). Thus in the second
	example the data value 100 is interpreted as a code for
	<quote>missing</quote>, but only for the variable
	<lit>x2</lit>.
      </para>

      <para context="gui">
	Set a numerical value that will be interpreted as "missing" or
	"not applicable", either for a particular data series (under
	the Variable menu) or globally for the entire data set (under
	the Data menu).
      </para>

      <para context="gui">
	Gretl has its own internal coding for missing values, but
	sometimes imported data may employ a different code.  For
	example, if a particular series is coded such that a value of
	-1 indicates "not applicable", you can select "Set missing
	value code" under the Variable menu and type in the value "-1"
	(without the quotes).  Gretl will then read the -1s as missing
	observations.
      </para>

    </description>

    <gui-access>
      <menu-path>/Data/Set missing value code</menu-path>
    </gui-access>

  </command>

  <command name="setobs" section="Dataset" context="cli"
    label="Set frequency and starting observation">

    <usage>
      <altforms>
        <altform><lit>setobs</lit> <repl>periodicity</repl> <repl>startobs</repl></altform>
	<altform><lit>setobs</lit> <repl>unitvar</repl> <repl>timevar</repl> <lit>--panel-vars</lit></altform>
      </altforms>
      <options>
        <option>
	  <flag>--cross-section</flag>
	  <effect>interpret as cross section</effect>
        </option>
        <option>
	  <flag>--time-series</flag>
	  <effect>interpret as time series</effect>
        </option>
        <option>
	  <flag>--special-time-series</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--stacked-cross-section</flag>
	  <effect>interpret as panel data</effect>
        </option>
        <option>
	  <flag>--stacked-time-series</flag>
	  <effect>interpret as panel data</effect>
        </option>
        <option>
	  <flag>--panel-vars</flag>
	  <effect>use index variables, see below</effect>
        </option>
        <option>
	  <flag>--panel-time</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--panel-groups</flag>
	  <effect>see below</effect>
        </option>
      </options>
      <examples>
        <example>setobs 4 1990:1 --time-series</example>
        <example>setobs 12 1978:03</example>
	<example>setobs 1 1 --cross-section</example>
        <example>setobs 20 1:1 --stacked-time-series</example>
	<example>setobs unit year --panel-vars</example>
      </examples>
    </usage>

    <description>
      <para>
	This command forces the program to interpret the current data
	set as having a specified structure.
      </para>
      <para>
	In the first form of the command the <repl>periodicity</repl>,
	which must be an integer, represents frequency in the case of
	time-series data (1 = annual; 4 = quarterly; 12 = monthly; 52 =
	weekly; 5, 6, or 7 = daily; 24 = hourly).  In the case of panel
	data the periodicity means the number of lines per data block:
	this corresponds to the number of cross-sectional units in the
	case of stacked cross-sections, or the number of time periods in
	the case of stacked time series.  In the case of simple
	cross-sectional data the periodicity should be set to 1.
      </para>
      <para>
	The starting observation represents the starting date in the
	case of time series data.  Years may be given with two or four
	digits; subperiods (for example, quarters or months) should be
	separated from the year with a colon.  In the case of panel
	data the starting observation should be given as 1:1; and in
	the case of cross-sectional data, as 1.  Starting observations
	for daily or weekly data should be given in the form
	YYYY-MM-DD (or simply as 1 for undated data).
      </para>
      <para>
	Certain time-series periodicities have standard
	interpretations&mdash;for example, 12 = monthly and 4 =
	quarterly. If you have unusual time-series data to which the
	standard interpretation does not apply, you can signal this by
	giving the <opt>special-time-series</opt> option. In that case
	gretl will not (for example) report your frequency-12 data as
	being monthly.
      </para>
      <para>
	If no explicit option flag is given to indicate the structure
	of the data the program will attempt to guess the structure
	from the information given.
      </para>
      <para>
	The second form of the command (which requires the
	<opt>panel-vars</opt> flag) may be used to impose a panel
	interpretation when the data set contains variables that uniquely
	identify the cross-sectional units and the time periods.  The data
	set will be sorted as stacked time series, by ascending values of
	the units variable, <repl>unitvar</repl>.
      </para>
      <subhead>Panel-specific options</subhead>
      <para>
	The <opt>panel-time</opt> and <opt>panel-groups</opt> options
	can only be used with a dataset which has already been defined
	as a panel.
      </para>
      <para>
	The purpose of <opt>panel-time</opt> is to set extra
	information regarding the time dimension of the panel. This
	should be given on the pattern of the first form of
	<lit>setobs</lit> noted above. For example, the following may
	be used to indicate that the time dimension of a panel is
	quarterly, starting in the first quarter of 1990.
      </para>
      <code>
	setobs 4 1990:1 --panel-time
      </code>
      <para>
	The purpose of <opt>panel-groups</opt> is to create a
	string-valued series holding names for the groups
	(individuals, cross-sectional units) in the panel. (This will
	be used where appropriate in panel graphs.) With this
	option you supply either one or two arguments as follows.
      </para>
      <para>
	First case: the (single) argument is the name of a
	string-valued series. If the number of distinct values equals
	the number of groups in the panel this series is used to
	define the group names. If necessary, the numerical content of
	the series will be adjusted such that the values are all 1s
	for the first group, all 2s for the second, and so on. If the
	number of string values doesn't match the number of groups an
	error is flagged.
      </para>
      <para>
	Second case: the first argument is the name of a series and
	the second is a string literal or variable holding a name for
	each group. The series will be created if it does not already
	exist. If the second argument is a string literal or string
	variable the group names should be separated by spaces; if a
	name includes spaces it should be wrapped in backslash-escaped
	double-quotes. Alternatively the second argument may be an
	array of strings.
      </para>
      <para>
	For example, the following will create a series named
	<lit>country</lit> in which the names in <lit>cstrs</lit> are
	each repeated <math>T</math> times, <math>T</math> being the
	time-series length of the panel.
      </para>
      <code>
	string cstrs = sprintf("France Germany Italy \"United Kingdom\"")
	setobs country cstrs --panel-groups
      </code>
    </description>

    <gui-access>
      <menu-path>/Data/Dataset structure</menu-path>
    </gui-access>

  </command>

  <command name="setopt" section="Programming" context="cli"
    label="Set options for next command">

    <usage>
      <arguments>
	<argument>command</argument>
	<argument optional="true">action</argument>
	<argument>options</argument>
      </arguments>
      <examples>
        <example>setopt mle --hessian</example>
        <example>setopt ols persist --quiet</example>
	<example>setopt ols clear</example>
	<demos>
	  <demo>gdp_midas.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	This command enables the pre-setting of options for a
	specified command. Ordinarily this is not required, but it may
	be useful for the writers of hansl functions when they wish to
	make certain command options conditional on the value of an
	argument supplied by the caller.
      </para>
      <para>
	For example, suppose a function offers a boolean
	<quote><lit>quiet</lit></quote> switch, whose intended effect
	is to suppress the printing of results from a certain
	regression executed within the function. In that case one
	might write:
      </para>
      <code>
	if quiet
	  setopt ols --quiet
	endif
	ols ...
      </code>
      <para>
	The <opt>quiet</opt> option will then be applied to the
	next <lit>ols</lit> command if and only if the variable
	<lit>quiet</lit> has a non-zero value.
      </para>
      <para>
	By default, options set in this way apply only to the
	following instance of <repl>command</repl>; they are not
	persistent. However if you give <lit>persist</lit> as the
	value for <repl>action</repl> the options will continue to
	apply to the given command until further notice.  The antidote
	to the <lit>persist</lit> action is <lit>clear</lit>: this
	erases any stored setting for the specified command.
      </para>
      <para>
	It should be noted that options set via <lit>setopt</lit> are
	compounded with any options attached to the target command
	directly. So for example one might append the
	<opt>hessian</opt> option to an <lit>mle</lit> command
	unconditionally but use <lit>setopt</lit> to add
	<opt>quiet</opt> conditionally.
      </para>
    </description>
  </command>

  <command name="shell" section="Utilities"
    label="Execute shell commands" context="cli">

    <usage>
      <arguments>
        <argument>shellcommand</argument>
      </arguments>
      <examples>
        <example>! ls -al</example>
	<example>! dir c:\users</example>
	<example>launch notepad</example>
	<example>launch emacs myfile.txt</example>
      </examples>
    </usage>

    <description>
      <para>
	The facility described here is not activated by default. See
	below for details.
      </para>
      <para>
	An exclamation mark, <cmd>!</cmd>, at the beginning of a
	command line is interpreted as an escape to the user's shell.
	Thus arbitrary shell commands can be executed from within
	<program>gretl</program>. The <repl>shellcommand</repl>
	argument is passed to <lit>/bin/sh</lit> on unix-type systems
	such as Linux and macOS or to <lit>cmd.exe</lit> on MS
	Windows. It is executed in synchronous
	mode&mdash;<program>gretl</program> waits for it to complete
	before proceeding. If the command outputs any text this is
	printed to the console or script output window.
      </para>
      <para>
	A variant of synchronous shell access allows the user to
	<quote>grab</quote> the output of a command into a string
	variable. This is achieved by wrapping the command in
	parentheses, preceded by a dollar sign, as in
      </para>
      <code>
	string s = $(ls -l $HOME)
      </code>
      <para>
	The <cmd>launch</cmd> keyword, on the other hand, executes an
	external program asynchronously (without waiting for
	completion), as in the third and fourth examples above. This
	is designed for opening an application in interactive mode.
	The user's <lit>PATH</lit> is searched for the specified
	executable. On MS Windows the command is executed directly,
	not passed to <lit>cmd.exe</lit> (so environment variables are
	not expanded automatically).
      </para>
      <subhead>Activation</subhead>
      <para>
	For reasons of security the shell-access facility is not
	enabled by default.  To activate it, check the box titled
	<quote>Allow shell commands</quote> under
	Tools/Preferences/General in the GUI program.  This also makes
	shell commands available in the command-line program (and is
	the only way to do so).
      </para>
    </description>

  </command>

  <command name="smpl" section="Dataset"
    label="Set the sample range" context="cli">

    <!-- don't break the lines below or the text version will get messed
    up -->

    <usage>
      <altforms>
	<altform><lit>smpl</lit> <repl>startobs endobs</repl></altform>
	<altform><lit>smpl</lit> <repl>+i -j</repl></altform>
	<altform><lit>smpl</lit> <repl>dumvar</repl> <lit>--dummy</lit></altform>
	<altform><lit>smpl</lit> <repl>condition</repl> <lit>--restrict</lit></altform>
	<altform><lit>smpl</lit> <lit>--no-missing [ </lit><repl>varlist</repl> <lit>]</lit></altform>
	<altform><lit>smpl</lit> <lit>--no-all-missing [ </lit><repl>varlist</repl> <lit>]</lit></altform>
	<altform><lit>smpl</lit> <lit>--contiguous [ </lit><repl>varlist</repl> <lit>]</lit></altform>
	<altform><lit>smpl</lit> <repl>n</repl> <lit>--random</lit></altform>
	<altform><lit>smpl full</lit></altform>
      </altforms>
      <options>
        <option>
	  <flag>--dummy</flag>
	  <effect>argument is a dummy variable</effect>
        </option>
        <option>
	  <flag>--restrict</flag>
	  <effect>apply boolean restriction</effect>
        </option>
        <option>
	  <flag>--replace</flag>
	  <effect>replace any existing boolean restriction</effect>
        </option>
        <option>
	  <flag>--no-missing</flag>
	  <effect>restrict to valid observations</effect>
        </option>
        <option>
	  <flag>--no-all-missing</flag>
	  <effect>omit empty observations (see below)</effect>
        </option>
        <option>
	  <flag>--contiguous</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--random</flag>
	  <effect>form random sub-sample</effect>
        </option>
        <option>
	  <flag>--permanent</flag>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--preserve-panel</flag>
	  <effect>panel data: see below</effect>
        </option>
        <option>
	  <flag>--unit</flag>
	  <effect>panel data: sample in cross-sectional dimension</effect>
        </option>
        <option>
	  <flag>--time</flag>
	  <effect>panel data: sample in time-series dimension</effect>
        </option>
        <option>
	  <flag>--dates</flag>
	  <effect>interpret observation numbers as dates</effect>
        </option>
	<option>
	  <flag>--quiet</flag>
	  <effect>don't report sample range</effect>
	</option>
      </options>
      <examples>
        <example>smpl 3 10</example>
	<example>smpl 1960:2 1982:4</example>
	<example>smpl +1 -1</example>
	<example>smpl x &gt; 3000 --restrict</example>
	<example>smpl y &gt; 3000 --restrict --replace</example>
	<example>smpl 100 --random</example>
      </examples>
    </usage>

    <description>
      <para>
	Resets the sample range.  The new range can be defined in
	several ways.  In the first alternate form (and the first two
	examples) above, <repl>startobs</repl> and <repl>endobs</repl>
	must be consistent with the periodicity of the data.  Either
	one may be replaced by a semicolon to leave the value
	unchanged. (For more on <repl>startobs</repl> and
	<repl>endobs</repl> see the section titled <quote>Dates versus
	sequential indices</quote> below.)  In the second form, the
	integers <repl>i</repl> and <repl>j</repl> (which may be
	positive or negative, and must be signed) are taken as offsets
	relative to the existing sample range. In the third form
	<repl>dummyvar</repl> must be an indicator variable with
	values 0 or 1 at each observation; the sample will be
	restricted to observations where the value is 1. The fourth
	form, using <opt>restrict</opt>, restricts the sample to
	observations that satisfy the given Boolean condition.
      </para>
      <para>
	The options <opt>no-missing</opt> and
	<opt>no-all-missing</opt> may be used to exclude from the
	sample observations for which data are missing. The first
	variant excludes those rows in the dataset for which at least
	one variable has a missing value, while the second excludes
	just those rows on which <emphasis>all</emphasis> variables
	have missing values. In each case the test is confined to the
	variables in <repl>varlist</repl> if this argument is given,
	otherwise it is applied to all series&mdash;with the
	qualification that in the case of <opt>no-all-missing</opt>
	and no <repl>varlist</repl>, the generic variables
	<lit>index</lit> and <lit>time</lit> are ignored.
      </para>
      <para>
	The <opt>contiguous</opt> form of <lit>smpl</lit> is intended
	for use with time series data.  The effect is to trim any
	observations at the start and end of the current sample range
	that contain missing values (either for the variables in
	<repl>varlist</repl>, or for all data series if no
	<repl>varlist</repl> is given).  Then a check is performed to
	see if there are any missing values in the remaining range; if
	so, an error is flagged.
      </para>
      <para>
	With the <opt>random</opt> flag, the specified number of cases
	are selected from the current dataset at random (without
	replacement).  If you wish to be able to replicate this
	selection you should set the seed for the random number
	generator first (see the <cmdref targ="set"/> command).
      </para>
      <para>
	The final form, <lit>smpl full</lit>, restores the full data
	range.
      </para>
      <para>
	Note that sample restrictions are, by default, cumulative: the
	baseline for any <lit>smpl</lit> command is the current
	sample. If you wish the command to act so as to replace any
	existing restriction you can add the option flag
	<opt>replace</opt> to the end of the command. (But this option
	is not compatible with the <opt>contiguous</opt> option.)
      </para>
      <para>
	The internal variable <lit>obs</lit> may be used with the
	<opt>restrict</opt> form of <lit>smpl</lit> to exclude particular
	observations from the sample.  For example
      </para>
      <code>
	smpl obs!=4 --restrict
      </code>
      <para>
	will drop just the fourth observation. If the data points are
	identified by labels,
      </para>
      <code>
	smpl obs!="USA" --restrict
      </code>
      <para>will drop the observation with label <quote>USA</quote>.
      </para>
      <para>
	One point should be noted about the <opt>dummy</opt>,
	<opt>restrict</opt> and <opt>no-missing</opt> forms of
	<lit>smpl</lit>: <quote>structural</quote> information in the
	data file (regarding the time series or panel nature of the
	data) is likely to be lost when this command is issued.  You
	may reimpose structure with the <cmdref targ="setobs"/>
	command, but also see the <opt>preserve-panel</opt> option
	below.
      </para>

      <subhead>Dates versus sequential indices</subhead>
      <para>
	The <opt>dates</opt> option can be used to resolve a potential
	ambiguity in the interpretation of <repl>startobs</repl> and
	<repl>endobs</repl> in the case of annual time-series data.
	For example, should <lit>2010</lit> be taken to refer to the
	year 2010, or to the two-thousand-and-tenth observation?  In
	most cases this should come out right automatically but you
	can force the date interpretation if needed. This option can
	also be used with dated daily data, to get <lit>smpl</lit> to
	interpret, for example, 20100301 as the first of March 2010
	rather than a plain sequential index.  Note that this
	ambiguity does not arise with time series frequencies other
	than annual and daily; dates such as 1980:3 (third quarter of
	1980) and 2020:03 (March 2020) cannot be confused with
	plain indices.
      </para>

      <subhead>Panel-specific options</subhead>
      <para>
	The <opt>unit</opt> and <opt>time</opt> options are specific
	to panel data. They allow you to specify, respectively, a
	range of <quote>units</quote> or time-periods. For example:
      </para>
      <code>
	# limit the sample to the first 50 units
	smpl 1 50 --unit
	# limit the sample to periods 2 to 20
	smpl 2 20 --time
      </code>
      <para>
	If the time dimension of a panel dataset has been specified
	via the <cmdref targ="setobs"/> command with the
	<opt>panel-time</opt> option, <lit>smpl</lit> with the
	<opt>time</opt> option can be expressed in terms of dates
	rather than plain observation numbers. Here's an example:
      </para>
      <code>
	# specify panel time as quarterly, starting in Q1 of 1990
	setobs 4 1990:1 --panel-time
	# limit the sample to 2000:1 to 2007:1
	smpl 2000:1 2007:1 --time
      </code>
      <para>
	In gretl, a panel dataset must always be <quote>nominally
	balanced</quote>&mdash;that is, each unit must have the same
	number of data rows, even if some rows contain nothing but
	<lit>NA</lit>s. Sub-sampling via the <opt>restrict</opt> or
	<opt>dummy</opt> options may destroy this structure. In that
	case the <opt>preserve-panel</opt> flag can be added to
	request that a nominally balanced panel is reconstituted, via
	the insertion of <quote>missing rows</quote> if needed.
      </para>

      <subhead>Permanent versus temporary sampling</subhead>
      <para>
	By default, restrictions on the current sample range can be
	undone: you can restore the full dataset via <lit>smpl
	full</lit>.  However, the <opt>permanent</opt> flag can be
	used to substitute the restricted dataset for the original. If
	you give the <opt>permanent</opt> option with no other
	arguments or options the effect is to shrink the dataset to
	the current sample range.
      </para>

      <para>
	Please see <guideref targ="chap:sampling"/> for further details.
      </para>

    </description>

    <gui-access>
      <menu-path>/Sample</menu-path>
    </gui-access>

  </command>

  <command name="spearman" section="Statistics"
    label="Spearmans's rank correlation">

    <usage>
      <arguments>
        <argument>series1</argument>
        <argument>series2</argument>
      </arguments>
      <options>
        <option>
	  <flag>--verbose</flag>
	  <effect>print ranked data</effect>
        </option>
      </options>
    </usage>

    <description>
      <para context="cli">
	Prints Spearman's rank correlation coefficient for the
	series <repl>series1</repl> and <repl>series2</repl>. The
	variables do not have to be ranked manually in advance; the
	function takes care of this.
      </para>
      <para context="gui">
	Prints Spearman's rank correlation coefficient for a specified
	pair of series.  The series do not have to be ranked
	manually in advance; the function takes care of this.
      </para>
      <para>
	The automatic ranking is from largest to smallest (&ie; the
	largest data value gets rank 1).  If you need to invert this
	ranking, create a new variable which is the negative of the
	original.  For example:
      </para>
      <code>
	series altx = -x
	spearman altx y
      </code>
    </description>

    <gui-access>
      <menu-path>/Tools/Nonparametric tests/Correlation</menu-path>
    </gui-access>

  </command>

  <command name="sprintf" section="Printing" context="cli">
    <description>
      <para>
	Obsolete command: please use the <fncref targ="sprintf"/>
	function instead.
      </para>
    </description>
  </command>

  <command name="square" section="Transformations"
    label="Create squares of variables" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--cross</flag>
	  <effect>generate cross-products as well as squares</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Generates new series which are squares of the series in
	<repl>varlist</repl> (plus cross-products if the
	<opt>cross</opt> option is given).  For example, <cmd>square
	  x y</cmd> will generate <lit>sq_x</lit> = <lit>x</lit>
	squared, <lit>sq_y</lit> = <lit>y</lit> squared and
	(optionally) <lit>x_y</lit> = <lit>x</lit> times <lit>y</lit>.
	If a particular variable is a dummy variable it is not squared
	because we will get the same variable.
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Squares of selected variables</menu-path>
    </gui-access>

  </command>

  <command name="stdize" section="Transformations"
    label="Standardize series">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--no-df-corr</flag>
	  <effect>no degrees of freedom correction</effect>
        </option>
        <option>
	  <flag>--center-only</flag>
	  <effect>don't divide by s.d.</effect>
        </option>
      </options>
    </usage>

    <description context="gui">
      <para>
	By default this action adds to dataset new series which are
	standardized versions of the originals, named with a prefix
	of <lit>s_</lit>. For example, <lit>s_x</lit> is formed by
	subtracting the mean from <lit>x</lit> and dividing by its
	sample standard deviation (with a degrees of freedom
	correction of 1). But you can choose to divide by the
	standard deviation without a degrees of freedom correction,
	the maximum likelihood estimator.
      </para>
      <para>
	You also have the option of forming series which are just
	centered (the mean is subtracted but no scaling is applied).
	In this case the new series names have prefix <lit>c_</lit>
	rather than <lit>s_</lit>.
      </para>
    </description>

    <description context="cli">
      <para>
	By default a standardized version of each of the series in
	<repl>varlist</repl> is obtained and the result stored in a
	new series with the prefix <lit>s_</lit>.  For example,
	<cmd>stdize x y</cmd> creates the new series <lit>s_x</lit>
	and <lit>s_y</lit>, each of which is centered and divided by
	its sample standard deviation (with a degrees of freedom
	correction of 1).
      </para>
      <para>
	If the <opt>no-df-corr</opt> option is given no degrees of
	freedom correction is applied; the standard deviation used is
	the maximum likelihood estimator. If <opt>center-only</opt> is
	given the series just have their means subtracted, and in that
	case the output names have prefix <lit>c_</lit> rather than
	<lit>s_</lit>.
      </para>
      <para>
	The functionality of this command is available in somewhat
	more flexible form via the <fncref targ="stdize"/> function.
      </para>
    </description>

    <gui-access>
      <menu-path>/Add/Standardize selected variables</menu-path>
    </gui-access>

  </command>

  <command name="store" section="Dataset" label="Save data">

    <usage>
      <arguments>
        <argument>filename</argument>
        <argument optional="true">varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--omit-obs</flag>
	  <effect>see below, on CSV format</effect>
        </option>
        <option>
	  <flag>--no-header</flag>
	  <effect>see below, on CSV format</effect>
        </option>
        <option>
	  <flag>--gnu-octave</flag>
	  <effect>use GNU Octave format</effect>
        </option>
        <option>
	  <flag>--gnu-R</flag>
	  <effect>format friendly for read.table</effect>
        </option>
        <option>
	  <flag>--gzipped</flag>
	  <optparm optional="true">level</optparm>
	  <effect>apply gzip compression</effect>
        </option>
        <option>
	  <flag>--jmulti</flag>
	  <effect>use JMulti ASCII format</effect>
        </option>
        <option>
	  <flag>--dat</flag>
	  <effect>use PcGive ASCII format</effect>
        </option>
        <option>
	  <flag>--decimal-comma</flag>
	  <effect>use comma as decimal character</effect>
        </option>
        <option>
	  <flag>--database</flag>
	  <effect>use gretl database format</effect>
        </option>
        <option>
	  <flag>--overwrite</flag>
	  <effect>see below, on database format</effect>
        </option>
        <option>
	  <flag>--comment</flag>
	  <optparm>string</optparm>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>matrix-name</optparm>
	  <effect>see below</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Save data to <repl>filename</repl>. By default all currently
	defined series are saved but the optional <repl>varlist</repl>
	argument can be used to select a subset of series. If the
	dataset is sub-sampled, only the observations in the current
	sample range are saved.
      </para>
      <para>
	The output file will be written in the currently set <cmdref
	targ="workdir"/>, unless the <repl>filename</repl> string
	contains a full path specification.
      </para>
      <para>
	Note that the <lit>store</lit> command behaves in a special
	manner in the context of a <quote>progressive loop</quote>;
	see <guideref targ="chap:looping"/> for details.
      </para>
      <subhead>Native formats</subhead>
      <para>
	If <repl>filename</repl> has extension <lit>.gdt</lit> or
	<lit>.gtdb</lit> this implies saving the data in one of
	gretl's native formats. In addition, if no extension is given
	<lit>.gdt</lit> is taken to be implicit and the suffix is
	added automatically. The <lit>gdt</lit> format is XML,
	optionally gzip-compressed, while the <lit>gdtb</lit> format
	is binary. The former is recommended for datasets of moderate
	size (say, up to several hundred kilobytes of data); the
	binary format is much faster for very large datasets.
      </para>
      <para>
	When data are saved in <lit>gdt</lit> format the
	<opt>gzipped</opt> option may be used for data compression.
	The optional parameter for this flag controls the level of
	compression (from 0 to 9): higher levels produce a smaller
	file, but compression takes longer. The default level is 1; a
	level of 0 means that no compression is applied.
      </para>
      <para>
	A special sort of <quote>native</quote> save is supported in
	the GUI program: if <repl>filename</repl> has extension
	<lit>.gretl</lit> and the <repl>varlist</repl> argument is
	omitted, then a gretl session file is written. Such files
	include the current dataset along with any named objects such
	as models, graphs and matrices.
      </para>
      <subhead>Other formats</subhead>
      <para>
	The format in which the data are written may be controlled to
	a degree by the extension or suffix of <repl>filename</repl>,
	as follows:
      </para>
      <ilist>
	<li>
	  <para>
	    <lit>.csv</lit>: comma-separated values (CSV).
	  </para>
	</li>
	<li>
	  <para>
	    <lit>.txt</lit> or <lit>.asc</lit>: space-separated
	    values.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>.m</lit>: GNU Octave matrix format.
	  </para>
	</li>
	<li>
	  <para>
	    <lit>.dta</lit>: Stata dta format (version 113).
	  </para>
	</li>
      </ilist>
      <para>
	The format-related option flags shown above can be used to
	force the choice of format independently of the filename (or
	to get gretl to write in the formats of PcGive or JMulTi).
      </para>
      <subhead>CSV options</subhead>
      <para>
	The option flags <opt>omit-obs</opt> and <opt>no-header</opt>
	are specific to saving data in CSV format.  By default, if the
	data are time series or panel, or if the dataset includes
	specific observation markers, the output file includes a first
	column identifying the observations (&eg; by date).  If the
	<opt>omit-obs</opt> flag is given this column is omitted. The
	<opt>no-header</opt> flag suppresses the usual printing of the
	names of the variables at the top of the columns.
      </para>
      <para>
	The option flag <opt>decimal-comma</opt> is also confined to
	CSV. Its effect is to replace the decimal point with decimal
	comma; in addition the column separator is forced to be a
	semicolon rather than a comma.
      </para>
      <subhead>Storing to a database</subhead>
      <para>
	The option of saving in gretl database format is intended to
	help with the construction of large sets of series with mixed
	frequencies and ranges of observations.  At present this
	option is available only for annual, quarterly or monthly
	time-series data. If you save to a file that already exists,
	the default action is to append the newly saved series to the
	existing content of the database.  In this context it is an
	error if one or more of the variables to be saved has the same
	name as a variable that is already present in the
	database. The <opt>overwrite</opt> flag has the effect that,
	if there are variable names in common, the newly saved
	variable replaces the variable of the same name in the
	original dataset.
      </para>
      <para>
	The <opt>comment</opt> option is available when saving data as
	a database or as CSV. The required parameter is a
	double-quoted one-line string, attached to the option flag
	with an equals sign. The string is inserted as a comment into
	the database index file or at the top of the CSV output.
      </para>
      <subhead>Writing a matrix as a dataset</subhead>
      <para>
	The <opt>matrix</opt> option requires a parameter, the name of
	a (non-empty) matrix. The effect of <lit>store</lit> is then,
	in effect, to turn the matrix into a dataset <quote>in the
	background</quote> and write it to file as such. Matrix
	columns become series; their names are taken from column-names
	attached to the matrix, if any, or by default are assigned as
	<lit>v1</lit>, <lit>v2</lit> and so on. If the matrix has row
	names attached these are used as <quote>observation
	markers</quote> in the dataset.
      </para>
      <para>
	Note that matrices can be written to file in their own right,
	see the <fncref targ="mwrite"/> function. But in some cases
	it may be useful to write them in dataset mode.
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Save data; /File/Export data</menu-path>
    </gui-access>

  </command>

  <command name="summary" section="Statistics"
    label="Descriptive statistics" context="cli">

    <usage>
      <altforms>
	<altform><lit>summary [</lit> <repl>varlist</repl> ]</altform>
	<altform><lit>summary --matrix=</lit><repl>matname</repl></altform>
      </altforms>
      <options>
        <option>
	  <flag>--simple</flag>
	  <effect>basic statistics only</effect>
        </option>
        <option>
	  <flag>--weight</flag>
	  <optparm>wvar</optparm>
	  <effect>weighting variable</effect>
        </option>
        <option>
	  <flag>--by</flag>
	  <optparm>byvar</optparm>
	  <effect>see below</effect>
        </option>
      </options>
	  <examples>
	<demos>
	  <demo>frontier.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	In its first form, this command prints summary statistics for
	the variables in <repl>varlist</repl>, or for all the
	variables in the data set if <repl>varlist</repl> is omitted.
	By default, output consists of the mean, standard deviation
	(sd), coefficient of variation (= sd/mean), median, minimum,
	maximum, skewness coefficient, and excess kurtosis.  If the
	<opt>simple</opt> option is given, output is restricted to
	the mean, minimum, maximum and standard deviation.
      </para>
      <para>
	If the <opt>by</opt> option is given (in which case the
	parameter <repl>byvar</repl> should be the name of a discrete
	variable), then statistics are printed for sub-samples
	corresponding to the distinct values taken on by
	<repl>byvar</repl>.  For example, if <repl>byvar</repl> is a
	(binary) dummy variable, statistics are given for the cases
	<lit>byvar = 0</lit> and <lit>byvar = 1</lit>. Note: at
	present, this option is incompatible with the
	<opt>weight</opt> option.
      </para>
      <para>
	If the alternative form is given, using a named matrix, then
	summary statistics are printed for each column of the matrix.
	The <opt>by</opt> option is not available in this case.
      </para>
      <para>
	The table of statistics produced by <lit>summary</lit> can be
	retrieved in matrix form via the <fncref targ="$result"/>
	accessor.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Summary statistics</menu-path>
      <other-access>Main window pop-up menu</other-access>
    </gui-access>

  </command>

  <command name="system" section="Estimation" label="Systems of equations">

    <usage>
      <altforms>
	<altform><lit>system method=</lit><repl>estimator</repl></altform>
	<altform><repl>sysname</repl><lit> &lt;- system</lit></altform>
      </altforms>
      <examples>
	<example>"Klein Model 1" &lt;- system</example>
        <example>system method=sur</example>
	<example>system method=3sls</example>
	<demos>
	  <demo>klein.inp</demo>
	  <demo>kmenta.inp</demo>
	  <demo>greene14_2.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>

      <para context="gui">
	In this window you can define a system of equations and choose an
	estimator for the system.  Four sorts of statement may be given here, as
	follows:
      </para>

      <para context="cli">
	Starts a system of equations.  Either of two forms of the
	command may be given, depending on whether you wish to save
	the system for estimation in more than one way or just
	estimate the system once.</para>

      <para context="cli">
	To save the system you should assign it a name, as in the first
	example (if the name contains spaces it must be surrounded by
	double quotes).  In this case you estimate the system using
	the <cmdref targ="estimate"/> command.  With a saved system of
	equations, you are able to impose restrictions (including
	cross-equation restrictions) using the <cmdref
	targ="restrict"/> command.
      </para>

      <para context="cli">
	Alternatively you can specify an estimator for the system
	using <lit>method=</lit> followed by a string identifying one
	of the supported estimators: <cmd>ols</cmd> (Ordinary Least
	Squares), <cmd>tsls</cmd> (Two-Stage Least Squares)
	<cmd>sur</cmd> (Seemingly Unrelated Regressions),
	<cmd>3sls</cmd> (Three-Stage Least Squares), <cmd>fiml</cmd>
	(Full Information Maximum Likelihood) or <cmd>liml</cmd>
	(Limited Information Maximum Likelihood).  In this case the
	system is estimated once its definition is complete.
      </para>

      <para context="cli">
	An equation system is terminated by the line <cmd>end system</cmd>.
	Within the system four sorts of statement may be given, as follows.
      </para>

      <ilist>
	<li><para><cmdref targ="equation"/>: specify an equation
	    within the system.</para>
	</li>
	<li><para><cmd>instr</cmd>: for a system to be estimated via
	    Three-Stage Least Squares, a list of instruments (by
	    variable name or number). Alternatively, you can put this
	    information into the <cmd>equation</cmd> line using the
	    same syntax as in the <cmdref targ="tsls"/>
	    command.</para>
	</li>
	<li><para><cmd>endog</cmd>: for a system of simultaneous
	    equations, a list of endogenous variables.  This is
	    primarily intended for use with FIML estimation, but with
	    Three-Stage Least Squares this approach may be used
	    instead of giving an <cmd>instr</cmd> list; then all the
	    variables not identified as endogenous will be used as
	    instruments.</para>
	</li>
	<li><para><cmd>identity</cmd>: for use with FIML, an identity
	    linking two or more of the variables in the system.  This
	    sort of statement is ignored when an estimator other than
	    FIML is used.
	  </para>
	</li>
      </ilist>

      <para context="cli">
	After estimation using the <cmd>system</cmd> or
	<cmd>estimate</cmd> commands the following accessors can be used to
	retrieve additional information:
      </para>

      <ilist context="cli">
	<li><para><lit>$uhat</lit>: the matrix of residuals, one column
	    per equation.
	  </para>
	</li>
	<li><para><lit>$yhat</lit>: matrix of fitted values, one column
	    per equation.
	  </para>
	</li>
	<li><para><lit>$coeff</lit>: column vector of coefficients (all
	    the coefficients from the first equation, followed by those
	    from the second equation, and so on).
	  </para>
	</li>
	<li><para><lit>$vcv</lit>: covariance matrix of the coefficients.
	    If there are <math>k</math> elements in the
	    <lit>$coeff</lit> vector, this matrix is <math>k</math>
	    by <math>k</math>.
	  </para>
	</li>
	<li><para><lit>$sigma</lit>: cross-equation residual covariance
	    matrix.
	  </para>
	</li>
	<li><para><lit>$sysGamma</lit>, <lit>$sysA</lit> and <lit>$sysB</lit>:
	    structural-form coefficient matrices (see below).
	  </para>
	</li>
      </ilist>

      <para context="cli">
	If you want to retrieve the residuals or fitted values for a
	specific equation as a data series, select a column from the
	<lit>$uhat</lit> or <lit>$yhat</lit> matrix and assign it to
	a series, as in
      </para>
      <code context="cli">
	series uh1 = $uhat[,1]
      </code>

      <para context="cli">
	The structural-form matrices correspond to the following
	representation of a simultaneous equations model:
	<equation status="display"
	  tex="\[\Gamma y_t=Ay_{t-1}+Bx_t+\epsilon_t\]"
	  ascii="Gamma y(t) = A y(t-1) + B x(t) + e(t)"
	  graphic="structural"/>
	If there are <math>n</math> endogenous variables and
	<math>k</math> exogenous variables,
	&Gamma; is an <by r="n" c="n"/> matrix and <math>B</math>
	is <by r="n" c="k"/>. If the system contains no lags of the endogenous
	variables then the <math>A</math> matrix is not present.  If the
	maximum lag of an endogenous regressor is <math>p</math>,
	the <math>A</math> matrix is <by r="n" c="np"/>.
      </para>

    </description>

    <gui-access>
      <menu-path>/Model/Simultaneous equations</menu-path>
    </gui-access>

  </command>

  <command name="tabprint" section="Printing"
    label="Print model in tabular form" context="cli">

    <usage>
      <options>
        <option>
	  <flag>--output</flag>
	  <optparm>filename</optparm>
	  <effect>send output to specified file</effect>
        </option>
        <option>
	  <flag>--format="f1|f2|f3|f4"</flag>
	  <effect>Specify custom TeX format</effect>
        </option>
	<option>
	  <flag>--complete</flag>
	  <effect>TeX-related, see below</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	Must follow the estimation of a model.  Prints the model in
	tabular form. The format is governed by the extension of the
	specified <repl>filename</repl>:
	<quote><lit>.tex</lit></quote> for &latex;,
	<quote><lit>.rtf</lit></quote> for RTF (Microsoft's Rich Text
	Format), or <quote><lit>.csv</lit></quote> for
	comma-separated.  The file will be written in the currently
	set <cmdref targ="workdir"/>, unless <repl>filename</repl>
	contains a full path specification.
      </para>
      <para>
	If CSV format is selected, values are comma-separated unless
	the decimal comma is in force, in which case the separator is
	the semicolon.
      </para>
      <subhead>Options specific to &latex; output</subhead>
      <para>
	If the <opt>complete</opt> flag is given the &latex; file is
	a complete document, ready for processing; otherwise it must
	be included in a document.
      </para>
      <para>
	If you wish alter the appearance of the tabular output, you can
	specify a custom row format using the <opt>format</opt> flag.
	The format string must be enclosed in double quotes and must be
	tied to the flag with an equals sign.  The pattern for the format
	string is as follows.  There are four fields, representing the
	coefficient, standard error, <math>t</math>-ratio and
	p-value respectively.  These fields should be separated by
	vertical bars; they may contain a <lit>printf</lit>-type
	specification for the formatting of the numeric value in question,
	or may be left blank to suppress the printing of that column
	(subject to the constraint that you can't leave all the columns
	blank).  Here are a few examples:
      </para>
      <code>
	--format="%.4f|%.4f|%.4f|%.4f"
	--format="%.4f|%.4f|%.3f|"
	--format="%.5f|%.4f||%.4f"
	--format="%.8g|%.8g||%.4f"
      </code>
      <para>
	The first of these specifications prints the values in all columns
	using 4 decimal places.  The second suppresses the p-value and
	prints the <math>t</math>-ratio to 3 places.  The third
	omits the <math>t</math>-ratio.  The last one again omits
	the <math>t</math>, and prints both coefficient and standard
	error to 8 significant figures.
      </para>
      <para>
	Once you set a custom format in this way, it is remembered and
	used for the duration of the gretl session.  To revert to
	the default format you can use the special variant
	<opt>format=default</opt>.
      </para>
    </description>

    <gui-access>
      <menu-path>Model window, /LaTeX</menu-path>
    </gui-access>

  </command>

  <command name="tdisagg" section="Dataset" context="gui"
    label="Temporal disaggregation">

    <description>
      <para>
	Here's a walk-through of the main choices in the temporal
	disaggregation dialog box. For detailed explanation see
	<guideref targ="chap:tdisagg"/>.
      </para>
      <ilist>
	<li>
	  <para>
	    Output name: You can choose to overwrite an existing
	    series with the disaggregated version, or give a new name
	    to the output.
	  </para>
	</li>
	<li>
	  <para>
	    Aggregation type: Choose <lit>sum</lit> if the high
	    frequency values should sum to the given low frequency
	    value (flows, such as GDP and its components).  Choose
	    <lit>avg</lit> if the high frequency values should have
	    mean equal to the given low frequency value (index
	    variables, ratios, or flows <quote>at an annual
	    rate</quote>). Choose <lit>last</lit> or <lit>first</lit>
	    if the last or first high frequency value should equal the
	    low frequency value (which may be the case with stock
	    variables, such as money stock).
	  </para>
	</li>
	<li>
	  <para>
	    Regression based versus Denton: The first is generally
	    recommended.  If you choose <quote>Regression
	    based</quote> you can select the Fernández method if you
	    believe the series in question has a unit root; otherwise
	    go with Chow&ndash;Lin. If you choose Denton we assume you
	    know what you're doing. The Denton methods are in first
	    differences, as modified by Cholette.
	  </para>
	</li>
      </ilist>
    </description>
  </command>

  <command name="textplot" section="Graphs"
    label="ASCII plot" context="cli">

    <usage>
      <arguments>
        <argument>varlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--time-series</flag>
	  <effect>plot by observation</effect>
        </option>
        <option>
	  <flag>--one-scale</flag>
	  <effect>force a single scale</effect>
        </option>
        <option>
	  <flag>--tall</flag>
	  <effect>use 40 rows</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Quick and simple ASCII graphics.  Without the <opt>time-series</opt>
	flag, <repl>varlist</repl> must contain at least two series, the last
	of which is taken as the variable for the <math>x</math> axis, and a
	scatter plot is produced. In this case the <opt>tall</opt> option
	may be used to produce a graph in which the <math>y</math> axis is
	represented by 40 rows of characters (the default is 20 rows).
      </para>
      <para>
	With the <opt>time-series</opt>, a plot by observation is produced.
	In this case the option <opt>one-scale</opt> may be used to force
	the use of a single scale; otherwise if <repl>varlist</repl> contains
	more than one series the data may be scaled. Each line represents an
	observation, with the data values plotted horizontally.
      </para>
      <para>
	See also <cmdref targ="gnuplot"/>.
      </para>
    </description>

  </command>


  <command name="tobit" section="Estimation" label="Tobit model">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--llimit</flag>
	  <optparm>lval</optparm>
	  <effect>specify left bound</effect>
        </option>
        <option>
	  <flag>--rlimit</flag>
	  <optparm>rval</optparm>
	  <effect>specify right bound</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--opg</flag>
	  <effect>see below</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>see <cmdref targ="logit"/> for explanation</effect>
        </option>
        <option>
	  <flag>--verbose</flag>
	  <effect>print details of iterations</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
      </options>
    </usage>

    <description>
      <para>
	Estimates a Tobit model, which may be appropriate when the
	dependent variable is <quote>censored</quote>.  For example,
	positive and zero values of purchases of durable goods on the part
	of individual households are observed, and no negative values, yet
	decisions on such purchases may be thought of as outcomes of an
	underlying, unobserved disposition to purchase that may be
	negative in some cases.
      </para>
      <para context="cli">
	By default it is assumed that the dependent variable is
	censored at zero on the left and is uncensored on the
	right. However you can use the options <opt>llimit</opt>
	and <opt>rlimit</opt> to specify a different pattern
	of censoring. Note that if you specify a right bound only,
	the assumption is then that the dependent variable is
	uncensored on the left.
      </para>
      <para context="gui">
	By default it is assumed that the dependent variable is
	censored at zero on the left and is uncensored on the
	right. However you can use the entry boxes marked <quote>left
	bound</quote> and <quote>right bound</quote> to specify a
	different pattern of censoring. Enter either a numerical value
	or <lit>NA</lit> for no censoring.
      </para>
      <para>
	The Tobit model is a special case of interval regression.
	Please see the <cmdref targ="intreg"/> command for further
	details, including an account of the <opt>robust</opt> and
	<opt>opg</opt> options.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Limited dependent variable/Tobit</menu-path>
    </gui-access>

  </command>

  <command name="transpos" section="Dataset" label="Transpose data"
    context="gui">

    <description>
      <para>
	Transposes the current data set.  That is, each observation
	(row) in the current data set will be treated as a variable
	(column), and each variable as an observation.  This command
	may be useful if data have been read from some external source
	in which the rows of the data table represent variables.
      </para>
      <para>
	See also <cmdref targ="dataset"/>.
      </para>
    </description>

    <gui-access>
      <menu-path>/Data/Transpose data</menu-path>
    </gui-access>

  </command>

  <command name="tsls" section="Estimation"
    label="Instrumental variables regression">

    <usage>
      <arguments>
        <argument>depvar</argument>
        <argument>indepvars</argument>
	<argument separated="true">instruments</argument>
      </arguments>
      <options>
        <option>
	  <flag>--no-tests</flag>
	  <effect>don't do diagnostic tests</effect>
        </option>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print results</effect>
        </option>
        <option>
	  <flag>--no-df-corr</flag>
	  <effect>no degrees-of-freedom correction</effect>
        </option>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--cluster</flag>
	  <optparm>clustvar</optparm>
	  <effect>clustered standard errors</effect>
        </option>
	<option>
	  <flag>--liml</flag>
	  <effect>use Limited Information Maximum Likelihood</effect>
        </option>
	<option>
	  <flag>--gmm</flag>
	  <effect>use the Generalized Method of Moments</effect>
        </option>
      </options>
      <examples>
        <example>tsls y1 0 y2 y3 x1 x2 ; 0 x1 x2 x3 x4 x5 x6</example>
	<demos>
	  <demo>penngrow.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para context="cli">
	Computes Instrumental Variables (IV) estimates, by default using
	two-stage least squares (TSLS) but see below for further options.  The
	dependent variable is <repl>depvar</repl>, <repl>indepvars</repl> is
	the list of regressors (which is presumed to include at least one
	endogenous variable); and <repl>instruments</repl> is the list of
	instruments (exogenous and/or predetermined variables). If the
	<repl>instruments</repl> list is not at least as long as
	<repl>indepvars</repl>, the model is not identified.
      </para>

      <para context="cli">
	In the above example, the <lit>y</lit>s are endogenous and the
	<lit>x</lit>s are the exogenous variables. Note that exogenous
	regressors should appear in both lists.
      </para>

      <para context="gui">
	This command requires the selection of two lists of variables: the
	independent variables to appear in the given model and a set of
	instruments.  Note that any exogenous regressors should appear in both
	lists.
      </para>

      <para>
	Output for two-stage least squares estimates includes the Hausman
	test and, if the model is over-identified, the Sargan
	over-identification test.  In the Hausman test, the null
	hypothesis is that OLS estimates are consistent, or in other words
	estimation by means of instrumental variables is not really
	required.  A model of this sort is over-identified if there are
	more instruments than are strictly required.  The Sargan test is
	based on an auxiliary regression of the residuals from the
	two-stage least squares model on the full list of instruments.
	The null hypothesis is that all the instruments are valid, and
	suspicion is thrown on this hypothesis if the auxiliary regression
	has a significant degree of explanatory power. For a good
	explanation of both tests see chapter 8 of <cite
	key="davidson-mackinnon04">Davidson and MacKinnon (2004)</cite>.
      </para>

      <para>
	For both TSLS and LIML estimation, an additional test result is
	shown provided that the model is estimated under the assumption of
	i.i.d. errors (that is, the <opt>robust</opt> option is not
	selected). This is a test for weakness of the instruments.  Weak
	instruments can lead to serious problems in IV regression: biased
	estimates and/or incorrect size of hypothesis tests based on the
	covariance matrix, with rejection rates well in excess of the
	nominal significance level <cite key="stock-wright-yogo02"
	p="true">(Stock, Wright and Yogo, 2002)</cite>.  The test
	statistic is the first-stage <math>F</math>-test if the model
	contains just one endogenous regressor, otherwise it is the
	smallest eigenvalue of the matrix counterpart of the first stage
	<math>F</math>. Critical values based on the Monte Carlo analysis
	of <cite key="stock-yogo03">Stock and Yogo (2003)</cite> are shown
	when available.
      </para>

      <para>
	The R-squared value printed for models estimated via two-stage least
	squares is the square of the correlation between the dependent
	variable and the fitted values.
      </para>

      <para context="cli">
	For details on the effects of the <opt>robust</opt> and
	<opt>cluster</opt> options, please see the help for
	<cmdref targ="ols"/>.
      </para>

      <para context="cli">
	As alternatives to TSLS, the model may be estimated via Limited
	Information Maximum Likelihood (the <opt>liml</opt> option) or via
	the Generalized Method of Moments (<opt>gmm</opt> option). Note that
	if the model is just identified these methods should produce the same
	results as TSLS, but if it is over-identified the results will differ
	in general.
      </para>

      <para context="cli">
	If GMM estimation is selected, the following additional options become
	available:
      </para>

      <ilist context="cli">
	<li>
	  <para>
	    <opt>two-step</opt>: perform two-step GMM rather than the
	    default of one-step.
	  </para>
	</li>
	<li>
	  <para>
	    <opt>iterate</opt>: Iterate GMM to convergence.
	  </para>
	</li>
	<li>
	  <para>
	    <opt>weights=</opt><repl>Wmat</repl>: specify a square matrix of
	    weights to be used when computing the GMM criterion function. The
	    dimension of this matrix must equal the number of instruments. The
	    default is an appropriately sized identity matrix.
	  </para>
	</li>
      </ilist>

    </description>

    <gui-access>
      <menu-path>/Model/Instrumental variables</menu-path>
    </gui-access>

  </command>

  <command name="var" section="Estimation"
    label="Vector Autoregression">

    <usage>
      <arguments>
        <argument>order</argument>
        <argument>ylist</argument>
	<argument separated="true" optional="true">xlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--nc</flag>
	  <effect>do not include a constant</effect>
        </option>
        <option>
	  <flag>--trend</flag>
	  <effect>include a linear trend</effect>
        </option>
        <option>
	  <flag>--seasonals</flag>
	  <effect>include seasonal dummy variables</effect>
        </option>
        <option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
        <option>
	  <flag>--robust-hac</flag>
	  <effect>HAC standard errors</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>skip output of individual equations</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
        </option>
        <option>
	  <flag>--impulse-responses</flag>
	  <effect>print impulse responses</effect>
        </option>
        <option>
	  <flag>--variance-decomp</flag>
	  <effect>print variance decompositions</effect>
        </option>
        <option>
	  <flag>--lagselect</flag>
	  <effect>show criteria for lag selection</effect>
        </option>
        <option>
	  <flag>--minlag</flag>
	  <optparm>minimum lag</optparm>
	  <effect>lag selection only, see below</effect>
        </option>
      </options>
      <examples>
        <example>var 4 x1 x2 x3 ; time mydum</example>
	<example>var 4 x1 x2 x3 --seasonals</example>
	<example>var 12 x1 x2 x3 --lagselect</example>
	<demos>
	  <demo>sw_ch14.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para context="gui">
	This command requires specification of:
      </para>
      <ilist context="gui">
	<li><para context="gui">- the lag order, that is, the number of
	    lags of each variable that should be included in the
	    system;</para>
	</li>
	<li><para context="gui">- any exogenous variables (but note that a
	    constant is included automatically unless you specify otherwise, a
	    trend can be added using the trend checkbox, and seasonal dummy
	    variables can be added using the seasonals checkbox); and
	  </para>
	</li>
	<li><para context="gui">- a list of endogenous variables, lags
	    of which will be included on the right-hand side of each
	    equation (note: do not include lagged variables in this
	    list -- they will be added automatically).</para>
	</li>
      </ilist>
      <para context="gui">
	A separate regression will be run for each variable in the system.
	Output for each equation includes F-tests for zero restrictions on
	all lags of each of the variables and an F-test for the maximum
	lag, along with (optionally) forecast variance decompositions and
	impulse response functions.
      </para>
      <para context="cli">
	Sets up and estimates (using OLS) a vector autoregression
	(VAR).  The first argument specifies the lag order &mdash; or
	the maximum lag order in case the <opt>lagselect</opt>
	option is given (see below).  The order may be given
	numerically, or as the name of a pre-existing scalar variable.
	Then follows the setup for the first equation.  Do not include
	lags among the elements of <repl>ylist</repl> &mdash; they
	will be added automatically.  The semi-colon separates the
	stochastic variables, for which <repl>order</repl> lags will
	be included, from any exogenous variables in
	<repl>xlist</repl>.  Note that a constant is included
	automatically unless you give the <opt>nc</opt> flag, a
	trend can be added with the <opt>trend</opt> flag, and
	seasonal dummy variables may be added using the
	<opt>seasonals</opt> flag.
      </para>
      <para context="cli">
	While a VAR specification usually includes all lags from 1
	to a given maximum, it is possible to select a specific
	set of lags. To do this, substitute for the regular
	(scalar) <repl>order</repl> argument either the name of
	a predefined vector or a comma-separated list of lags,
	enclosed in braces. We show below two ways of specifying
	that a VAR should include lags 1, 2 and 4 (but not lag 3):
      </para>
      <code context="cli">
	var {1,2,4} ylist
	matrix p = {1,2,4}
	var p ylist
      </code>
      <para context="cli">
	A separate regression is reported for each variable in
	<repl>ylist</repl>.  Output for each equation includes
	<math>F</math>-tests for zero restrictions on all lags of each
	of the variables, an <math>F</math>-test for the significance
	of the maximum lag, and, if the <opt>impulse-responses</opt>
	flag is given, forecast variance decompositions and impulse
	responses.
      </para>
      <para>
	Forecast variance decompositions and impulse responses are
	based on the Cholesky decomposition of the contemporaneous
	covariance matrix, and in this context the order in which the
	(stochastic) variables are given matters.  The first variable
	in the list is assumed to be <quote>most exogenous</quote>
	within-period. The horizon for variance decompositions and
	impulse responses can be set using the <cmdref targ="set"/>
	command.  For retrieval of a specified impulse response
	function in matrix form, see the <fncref targ="irf"/>
	function.
      </para>
      <para context="cli">
	If the <opt>robust</opt> option is given, standard errors are
	corrected for heteroskedasticity. Alternatively, the
	<opt>robust-hac</opt> option can be given to produce standard
	errors that are robust with respect to both heteroskedasticity
	and autocorrelation (HAC). In general the latter correction
	should not be needed if the VAR includes sufficient lags.
      </para>
      <para context="cli">
	If the <opt>lagselect</opt> option is given, the first
	parameter to the <lit>var</lit> command is taken as the
	maximum lag order.  Output consists of a table showing the
	values of the Akaike (AIC), Schwarz (BIC) and
	Hannan&ndash;Quinn (HQC) information criteria, by default
	computed from VARs of order 1 to the given maximum.  This is
	intended to help with the selection of the optimal lag order.
	The usual VAR output is not presented. The table of
	information criteria may be retrieved as a matrix via the
	<fncref targ="$test"/> accessor. In this context (only) the
	<opt>minlag</opt> option can be used to adjust the minimum lag
	order. Set this to 0 to allow for the possibility that the
	optimal lag order is zero, meaning that a VAR is not really
	called for at all. Conversely you could set
	<opt>minlag=4</opt> if you believe you need at least 4 lags,
	thereby saving a little compute time.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Multivariate time series</menu-path>
    </gui-access>

  </command>

  <command name="VAR-lagselect" section="Tests" context="gui"
    label="VAR lag-length selection">

    <description>
      <para>
	In this dialog box you specify a VAR as usual, but use the lag order
	spin button to set the maximum number of lags to test.
      </para>
      <para>
	Output will consist of a table showing the values of the Akaike (AIC),
	Schwarz (BIC) and Hannan&ndash;Quinn (HQC) information criteria computed
	from VARs of order 1 to the chosen maximum.  This is intended to help with
	the selection of the optimal lag order.
      </para>
    </description>

  </command>

  <command name="VAR-omit" section="Tests" context="gui"
    label="Test exogenous variables in VAR">

    <description>
      <para>
	Use this dialog box to specify a subset of exogenous variables in a VAR.
	These variables will be omitted from the original VAR, and the system
	re-estimated.
      </para>
      <para>
	A Likelihood Ratio test is reported, where the null hypothesis is that
	the true parameter values are zero, in all equations of the VAR, for the
	omitted variables.  The test is based on the difference between the
	log-determinant of the variance matrix for the unrestricted system, and
	that for the restricted system with the selected variables omitted.
      </para>
    </description>

  </command>

  <command name="varlist" section="Dataset"
    label="Listing of variables" context="cli">

    <usage>
      <options>
	<option>
	  <flag>--type</flag>
	  <optparm>typename</optparm>
	  <effect>scope of listing</effect>
	</option>
      </options>
    </usage>

    <description>
      <para>
	By default, prints a listing of the series in the current
	dataset (if any); <cmd>ls</cmd> may be used as an alias.
      </para>
      <para>
	If the <opt>type</opt> option is given, it should be followed
	(after an equals sign) by one of the following typenames:
	<lit>series</lit>, <lit>scalar</lit>, <lit>matrix</lit>,
	<lit>list</lit>, <lit>string</lit>, <lit>bundle</lit>,
	<lit>array</lit> or <lit>accessor</lit>. The effect is to
	print the names of all currently defined objects of the named
	type.
      </para>
      <para>
	As a special case, if the typename is <lit>accessor</lit>, the
	names printed are those of the internal variables currently
	available as <quote>accessors</quote>, such as <fncref
	targ="$nobs"/> and <fncref targ="$uhat"/>, regardless of their
	specific type.
      </para>
    </description>

  </command>

  <command name="vartest" section="Tests"
    label="Difference of variances">

    <usage>
      <arguments>
        <argument>series1</argument>
        <argument>series2</argument>
      </arguments>
    </usage>

    <description>
      <para context="cli">
	Calculates the <math>F</math> statistic for the null
	hypothesis that the population variances for the variables
	<repl>series1</repl> and <repl>series2</repl> are equal, and
	shows its p-value. The test statistics and the p-value can be
	retrieved through the accessors <fncref targ="$test"/>
	and <fncref targ="$pvalue"/>, respectively. The following code
      </para>
      <code>
      	open AWM18.gdt
		vartest EEN EXR
		eval $test
		eval $pvalue
      </code>
    <para>
      computes the test and shows how to retrieve the test statistics and
      corresponding p-value afterwards:
  	</para>
  	<code>
		Equality of variances test

		EEN: Number of observations = 192
		EXR: Number of observations = 188
		Ratio of sample variances = 3.70707
		Null hypothesis: The two population variances are equal
		Test statistic: F(191,187) = 3.70707
		p-value (two-tailed) = 1.94866e-18

		3.7070716
		1.9486605e-18
	</code>
      <para context="gui">
	Calculates the <math>F</math> statistic for the null
	hypothesis that the population variances are equal for the
	two selected series, and shows its p-value.
      </para>
    </description>

    <gui-access>
      <menu-path>/Tools/Test statistic calculator</menu-path>
    </gui-access>

  </command>

  <command name="vecm" section="Estimation"
    label="Vector Error Correction Model">

    <usage>
      <arguments>
        <argument>order</argument>
	<argument>rank</argument>
        <argument>ylist</argument>
	<argblock optional="true" separated="true">
	  <argument>xlist</argument>
	</argblock>
	<argblock optional="true" separated="true">
	  <argument>rxlist</argument>
	</argblock>
      </arguments>
      <options>
        <option>
	  <flag>--nc</flag>
	  <effect>no constant</effect>
        </option>
        <option>
	  <flag>--rc</flag>
	  <effect>restricted constant</effect>
        </option>
        <option>
	  <flag>--uc</flag>
	  <effect>unrestricted constant</effect>
        </option>
        <option>
	  <flag>--crt</flag>
	  <effect>constant and restricted trend</effect>
        </option>
        <option>
	  <flag>--ct</flag>
	  <effect>constant and unrestricted trend</effect>
        </option>
        <option>
	  <flag>--seasonals</flag>
	  <effect>include centered seasonal dummies</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>skip output of individual equations</effect>
        </option>
        <option>
	  <flag>--silent</flag>
	  <effect>don't print anything</effect>
        </option>
        <option>
	  <flag>--impulse-responses</flag>
	  <effect>print impulse responses</effect>
        </option>
        <option>
	  <flag>--variance-decomp</flag>
	  <effect>print variance decompositions</effect>
        </option>
      </options>
      <examples>
        <example>vecm 4 1 Y1 Y2 Y3</example>
        <example>vecm 3 2 Y1 Y2 Y3 --rc</example>
	<example>vecm 3 2 Y1 Y2 Y3 ; X1 --rc</example>
	<demos>
	  <demo>denmark.inp</demo>
	  <demo>hamilton.inp</demo>
	</demos>
      </examples>
    </usage>

    <description>
      <para>
	A VECM is a form of vector autoregression or VAR (see <cmdref
	  targ="var"/>), applicable where the variables in the model are
	individually integrated of order 1 (that is, are random walks, with or
	without drift), but exhibit cointegration.  This command is closely
	related to the Johansen test for cointegration (see <cmdref
	  targ="johansen"/>).
      </para>
      <para context="cli">
	The <repl>order</repl> parameter to this command represents the lag
	order of the VAR system.  The number of lags in the VECM itself (where
	the dependent variable is given as a first difference) is one less than
	<repl>order</repl>.
      </para>
      <para context="gui">
	The lag order selected in the VECM dialog box is that of the VAR system.
	The number of lags in the VECM itself (where the dependent variable is
	given as a first difference) is one less than this number.
      </para>
      <para context="cli">
	The <repl>rank</repl> parameter represents the cointegration rank, or in
	other words the number of cointegrating vectors.  This must be greater
	than zero and less than or equal to (generally, less than) the number of
	endogenous variables given in <repl>ylist</repl>.
      </para>
      <para context="gui">
	The <quote>rank</quote> represents the number of cointegrating
	vectors.  This must be greater than zero and less than or
	equal to (generally, less than) the number of endogenous
	variables selected.
      </para>
      <para context="cli">
	<repl>ylist</repl> supplies the list of endogenous variables,
	in levels. The inclusion of deterministic terms in the model
	is controlled by the option flags.  The default if no option
	is specified is to include an <quote>unrestricted
	constant</quote>, which allows for the presence of a non-zero
	intercept in the cointegrating relations as well as a trend in
	the levels of the endogenous variables.  In the literature
	stemming from the work of Johansen (see for example his 1995
	book) this is often referred to as <quote>case 3</quote>.  The
	first four options given above, which are mutually exclusive,
	produce cases 1, 2, 4 and 5 respectively.  The meaning of
	these cases and the criteria for selecting a case are
	explained in <guideref targ="chap:vecm"/>.
      </para>
      <para context="cli">
	The optional lists <repl>xlist</repl> and <repl>rxlist</repl>
	allow you to specify sets of exogenous variables which enter the
	model either unrestrictedly (<repl>xlist</repl>) or restricted to
	the cointegration space (<repl>rxlist</repl>). These lists are
	separated from <repl>ylist</repl> and from each other by
	semicolons.
      </para>
      <para context="gui">
	In the <quote>Endogenous variables</quote> box you select the
	vector of endogenous variables, in levels. The inclusion of
	deterministic terms in the model is controlled by the option
	buttons.  The default is to include an <quote>unrestricted
	constant</quote>, which allows for the presence of a non-zero
	intercept in the cointegrating relations as well as a trend in
	the levels of the endogenous variables.  In the literature
	stemming from the work of Johansen (see for example his 1995
	book) this is often referred to as <quote>case 3</quote>.  The
	other four options produce cases 1, 2, 4 and 5 respectively.
	The meaning of these cases and the criteria for selecting a
	case are explained in <guideref targ="chap:vecm"/>.
      </para>
      <para context="gui">
	In the <quote>Exogenous variables</quote> box you may add specific
	exogenous variables.  By default these enter the model in
	unrestricted form (indicated by a <lit>U</lit> next to the name of
	the variable).  If you want a certain exogenous variable to be
	restricted to the cointegrating space, right-click on it and
	select <quote>Restricted</quote> from the pop-up menu.  The symbol
	next to the variable will change to R.
      </para>
      <para context="cli">
	The <opt>seasonals</opt> option, which may be combined with any of the
	other options, specifies the inclusion of a set of centered seasonal
	dummy variables.  This option is available only for quarterly or monthly
	data.
      </para>
      <para context="gui">
	If the data are quarterly or monthly, a check box is shown that allows
	you to include a set of centered seasonal dummy variables.  In all
	cases, an additional check box (<quote>Show details</quote>) allows
	for the printing of the auxiliary regressions that form the starting
	point of the Johansen maximum likelihood estimation procedure.
      </para>
      <para context="cli">
	The first example above specifies a VECM with lag order 4 and a single
	cointegrating vector.  The endogenous variables are <lit>Y1</lit>,
	<lit>Y2</lit> and <lit>Y3</lit>.  The second example uses the same
	variables but specifies a lag order of 3 and two cointegrating vectors;
	it also specifies a <quote>restricted constant</quote>, which is
	appropriate if the cointegrating vectors may have a non-zero intercept
	but the <lit>Y</lit> variables have no trend.
      </para>
      <para context="cli">
	Following estimation of a VECM some special accessors are
	available: <lit>$jalpha</lit>, <lit>$jbeta</lit> and
	<lit>$jvbeta</lit> retrieve, respectively, the &agr; and &bgr;
	matrices and the estimated variance of &bgr;.  For retrieval
	of a specified impulse response function in matrix form, see
	the <fncref targ="irf"/> function.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Multivariate time series</menu-path>
    </gui-access>

  </command>

  <command name="vif" section="Tests" context="cli"
	   label="Variance Inflation Factors">
    <usage>
      <options>
        <option>
	  <flag>--quiet</flag>
	  <effect>don't print anything</effect>
        </option>
      </options>
	<examples>
	<demos>
	  <demo>longley.inp</demo>
	</demos>
    </examples>
    </usage>

    <description>
      <para>
	Must follow the estimation of a model which includes at least
	two independent variables. Calculates and displays diagnostic
	information pertaining to collinearity.
      </para>
      <para>
	The Variance Inflation Factor or VIF for regressor
	<math>j</math> is defined as <equation status="display"
	tex="\[\frac{1}{1-R_j^2}\]" ascii="1/(1 - Rj^2)"
	graphic="vif"/> where <math>R</math><sub>j</sub> is the
	coefficient of multiple correlation between regressor
	<math>j</math> and the other regressors. The factor has a
	minimum value of 1.0 when the variable in question is
	orthogonal to the other independent variables.  <cite
	key="neter-etal90">Neter, Wasserman, and Kutner (1990)</cite>
	suggest inspecting the largest VIF as a diagnostic for
	collinearity; a value greater than 10 is sometimes taken as
	indicating a problematic degree of collinearity.
      </para>
      <para>
	Following this command the <fncref targ="$result"/> accessor
	may be used to retrieve a column vector holding the VIFs.
	For a more sophisticated approach to diagnosing collinearity,
	see the <cmdref targ="bkw"/> command.
      </para>
    </description>
    <gui-access>
      <menu-path>Model window, /Analysis/Collinearity</menu-path>
    </gui-access>

  </command>

  <command name="wls" section="Estimation"
    label="Weighted Least Squares">

    <usage>
      <arguments>
        <argument>wtvar</argument>
        <argument>depvar</argument>
	<argument>indepvars</argument>
      </arguments>
      <options>
        <option>
	  <flag>--vcv</flag>
	  <effect>print covariance matrix</effect>
        </option>
	<option>
	  <flag>--robust</flag>
	  <effect>robust standard errors</effect>
        </option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printing of results</effect>
        </option>
	<option>
	  <flag>--allow-zeros</flag>
	  <effect>see below</effect>
	</option>
      </options>
    </usage>

    <description>
      <para context="cli">
	Computes weighted least squares (WLS) estimates using
	<repl>wtvar</repl> as the weight, <repl>depvar</repl> as the
	dependent variable, and <repl>indepvars</repl> as the list of
	independent variables.  Let <repl>w</repl> denote the positive
	square root of <lit>wtvar</lit>; then WLS is basically equivalent
	to an OLS regression of <repl>w</repl> <lit>*</lit>
	<repl>depvar</repl> on <repl>w</repl> <lit>*</lit>
	<repl>indepvars</repl>.  The <emphasis>R</emphasis>-squared,
	however, is calculated in a special manner, namely as
	<equation status="display"
	  tex="\[R^2 = 1 - \frac{\rm ESS}{\rm WTSS}\]"
	  ascii="R^2 = 1 - ESS / WTSS"
	  graphic="wlsr2"/> where ESS is the error sum of squares (sum of
	squared residuals) from the weighted regression and WTSS denotes
	the <quote>weighted total sum of squares</quote>, which equals the
	sum of squared residuals from a regression of the weighted
	dependent variable on the weighted constant alone.
      </para>
      <para context="cli">
	As a special case, if <repl>wtvar</repl> is a 0/1 dummy
	variable, WLS estimation is equivalent to OLS on a sample that
	excludes all observations with value zero for
	<repl>wtvar</repl>. Otherwise including weights of zero is
	considered an error, but if you really want to mix zero
	weights with positive ones you can append the
	<opt>allow-zeros</opt> option.
      </para>
      <para context="cli">
	For weighted least squares estimation applied to panel data
	and based on the unit specific error variances please see the
	<cmdref targ="panel"/> command with the
	<opt>unit-weights</opt> option.
      </para>
      <para context="gui">
	Let "wtvar" denote the variable selected in the "Weight variable"
	box.  An OLS regression is run, where the dependent variable is
	the product of the positive square root of wtvar and the selected
	dependent variable, and the independent variables are also
	multiplied by the square root of wtvar. Statistics such as
	<emphasis>R</emphasis>-squared are based on the weighted
	data.  If wtvar is a dummy variable, weighted least squares
	estimation is equivalent to eliminating all observations with
	value zero for wtvar.
      </para>
    </description>

    <gui-access>
      <menu-path>/Model/Other linear models/Weighted Least Squares</menu-path>
    </gui-access>

  </command>

  <command name="workdir" section="Utilities" label="Working directory"
    context="gui">

    <description>
      <para>
	The working directory (or <quote>workdir</quote>) is where
	gretl looks by default when reading or writing data files or
	scripts via the file Open and Save dialogs. In addition it is
	the default location for
      </para>
      <ilist>
	<li>
	  <para>
	    reading files via commands such as <lit>append</lit>,
	    <lit>open</lit>, <lit>run</lit> and <lit>include</lit>;
	    and
	  </para>
	</li>
	<li>
	  <para>
	    writing files via commands such as <lit>eqnprint</lit>,
	    <lit>tabprint</lit>, <lit>gnuplot</lit>,
	    <lit>outfile</lit> and <lit>store</lit>.
	  </para>
	</li>
      </ilist>
      <para>
	The working directory can be set in either of two ways:
	using the dialog accessed by the <quote>Working
	directory</quote> item under the File menu, or using
	the <cmdref targ="set"/> command, as in
      </para>
      <code>
	set workdir /path/to/somewhere
      </code>
      <para>
	The current value of the <lit>workdir</lit> variable can be
	inspected in the dialog just mentioned or via the command
      </para>
      <code>
	eval $workdir
      </code>
      <para>
	By default the value of <lit>workdir</lit> is preserved across
	gretl sessions. However, users who like to work from the
	command prompt (launching gretl from a terminal window) may
	prefer to have the working directory set automatically as the
	current directory (according to the shell) at start-up.
	This option can be selected in the dialog or by the command
      </para>
      <code>
	set use_cwd on
      </code>
      <para>
	(<quote>cwd</quote> = current working directory).
      </para>
      <para>
	The working directory dialog also allows you to set the
	behavior of the GUI file selector: when you open or save a
	file in a given folder, should the selector remember and
	return to the same folder on the next invocation?  Or should
	the selector always visit the chosen working directory?
      </para>
    </description>

    <gui-access>
      <menu-path>/File/Working directory</menu-path>
    </gui-access>

  </command>

  <command name="x12a" section="Utilities" context="gui"
    label="X-12-ARIMA">

    <description>
      <para>
	There are two procedural options here, controlled by the
	lower set of radio-buttons.
      </para>
      <para>
	If you select <quote>Execute X-12-ARIMA directly</quote> then
	gretl writes a command file for X-12-ARIMA and calls the
	x12a program to execute the commands. In this case you have the
	option of producing a graph and/or saving selected output series
	to the gretl dataset.
      </para>
      <para>
	If you select <quote>Make X-12-ARIMA command file</quote>
	gretl writes a command file for X-12-ARIMA, as above, but then
	opens this file in an editor window. In that window you are
	able to make changes and to save the file under a chosen
	name. You are also able to send the file for execution by x12a
	(by clicking the <quote>Run</quote> button on the editor
	window toolbar) and view the output. But in this case you do
	not have the option of saving data as gretl series or
	producing a gretl graph.
      </para>
    </description>

  </command>

  <command name="xcorrgm" section="Statistics" label="Cross-correlogram">

    <usage>
      <arguments>
        <argument>series1</argument>
        <argument>series2</argument>
        <argument optional="true">order</argument>
      </arguments>
      <options>
       <option>
	  <flag>--plot</flag>
	  <optparm>mode-or-filename</optparm>
	  <effect>see below</effect>
        </option>
        <option>
	  <flag>--quiet</flag>
	  <effect>suppress plot</effect>
        </option>
      </options>
      <examples>
        <example>xcorrgm x y 12</example>
      </examples>
    </usage>

    <description>
      <para>
	Prints and graphs the cross-correlogram for
	<repl>series1</repl> and <repl>series2</repl>, which may be
	specified by name or number.  The values are the sample
	correlation coefficients between the current value of
	<repl>series1</repl> and successive leads and lags of
	<repl>series2</repl>.
      </para>
      <para>
	If an <repl>order</repl> value is specified the length of the
	cross-correlogram is limited to at most that number of leads and
	lags, otherwise the length is determined automatically, as a
	function of the frequency of the data and the number of
	observations.
      </para>
      <para>
	By default, a plot of the cross-correlogram is produced: a
	gnuplot graph in interactive mode or an ASCII graphic in batch
	mode.  This can be adjusted via the <opt>plot</opt>
	option. The acceptable parameters to this option are
	<lit>none</lit> (to suppress the plot); <lit>ascii</lit> (to
	produce a text graphic even when in interactive mode);
	<lit>display</lit> (to produce a gnuplot graph even when in
	batch mode); or a file name. The effect of providing a file
	name is as described for the <opt>output</opt> option of the
	<cmdref targ="gnuplot"/> command.
      </para>
    </description>

    <gui-access>
      <menu-path>/View/Cross-correlogram</menu-path>
      <other-access>Main window pop-up menu (multiple selection)</other-access>
    </gui-access>

  </command>

  <command name="xtab" section="Statistics" label="Cross-tabulate variables">

    <usage>
      <arguments>
        <argument>ylist</argument>
	<argument optional="true" separated="true">xlist</argument>
      </arguments>
      <options>
        <option>
	  <flag>--row</flag>
	  <effect>display row percentages</effect>
        </option>
        <option>
	  <flag>--column</flag>
	  <effect>display column percentages</effect>
        </option>
        <option>
	  <flag>--zeros</flag>
	  <effect>display zero entries</effect>
        </option>
        <option>
	  <flag>--no-totals</flag>
	  <effect>suppress printing of marginal counts</effect>
        </option>
        <option>
	  <flag>--matrix</flag>
	  <optparm>matname</optparm>
	  <effect>use frequencies from named matrix</effect>
        </option>
	<option>
	  <flag>--quiet</flag>
	  <effect>suppress printed output</effect>
	</option>
	<option>
	  <flag>--tex</flag>
	  <optparm optional="true">filename</optparm>
	  <effect>output as &latex;</effect>
	</option>
	<option>
	  <flag>--equal</flag>
	  <effect>see the &latex; case below</effect>
	</option>
      </options>
      <examples>
	<example>xtab 1 2</example>
	<example>xtab 1 ; 2 3 4</example>
	<example>xtab --matrix=A</example>
	<example>xtab 1 2 --tex="xtab.tex"</example>
	<demos>
	  <demo>ooballot.inp</demo>
	</demos>
      </examples>
    </usage>

    <description context="cli">
      <para>
	Given just the <repl>ylist</repl> argument, computes (and by
	default prints) a contingency table or cross-tabulation for
	each combination of the variables included in the list.  If a
	second list <repl>xlist</repl> is given, each variable in
	<repl>ylist</repl> is cross-tabulated by row against each
	variable in <repl>xlist</repl> (by column).  Variables in
	these lists can be referenced by name or by number.  Note that
	all the variables must have been marked as discrete.
	Alternatively, if the <opt>matrix</opt> option is given, the
	named matrix is treated as a precomputed set of frequencies,
	to be displayed as a cross-tabulation (see also the <fncref
	targ="mxtab"/> function). In this case the <repl>list</repl>
	argument(s) should be omitted.
      </para>
      <para>
	By default the cell entries are given as frequency counts. The
	<opt>row</opt> and <opt>column</opt> options (which are
	mutually exclusive) replace the counts with the percentages
	for each row or column, respectively.  By default, cells with
	a zero count are left blank but the <opt>zeros</opt> option
	has the effect of showing zero counts explicitly, which may be
	useful for importing the table into another program, such as a
	spreadsheet.
      </para>
      <para>
        Pearson's chi-square test for independence is shown if the
        expected frequency under independence is at least 1.0e-7 for
        all cells.  A common rule of thumb for the validity of this
        statistic is that at least 80 percent of cells should have
        expected frequencies of 5 or greater; if this criterion is not
        met a warning is printed.
      </para>
      <para>
	If the contingency table is 2 by 2, Fisher's Exact Test for
	independence is shown.  Note that this test is based on the
	assumption that the row and column totals are fixed, which may
	or may not be appropriate depending on how the data were
	generated.  The left p-value should be used when the
	alternative to independence is negative association (values
	tend to cluster in the lower left and upper right cells), the
	right p-value when the alternative is positive association.
	The two-tailed p-value for this test is calculated by method
	(b) in section 2.1 of <cite key="agresti92">Agresti
	(1992)</cite>: it is the sum of the probabilities of all
	possible tables with the given row and column totals and a
	probability no greater than that of the observed table.
      </para>

      <subhead>The bivariate case</subhead>
      <para>
	In the case of a bivariate cross-tabulation (only one list is
	given, and it has two members) certain results are stored. The
	contingency table may be retrieved in matrix form via the
	<fncref targ="$result"/> accessor. In addition, if the minimum
	expected value condition is met, the Pearson chi-square test
	and its p-value may be retrieved via the <fncref
	targ="$test"/> and <fncref targ="$pvalue"/> accessors. If it's
	these results that are of interest, the <opt>quiet</opt>
	option can be used to suppress the usual printout.
      </para>

      <subhead>&latex; output</subhead>
      <para>
	If the <opt>tex</opt> option is given the cross-tabulation is
	printed in the form of a &latex; <lit>tabular</lit>
	environment, either inline (from where it may be copied and
	pasted) or, if the <repl>filename</repl> parameter is
	appended, to the specified file. (If <repl>filename</repl>
	does not specify a full path the file is written in the
	currently set <cmdref targ="workdir"/>.)  No test statistic is
	computed. The additional option <opt>equal</opt> can be used
	to flag, by printing in boldface, the count or percentage for
	cells in which the row and column variables have the same
	numerical value. This option is ignored unless the
	<opt>tex</opt> option is given, and also when one or both of
	the cross-tabulated variables are string-valued.
      </para>
    </description>

    <description context="gui">
      <para>
        Displays a contingency table or cross-tabulation for each
	combination of the selected variables.  Note that all the
	variables must be discrete.
      </para>
      <para>
	By default, frequency count values are shown in the cells and
	on the margins of the table.  However, you can choose to
	display either row or column percentages instead.
      </para>
      <para>
	By default, cells with a zero count are shown as empty, but
	you can choose to show zero values explicitly.
      </para>
      <para>
        Pearson's chi-square test for independence is displayed if the
        expected frequency under independence is at least 1.0e-7 for
        all cells.  A common rule of thumb for the validity of this
        statistic is that at least 80 percent of cells should have
        expected frequencies of 5 or greater; if this criterion is not
        met a warning is printed.
      </para>
      <para>
	If the contingency table is 2 by 2, Fisher's Exact Test for
	independence is shown.  Note that this test is based on the
	assumption that the row and column totals are fixed, which may
	or may not be appropriate depending on how the data were
	generated.  The left p-value should be used when the
	alternative to independence is negative association (values
	tend to cluster in the lower left and upper right cells); the
	right p-value should be used if the alternative is positive
	association.  The two-tailed p-value for this test is
	calculated by method (b) in section 2.1 of <cite
	key="agresti92">Agresti (1992)</cite>: it is the sum of the
	probabilities of all possible tables having the given row and
	column totals and having a probability less than or equal to
	that of the observed table.
      </para>
    </description>

  </command>

</commandref>
