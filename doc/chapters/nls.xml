<?PSGML NOFILL programlisting example informalequation?>

  <chapter id="nlschap"><title>Nonlinear least squares</title>

  <sect1 id="nls-intro"><title>Introduction and examples</title>

    <para>
      As of version 1.0.9, <application>gretl</application> supports
      nonlinear least squares (NLS) using a variant of the
      Levenberg&ndash;Marquandt algorithm.  The user must supply a
      specification of the regression function; prior to giving this
      specification the parameters to be estimated must be
      <quote>declared</quote> and given initial values.  Optionally,
      the user may supply analytical derivatives of the regression
      function with respect to each of the parameters.  The tolerance
      (criterion for terminating the iterative estimation procedure)
      can be set using the <command>genr</command> command.</para>

    <para>The syntax for specifying the function to be estimated is
    the same as for the <command>genr</command> command.  Here are two
    examples, with accompanying derivatives.</para>

    <example id="nls-cons"><title>Consumption function from
	  Greene</title>
	<programlisting>
	  nls C = alpha + beta * Y^gamma
	  deriv alpha = 1
	  deriv beta = Y^gamma
	  deriv gamma = beta * Y^gamma * log(Y)
	  end nls
	</programlisting>
    </example>

    <example id="nls-ects"><title>Nonlinear function from Russell
	Davidson</title>
	<programlisting>
	  nls y = alpha + beta * x1 + (1/beta) * x2
	  deriv alpha = 1
	  deriv beta = x1 - x2/(beta*beta)
	  end nls
	</programlisting>
    </example> 

    <para>
      Note the command words <command>nls</command> (which introduces
      the regression function), <command>deriv</command> (which
      introduces the specification of a derivative), and <command>end
      nls</command>, which terminates the specification and calls for
      estimation. If the <command>-o</command> flag is appended to the
      last line the covariance matrix of the parameter estimates is
      printed.</para>

  </sect1>

  <sect1 id="nls-param"><title>Initializing the parameters</title>

    <para>The parameters of the regression function must be given
      initial values prior to the <command>nls</command> command.
      This can be done using the <command>genr</command> command (or,
      in the GUI program, via the menu item <quote>Define new
	variable</quote>).  In some cases, where the nonlinear
      function is a generalization of (or a restricted form of) a
      linear model, it may be convenient to run an
      <command>ols</command> and initialize the parameters from the
      OLS coefficient estimates.  In relation to the first example
      above, one might do:</para>

    <programlisting>
      ols C 0 Y
      genr alpha = coeff(0)
      genr beta = coeff(Y)
      genr gamma = 1
    </programlisting>

    <para>And in relation to the second example one might do:</para>

    <programlisting>
      ols y 0 x1 x2
      genr alpha = coeff(0)
      genr beta = coeff(x1)
    </programlisting> 

  </sect1>

  <sect1 id="nls-gui"><title>NLS dialog window</title>

    <para>
      It is probably most convenient to compose the commands for NLS
      estimation in the form of a <application>gretl</application>
      script but you can also do so interactively, by selecting the
      item <quote>Nonlinear Least Squares</quote> under the Model
      menu.  This opens a dialog box where you can type the function
      specification (possibly prefaced by <command>genr</command>
      lines to set the initial parameter values) and the derivatives,
      if available.  An example of this is shown in <xref
	linkend="fig-nls-dialog"/>.  Note that in this context you do
      not have to supply the <command>nls</command> and <command>end
      nls</command> tags.</para>

    <figure id="fig-nls-dialog" float="1" pgwide="1">
      <title>NLS dialog box</title>
      <screenshot>
        <screeninfo>Nonlinear least squares dialog</screeninfo>
        <graphic fileref="figures/nls_window" align="center"/>
      </screenshot>
    </figure>

  </sect1>


   <sect1 id="nls-deriv"><title>Analytical and numerical derivatives</title>

    <para>If you are able to figure out the derivatives of the
      regression function with respect to the parameters, it is
      advisable to supply those derivatives as shown in the examples
      above.  If that is not possible,
      <application>gretl</application> will compute approximate
      numerical derivatives.  The properties of the NLS
      algorithm may not be so good in this case (see 
      <xref linkend="nls-accuracy"/>).</para>

    <para>If analytical derivatives are supplied, they are checked for
      consistency with the given nonlinear function.  If the
      derivatives are clearly incorrect estimation is aborted with an
      error message.  If the derivatives are <quote>suspicious</quote>
      a warning message is issued but estimation proceeds.  This
      warning may sometimes be triggered by incorrect derivatives, but
      it may also be triggered by a high degree of collinearity among
      the derivatives.</para>

    <para>Note that you cannot mix analytical and numerical
      derivatives: you should supply expressions for all of the
      derivatives or none.</para>

  </sect1>
    
   <sect1 id="nls-toler"><title>Controlling termination</title>

    <para>The NLS estimation procedure is an iterative process.
      Iteration is terminated when a convergence criterion is met or
      when a set maximum number of iterations is reached, whichever
      comes first.  The maximum number of iterations is
      <command>100*(k+1)</command> when analytical derivatives are
      given and <command>200*(k+1)</command> when numerical
      derivatives are used, where <varname>k</varname> denotes the
      number of parameters being estimated.  The convergence criterion
      is that the relative error in the sum of squares, and/or the
      relative error between the the coefficient vector and the
      solution, is estimated to be no larger than some small value.
      This <quote>small value</quote> is by default the machine
      precision to the power 3/4, but it can be set with the
      <command>genr</command> command using the special variable
      <varname>toler</varname>.  For example</para>

    <programlisting>genr toler = .0001</programlisting> 

    <para>will relax the tolerance to 0.0001.</para>

  </sect1>

   <sect1 id="nls-code"><title>Details on the code</title>  

    <para>The underlying engine for NLS estimation is based on the
      <application>minpack</application> suite of functions, available
      from <ulink
	url="http://www.netlib.org/minpack/">netlib.org</ulink>.
      Specifically, the following <application>minpack</application>
      functions are called:</para>

    <itemizedlist>
      <listitem><para><filename>lmder</filename>: 
	  Levenberg&ndash;Marquandt algorithm with analytical
	  derivatives.</para></listitem>
      <listitem><para><filename>chkder</filename>: For checking the
	  supplied analytical derivatives.</para></listitem>
      <listitem><para><filename>lmdif</filename>: 
	  Levenberg&ndash;Marquandt algorithm with numerical
	  derivatives.</para></listitem>
      <listitem><para><filename>fdjac2</filename>: Compute the
	  final approximate Jacobian when using numerical
	  derivatives.</para></listitem>
      <listitem><para><filename>dpmpar</filename>: Determine the
	  machine precision.</para></listitem>
      </itemizedlist>

    <para>On successful completion of the Levenberg&ndash;Marquandt
	iteration, a Gauss&ndash;Newton regression is used to
	calculate the covariance matrix for the parameter estimates.
	Since NLS results are asymptotic, there is room for debate
	over whether or not a correction for degrees of freedom should
	be applied when calculating the standard error of the
	regression (and the standard errors of the parameter
	estimates).  For comparability with OLS, and in light of the
	reasoning given in Davidson and MacKinnon (1993), the
	estimates shown in <application>gretl</application>
	<emphasis>do</emphasis> use a degrees of freedom
	correction.</para>

  </sect1>

  <sect1 id="nls-accuracy"><title>Numerical accuracy</title>

    <para><xref linkend="tab-nls"/> shows the results of running the
      <application>gretl</application> NLS procedure on the 27
      Statistical Reference Datasets made available by the U.S.
      National Institute of Standards and Technology (NIST) for
      testing nonlinear regression software.<footnote><para>For a
	  discussion of <application>gretl</application>'s accuracy in
	  the estimation of linear models, see <xref
	    linkend="app-accuracy"/>.</para>
      </footnote> For each dataset, two sets of starting values for
      the parameters are given in the test files, so the full test
      comprises 54 runs.  Two full tests were performed, one using all
      analytical derivatives and one using all numerical
      approximations. In each case the default tolerance was
      used.<footnote><para>The data shown in the table were gathered
	  from a pre-release build of <application>gretl</application>
	  version 1.0.9, compiled with <application>gcc</application>
	  3.3, linked against <application>glibc</application> 2.3.2,
	  and run under Linux on an i686 PC (IBM ThinkPad
	  A21m).</para>
      </footnote></para>

    <para>Out of the 54 runs, <application>gretl</application> failed
      to produce a solution in 4 cases when using analytical
      derivatives, and in 7 cases when using numeric approximation. Of
      the four failures in analytical derivatives mode, two were due
      to non-convergence of the Levenberg&ndash;Marquandt algorithm
      after the maximum number of iterations (on
      <filename>MGH09</filename> and <filename>Bennett5</filename>,
      both described by NIST as of <quote>Higher difficulty</quote>)
      and two were due to generation of range errors (out-of-bounds
      floating point values) when computing the Jacobian (on
      <filename>BoxBOD</filename> and <filename>MGH17</filename>,
      described as of <quote>Higher difficulty</quote> and
      <quote>Average difficulty</quote> respectively).  The additional
      failures in numerical approximation mode were on
      <filename>MGH10</filename> (<quote>Higher difficulty</quote>,
      maximum number of iterations reached) and
      <filename>Lanczos1</filename> (<quote>Average
	difficulty</quote>, excessive collinearity in the
      Gauss&ndash;Newton regression for computing the standard
      errors).</para>

    <para>The table gives information on several aspects of the tests:
      the number of outright failures, the average number of
      iterations taken to produce a solution and two sorts of measure
      of the accuracy of the estimates for both the parameters and the
      standard errors of the parameters.</para>

    <para>For each of the 54 runs in each mode, if the run produced a
      solution the parameter estimates obtained by
      <application>gretl</application> were compared with the NIST
      certified values.  We define the <quote>minimum correct
	figures</quote> for a given run as the number of significant
      figures to which the <emphasis>least accurate</emphasis>
      <application>gretl</application> estimate agreed with the
      certified value, for that run. The table shows both the average
      and the worst case value of this variable across all the runs
      that produced a solution.  The same information is shown for the
      estimated standard errors.</para>
      
    <para>
      The second measure of accuracy shown is the percentage of cases,
      taking into account all parameters from all successful runs, in
      which the <application>gretl</application> estimate agreed with
      the certified value to at least the 6 significant figures which
      are printed by default in the <application>gretl</application>
      regression output.</para>

    <table id="tab-nls" frame="none">
      <title>Nonlinear regression: the NIST tests</title>
      <tgroup cols="3">
        <thead>
          <row>
            <entry>&nbsp;</entry>
	    <entry>Analytical derivatives</entry>
	    <entry>Numerical derivatives</entry>
          </row>
        </thead>
        <tbody>
          <row>
	    <entry>Failures in 54 tests</entry>
	    <entry>4</entry>
	    <entry>7</entry>
	  </row>
          <row>
	    <entry>Average iterations</entry>
	    <entry>32</entry>
	    <entry>120</entry>
	  </row>
          <row>
	    <entry>Avg. of min. correct figures, parameters</entry>
	    <entry>7.960</entry>
	    <entry>6.809</entry>
	  </row>
          <row>
	    <entry>Worst of min. correct figures, parameters</entry>
	    <entry>4</entry>
	    <entry>3</entry>
	  </row>
          <row>
	    <entry>Avg. of min. correct figures, standard
	      errors</entry>
	    <entry>7.760</entry>
	    <entry>5.702</entry>
	  </row>
          <row>
	    <entry>Worst of min. correct figures, standard
	      errors</entry>
	    <entry>5</entry>
	    <entry>2</entry>
	  </row>
          <row>
	    <entry>Percent correct to at least 6 figures,
	      parameters</entry>
	    <entry>96.5</entry>
	    <entry>91.5</entry>
	  </row>
          <row>
	    <entry>Percent correct to at least 6 figures, standard
	      errors</entry>
	    <entry>92.5</entry>
	    <entry>77.3</entry>
	  </row>
	</tbody>
      </tgroup>
    </table>

    <para>The worst case parameter estimate using analytical
      derivatives occurred on the <filename>ENSO</filename> dataset.
      On the test machine this estimate was improved from 4 to 6
      correct figures by tightening the convergence tolerance to
      1.0e&minus;14.  The worst case parameter estimate using
      numerical derivatives (3 digits correct for one of the
      <filename>Bennett5</filename> parameters) could not be improved
      in this way.</para>

    <para>As regards the worst case standard error estimates, the
      poorest figure using analytical derivatives (on the
      <filename>ENSO</filename> dataset again) was improved from 5 to
      6 correct figures by setting a tolerance of 1.0e&minus;13.  The
      worst standard error estimate using numerical derivatives (on
      the <filename>Lanczos3</filename> dataset) was improved from 2
      to 3 figures at a tolerance of 1.0e&minus;13.
      </para>

    <para>Note the overall superiority of analytical derivatives: on
      average solutions to the test problems were obtained with
      substantially fewer iterations and the results were more
      accurate (most notably for the estimated standard errors).  Note
      also that the six-digit results printed by
      <application>gretl</application> are not 100 percent reliable
      for difficult nonlinear problems (in particular when using
      numerical derivatives).  Having registered this caveat, the
      percentage of cases where the results were good to six digits or
      better seems high enough to justify their printing in this
      form.</para>

  </sect1>

  </chapter>

<!-- Keep this comment at the end of the file
Local variables:
sgml-default-dtd-file:"../manual.ced"
mode: xml
sgml-parent-document:("../manual.xml" "book" "chapter")
End:
-->

