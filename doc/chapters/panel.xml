<?PSGML NOFILL programlisting example informalequation?>

  <chapter id="chap-panel"><title>Panel data</title>

  <sect1 id="panel-structure"><title>Panel structure</title>

    <para>Panel data (pooled cross-section and time-series) require
      special care.  Here are some pointers.</para>

    <para>Consider a data set composed of observations on each of
      <emphasis>n</emphasis> cross-sectional units (countries, states,
      persons or whatever) in each of <emphasis>T</emphasis> periods.
      Let each observation comprise the values of
      <emphasis>k</emphasis> variables of interest.  The data set then
      contains <emphasis>knT</emphasis> values.</para>

    <para>The data should be arranged <quote>by observation</quote>:
      each row represents an observation; each column contains the
      values of a particular variable. The data matrix then has
      <emphasis>nT</emphasis> rows and <emphasis>k</emphasis> columns.
      That leaves open the matter of how the rows should be arranged.
      There are two possibilities.<footnote><para>If you don't intend
	  to make any conceptual or statistical distinction between
	  cross-sectional and temporal variation in the data you can
	  arrange the rows arbitrarily, but this is probably wasteful
	  of information.</para>
      </footnote></para>

    <itemizedlist>
      <listitem><para>Rows grouped by <emphasis>unit</emphasis>. Think
	  of the data matrix as composed of <emphasis>n</emphasis>
	  blocks, each having <emphasis>T</emphasis> rows. The first
	  block of <emphasis>T</emphasis> rows contains the
	  observations on cross-sectional unit 1 for each of the
	  periods; the next block contains the observations on unit 2
	  for all periods; and so on.  In effect, the data matrix is a
	  set of time-series data sets, stacked
	  vertically.</para></listitem>
      <listitem><para>Rows grouped by <emphasis>period</emphasis>.
	  Think of the data matrix as composed of
	  <emphasis>T</emphasis> blocks, each having
	  <emphasis>n</emphasis> rows. The first
	  <emphasis>n</emphasis> rows contain the observations for
	  each of the cross-sectional units in period 1; the next
	  block contains the observations for all units in period 2;
	  and so on.  The data matrix is a set of cross-sectional data
	  sets, stacked vertically.</para></listitem>
    </itemizedlist>

    <para>You may use whichever arrangement is more convenient.  The
      first is perhaps easier to keep straight.  If you use the second
      then of course you must ensure that the cross-sectional units
      appear in the same order in each of the period data blocks.
      Under <application>gretl</application>'s
      <guimenu>Sample</guimenu> menu you will find an item
      <quote>Restructure panel</quote> which allows you to convert
      from stacked cross-section form to stacked time series.</para>

    <para>When you import panel data into
    <application>gretl</application> from a spreadsheet or comma
    separated format, the panel nature of the data will not be
    recognized automatically (most likely the data will be treated as
    <quote>undated</quote>).  Getting the data recognized correctly is
    a two-step process: first, establish the periodicity of the data
    and the starting observation; second, establish the structure of
    the data (stacked time series or stacked cross-sections).</para>

    <orderedlist>
      <listitem>
	<para>For panel data the <emphasis>periodicity</emphasis>
	  equals the number of time periods, in the case of stacked
	  time series, or the number of cross-sectional unit, in the
	  case of stacked cross-sections.  (In either case it is the
	  number of rows in each block of the data matrix.)</para> 
	<para>The <emphasis>starting observation</emphasis> should be
	  set in the form <literal>1:1</literal> (for periodicity less
	  than 10) or <literal>1:01</literal> (for periodicity between
	  10 and 99; add another leading zero if the periodicity is
	  100 or greater).  In this colon-separated pair of numbers,
	  the leading number represents the data-block and the
	  trailing number represents the entry within that block.
	  (Thus for example, with stacked time series the observation
	  label <literal>3:02</literal> denotes the observation for
	  unit 3, period 2.)</para>
	<para>The periodicity and starting observation can be set
	  using the script command <command>setobs</command> or the
	  GUI menu item <quote>Sample, Set frequency,
	    startobs&hellip;</quote>.</para>
      </listitem>
      <listitem>
	<para>Once the periodicity and starting observation are set
	  appropriately, you can impose the correct interpretation of
	  the data structure using the script command
	  <command>panel</command> or the GUI menu item <quote>Sample,
	    interpret as panel</quote>.  The <command>panel</command>
	  takes an option, either <literal>--time-series</literal>
	  (for stacked time series) or
	  <literal>--cross-section</literal> (for stacked
	  cross-sections).  If no option is given, stacked time series
	  is assumed.  The <quote>interpret an panel</quote> menu item
	  brings up a dialog box where you select stacked time series
	  or stacked cross-sections.</para>
	</listitem>
    </orderedlist>

  </sect1>

  <sect1 id="dummies"><title>Dummy variables</title>

    <para>In a panel study you may wish to construct dummy variables
      of one or both of the following sorts: (a) dummies as unique
      identifiers for the cross-sectional units, and (b) dummies as
      unique identifiers of the time periods.  The former may be used
      to allow the intercept of the regression to differ across the
      units, the latter to allow the intercept to differ across
      periods.</para>

    <para>You can use two special functions to create such dummies.
      These are found under the <quote>Data, Add variables</quote>
      menu in the GUI, or under the <command>genr</command> command in
      script mode or <application>gretlcli</application>.</para>

    <orderedlist>
      <listitem><para><quote>periodic dummies</quote> (script command
	  <command>genr dummy</command>).  The common use for this
	  command is to create a set of periodic dummy variables up to
	  the data frequency in a time-series study (for instance a
	  set of quarterly dummies for use in seasonal adjustment).
	  But it also works with panel data. Note that the
	  interpretation of the dummies created by this command
	  differs depending on whether the data rows are grouped by
	  unit or by period. If the grouping is by
	  <emphasis>unit</emphasis> (frequency <emphasis>T</emphasis>)
	  the resulting variables are <emphasis>period
	    dummies</emphasis> and there will be
	  <emphasis>T</emphasis> of them. For instance
	  <varname>dummy_2</varname> will have value 1 in each data
	  row corresponding to a period 2 observation, 0 otherwise. If
	  the grouping is by <emphasis>period</emphasis> (frequency
	  <emphasis>n</emphasis>) then <emphasis>n</emphasis>
	  <emphasis>unit dummies</emphasis> will be generated:
	  <varname>dummy_2</varname> will have value 1 in each data
	  row associated with cross-sectional unit 2, 0
	  otherwise.</para></listitem>
      <listitem><para><quote>panel dummies</quote> (script command
	  <command>genr paneldum</command>).  This creates all the
	  dummies, unit and period, at a stroke.  The
	  unit dummies are named <varname>du_1</varname>,
	  <varname>du_2</varname> and so on, while the period dummies
	  are named <varname>dt_1</varname>, <varname>dt_2</varname>,
	  etc.</para>
      </listitem>
    </orderedlist>

    <para>If a panel data set has the <literal>YEAR</literal> of the
      observation entered as one of the variables you can create a
      periodic dummy to pick out a particular year, e.g. <command>genr
	dum = (YEAR=1960)</command>.  You can also create periodic
      dummy variables using the modulus operator,
      <literal>%</literal>.  For instance, to create a dummy with
      value 1 for the first observation and every thirtieth
      observation thereafter, 0 otherwise, do</para>

    <programlisting>
      genr index 
      genr dum = ((index-1)%30) = 0
    </programlisting>

  </sect1>

  <sect1 id="panel-lagged"><title>Lags and differences with panel
      data</title>

    <para>If the time periods are evenly spaced you may want to use
      lagged values of variables in a panel regression; you may also
      with to construct first differences of variables of
      interest.</para>

    <para>Once a dataset is properly identified as a panel (as
      described in the previous section),
      <application>gretl</application> will handle the generation of
      such variables correctly.  For example the command <command>genr
	x1_1 = x1(-1)</command> will create a variable that contains
      the first lag of <literal>x1</literal> where available, and the
      missing value code where the lag is not available.
    </para>

    <para>If a lag of this sort is to be included in a regression you
      must ensure that the first observation from each unit block is
      dropped. One way to achieve this is to use Weighted Least
      Squares (<command>wls</command>) using an appropriate dummy
      variable as weight.  This dummy (call it
      <command>lagdum</command>) should have value 0 for the
      observations to be dropped, 1 otherwise.  In other words, it is
      complementary to a dummy variable for period 1.  Thus if you
      have already issued the command <command>genr dummy</command>
      you can now do <command>genr lagdum = 1 - dummy_1</command>.  If
      you have used <command>genr paneldum</command> you would now say
      <command>genr lagdum = 1 - dt_1</command>. Either way, you can
      now do</para>

    <para>
      <command>wls lagdum y const x1_1 ...</command>
    </para>

    <para>to get a pooled regression using the first lag of
      <varname>x1</varname>, dropping all observations from period
      1.</para>

    <para>Another option is to use the <command>smpl</command> with
      the <command>--restrict</command> or <command>--dummy</command>
      option.
      <xref linkend="examp-pwt"/> shows illustrative commands,
      assuming the unit data blocks each contain 30 observations and
      we want to drop the first row of each.  You can then run
      regressions on the restricted data set without having to use the
      <command>wls</command> command.  If you plan to reuse the
      restricted data set you may wish to save it using the
      <command>store</command> command (see <xref
	linkend="cmdref"/> below).
    </para>

    <example id="examp-panel-lags">
      <title>Lags with panel data</title>
    <programlisting>
      # create index variable 
      genr index 
      # create dum = 0 for every 30th obs 
      genr dum = ((index-1)%30) > 0 
      # sample based on this dummy
      smpl --dummy dum 
      # recreate the obs. structure, for 29 periods
      setobs 29 1.01 
    </programlisting>
    </example>

  </sect1>

  <sect1 id="pooled-est"><title>Pooled estimation</title>

    <para>There is a special purpose estimation command for use with
      panel data, the <quote>Pooled OLS</quote> option under the
      <guimenu>Model</guimenu> menu. This command is available only if
      the data set is recognized as a panel.  To take advantage of it,
      you should specify a model without any dummy variables
      representing cross-sectional units.  The routine presents
      estimates for straightforward pooled OLS, which treats
      cross-sectional and time-series variation at par.  This model
      may or may not be appropriate.  Under the
      <guimenu>Tests</guimenu> menu in the model window, you will find
      an item <quote>panel diagnostics</quote>, which tests pooled OLS
      against the principal alternatives, the fixed effects and random
      effects models.</para>

    <para>The fixed effects model adds a dummy variable for all but
      one of the cross-sectional units, allowing the intercept of the
      regression to vary across the units.  An
      <emphasis>F</emphasis>-test for the joint significance of these
      dummies is presented: if the p-value for this test is small,
      that counts against the null hypothesis (that the simple pooled
      model is adequate) and in favor of the fixed effects
      model.</para>

    <para>The random effects model, on the other hand, decomposes the
      residual variance into two parts, one part specific to the
      cross-sectional unit or <quote>group</quote> and the other
      specific to the particular observation.  (This estimator can be
      computed only if the panel is <quote>wide</quote> enough, that
      is, if the number of cross-sectional units in the data set
      exceeds the number of parameters to be estimated.)  The
      Breusch&ndash;Pagan LM statistic tests the null hypothesis
      (again, that the pooled OLS estimator is adequate) against the
      random effects alternative.</para>

    <para>It is quite possible that the pooled OLS model is rejected
      against both of the alternatives, fixed effects and random
      effects. How, then, to assess the relative merits of the two
      alternative estimators?  The Hausman test (also reported,
      provided the random effects model can be estimated) addresses
      this issue.  Provided the unit- or group-specific error is
      uncorrelated with the independent variables, the random effects
      estimator is more efficient than the fixed effects estimator;
      otherwise the random effects estimator is inconsistent, in which
      case the fixed effects estimator is to be preferred.  The null
      hypothesis for the Hausman test is that the group-specific error
      is not so correlated (and therefore the random effects model is
      preferable).  Thus a low p-value for this tests counts against
      the random effects model and in favor of fixed effects.</para>

    <para>For a rigorous discussion of this topic, see Greene (2000),
      chapter 14.</para>

  </sect1>

  <sect1 id="PWT"><title>Illustration: the Penn World Table</title>

    <para>The Penn World Table (homepage at <ulink
	url="http://pwt.econ.upenn.edu/">pwt.econ.upenn.edu</ulink>)
      is a rich macroeconomic panel dataset, spanning 152 countries
      over the years 1950&ndash;1992.  The data are available in
      <application>gretl</application> format; please see the
      <application>gretl</application> <ulink
	url="http://ricardo.ecn.wfu.edu/gretl/gretl_data.html">data
	site</ulink> (this is a free download, although it is not
      included in the main <application>gretl</application>
      package).</para>  

    <para><xref linkend="examp-pwt"/> below opens
      <filename>pwt56_60_89.gdt</filename>, a subset of the pwt
      containing data on 120 countries, 1960&ndash;89, for 20
      variables, with no missing observations (the full data set,
      which is also supplied in the pwt package for
      <application>gretl</application>, has many missing
      observations). Total growth of real GDP, 1960&ndash;89, is
      calculated for each country and regressed against the 1960 level
      of real GDP, to see if there is evidence for
      <quote>convergence</quote> (i.e. faster growth on the part of
      countries starting from a low base).</para>

    <example id="examp-pwt">
      <title>Use of the Penn World Table</title>
      <programlisting>
	  open pwt56_60_89.gdt 
	  # for 1989 (the last obs), lag 29 gives 1960, the first obs 
	  genr gdp60 = RGDPL(-29) 
	  # find total growth of real GDP over 30 years
	  genr gdpgro = (RGDPL - gdp60)/gdp60
	  # restrict the sample to a 1989 cross-section 
	  smpl --restrict YEAR=1989 
	  # convergence: did countries with a lower base grow faster?  
	  ols gdpgro const gdp60 
	  # result: No! Try an inverse relationship?
	  genr gdp60inv = 1/gdp60 
	  ols gdpgro const gdp60inv 
	  # no again.  Try treating Africa as special? 
	  genr afdum = (CCODE = 1)
	  genr afslope = afdum * gdp60 
	  ols gdpgro const afdum gdp60 afslope 
      </programlisting>
    </example>
  </sect1>

  </chapter>

<!-- Keep this comment at the end of the file
Local variables:
sgml-default-dtd-file:"../manual.ced"
mode: xml
sgml-parent-document:("../manual.xml" "book" "chapter")
End:
-->

