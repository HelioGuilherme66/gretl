\chapter{Modelli per serie storiche}
\label{chap:timeser}

\section{Modelli ARIMA}
\label{arma-estimation}

\subsection{Rappresentazione e sintassi}
\label{arma-repr}

Il comando \cmd{arma} effettua la stima di modelli autoregressivi integrati a media mobile
(ARIMA). Questi modelli possono essere scritti come
\begin{equation}
  \label{eq:plain-0-arma}
  \phi(L) y_t = \theta(L) \epsilon_t
\end{equation}
dove $\phi(L)$ e $\theta(L)$ sono polinomi nell'operatore ritardo, $L$, definito
in modo che $L^n x_t = x_{t-n}$, e $\epsilon_t$ è un processo di rumore bianco.
Il contenuto esatto di $y_t$, del polinomio AR $\phi()$ e del polinomio MA $\theta()$
verrà spiegato in seguito.

\subsection{Mean terms}
\label{sec:arma-nonzeromean}

The process $y_t$ as written in equation (\ref{eq:plain-0-arma}) has,
without further qualifications, mean zero. If the model is to be
applied to real data, it is necessary to include some term to handle
the possibility that $y_t$ has non-zero mean. There are two possible
ways to represent processes with nonzero mean: one is to define $\mu_t$
as the \emph{unconditional} mean of $y_t$, namely the central value of
is marginal distribution. Therefore, the series $\tilde{y}_t = y_t -
\mu_t$ has mean 0, and the model (\ref{eq:plain-0-arma}) applies to
$\tilde{y}_t$. In practice, assuming that $\mu_t$ is a linear function
of some observable variables $x_t$, the model becomes
\begin{equation}
  \label{eq:arma-with-x}
  \phi(L) (y_t - x_t \beta) = \theta(L) \epsilon_t
\end{equation}
This is sometimes known as a ``regression model with ARMA errors'';
its structure may be more apparent if we represent it using two
equations:
\begin{eqnarray*}
  y_t & = & x_t \beta + u_t \\
  \phi(L) u_t & = & \theta(L) \epsilon_t
\end{eqnarray*}

The model just presented is also sometimes known as ``ARMAX'' (ARMA +
eXogenous variables).  It seems to us, however, that this label is
more appropriately applied to a different model: another way to
include a mean term in (\ref{eq:plain-0-arma}) is to base the
representation on the \emph{conditional} mean of $y_t$, that is the
central value of the distribution of $y_t$ \emph{given its own past}.
Assuming, again, that this can be represented as a linear combination
of some observable variables $z_t$, the model would expand to
\begin{equation}
  \label{eq:arma-with-z}
  \phi(L) y_t = z_t \gamma + \theta(L) \epsilon_t
\end{equation}
The formulation (\ref{eq:arma-with-z}) has the advantage that $\gamma$
can be immediately interpreted as the vector of marginal effects of
the $z_t$ variables on the conditional mean of $y_t$.  And by adding
lags of $z_t$ to this specification one can estimate \emph{Transfer
  Function models} (which generalize ARMA by adding the effects of
exogenous variable distributed across time).

\app{Gretl} provides a way to estimate both forms. Models written as
in (\ref{eq:arma-with-x}) are estimated by maximum likelihood; models
written as in (\ref{eq:arma-with-z}) are estimated by conditional
maximum likelihood. (For more on these options see the section on
``Estimation'' below.)  

In the special case when $x_t = z_t = 1$ (that is, the models include
a constant but no exogenous variables) the two specifications discussed
above reduce to
\begin{equation}
  \phi(L) (y_t - \mu) = \theta(L) \epsilon_t
  \label{eq:arma-with-xconst} 
\end{equation}
and
\begin{equation}
  \phi(L) y_t = \alpha + \theta(L) \epsilon_t
  \label{eq:arma-with-zconst}
\end{equation}
respectively.  These formulations are essentially equivalent, but if
they represent one and the same process $\mu$ and $\alpha$ are, fairly
obviously, not numerically identical; rather
\[
\alpha = \left(1 - \phi_1 - \ldots - \phi_p\right) \mu
\]

The \app{gretl} syntax for estimating (\ref{eq:arma-with-xconst}) is simply
\begin{code}
  arma p q ; y
\end{code}
The AR and MA lag orders, \verb|p| and \verb|q|, can be given either as
numbers or as pre-defined scalars. The parameter $\mu$ can be dropped
if necessary by appending the option \cmd{--nc} (``no constant'') to
the command. If estimation of (\ref{eq:arma-with-zconst}) is needed,
the switch \texttt{--conditional} must be appended to the command, as
in 
\begin{code}
  arma p q ; y --conditional
\end{code}

Generalizing this principle to the estimation of
(\ref{eq:arma-with-x}) or (\ref{eq:arma-with-z}), you get that
\begin{code}
  arma p q ; y const x1 x2
\end{code}
would estimate the following model:
\[
  y_t - x_t \beta = \phi_1 \left(y_{t-1} - x_{t-1} \beta \right) + \ldots + 
   \phi_p \left( y_{t-p} - x_{t-p} \beta \right) + 
  \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q}
\]
where in this instance $x_t \beta = \beta_0 + x_{t,1} \beta_1 +
x_{t,2} \beta_2$. Appending the \texttt{--conditional} switch, as in 
\begin{code}
  arma p q ; y const x1 x2 --conditional
\end{code}
would estimate the following model:
\[
  y_t = x_t \gamma + \phi_1 y_{t-1} + \ldots +  \phi_p y_{t-p} + 
  \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q}
\]

Ideally, the issue broached above could be made moot by writing a more
general specification that nests the alternatives; that is
\begin{equation}
 \label{armax-general}
  \phi(L) \left(y_t - x_t \beta\right) = z_t \gamma  + \theta(L) \epsilon_t ;
\end{equation}
we would like to generalize the \cmd{arma} command so that
the user could specify, for any estimation method, whether certain
exogenous variables should be treated as $x_t$s or $z_t$s, but we're
not yet at that point (and neither are most other software packages).

\subsection{Seasonal models}

A more flexible lag structure is desirable when analyzing time series
that display strong seasonal patterns. Model (\ref{eq:plain-0-arma})
can be expanded to
\begin{equation}
  \label{eq:seasonal-arma}
  \phi(L) \Phi(L^s) y_t = \theta(L) \Theta(L^s) \epsilon_t .
\end{equation}
For such cases, a fuller form of the syntax is available, namely,
\begin{code}
  arma p q ; P Q ; y
\end{code}
where \texttt{p} and \texttt{q} represent the non-seasonal AR and MA
orders, and \texttt{P} and \texttt{Q} the seasonal orders.  For
example,
\begin{code}
  arma 1 1 ; 1 1 ; y
\end{code}
would be used to estimate the following model:
\[
  (1 -\phi L)(1 -\Phi L^s) (y_t - \mu) = (1 + \theta L)(1 + \Theta L^s) \epsilon_t
\]
If $y_t$ is a quarterly series (and therefore $s=4$), the above
equation can be written more explicitly as
\[
y_t - \mu = \phi (y_{t-1} - \mu) + \Phi (y_{t-4} - \mu) - (\phi
  \cdot \Phi) (y_{t-5} - \mu) + \epsilon_t + \theta \epsilon_{t-1} + \Theta
  \epsilon_{t-4} + (\theta \cdot \Theta) \epsilon_{t-5}
\]
Such a model is known as a ``multiplicative seasonal ARMA model''.

\subsection{Differencing and ARIMA}

The above discussion presupposes that the time series $y_t$ has
already been subjected to all the transformations deemed necessary for
ensuring stationarity (see also section \ref{sec:uroot}). Differencing
is the most common of these transformations, and \app{gretl} provides
a mechanism to include this step into the \cmd{arma} command: the
syntax
\begin{code}
  arma p d q ; y 
\end{code}
would estimate an ARMA$(p,q)$ model on $\Delta^d y_t$. It is
functionally equivalent to 
\begin{code}
  series tmp = y
  loop for i=1..d
    tmp = diff(tmp)
  end loop
  arma p q ; tmp 
\end{code}
except with regard to forecasting after estimation (see below).

When the series $y_t$ is differenced before performing the analysis
the model is known as ARIMA (``I'' for Integrated); for this reason,
\app{gretl} provides the \cmd{arima} command as an alias for
\cmd{arma}.

Seasonal differencing is handled similarly, with the syntax
\begin{code}
  arma p d q ; P D Q ; y 
\end{code}
where \texttt{D} is the order for seasonal differencing.  Thus, the
command
\begin{code}
  arma 1 0 0 ; 1 1 1 ; y 
\end{code}
would produce the same parameter estimates as
\begin{code}
  genr dsy = sdiff(y)
  arma 1 0 ; 1 1 ; dsy 
\end{code}
where we use the \texttt{sdiff} function to create a seasonal
difference (e.g.\ for quarterly data, $y_t - y_{t-4}$).

\subsection{Stima}
\label{arma-est}

%TODO
The default estimation method for ARMA models is exact maximum
likelihood estimation (under the assumption that the error term is
normally distributed), using the Kalman filter in conjunction with the
BFGS maximization algorithm.  The gradient of the log-likelihood with
respect to the parameter estimates is approximated numerically.  This
method produces results that are directly comparable with many other
software packages.  The constant, and any exogenous variables, are
treated as in equation (\ref{eq:arma-with-x}).  The covariance matrix
for the parameters is computed via the OPG (Outer Product of the
Gradients) method.

The alternative method, invoked with the \verb|--conditional| switch,
is conditional maximum likelihood (CML), also known as ``conditional
sum of squares'' --- see Hamilton (1994, p.\ 132).  This method was
exemplified in the script~\ref{jack-arma}, and only a brief
description will be given here.  Given a sample of size $T$, the CML
method minimizes the sum of squared one-step-ahead prediction errors
generated by the model for the observations $t_0, \ldots,
T$.\footnote{The numerical method used is BHHH.  As with exact ML, the
  covariance matrix is computed via the OPG method.}  The starting
point $t_0$ depends on the orders of the AR polynomials in the model.

Questo metodo è quasi equivalente a quello della massima verosimiglianza in
ipotesi di normalità; la differenza sta nel fatto che le prime $(t_0 - 1)$
osservazioni sono considerate fisse ed entrano nella funzione di verosimiglianza
solo come variabili condizionanti. Di conseguenza, i due metodi sono
asintoticamente equivalenti sotto le consuete ipotesi, tranne per il fatto,
discusso sopra, che la nostra implementazione CML tratta la costante e le
variabili esogente come mostrato nell'equazione (\ref{eq:arma-with-z}).

The two methods can be compared as in the following example
\begin{code}
  open data10-1
  arma 1 1 ; r
  arma 1 1 ; r --conditional
\end{code}
which produces the estimates shown in Table~\ref{tab:ml-cml}.  As you
can see, the estimates of $\phi$ and $\theta$ are quite similar.  The
reported constants differ widely, as expected --- see the discussion
following equations (\ref{eq:arma-with-xconst}) and
(\ref{eq:arma-with-zconst}).  However, dividing the CML constant by
$1-\phi$ we get 7.38, which is not far from the ML estimate of 6.93.

\begin{table}[htbp]
\caption{ML and CML estimates}
\label{tab:ml-cml}
\begin{center}
  \begin{tabular}{crrrr}
    \hline
    Parameter & \multicolumn{2}{c}{ML} &
    \multicolumn{2}{c}{CML} \\
    \hline 
    $\mu$ & 6.93042 & (0.673202) & 1.07322 & (0.488661) \\
    $\phi$ & 0.855360 & (0.0512026) & 0.852772 & (0.0450252) \\
    $\theta$ & 0.588056 & (0.0809769) & 0.591838 & (0.0456662) \\
    \hline
  \end{tabular}
\end{center}
\end{table}

\subsection{Estimation via X-12-ARIMA}

As an alternative to estimating ARMA models using ``native'' code,
\app{gretl} offers the option of using the external program
\app{X-12-ARIMA}.  This is the seasonal adjustment software produced
and maintained by the U.S. Census Bureau; it is used for all official
seasonal adjustments at the Bureau.

\app{Gretl} includes a module which interfaces with \app{X-12-ARIMA}:
it translates \cmd{arma} commands using the syntax outlined above into
a form recognized by \app{X-12-ARIMA}, executes the program, and
retrieves the results for viewing and further analysis within
\app{gretl}.  To use this facility you have to install
\app{X-12-ARIMA} separately.  Packages for both MS Windows and
GNU/Linux are available from the \app{gretl} website,
\url{http://gretl.sourceforge.net/}.

To invoke \app{X-12-ARIMA} as the estimation engine, append the flag
\verb|--x-12-arima|, as in
\begin{code}
  arma p q ; y --x-12-arima
\end{code}
As with native estimation, the default is to use exact ML but there is
the option of using conditional ML with the \verb|--conditional| flag.
However, please note that when \app{X-12-ARIMA} is used in conditional
ML mode, the comments above regarding the variant treatments of the
mean of the process $y_t$ \textit{do not apply}.  That is, when you
use \app{X-12-ARIMA} the model that is estimated is
(\ref{eq:arma-with-x}), regardless of whether estimation is by exact
ML or conditional ML.


\subsection{Forecasting}
\label{arma-fcast}

ARMA models are often used for forecasting purposes.  The
autoregressive component, in particular, offers the possibility of
forecasting a process ``out of sample'' over a substantial time
horizon.

\app{Gretl} supports forecasting on the basis of ARMA models using the
method set out by Box and Jenkins (1976).\footnote{See in particular
  their ``Program 4'' on p.\ 505ff.}  The Box and Jenkins algorithm
produces a set of integrated AR coefficients which take into account
any differencing of the dependent variable (seasonal and/or
non-seasonal) in the ARIMA context, thus making it possible to
generate a forecast for the level of the original variable.  By
contrast, if you first difference a series manually and then apply
ARMA to the differenced series, forecasts will be for the differenced
series, not the level.  This point is illustrated
in Example~\ref{arima-fcast-script}.  The parameter estimates are identical
for the two models.  The forecasts differ but are mutually consistent:
the variable \texttt{fcdiff} emulates the ARMA forecast (static,
one step ahead within the sample range, and dynamic out of sample).

\begin{script}[htbp]
  \caption{ARIMA forecasting}
  \label{arima-fcast-script}
\begin{code}
  open greene18_2.gdt
  # log of quarterly U.S. nominal GNP, 1950:1 to 1983:4
  genr y = log(Y)
  # and its first difference
  genr dy = diff(y)
  # reserve 2 years for out-of-sample forecast
  smpl ; 1981:4
  # Estimate using ARIMA
  arima 1 1 1 ; y 
  # forecast over full period
  smpl --full
  fcast fc1
  # Return to sub-sample and run ARMA on the first difference of y
  smpl ; 1981:4
  arma 1 1 ; dy
  smpl --full
  fcast fc2
  genr fcdiff = (t<=1982:1)*(fc1 - y(-1)) + (t>1982:1)*(fc1 - fc1(-1))
  # compare the forecasts over the later period
  smpl 1981:1 1983:4
  print y fc1 fc2 fcdiff --byobs
\end{code}
The output from the last command is:
\begin{code}
                    y          fc1          fc2       fcdiff

  1981:1      7.964086     7.940930      0.02668      0.02668
  1981:2      7.978654     7.997576      0.03349      0.03349
  1981:3      8.009463     7.997503      0.01885      0.01885
  1981:4      8.015625     8.033695      0.02423      0.02423
  1982:1      8.014997     8.029698      0.01407      0.01407
  1982:2      8.026562     8.046037      0.01634      0.01634
  1982:3      8.032717     8.063636      0.01760      0.01760
  1982:4      8.042249     8.081935      0.01830      0.01830
  1983:1      8.062685     8.100623      0.01869      0.01869
  1983:2      8.091627     8.119528      0.01891      0.01891
  1983:3      8.115700     8.138554      0.01903      0.01903
  1983:4      8.140811     8.157646      0.01909      0.01909
\end{code}
\end{script}


\subsection{Limitations}

The structure of \app{gretl}'s \cmd{arma} command does not allow you
to specify models with gaps in the lag structure, other than via the
seasonal specification discussed above.  For example, if you have a
monthly time series, you cannot estimate an ARMA model with AR terms
(or MA terms) at just lags 1, 3 and 5.

At a pinch, you could circumvent this limitation in respect of the AR
part of the specification by the trick of including lags of the
dependent variable in the list of ``exogenous'' variables.  For
example, the following command
\begin{code}
  arma 0 0 ; 0 1 ; y const y(-2)
\end{code}
on a quarterly series would estimate the parameters of the model
\[
  y_t - \mu = \phi \left(y_{t-2} - \mu\right) + \epsilon_t + \Theta \epsilon_{t-4}
\]
However, this workaround is not really recommended: it should deliver
correct estimates, but will break the existing mechanism for
forecasting.

\section{Test per radici unitarie}
\label{sec:uroot}

(da completare)

\subsection{Il test ADF}
\label{sec:ADFtest}

Il test ADF (Augmented Dickey-Fuller) è implementato in \app{gretl} sotto forma
della statistica $t$ su $\varphi$ nella regressione seguente:
\begin{equation}
  \label{eq:ADFtest}
  \Delta y_t = \mu_t + \varphi y_{t-1} + \sum_{i=1}^p \gamma_i \Delta
  y_{t-i} + \epsilon_t .
\end{equation}

Questa statistica test è probabilmente il più famoso e utilizzato test per
radici unitarie. È un test a una coda la cui ipotesi nulla è
$\varphi = 0$, mentre quella alternativa è $\varphi < 0$. Sotto l'ipotesi nulla,
$y_t$ deve essere differenziata almeno una volta per raggiungere la
stazionarietà. Sotto l'ipotesi alternativa, $y_t$ è già stazionaria e non
richiede differenziazione. Quindi, grandi valori negativi della statistica test
portano a rifiutare l'ipotesi nulla.

Un aspetto peculiare di questo test è che la sua distribuzione limite non è
standard sotto l'ipotesi nulla: inoltre, la forma della distribuzione, e quindi
i valori critici per il test, dipendono dalla forma del termine
$\mu_t$. Un'eccellente analisi di tutti i casi possibili è contenuta in
Hamilton (1994), ma il soggetto è trattato anche in qualsiasi testo recente
sulle serie storiche. Per quanto riguarda \app{gretl}, esso permette all'utente
di scegliere la specificazione di $\mu_t$ tra quattro alternative:

\begin{center}
  \begin{tabular}{cc}
    \hline
    $\mu_t$ & Opzione del comando \\
    \hline
    0 & \verb|--nc| \\
    $\mu_0$ &  \verb|--c| \\
    $\mu_0 + \mu_1 t$ &  \verb|--ct| \\
    $\mu_0 + \mu_1 t + \mu_1 t^2$ &  \verb|--ctt| \\
    \hline
  \end{tabular}
\end{center}

Queste opzioni non sono mutualmente esclusive e possono essere usate insieme; in
questo caso, la statistica verrà calcolata separatamente per ognuno dei casi.
La scelta predefinita in \app{gretl} è quella di usare la combinazione
\verb|--c --ct --ctt|. Per ognuno dei casi, vengono calcolati p-value
approssimativi usando l'algoritmo descritto in MacKinnon 1996.

Il comando di \app{gretl} da usare per eseguire il test è \cmd{adf}; ad esempio
\begin{code}
  adf 4 x1 --c --ct
\end{code}
calcola la statistica test come statistica-t per $\varphi$ nell'equazione
\ref{eq:ADFtest} con $p=4$ nei due casi $\mu_t = \mu_0$ e
$\mu_t = \mu_0 + \mu_1 t$.

Il numero di ritardi ($p$ nell'equazione \ref{eq:ADFtest}) deve essere scelto
per assicurarsi che la (\ref{eq:ADFtest}) sia una parametrizzazione abbastanza
flessibile per rappresentare adeguatamente la persistenza di breve termine di
$\Delta y_t$. Scegliere un $p$ troppo basso può portare a distorsioni di
dimensione nel test, mentre sceglierlo troppo alto porta a una perdita di
potenza del test. Per comodità dell'utente, il parametro $p$ può essere
determinato automaticamente. Impostando $p$ a un numero negativo viene attivata
una procedura sequenziale che parte da $p$ ritardi e decrementa $p$ fino a quando la statistica $t$
per il parametro $\gamma_p$ supera 1.645 in valore assoluto.

\subsection{Il test KPSS}
\label{sec:KPSStest}

Il test KPSS (Kwiatkowski, Phillips, Schmidt e Shin, 1992) è un test per radici
unitarie in cui l'ipotesi nulla è l'opposto di quella del test ADF: l'ipotesi
nulla è che la serie sia stazionaria, mentre l'ipotesi alternativa è che la serie
sia $I(1)$.
 
L'intuizione alla base di questa statistica test è molto semplice: se $y_t$ può
essere scritta come $y_t = \mu + u_t$, dove $u_t$ è un qualche processo
stazionario a media nulla, non solo la media campionaria di $y_t$ fornisce uno
stimatore consistente di $\mu$, ma la varianza di lungo periodo di $u_t$ è un
numero finito. Nessuna di queste proprietà è valida nel caso dell'ipotesi
alternativa.
 
Il test si basa sulla seguente statistica:

\begin{equation}
  \label{eq:KPSStest}
  \eta = \frac{\sum_{i=1}^T S_t^2 }{ T^2 \bar{\sigma}^2 }
\end{equation}
dove $S_t = \sum_{s=1}^t e_s$ e $\bar{\sigma}^2$ è una stima della varianza di
lungo periodo di $e_t = (y_t - \bar{y})$. Sotto l'ipotesi nulla, questa
statistica ha una distribuzione asintotica ben definita (non standard), che non
dipende da parametri di disturbo ed è stata tabulata con metodi di simulazione.
Sotto l'ipotesi alternativa, la statistica diverge.

Di conseguenza, è possibile costruire un test a una coda basato su
$\eta$, dove $H_0$ è rifiutata se $\eta$ è maggiore del valore critico
desiderato; \app{gretl} fornisce i quantili del 90\%, 95\%,
97.5\% e 99\%.

Esempio di uso:
\begin{code}
  kpss m y
\end{code}
dove \verb|m| è un intero che rappresenta la larghezza di banda, o la dimensione
della finestra usata nella formula per stimare la varianza di lungo periodo:
\[
  \bar{\sigma}^2 = \sum_{i=-m}^m \left( 1 - \frac{|i|}{m+1} \right) \hat{\gamma}_i
\]
I termini $\hat{\gamma}_i$ denotano le autocovarianze empiriche di $e_t$
dall'ordine $-m$ fino al $m$.  Affinché questo stimatore sia consistente, $m$
deve essere abbastanza grande da accomodare la persistenza di breve periodo di
$e_t$, ma non troppo grande se paragonato all'ampiezza campionaria $T$.
Nell'interfaccia grafica di \app{gretl}, il valore predefinito è pari alla parte
intera di $4 \left( \frac{T}{100} \right)^{1/4}$.

Il concetto visto sopra può essere generalizzato al caso in cui $y_t$ è
stazionario attorno a un trend deterministico. In questo caso, la
formula (\ref{eq:KPSStest}) rimane invariata, ma la serie $e_t$ è definita come
residui della regressione OLS di $y_t$ su una costante e un trend lineare.
Questa seconda forma del test si ottiene aggiungendo l'opzione
\verb|--trend| al comando \cmd{kpss}:
\begin{code}
  kpss n y --trend
\end{code}
Si noti che in questo caso la distribuzione asintotica del test è diversa, e i
valori critici riportati da \app{gretl} sono corretti di conseguenza.

\subsection{I test di Johansen}
\label{sec:Joh-test}

In senso stretto, questi sono test per la cointegrazione, ma possono essere
usati anche come test multivariati per radici unitarie, visto che sono la
generalizzazione multivariata del test ADF.
\begin{equation}
  \label{eq:Joh-tests}
  \Delta y_t = \mu_t + \Pi y_{t-1} + \sum_{i=1}^p \Gamma_i \Delta
  y_{t-i} + \epsilon_t
\end{equation}
Se il rango di $\Pi$ è 0, i processi sono tutti I(1); se il rango di
$\Pi$ è pieno, i processi sono tutti I(0); nei casi intermedi, $\Pi$ può essere
scritto come $\alpha \beta'$ e si ha cointegrazione.

Il rango di $\Pi$ viene analizzato calcolando gli autovalori di una matrice ad
essa strettamente legata (chiamata $M$) che ha rango pari a quello di $\Pi$:
per costruzione, $M$ è simmetrica e semidefinita positiva, quindi tutti i suoi
autovalori sono reali e non negativi e i test sul rango di $\Pi$ possono quindi
essere condotti verificando quanti autovalori di $M$ sono pari a 0.

Se tutti gli autovalori sono significativamente diversi da 0, tutti i processi
sono stazionari. Se, al contrario, c'è almeno un autovalore pari a 0, allora
il processo $y_t$ è integrato, anche se qualche combinazione lineare $\beta'y_t$
potrebbe essere stazionaria. All'estremo opposto, se non ci sono autovalori
significativamente diversi da 0, non solo il processo $y_t$ è non-stazionario,
ma vale lo stesso per qualsiasi combinazione lineare $\beta'y_t$; in altre
parole non c'è alcuna cointegrazione.

I due test di Johansen sono i test ``$\lambda$-max'', per le ipotesi sui singoli
autovalori, e il test ``trace'', per le ipotesi congiunte.  Il comando
\cmd{coint2} di \app{gretl} esegue questi due test. 
 
Come nel test ADF, la distribuzione asintotica dei test varia a seconda
dell'elemento deterministico $\mu_t$ incluso nel VAR. \app{gretl} fornisce le
opzioni seguenti (per una breve discussione del significato delle cinque
opzioni, si veda la sezione \ref{sec:johansen-test}):
\begin{center}
  \begin{tabular}{cc}
    \hline
    $\mu_t$ & Opzione del comando \\
    \hline
    0 & \verb|--nc| \\
    $\mu_0, \alpha_{\perp}'\mu_0 = 0 $ &  \verb|--rc| \\
    $\mu_0$ &  predefinito \\
    $\mu_0 + \mu_1 t , \alpha_{\perp}'\mu_1 = 0$ &  \verb|--crt| \\
    $\mu_0 + \mu_1 t$ &  \verb|--ct| \\
    \hline
  \end{tabular}
\end{center}
Si noti che per questo comando le opzioni presentate sopra sono mutualmente
esclusive. Inoltre, si può usare l'opzione \verb|--seasonal| per aggiungere a
$\mu_t$ delle variabili dummy stagionali centrate. In ognuno dei casi, i p-value
sono calcolati con le approssimazioni di Doornik (1998).
 
Il codice seguente usa il database \cmd{denmark}, fornito con \app{gretl}, per
replicare l'esempio di Johansen contenuto nel suo libro del 1995.
\begin{code}
  open denmark
  coint2 2 LRM LRY IBO IDE --rc --seasonal
\end{code}

In questo caso, il vettore $y_t$ nell'equazione (\ref{eq:Joh-tests}) comprende
le quattro variabili \cmd{LRM}, \cmd{LRY}, \cmd{IBO}, \cmd{IDE}. Il numero dei ritardi
è pari a $p$ nella (\ref{eq:Joh-tests}) più uno. Di seguito è riportata parte
dell'output:
\begin{center}
\begin{code}
Test di Johansen:
Numero di equazioni = 4
Ordine dei ritardi = 2
Periodo di stima: 1974:3 - 1987:3 (T = 53)

Caso 2: costante vincolata
Rango Autovalore Test traccia p-value   Test Lmax  p-value
   0    0,43317     49,144 [0,1284]     30,087 [0,0286]
   1    0,17758     19,057 [0,7833]     10,362 [0,8017]
   2    0,11279     8,6950 [0,7645]     6,3427 [0,7483]
   3   0,043411     2,3522 [0,7088]     2,3522 [0,7076]

autovalore     0.43317      0.17758      0.11279     0.043411 
\end{code}
\end{center}
Visto che sia la traccia, sia $\lambda$-max portano ad accettare l'ipotesi nulla
che il più piccolo autovalore vale 0, possiamo concludere che le serie non sono
stazionarie. Tuttavia, qualche loro combinazione lineare potrebbe essere $I(0)$,
come indicato dal rifiuto del $\lambda$-max dell'ipotesi che il rango di
$\Pi$ sia 0 (il test traccia dà un'indicazione meno netta in questo senso).

\section{ARCH e GARCH}
\label{sec:arch-garch}

Il fenomeno dell'eteroschedasticità rappresenta la varianza non costante del
termine di errore in un modello di regressione. L'eteroschedasticità
condizionale autoregressiva (ARCH) è un fenomeno specifico dei modelli per serie
storiche, in cui la varianza del termine di errore presenta un comportamento
autoregressivo, ad esempio, la serie presenta periodi in cui la varianza
dell'errore è relativamente ampia e periodi in cui è relativamente piccola.
Questo tipo di comportamento si verifica spesso sui mercati finanziari: una
notizia inaspettata può provocare periodi di maggior volatilità del mercato.

Un processo di errore ARCH di ordine $q$ può essere rappresentato come
\[
u_t = \sigma_t \varepsilon_t; \qquad
\sigma^2_t \equiv {\rm E}(u^2_t|\Omega_{t-1}) = 
\alpha_0 + \sum_{i=1}^q \alpha_i u^2_{t-i}
\]
dove le $\varepsilon_t$ sono indipendenti e identicamente distribuite (iid)
con media zero e varianza uno, e dove $\sigma_t$ è la radice quadrata di
$\sigma^2_t$. $\Omega_{t-1}$ denota il set informativo al tempo $t-1$ e
$\sigma^2_t$ è la varianza condizionale: ossia, la varianza condizionata
all'informazione che risale al tempo $t-1$ e precedente.

È importante notare la differenza tra un processo ARCH e un normale processo di
errore autoregressivo. Quest'ultimo, nel caso più semplice (del primo ordine),
può essere scritto come
\[
u_t = \rho u_{t-1} + \varepsilon_t; \qquad -1 < \rho < 1
\]
dove le $\varepsilon_t$ sono iid con media zero e varianza costante $\sigma^2$.
Con un errore AR(1), se $\rho$ è positivo un valore positivo di $u_t$ tenderà ad
essere seguito, con probabilità maggiore di 0.5, da un valore positivo
$u_{t+1}$.  Con un processo di errore ARCH, un errore $u_t$ dal valore assoluto
elevato tenderà ad essere seguito da valori a loro volta elevati, ma senza
assumere che i valori successivi siano dello stesso segno. La presenza di
processi ARCH nelle serie finanziarie è un ``fatto stilizzato'' ed è coerente
con l'ipotesi di efficienza dei mercati; d'altra parte, un comportamento
autoregressivo dei prezzi dei titoli violerebbe l'efficienza del mercato.

È possibile testare per l'esistenza di un ARCH di ordine $q$ nel modo seguente:
\begin{enumerate}
\item Stimare il modello in esame con OLS e salvare i quadrati dei residui,
  $\hat{u}^2_t$.
\item Eseguire una regressione ausiliaria in cui i quadrati dei residui sono
  regrediti su una costante e su $q$ ritardi propri.
\item Trovare il valore $TR^2$ (ampiezza campionaria moltiplicata per $R^2$ non
  corretto) per la regressione ausiliaria.
\item Confrontare il valore $TR^2$ con la distribuzione $\chi^2$ con $q$ gradi
  di libertà, e se il p-value è ``abbastanza piccolo'' rifiutare l'ipotesi nulla
  di omoschedasticità, in favore dell'ipotesi alternativa dell'esistenza di un
  processo ARCH($q$).
\end{enumerate}

Questo test è implementato in \app{gretl} con il comando \cmd{arch}. Questo
comando può essere eseguito dopo la stima OLS di un modello di serie storiche, o
selezionandolo dal menù ``Test'' della finestra del modello (sempre dopo una
stima OLS). Verrà mostrato il risultato del test, e, se il valore
$TR^2$ dalla regressione ausiliaria ha un p-value minore di 0.10,
vengono mostrate anche le stime ARCH. Queste stime sono sotto forma di minimi
quadrati generalizzati (Generalized Least Squares, GLS), in particolare di
minimi quadrati ponderati, dove i pesi sono inversamente proporzionali alle
deviazioni standard stimate degli errori, $\hat{\sigma}_t$, derivate dalla
regressione ausiliaria.

Inoltre, il test ARCH è disponibile dopo aver stimato un'autoregressione
vettoriale (VAR). In questo caso però non viene eseguita la stima GLS.

\subsection{GARCH}
\label{subsec:garch}

Il semplice processo ARCH($q$) è utile per introdurre il concetto generale di
eteroschedasticità condizionale nelle serie storiche, ma si è rivelato
insufficiente per il lavoro empirico. La dinamica della varianza dell'errore
consentita dal modello ARCH($q$) non è abbastanza ricca per rappresentare
l'andamento tipico dei dati finanziari. Oggi è di uso più comune il modello ARCH
generalizzato, o GARCH.

La rappresentazione della varianza di un processo nel modello GARCH è abbastanza
(ma non esattamente) simile alla rappresentazione ARMA del livello di una serie
storica. La varianza al tempo $t$ può dipendere sia dai valori passati della
varianza, sia dai valori passati dei quadrati degli errori, come mostra il
seguente sistema di equazioni:
\begin{eqnarray}
  \label{eq:garch-meaneq}
  y_t &  = & X_t \beta + u_t \\
  \label{eq:garch-epseq}
  u_t &  = & \sigma_t \varepsilon_t \\
  \label{eq:garch-vareq}
  \sigma^2_t & = & \alpha_0 + \sum_{i=1}^q \alpha_i u^2_{t-i} +
	  \sum_{j=1}^p \delta_i \sigma^2_{t-j}
\end{eqnarray}

Come nel caso precedente, $\varepsilon_t$ è una variabile iid con varianza unitaria.
$X_t$ è una matrice di regressori (o nel caso più semplice un vettore con
elementi pari a 1, che consente una media di $y_t$ diversa da zero). Si noti che
se $p=0$, il GARCH si riduce a un ARCH($q$): la generalizzazione è incorporata
nei termini $\delta_i$ che moltiplicano i valori precedenti della varianza
dell'errore.

In linea di principio, l'innovazione sottostante, $\varepsilon_t$, potrebbe
seguire una qualsiasi distribuzione di probabilità, e oltre all'ovvia candidata
rappresentata dalla normale, o Gaussiana, anche la distribuzione $t$ è stata
usata in questo contesto. Al momento \app{gretl} gestisce solo il caso in cui
$\varepsilon_t$ viene ipotizzata Gaussiana. Però se si usa l'opzione
\verb|--robust| del comando \cmd{garch}, lo stimatore usato da \app{gretl} per
la matrice di covarianza può essere considerato uno stimatore di quasi massima
verosimiglianza anche con disturbi non normali. Si veda oltre per le altre
opzioni che riguardano la matrice di covarianza GARCH.

Esempio:
\begin{code}
  garch p q ; y const x
\end{code}
dove \verb|p| $\ge 0$ e \verb|q| $>0$ denotano i rispettivi ordini di ritardo
come mostrati nell'equazione (\ref{eq:garch-vareq}).  Questi valori possono
essere forniti in forma numerica o come nomi di variabili scalari preesistenti.

\subsection{Stima GARCH}
\label{subsec:garch-est}

La stima dei parametri di un modello GARCH non è per nulla un compito semplice.
Si consideri l'equazione~\ref{eq:garch-vareq}: la varianza condizionale in ogni
periodo, $\sigma^2_t$, dipende dalla varianza condizionale nei periodi
precedenti, ma $\sigma^2_t$ non è osservata e deve essere stimata con qualche
tipo di procedura di massima verosimiglianza. \app{Gretl} usa il metodo proposto
da Fiorentini, Calzolari e Panattoni (1996)\footnote{L'algoritmo si basa sul
codice Fortran rilasciato dagli autori nell'archivio del \textit{Journal of
Applied Econometrics} ed è usato per gentile concessione del Professor
Fiorentini.} che è stato adottato come metro di paragone nello studio dei
risultati GARCH di McCullough e Renfro (1998). Esso usa le derivate analitiche
prime e seconde della log-verosimiglianza, adotta un algoritmo del gradiente
misto, sfruttando la matrice informativa nelle prime iterazioni, e quindi passa
all'Hessiano in prossimità della massima verosimiglianza (questo andamento può
essere verificato usando l'opzione \verb|--verbose| del comando \cmd{garch} di
\app{gretl}).

Sono disponibili varie opzioni per il calcolo della matrice di covarianza delle
stime dei parametri ottenute con il comando \cmd{garch}. Per prima cosa, è
possibile scegliere tra uno stimatore ``standard'' e uno ``robusto''.
La scelta predefinita è quella dell'Hessiano, a meno che non si usi l'opzione
\verb|--robust|, nel cui caso viene usato lo stimatore QML. Una scelta più
dettagliata è disponibile usando il comando \cmd{set}, come mostrato nella
tabella~\ref{tab:garch-vcv}.

\begin{table}[htbp]
\caption{Opzioni per la matrice di covarianza GARCH}
\label{tab:garch-vcv}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\textit{Commando}} &
\multicolumn{1}{c}{\textit{Effetto}} \\ [4pt]
\texttt{set garch\_vcv hessian} & Usa l'Hessiano \\
\texttt{set garch\_vcv im} & Usa la matrice di informazione \\
\texttt{set garch\_vcv op} & Usa il prodotto esterno del gradiente \\
\texttt{set garch\_vcv qml} & Stimatore QML \\
\texttt{set garch\_vcv bw} & Stimatore ``sandwich'' di Bollerslev--Wooldridge
\end{tabular}
\end{center}
\end{table}

Non è infrequente, nella stima di un modello GARCH, che il calcolo iterativo
delle stime fallisca nel raggiungere la convergenza. Affinché un modello GARCH
abbia senso, sono necessari vincoli stringenti sui valori ammissibili dei
parametri, e non sempre esiste un insieme di valori all'interno dello spazio dei
parametri per cui la verosimiglianza viene massimizzata.

I vincoli in questione possono essere spiegati riferendosi al più semplice (e
più comune) modello GARCH, dove $p = q = 1$. Nel modello GARCH(1, 1), la varianza
condizionale è
\begin{equation}
\label{eq:condvar}
\sigma^2_t = \alpha_0 + \alpha_1 u^2_{t-1} + \delta_1 \sigma^2_{t-1}
\end{equation}
Prendendo il valore atteso non condizionale della (\ref{eq:condvar}) si ottiene
\[
\sigma^2 = \alpha_0 + \alpha_1 \sigma^2 + \delta_1 \sigma^2
\]
così che
\[
\sigma^2 = \frac{\alpha_0}{1 - \alpha_1 - \delta_1}
\]
Affinché questa varianza non condizionale esista, occorre che
$\alpha_1 + \delta_1 < 1$, e quindi occorre richiedere $\alpha_0 > 0$.

Un motivo comune per la non convergenza delle stime GARCH (ossia, un motivo
comune per la non esistenza di valori $\alpha_i$ e $\delta_i$ che soddisfano le
condizioni precedenti e nello stesso tempo massimizzano la verosimiglianza dei
dati) è la cattiva specificazione del modello. È importante rendersi conto che
il modello GARCH in sé prevede \textit{solamente} che la volatilità dei dati dipende 
dal tempo. Se la \textit{media} della serie in questione non è costante, o se il
processo di errore non è solo eteroschedastico ma è anche autoregressivo, è
necessario tener conto di questi fatti per formulare un modello appropriato. Ad
esempio, può essere necessario dover prendere la differenza prima della
variabile in questione e/o aggiungere opportuni regressori $X_t$ al modello,
come mostrato nella (\ref{eq:garch-meaneq}).

\section{Cointegrazione e modelli vettoriali a correzione d'errore}
\label{vecm-explanation}

\subsection{Il test di cointegrazione di Johansen}
\label{sec:johansen-test}

Il test di Johansen per la cointegrazione deve tenere conto di quali
ipotesi vengono fatte a proposito dei termini deterministici, per cui
si possono individuare i ben noti ``cinque casi''. Una presentazione
approfondita dei cinque casi richiede una certa quantità di algebra
matriciale, ma è possibile dare un'intuizione del problema per mezzo
di un semplice esempio.
    
Si consideri una serie $x_t$ che si comporta nel modo seguente
%      
\[ x_t = m + x_{t-1} + \varepsilon_t \]
%
dove $m$ è un numero reale e $\varepsilon_t$ è un processo ``white
noise''.  Come si può facilmente mostrare, $x_t$ è un ``random walk''
che fluttua intorno a un trend deterministico con pendenza $m$. Nel
caso particolare in cui $m$ = 0, il trend deterministico scompare e
$x_t$ è un puro random walk.
    
Si consideri ora un altro processo $y_t$, definito da
%      
\[ y_t = k + x_t + u_t \]
%
dove, ancora, $k$ è un numero reale e $u_t$ è un processo white noise.
Poiché $u_t$ è stazionario per definizione, $x_t$ e $y_t$ sono
cointegrate, ossia la loro differenza
%      
\[ z_t = y_t - x_t = k + u_t \]
%	
è un processo stazionario. Per $k$ = 0, $z_t$ è un semplice white
noise a media zero, mentre per $k$ $\ne$ 0 il processo $z_t$ è white
noise con media diversa da zero.
  
Dopo alcune semplici sostituzioni, le due equazioni precedenti possono
essere rappresentate congiuntamente come un sistema VAR(1)
%      
\[ \left[ \begin{array}{c} y_t \\ x_t \end{array} \right] = \left[
  \begin{array}{c} k + m \\ m \end{array} \right] + \left[
  \begin{array}{rr} 0 & 1 \\ 0 & 1 \end{array} \right] \left[
  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + \left[
  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array}
\right] \]
%	
o in forma VECM
%      
\begin{eqnarray*}
  \left[  \begin{array}{c} \Delta y_t \\ \Delta x_t \end{array} \right]  & = & 
  \left[  \begin{array}{c} k + m \\ m \end{array} \right] +
  \left[  \begin{array}{rr} -1 & 1 \\ 0 & 0 \end{array} \right] 
  \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + 
  \left[  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array} \right] = \\
  & = & 
  \left[  \begin{array}{c} k + m \\ m \end{array} \right] +
  \left[  \begin{array}{r} -1 \\ 0 \end{array} \right]
  \left[  \begin{array}{rr} 1 & -1 \end{array} \right] 
  \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + 
  \left[  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array} \right] = \\
  & = &
  \mu_0 + \alpha \beta^{\prime} \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + \eta_t = 
  \mu_0 + \alpha z_{t-1} + \eta_t ,
\end{eqnarray*}
%	
dove $\beta$ è il vettore di cointegrazione e $\alpha$ è il vettore
dei ``loading'' o ``aggiustamenti''.
     
Possiamo ora considerare tre casi possibili:
    
\begin{enumerate}
\item $m \ne 0$: in questo caso $x_t$ ha un trend, come abbiamo appena
  visto; ne consegue che anche $y_t$ segue un trend lineare perché in
  media si mantiene a una distanza di $k$ da $x_t$.  Il vettore
  $\mu_0$ non ha restrizioni. Questo è il caso predefinito per il
  comando \cmd{vecm} di gretl.
	
\item $m = 0$ e $k \ne 0$: in questo caso, $x_t$ non ha un trend, e di
  conseguenza neanche $y_t$.  Tuttavia, la distanza media tra $y_t$ e
  $x_t$ è diversa da zero. Il vettore $\mu_0$ è dato da
%	  
  \[
  \mu_0 = \left[ \begin{array}{c} k \\ 0 \end{array} \right]
  \]
%	    
  che è non nullo, quindi il VECM mostrato sopra ha un termine
  costante.  La costante, tuttavia è soggetta alla restrizione che il
  suo secondo elemento deve essere pari a 0. Più in generale,
  $\mu$\ensuremath{_{0}} è un multiplo del vettore $\alpha$. Si noti
  che il VECM potrebbe essere scritto anche come
%	  
  \[
  \left[ \begin{array}{c} \Delta y_t \\ \Delta x_t \end{array} \right]
  = \left[ \begin{array}{r} -1 \\ 0 \end{array} \right] \left[
    \begin{array}{rrr} 1 & -1 & -k \end{array} \right] \left[
    \begin{array}{c} y_{t-1} \\ x_{t-1} \\ 1 \end{array} \right] +
  \left[ \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t
    \end{array} \right]
  \]
%	   
  che incorpora l'intercetta nel vettore di cointegrazione. Questo è
  il caso di ``costante vincolata''; può essere specificato nel
  comando \cmd{vecm} di gretl usando l'opzione \verb+--rc+.
	
\item $m = 0$ e $k = 0$: questo caso è il più vincolante: chiaramente,
  né $x_t$ né $y_t$ hanno un trend, e la loro distanza media è zero.
  Anche il vettore $\mu_0$ vale 0, quindi questo caso può essere
  chiamato ``senza costante''.  Questo caso è specificato usando
  l'opzione \verb+--nc+ con \cmd{vecm}.
	
\end{enumerate}


Nella maggior parte dei casi, la scelta tra le tre possibilità si basa
su un misto di osservazione empirica e di ragionamento economico. Se
le variabili in esame sembrano seguire un trend lineare, è opportuno
non imporre alcun vincolo all'intercetta. Altrimenti occorre chiedersi
se ha senso specificare una relazione di cointegrazione che includa
un'intercetta diversa da zero. Un esempio appropriato potrebbe essere
la relazione tra due tassi di interesse: in generale questi non hanno
un trend, ma il VAR potrebbe comunque avere un'intercetta perché la
differenza tra i due (lo ``spread'' sui tassi d'interesse) potrebbe
essere stazionaria attorno a una media diversa da zero (ad esempio per
un premio di liquidità o di rischio).
    
L'esempio precedente può essere generalizzato in tre direzioni:
    
\begin{enumerate}
\item Se si considera un VAR di ordine maggiore di 1, l'algebra si
  complica, ma le conclusioni sono identiche.
\item Se il VAR include più di due variabili endogene, il rango di
  cointegrazione $r$ può essere maggiore di 1. In questo caso $\alpha$
  è una matrice con $r$ colonne e il caso con la costante vincolata
  comporta che $\mu_0$ sia una combinazione lineare delle colonne di
  $\alpha$.
\item Se si include un trend lineare nel modello, la parte
  deterministica del VAR diventa $\mu_0$ + $\mu_1$. Il ragionamento è
  praticamente quello visto sopra, tranne per il fatto che
  l'attenzione è ora posta su $\mu_1$ invece che su $\mu_0$. La
  controparte del caso con ``costante vincolata'' discusso sopra è un
  caso con ``trend vincolato'', così che le relazioni di
  cointegrazione includono un trend, ma la differenza prima delle
  variabili in questione no. Nel caso di un trend non vincolato, il
  trend appare sia nelle relazioni di cointegrazione sia nelle
  differenze prime, il che corrisponde alla presenza di un trend
  quadratico nelle variabili (espresse in livelli). Questi due casi
  sono specificati rispettivamente con le opzioni \verb+--crt+ e
  \verb+--ct+ del comando \cmd{vecm}.
\end{enumerate}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "gretl-guide-it"
%%% End: 

