\chapter{Discrete and censored dependent variables}
\label{discr-models}

\section{Logit and probit models}
\label{sec:logit-probit}

It often happens that one wants to specify and estimate a model in
which the dependent variable is not continuous, but discrete. A
typical example is a model in which the dependent variable is the
occupational status of an individual (1 = employed, 0 = unemployed). A
convenient way of formalizing this situation is to consider the
variable $y_i$ as a Bernoulli random variable and analyze its
conditional distribution on the explanatory variables $x_i$, that is
\begin{equation}
  \label{eq:qr-Bernoulli}
  y_i \left\{ 
    \begin{array}{ll} 1 & P_i \\ 0 & 1 - P_i \end{array}
    \right. 
\end{equation}
where $P_i = P(y_i = 1 | x_i) $ is a given function of the explanatory
variables $x_i$.

In most cases, the function $P_i$ is a cumulative distribution
function $F$, applied to a linear combination of the $x_i$'s. In the
probit model, the Normal cdf is used, while the logit model employs
the logistic function $\Lambda()$. Therefore, we have
\begin{eqnarray}
  \label{eq:qr-link}
  \textrm{probit} & \qquad & P_i = F(z_i) = \Phi(z_i)  \\
  \textrm{logit}  & \qquad & P_i = F(z_i) = \Lambda(z_i) = \frac{1}{1 + e^{-z_i}} \\
  & &z_i = \sum_{j=1}^k x_{ij} \beta_j
\end{eqnarray}
where $z_i$ is commonly known as the \emph{index} function. Note that
in this case the coefficients $\beta_j$ cannot be interpreted as the
partial derivatives of $E(y_i | x_i)$ with respect to
$x_{ij}$. However, for a given value of $x_i$ it is possible to
compute the vector of ``slopes'', that is
\[
  \mathrm{slope}_j(\bar{x}) = \left. \pder{F(z)}{x_j} \right|_{z =
    \bar{z}} ;
\]
\app{gretl} automatically computes the slopes by setting each
explanatory variable at its sample mean.

Another
equivalent way of putting this model is to think that there is an
unobserved variable $y^*_i$ which can be described via the model
\begin{equation}
  \label{eq:qr-latent}
  y^*_i = \sum_{j=1}^k x_{ij} \beta_j + \varepsilon_i = z_i  +
  \varepsilon_i ;
\end{equation}
we observe $y_i = 1$ whenever $y^*_i > 0$ and $y_i = 0$ otherwise. If
$\varepsilon_i$ is assumed to be normal, then we have the probit
model. The logit model arises when assuming that the density function of
$\varepsilon_i$ is 
\[
  \lambda(\varepsilon_i) =
  \pder{\Lambda(\varepsilon_i)}{\varepsilon_i} =
  \frac{e^{-\varepsilon_i}}{(1 + e^{-\varepsilon_i})^2} .
\]

Both the probit and logit model are estimated in \app{gretl} via
maximum likelihood; since the score equations do not have a closed
form solution, numerical optimization is used. However, in most cases
this is totally transparent to the user, since only a few iterations
are normally needed to ensure convergence. The \texttt{--verbose}
switch can be used to track the maximization algorithm.

\begin{script}[htbp]
  \caption{Estimation of simple logit and probit models}
  \label{simple-QR}
\begin{code}
open greene19_1

logit GRADE const GPA TUCE PSI
probit GRADE const GPA TUCE PSI
\end{code}
\end{script}

As an example, we reproduce the results given in Greene (2000),
chapter 21, where the effectiveness of a program for teaching
economics is evaluated by the improvements of students' grades.
Running the code in example \ref{simple-QR} gives the following output:
\begin{code}

Model 1: Logit estimates using the 32 observations 1-32
Dependent variable: GRADE

      VARIABLE       COEFFICIENT        STDERROR      T STAT       SLOPE
                                                                  (at mean)
  const               -13.0213           4.93132      -2.641
  GPA                   2.82611          1.26294       2.238      0.533859   
  TUCE                  0.0951577        0.141554      0.672      0.0179755  
  PSI                   2.37869          1.06456       2.234      0.449339   

  Mean of GRADE = 0.344
  Number of cases 'correctly predicted' = 26 (81.2%)
  f(beta'x) at mean of independent vars = 0.189
  McFadden's pseudo-R-squared = 0.374038
  Log-likelihood = -12.8896
  Likelihood ratio test: Chi-square(3) = 15.4042 (p-value 0.001502)
  Akaike information criterion (AIC) = 33.7793
  Schwarz Bayesian criterion (BIC) = 39.6422
  Hannan-Quinn criterion (HQC) = 35.7227

           Predicted
             0    1
  Actual 0  18    3
         1   3    8

Model 2: Probit estimates using the 32 observations 1-32
Dependent variable: GRADE

      VARIABLE       COEFFICIENT        STDERROR      T STAT       SLOPE
                                                                  (at mean)
  const                -7.45232          2.54247      -2.931
  GPA                   1.62581          0.693883      2.343      0.533347   
  TUCE                  0.0517288        0.0838903     0.617      0.0169697  
  PSI                   1.42633          0.595038      2.397      0.467908   

  Mean of GRADE = 0.344
  Number of cases 'correctly predicted' = 26 (81.2%)
  f(beta'x) at mean of independent vars = 0.328
  McFadden's pseudo-R-squared = 0.377478
  Log-likelihood = -12.8188
  Likelihood ratio test: Chi-square(3) = 15.5459 (p-value 0.001405)
  Akaike information criterion (AIC) = 33.6376
  Schwarz Bayesian criterion (BIC) = 39.5006
  Hannan-Quinn criterion (HQC) = 35.581

           Predicted
             0    1
  Actual 0  18    3
         1   3    8

\end{code}

In this context, the \texttt{$\$$uhat} accessor function
takes a special meaning: it returns generalised residuals as defined
in Gourieroux et al, which can be interpreted as unbiased estimators
of the latent disturbances $\varepsilon_t$. These are defined as
\begin{equation}
  \label{eq:QR-genres}
  u_i = \left\{
    \begin{array}{ll}
      y_i - \hat{P}_i & \textrm{for the logit model} \\
      y_i\cdot \frac{\phi(\hat{z}_i)}{\Phi(\hat{z}_i)} - 
      ( 1 - y_i ) \cdot \frac{\phi(\hat{z}_i)}{1 - \Phi(\hat{z}_i)}
      & \textrm{for the probit model} \\
    \end{array}
    \right.
\end{equation}

Among other uses, generalised residuals are often used for diagnostic
purposes; for example, it is very easy to set up an omitted variables
test equivalent to the familiar LM test in the context of a linear
regression: example \ref{QR-add} shows how to perform a variable
addition test.

\begin{script}[htbp]
  \caption{Variable addition test in a probit model}
  \label{QR-add}
\begin{code}
open greene19_1

probit GRADE const GPA PSI
series u = $uhat 

ols u const GPA PSI TUCE -q
printf "Variable addition test for TUCE:\n"
printf "Rsq * T = %g (p. val. = %g)\n", $trsq, pvalue(X,1,$trsq) 
\end{code}
%$
\end{script}

\subsection{Ordered models}
\label{sec:ordered}

These models are simple variations of ordinary logit/probit models,
and are usually applied in case the dependent variable is a
discrete and ordered measurement, not necessarily quantitative. For
example, this models can be applied to analyse cases when the
dependent variable is a qualitative assessment like ``Good'',
``Average'' and ``Bad''. Assuming we have $p$ categories, the
probability that individual $i$ falls in the $j$-th category is given
by
\begin{equation}
  \label{eq:QR-ordered}
  P(y_i = j | x_i) = \left\{
    \begin{array}{ll}
      F(z_i + \mu_0) & \textrm{for } j = 0 \\
      F(z_i + \mu_j) -  F(z_i + \mu_{j-1}) & \textrm{for } 0 < j < p \\
      1 -  F(z_i + \mu_{p-1}) & \textrm{for } j = p 
    \end{array}
    \right.
\end{equation}
The unknown parameters $\mu_j$ are called the ``cutoff
points'' and are estimated together with the $\beta$'s. For
identification purposes, $\mu_0$ is assumed to be 0. In terms of the
unobserved variable $y^*_i$, the model can be equivalently cast as
$P(y_i = j | x_i) = P(\mu_{j-1} \le y^*_i < \mu_j)$. 

\begin{script}[htbp]
  \caption{Ordered probit model}
  \label{ex-oprobit}
\begin{code}
open pension.gdt
series pctstck = pctstck/50
discrete pctstck
probit pctstck const choice age educ female black married finc25 finc35 \
  finc50 finc75 finc100 finc101 wealth89 prftshr
\end{code}
\end{script}

In order to apply these models, the dependent variable must be marked
as discrete and its lowest value must be 0. Example \ref{ex-oprobit}
reproduces the estimation given in chap. 15 of Wooldridge (2002). Note
that \app{gretl} does not provide a separate command for ordered
models: the \texttt{logit} and \texttt{probit} commands automatically
estimate the ordered version if the dependent variable is not binary
(provided it has already been marked as discrete).

After estimating ordered models, the \texttt{$\$$uhat} accessor yields
generalized residuals as in binary models; additionally, the
\texttt{$\$$yhat} accessor function returns $\hat{z}_i$, so it is
possible to compute an unbiased estimator of the latent variable
$y^*_i$ simply by adding the two together.

\section{The Tobit model}
\label{sec:tobit}

\subsection{Generalized Tobit model}
\label{sec:heckit}

include Heckman example script

\section{Count data}
\label{sec:poisson}

also include example script for negative binomial (done in Verbeek
example files).



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 