\chapter{Time series models}
\label{chap:timeser}

\section{ARIMA models}
\label{arma-estimation}

\subsection{Representation and syntax}
\label{arma-repr}

The \cmd{arma} command performs estimation of autoregressive-moving
average (ARMA) models; the most general representation of a model
estimable by \app{gretl} is as follows:
\begin{equation}
  \label{eq:general-arma}
  A(L) B(L^s) y_t = x_t \beta + C(L) D(L^s) \epsilon_t ,
\end{equation}
where $L$ is the lag operator ($L^n x_t = x_{t-n}$), $s$ is the
number of subperiods for seasonal time series (for example, 12 for
monthly series), $x_t$ is a vector of exogenous variables and
$\epsilon_t$ is a white noise process.

The basic ARMA model is obtained when $x_t = 1$ and there are no
seasonal operators. In this case, $B(L^s) = D(L^s) = 1$ and the model
becomes
\begin{equation}
  \label{eq:plain-arma}
  A(L) y_t = \mu + C(L) \epsilon_t ,
\end{equation}
where, in customary notation, the vector $\beta$ reduces to the
intercept $\mu$. It is possible to write more explicitly the above
equation as
\[
  y_t = \mu + \phi_1 y_{t-1} + \ldots + \phi_p y_{t-p} + 
  \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q
  \epsilon_{t-q} ;
\]
the corresponding \app{gretl} syntax is simply
\begin{code}
  arma p q ; y
\end{code}
where \verb|p| and \verb|q| are the desired lag orders; these can be
either numbers or pre-defined scalars. The parameter $\mu$ can be
dropped if necessary by appending the option \cmd{--nc} to the command.

If you want to estimate a model with explanatory variables, the above
syntax must be extended to
\begin{code}
  arma p q ; y const x1 x2
\end{code}
This command would estimate the following model:
\[
  y_t = \beta_0 + x_{1,t} \beta_1 + x_{2,t} \beta_2 + 
  \phi_1 y_{t-1} + \ldots + \phi_p y_{t-p} + 
  \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q} .
\]
What \app{gretl} estimates in this case is known as an ARMAX (ARMA +
eXogenous variables) model, which is different from what some other packages
call ``regression model with ARMA errors''. The difference is apparent
by considering the model \app{gretl} estimates:
\begin{equation}
  \label{eq:armax}
  A(L) y_t = x_t \beta + C(L) \epsilon_t ,
\end{equation}
and a regression model with ARMA disturbances
\begin{eqnarray}
  \label{eq:reg-arma}
  y_t & = & x_t \beta + u_t \\
  A(L) u_t & = & C(L) \epsilon_t ;
\end{eqnarray}
the latter would translate into
\[
  A(L) y_t = A(L) \left(x_t \beta \right) + C(L) \epsilon_t ;
\]
the ARMAX formulation has the advantage that $\beta$ can be
immediately interpreted as the vector of marginal effects of the $x_t$
variables on the conditional mean of $y_t$.

The \cmd{arma} command can be therefore used also for estimating \emph{Transfer Function
  Models}, a type of generalization of autoregressive-moving average
(ARMA) models adding the effects of exogenous variable distributed
along time, as in:
\begin{equation}
  \label{eq:tfunc-model}
  \phi (L) \cdot \Phi (L^s) y_t = \sum_{i=1}^kv_{i}(L)x_{it} + \theta (L)\cdot \Theta (L^s) \epsilon_t ,
\end{equation}

The structure of the \cmd{arma} command does not let you specify
models with gaps in the lag structure. A more flexible lag
structure is especially desirable when analyzing time series that
display strong seasonal patterns. In these cases, the full model
(\ref{eq:general-arma}) can be used. For example, the syntax
\begin{code}
  arma 1 1 ; 1 1 ; y
\end{code}
would be used to estimate a model with four parameters:
\[
  ( 1 - \phi L )  ( 1 - \Phi L^s ) y_t = \mu + ( 1 + \theta L ) ( 1 + \Theta L^s ) \epsilon_t;
\]
assuming that $y_t$ is a quarterly series (and therefore $s=4$), the
above equation can be written more explicitly as
\[
  y_t = \mu + \phi y_{t-1} + \Phi y_{t-4} - (\phi \cdot \Phi) y_{t-5} + 
  \epsilon_t + \theta \epsilon_{t-1} + \Theta \epsilon_{t-4} +
  (\theta \cdot \Theta) \epsilon_{t-5} .
\]
Such a model is known as a ``multiplicative seasonal ARMA model''.

For more general models, this limitation can be circumvented for the
autoregressive part by including lags of the dependent variable in the
exogenous list. As an example, the following command 
\begin{code}
  arma 0 0 ; 0 1 ; y const y(-2)
\end{code}
on a quarterly series would estimate the parameters of the model
\[
  y_t = \mu + \phi y_{t-2} + \epsilon_t + \Theta \epsilon_{t-4}.
\]
However, this workaround is not recommended: although this would
deliver correct estimates, it would break the existing mechanism for
forecasting.

The above discussion presupposes that the time series $y_t$ has
already been subjected to all the transformations deemed necessary for
ensuring stationarity. Differencing is the most common of these
transformations, and \app{gretl} provides a mechanism to include this
step into the \cmd{arma} command: the syntax
\begin{code}
  arma p d q ; y 
\end{code}
would estimate and ARMA(p,q) model on $\Delta^d y_t$, and is
functionally equivalent to 
\begin{code}
  series tmp = y
  loop for i=1..d
    tmp = diff(tmp)
  end loop
  arma p q ; tmp 
\end{code}
Such a model is known as an ARIMA (AutoRegressive Integrated
Moving-Average) model; for this reason, \app{gretl} provides the
\cmd{arima} command as an alias for \cmd{arma}. Seasonal differencing
is handled similarly, with the syntax
\begin{code}
  arma p d q ; P D Q ; y 
\end{code}
Thus, the command 
\begin{code}
  arma 1 0 0 ; 1 1 1 ; y 
\end{code}
would produce the same results as
\begin{code}
  genr dsy = sdiff(y)
  arma 1 0 ; 1 1 ; dsy 
\end{code}

\subsection{Estimation}
\label{arma-est}

The algorithm \app{gretl} uses to estimate the parameters of an ARMA
model is conditional maximum likelihood (CML), also known as
``conditional sum of squares'' (see Hamilton, \emph{Time Series
  Analysis} (1994), page 132). This method was exemplified in the
script \ref{jack-arma}, and only a brief description will be given
here: given a sample of size $T$, the CML method minimizes the sum of
squared one-step-ahead prediction errors generated by the model for
the observations $t_0, \ldots, T$. The starting point $t_0$ depends by
the orders of the polynomials in the ARMA model.

This method is nearly equivalent to maximum likelihood under the
hypothesis of normality; the only difference is that the first $(t_0 -
1)$ observations are considered fixed and only enter the likelihood
function as conditioning variables. As a consequence, the two methods
are asymptotically equivalent under standard conditions.

The numerical method used for maximizing the log-likelihood is BHHH.
The covariance matrix for the parameters (hence the standard errors)
are computed via the OPG (Outer Product of Gradient) method.

Full ML estimation under normality is available in \app{gretl} via
the \verb|x-12| plugin, which is automatically used if the option
\verb|--x-12-arima| is appendend to the \cmd{arma} command. For
example, the following code
\begin{code}
  open data10-1
  arma 1 1 ; r
  arma 1 1 ; r --x-12-arima
\end{code}
produces the estimates below:
\begin{center}
  \begin{tabular}{crrrr}
    \hline
    Parameter & \multicolumn{2}{c}{\textrm{CML}} &
    \multicolumn{2}{c}{\textrm{ML (x-12 plugin)}} \\
    \hline 
    $\mu$ & 1.07322 & 0.488661 &  1.00232 & 0.133002 \\
    $\phi$ & 0.852772 & 0.0450252 & 0.855373  & 0.0496304 \\
    $\theta$ & 0.591838 & 0.0456662 & 0.587986 & 0.0799962 \\
    \hline
  \end{tabular}
\end{center}

\subsection{Forecasting}
\label{arma-fcast}

To be written

\section{Unit root tests}
\label{sec:uroot}

To be completed

\subsection{The ADF test}
\label{sec:ADFtest}

The ADF (Augmented Dickey-Fuller) test is, as implemented in
\app{gretl}, the $t$-statistic on $\varphi$ in the following regression:
\begin{equation}
  \label{eq:ADFtest}
  \Delta y_t = \mu_t + \varphi y_{t-1} + \sum_{i=1}^p \gamma_i \Delta
  y_{t-i} + \epsilon_t
\end{equation}
\begin{itemize}
\item $H_0$: $y_t$ is I(1);
\item one-sided test: $H_0$ is rejected if $t_{\varphi}$ is ``small'';
\item nonstandard distribution, also depends on the contents of
  $\mu_t$ --- p-values by MacKinnon.
\end{itemize}
Syntax:
\begin{code}
  adf n x1
\end{code}
\begin{itemize}
\item Determinstic kernel options;
\item auto-selection of $p$ via negative \verb|n|.
\end{itemize}
\subsection{The KPSS test}
\label{sec:KPSStest}

\begin{equation}
  \label{eq:KPSStest}
  \eta = \frac{\sum_{i=1}^T S_t^2 }{ T^2 \bar{\sigma}^2 }
\end{equation}
where $S_t = \sum_{s=1}^t e_s$ and $\bar{\sigma}^2$ is an
estimate of the long-run variance of $e_t = (y_t - \bar{y})$.
\begin{itemize}
\item $H_0$: $y_t$ is I(0);
\item one-sided test: $H_0$ is rejected if $\eta$ is ``big'';
\item extension to deterministic trend ($e_t$ are the residuals from
  an OLS regression of $y_t$ on a constant and a linear trend);
\item nonstandard distribution --- we provide 90\%, 95\%, 97.5\% and
  99\% quantiles.
\end{itemize}
Syntax:
\begin{code}
  kpss n x1
\end{code}
\begin{itemize}
\item \verb|--trend| option;
\item \verb|n| is used for estimating $\bar{\sigma}^2$; in the GUI,
  default is the integer part of $4 \left( \frac{T}{100}
  \right)^{1/4}$.
\end{itemize}

\subsection{The Johansen tests}
\label{sec:Joh-test}

Strictly speaking, these are tests for cointegration. However, they
can be used as multivariate unit-root tests since they are the
multivariate generalization of the ADF test.
\begin{equation}
  \label{eq:Joh-tests}
  \Delta y_t = \mu_t + \Pi y_{t-1} + \sum_{i=1}^p \Gamma_i \Delta
  y_{t-i} + \epsilon_t
\end{equation}
If the rank of $\Pi$ is 0, the processes are all I(1); If the rank of
$\Pi$ is full, the processes are all I(0); in between, you have
cointegration.

The rank of $\Pi$ is investigated by computing the eigenvalues of a
closely related matrix (call it $M$) whose rank is the same as $\Pi$:
tests on the rank of $\Pi$ can therefore be carried out by testing how
many eigenvalues of $M$ are 0. The two Johansen tests are the
``$\lambda$-max'' test, for hypotheses on individual eigenvalues, and
the ``trace'' test, for joint hypotheses.

The \app{gretl} command \cmd{coint2} performs these two tests.
Example:
\begin{code}
  coint2 n x1 x2 x3
\end{code}
sets $p$ in equation (\ref{eq:Joh-tests}) to $(n-1)$ and prints a table
with the two test batteries for the three series \verb|x1|, \verb|x2|
and \verb|x3|.
 
\ldots Options for the deterministic kernel \ldots
More on this in section \ref{sec:johansen-test}

\section{ARCH and GARCH}
\label{sec:arch}

Heteroskedasticity means a non-constant variance of the error term in
a regression model.  Autoregressive Conditional Heteroskedasticity
(ARCH) is a phenomenon specific to time series models, whereby the
variance of the error displays autoregressive behavior --- for
instance, the time series exhibits periods where the error variance is
relatively large, for several observations, and periods where it is
relatively small.

A GARCH model can be briefly described by the following equations:
\begin{eqnarray}
  \label{eq:garch-meaneq}
  y_t &  = & x_t \beta + \epsilon_t \\
  \label{eq:garch-epseq}
  \epsilon_t &  = & u_t \sigma_t \\
  \label{eq:garch-vareq}
  A(L) \sigma^2_t &  = & \omega + B(L) \epsilon_{t-1}^2 ,
\end{eqnarray}
where $u_t$ is an iid sequence with unit variance. Currently
\app{gretl} only handles models where $u_t$ is assumed to be a
Gaussian white noise. However, the \verb|--robust| option computes the
covariance matrix of the parameters via the Bollerslev-Wooldridge
``sandwich'' estimator, so that the estimator \app{gretl} uses can be
considered QML estimators even with non-normal disturbances.

Example:
\begin{code}
  garch p q ; y const x
\end{code}
where \verb|p| is the (non-negative) degree of $A(L)$ and \verb|q| is
the (strictly positive) degree of $B(L)$.

\section{Cointegration and Vector Error Correction Models}
\label{vecm-explanation}

\subsection{The Johansen cointegration test}
\label{sec:johansen-test}

The Johansen test for cointegration has to take into account what
hypotheses one is willing to make on the deterministic terms, which
leads to the famous ``five cases.'' A full and general illustration of
the five cases requires a fair amount of matrix algebra, but an
intuitive understanding of the issue can be gained by means of a
simple example.
    
Consider a series $x_t$ which behaves as follows
%      
\[ x_t = m + x_{t-1} + \varepsilon_t \] 
%
where $m$ is a real number and $\varepsilon_t$ is a white noise
process. As is easy to show, $x_t$ is a random walk which fluctuates
around a deterministic trend with slope $m$. In the special case $m$ =
0, the deterministic trend disappears and $x_t$ is a pure random walk.
    
Consider now another process $y_t$, defined by
%      
\[ y_t = k + x_t + u_t \] 
%
where, again, $k$ is a real number and $u_t$ is a white noise process.
Since $u_t$ is stationary by definition, $x_t$ and $y_t$ cointegrate:
that is, their difference
%      
\[ z_t = y_t - x_t = k + u_t \]
%	
is a stationary process. For $k$ = 0, $z_t$ is simple zero-mean white
noise, whereas for $k$ $\ne$ 0 the process $z_t$ is white noise with a
non-zero mean.
  
After some simple substitutions, the two equations above can be
represented jointly as a VAR(1) system
%      
\[ \left[ \begin{array}{c} y_t \\ x_t \end{array} \right] = \left[
  \begin{array}{c} k + m \\ m \end{array} \right] + \left[
  \begin{array}{rr} 0 & 1 \\ 0 & 1 \end{array} \right] \left[
  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + \left[
  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array}
\right] \]
%	
or in VECM form
%      
\begin{eqnarray*}
  \left[  \begin{array}{c} \Delta y_t \\ \Delta x_t \end{array} \right]  & = & 
  \left[  \begin{array}{c} k + m \\ m \end{array} \right] +
  \left[  \begin{array}{rr} -1 & 1 \\ 0 & 0 \end{array} \right] 
  \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + 
  \left[  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array} \right] = \\
  & = & 
  \left[  \begin{array}{c} k + m \\ m \end{array} \right] +
  \left[  \begin{array}{r} -1 \\ 0 \end{array} \right]
  \left[  \begin{array}{rr} 1 & -1 \end{array} \right] 
  \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + 
  \left[  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array} \right] = \\
  & = & 
  \mu_0 + \alpha \beta^{\prime} \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + \eta_t = 
  \mu_0 + \alpha z_{t-1} + \eta_t ,
\end{eqnarray*}
%	
where $\beta$ is the cointegration vector and $\alpha$ is the
``loadings'' or ``adjustments'' vector.
     
We are now in a position to consider three possible cases:
    
\begin{enumerate}
\item $m$ $\ne$ 0: In this case $x_t$ is trended, as we just saw; it
  follows that $y_t$ also follows a linear trend because on average it
  keeps at a distance $k$ from $x_t$. The vector
  $\mu_0$ is unrestricted.  This case is the default
  for gretl's \cmd{vecm} command.
	
\item $m$ = 0 and $k$ $\ne$ 0: In this case, $x_t$ is not trended and
  as a consequence neither is $y_t$. However, the mean distance
  between $y_t$ and $x_t$ is non-zero. The vector
  $\mu_0$ is given by
%	  
  \[
  \mu_0 = \left[ \begin{array}{c} k \\ 0 \end{array} \right]
  \]
%	    
  which is not null and therefore the VECM shown above does have a
  constant term. The constant, however, is subject to the restriction
  that its second element must be 0. More generally,
  $\mu_0$ is a multiple of the vector $\alpha$. Note
  that the VECM could also be written as
%	  
  \[
  \left[ \begin{array}{c} \Delta y_t \\ \Delta x_t \end{array} \right]
  = \left[ \begin{array}{r} -1 \\ 0 \end{array} \right] \left[
    \begin{array}{rrr} 1 & -1 & -k \end{array} \right] \left[
    \begin{array}{c} y_{t-1} \\ x_{t-1} \\ 1 \end{array} \right] +
  \left[ \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t
    \end{array} \right]
  \]
%	   
  which incorporates the intercept into the cointegration vector. This
  is known as the ``restricted constant'' case; it may be specified in
  gretl's \cmd{vecm} command using the option flag \verb+--rc+.
	
\item $m$ = 0 and $k$ = 0: This case is the most restrictive: clearly,
  neither $x_t$ nor $y_t$ are trended, and the mean distance between
  them is zero. The vector $\mu_0$ is also 0, which
  explains why this case is referred to as ``no constant.''  This case
  is specified using the option flag \verb+--nc+ with \cmd{vecm}.
	
\end{enumerate}

In most cases, the choice between the three possibilities is based on
a mix of empirical observation and economic reasoning. If the
variables under consideration seem to follow a linear trend then we
should not place any restriction on the intercept. Otherwise, the
question arises of whether it makes sense to specify a cointegration
relationship which includes a non-zero intercept. One example where
this is appropriate is the relationship between two interest rates:
generally these are not trended, but the VAR might still have an
intercept because the difference between the two (the ``interest rate
spread'') might be stationary around a non-zero mean (for example,
because of a risk or liquidity premium).
    
The previous example can be generalized in three directions:
    
\begin{enumerate}
\item If a VAR of order greater than 1 is considered, the algebra gets
  more convoluted but the conclusions are identical.
\item If the VAR includes more than two endogenous variables the
  cointegration rank $r$ can be greater than 1. In this case, $\alpha$
  is a matrix with $r$ columns, and the case with restricted constant
  entails the restriction that $\mu_0$ should be some
  linear combination of the columns of $\alpha$.
\item If a linear trend is included in the model, the deterministic
  part of the VAR becomes $\mu_0 + \mu_1 t$. The reasoning is
  practically the same as above except that the focus now centers on
  $\mu_1$ rather than $\mu_0$.  The counterpart to the ``restricted
  constant'' case discussed above is a ``restricted trend'' case, such
  that the cointegration relationships include a trend but the first
  differences of the variables in question do not.  In the case of an
  unrestricted trend, the trend appears in both the cointegration
  relationships and the first differences, which corresponds to the
  presence of a quadratic trend in the variables themselves (in
  levels).  These two cases are specified by the option flags
  \verb+--crt+ and \verb+--ct+, respectively, with the \cmd{vecm}
  command.
\end{enumerate}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "gretl-guide"
%%% End: 

