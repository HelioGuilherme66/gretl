\chapter{Time series models}
\label{chap:timeser}

\section{ARIMA models}
\label{arma-estimation}

\subsection{Representation and syntax}
\label{arma-repr}

The \cmd{arma} command performs estimation of autoregressive-moving
average (ARMA) models; the most general representation of an ARMA
model that may be estimated by \app{gretl} is as follows:
\begin{equation}
  \label{eq:general-arma}
  A(L) B(L^s) y_t = x_t \beta + C(L) D(L^s) \epsilon_t ,
\end{equation}
where $L$ is the lag operator ($L^n x_t = x_{t-n}$), $s$ is the
number of subperiods for seasonal time series (for example, 12 for
monthly series), $x_t$ is a vector of exogenous variables and
$\epsilon_t$ is a white noise process.

The basic ARMA model is obtained when $x_t = 1$ and there are no
seasonal operators. In this case, $B(L^s) = D(L^s) = 1$ and the model
becomes
\begin{equation}
  \label{eq:plain-arma}
  A(L) y_t = \mu + C(L) \epsilon_t ,
\end{equation}
where, in customary notation, the vector $\beta$ reduces to the
intercept $\mu$.  The equation above may be written more explicitly
as
\[
  y_t = \mu + \phi_1 y_{t-1} + \ldots + \phi_p y_{t-p} + 
  \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q
  \epsilon_{t-q} 
\]
The corresponding \app{gretl} syntax is simply
\begin{code}
  arma p q ; y
\end{code}
where \verb|p| and \verb|q| are the desired lag orders; these can be
either numbers or pre-defined scalars. The parameter $\mu$ can be
dropped if necessary by appending the option \cmd{--nc} to the command.

If you want to estimate a model with explanatory variables, the above
syntax must be extended to
\begin{code}
  arma p q ; y const x1 x2
\end{code}
This command would estimate the following model:
\[
  y_t = \beta_0 + x_{1,t} \beta_1 + x_{2,t} \beta_2 + 
  \phi_1 y_{t-1} + \ldots + \phi_p y_{t-p} + 
  \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q} .
\]
What \app{gretl} estimates in this case is known as an ARMAX (ARMA +
eXogenous variables) model, which is different from what some other packages
call ``regression model with ARMA errors''. The difference is apparent
by considering the model \app{gretl} estimates:
\begin{equation}
  \label{eq:armax}
  A(L) y_t = x_t \beta + C(L) \epsilon_t ,
\end{equation}
and a regression model with ARMA disturbances
\begin{eqnarray}
  \label{eq:reg-arma}
  y_t & = & x_t \beta + u_t \\
  A(L) u_t & = & C(L) \epsilon_t ;
\end{eqnarray}
the latter would translate into
\[
  A(L) y_t = A(L) \left(x_t \beta \right) + C(L) \epsilon_t ;
\]
the ARMAX formulation has the advantage that $\beta$ can be
immediately interpreted as the vector of marginal effects of the $x_t$
variables on the conditional mean of $y_t$.  Note, however, that
regression models with purely autoregressive errors can be estimated
(albeit not with maximum-likelihood techniques) by other \app{gretl}
commands such as \cmd{corc} and \cmd{pwe}. 

The \cmd{arma} command can be therefore used also for estimating
\emph{Transfer Function Models}, a type of generalization of
autoregressive-moving average (ARMA) models adding the effects of
exogenous variable distributed along time, as in:
\begin{equation}
  \label{eq:tfunc-model}
  \phi (L) \cdot \Phi (L^s) y_t = \sum_{i=1}^kv_{i}(L)x_{it} + \theta (L)\cdot \Theta (L^s) \epsilon_t ,
\end{equation}

The structure of the \cmd{arma} command does not let you specify
models with gaps in the lag structure. A more flexible lag
structure is especially desirable when analyzing time series that
display strong seasonal patterns. In these cases, the full model
(\ref{eq:general-arma}) can be used. For example, the syntax
\begin{code}
  arma 1 1 ; 1 1 ; y
\end{code}
would be used to estimate a model with four parameters:
\[
  ( 1 - \phi L )  ( 1 - \Phi L^s ) y_t = \mu + ( 1 + \theta L ) ( 1 + \Theta L^s ) \epsilon_t;
\]
assuming that $y_t$ is a quarterly series (and therefore $s=4$), the
above equation can be written more explicitly as
\[
  y_t = \mu + \phi y_{t-1} + \Phi y_{t-4} - (\phi \cdot \Phi) y_{t-5} + 
  \epsilon_t + \theta \epsilon_{t-1} + \Theta \epsilon_{t-4} +
  (\theta \cdot \Theta) \epsilon_{t-5} .
\]
Such a model is known as a ``multiplicative seasonal ARMA model''.

For more general models, this limitation can be circumvented for the
autoregressive part by including lags of the dependent variable in the
exogenous list. As an example, the following command 
\begin{code}
  arma 0 0 ; 0 1 ; y const y(-2)
\end{code}
on a quarterly series would estimate the parameters of the model
\[
  y_t = \mu + \phi y_{t-2} + \epsilon_t + \Theta \epsilon_{t-4}.
\]
However, this workaround is not recommended: although this would
deliver correct estimates, it would break the existing mechanism for
forecasting.

The above discussion presupposes that the time series $y_t$ has
already been subjected to all the transformations deemed necessary for
ensuring stationarity (see also section \ref{sec:uroot}). Differencing
is the most common of these transformations, and \app{gretl} provides
a mechanism to include this step into the \cmd{arma} command: the
syntax
\begin{code}
  arma p d q ; y 
\end{code}
would estimate and ARMA(p,q) model on $\Delta^d y_t$, and is
functionally equivalent to 
\begin{code}
  series tmp = y
  loop for i=1..d
    tmp = diff(tmp)
  end loop
  arma p q ; tmp 
\end{code}
Such a model is known as an ARIMA (AutoRegressive Integrated
Moving-Average) model; for this reason, \app{gretl} provides the
\cmd{arima} command as an alias for \cmd{arma}. Seasonal differencing
is handled similarly, with the syntax
\begin{code}
  arma p d q ; P D Q ; y 
\end{code}
Thus, the command 
\begin{code}
  arma 1 0 0 ; 1 1 1 ; y 
\end{code}
would produce the same results as
\begin{code}
  genr dsy = sdiff(y)
  arma 1 0 ; 1 1 ; dsy 
\end{code}

\subsection{Estimation}
\label{arma-est}

The algorithm \app{gretl} uses to estimate the parameters of an ARMA
model is conditional maximum likelihood (CML), also known as
``conditional sum of squares'' --- see Hamilton (1994, p.\ 132).  This
method was exemplified in the script \ref{jack-arma}, and only a brief
description will be given here.  Given a sample of size $T$, the CML
method minimizes the sum of squared one-step-ahead prediction errors
generated by the model for the observations $t_0, \ldots, T$. The
starting point $t_0$ depends on the orders of the AR polynomials in the
model.

This method is nearly equivalent to maximum likelihood under the
hypothesis of normality; the difference is that the first $(t_0 -
1)$ observations are considered fixed and only enter the likelihood
function as conditioning variables. As a consequence, the two methods
are asymptotically equivalent under standard conditions.

The numerical method used for maximizing the log-likelihood is BHHH.
The covariance matrix for the parameters (hence the standard errors)
are computed via the OPG (Outer Product of the Gradients) method.

Full ML estimation under normality is available in \app{gretl} via
the \verb|x-12| plugin, which is automatically used if the option
\verb|--x-12-arima| is appendend to the \cmd{arma} command. For
example, the following code
\begin{code}
  open data10-1
  arma 1 1 ; r
  arma 1 1 ; r --x-12-arima
\end{code}
produces the estimates shown in Table~\ref{tab:cml-ml}.

\begin{table}[htbp]
\caption{CML and ML estimates}
\label{tab:cml-ml}
\begin{center}
  \begin{tabular}{crrrr}
    \hline
    Parameter & \multicolumn{2}{c}{CML} &
    \multicolumn{2}{c}{ML (\app{X-12-ARIMA} plugin)} \\
    \hline 
    $\mu$ & 1.07322 & (0.488661) &  1.00232 & (0.133002) \\
    $\phi$ & 0.852772 & (0.0450252) & 0.855373  & (0.0496304) \\
    $\theta$ & 0.591838 & (0.0456662) & 0.587986 & (0.0799962) \\
    \hline
  \end{tabular}
\end{center}
\end{table}

For comparability with \app{gretl}'s own estimates, you can instruct
\app{X-12-ARIMA} to produce CML estimates by appending the option
\verb|--conditional| along with \verb|--x-12-arima|.

\subsection{Forecasting}
\label{arma-fcast}

To be written

\section{Unit root tests}
\label{sec:uroot}

To be completed

\subsection{The ADF test}
\label{sec:ADFtest}

The ADF (Augmented Dickey-Fuller) test is, as implemented in
\app{gretl}, the $t$-statistic on $\varphi$ in the following regression:
\begin{equation}
  \label{eq:ADFtest}
  \Delta y_t = \mu_t + \varphi y_{t-1} + \sum_{i=1}^p \gamma_i \Delta
  y_{t-i} + \epsilon_t .
\end{equation}

This test statistic is probably the best-known and most widely used
unit root test. It is a one-sided test whose null hypothesis is
$\varphi = 0$ versus the alternative $\varphi < 0$. Under the null,
$y_t$ must be differenced at least once to achieve stationarity;
under the alternative, $y_t$ is already stationary and no differencing
is required. Hence, large negative values of the test statistic lead
to the rejection of the null.

One peculiar aspect of this test is that its limit distribution is
non-standard under the null hypothesis: moreover, the shape of the
distribution, and consequently the critical values for the test,
depends on the form of the $\mu_t$ term.  A full analysis of the
various cases is inappropriate here: Hamilton (1994) contains an
excellent discussion, but any recent time series textbook covers
this topic. Suffice it to say that \app{gretl} allows the user to
choose the specification for $\mu_t$ among four different
alternatives:

\begin{center}
  \begin{tabular}{cc}
    \hline
    $\mu_t$ & command option \\
    \hline
    0 & \verb|--nc| \\
    $\mu_0$ &  \verb|--c| \\
    $\mu_0 + \mu_1 t$ &  \verb|--ct| \\
    $\mu_0 + \mu_1 t + \mu_1 t^2$ &  \verb|--ctt| \\
    \hline
  \end{tabular}
\end{center}

These options are not mutually exclusive and can be used together; in
this case, the statistic will be reported separately for each case.
By default, \app{gretl} uses by default the combination
\verb|--c --ct --ctt|. For each case, approximate p-values are
calculated by means of the algorithm developed in MacKinnon (1996).

The \app{gretl} command used to perform the test is \cmd{adf}; for example
\begin{code}
  adf 4 x1 --c --ct
\end{code}
would compute the test statistic as the t-statistic for $\varphi$ in
equation \ref{eq:ADFtest} with $p=4$ in the two cases $\mu_t = \mu_0$
and $\mu_t = \mu_0 + \mu_1 t$.

The number of lags ($p$ in equation \ref{eq:ADFtest}) should be chosen
as to ensure that (\ref{eq:ADFtest}) is a parametrization flexible
enough to represent adequately the short-run persistence of $\Delta
y_t$. Setting $p$ too low results in size distortions in the test,
whereas setting $p$ too high would lead to low power. As a convenience
to the user, the parameter $p$ can be automatically determined.
Setting $p$ to a negative number triggers a sequential procedure that
starts with $p$ lags and decrements $p$ until the $t$-statistic for
the parameter $\gamma_p$ exceeds 1.645 in absolute value.

\subsection{The KPSS test}
\label{sec:KPSStest}

The KPSS test (Kwiatkowski, Phillips, Schmidt and Shin, 1992) is a
unit root test in which the null hypothesis is opposite to that in the
ADF test: under the null, the series in question is stationary; the
alternative is that the series is $I(1)$.

The basic intuition behind this test statistic is very simple: if
$y_t$ can be written as $y_t = \mu + u_t$, where $u_t$ is some
zero-mean stationary process, then not only does the sample average of
the $y_t$'s provide a consistent estimator of $\mu$, but the long-run
variance of $u_t$ is a well-defined, finite number. Neither of these
properties hold under the alternative.

The test itself is based on the following statistic:
\begin{equation}
  \label{eq:KPSStest}
  \eta = \frac{\sum_{i=1}^T S_t^2 }{ T^2 \bar{\sigma}^2 }
\end{equation}
where $S_t = \sum_{s=1}^t e_s$ and $\bar{\sigma}^2$ is an estimate of
the long-run variance of $e_t = (y_t - \bar{y})$. Under the null, this
statistic has a well-defined (nonstandard) asymptotic distribution,
which is free of nuisance parameters and has been tabulated by
simulation. Under the alternative, the statistic diverges.

As a consequence, it is possible to construct a one-sided test based
on $\eta$, where $H_0$ is rejected if $\eta$ is bigger than the
appropriate critical value; \app{gretl} provides the 90\%, 95\%,
97.5\% and 99\% quantiles.

Usage example:
\begin{code}
  kpss m y
\end{code}
where \verb|m| is an integer representing the bandwidth or window
size used in the formula for estimating the long run variance:
\[
  \bar{\sigma}^2 = \sum_{i=-m}^m \left( 1 - \frac{|i|}{m+1} \right) \hat{\gamma}_i
\]
The $\hat{\gamma}_i$ terms denote the empirical autocovariances of
$e_t$ from order $-m$ through $m$.  For this estimator to be
consistent, $m$ must be large enough to accommodate the short-run
persistence of $e_t$, but not too large compared to the sample size
$T$.  In the GUI interface of \app{gretl}, this value defaults to the
integer part of $4 \left( \frac{T}{100} \right)^{1/4}$.

The above concept can be generalized to the case where $y_t$ is
thought to be stationary around a deterministic trend. In this case,
formula (\ref{eq:KPSStest}) remains unchanged, but the series $e_t$ is
defined as the residuals from an OLS regression of $y_t$ on a constant
and a linear trend. This second form of the test is obtained by
appending the \verb|--trend| option to the \cmd{kpss} command:
\begin{code}
  kpss n y --trend
\end{code}
Note that in this case the asymptotic distribution of the test is
different and the critical values reported by \app{gretl} differ
accordingly.


\subsection{The Johansen tests}
\label{sec:Joh-test}

Strictly speaking, these are tests for cointegration. However, they
can be used as multivariate unit-root tests since they are the
multivariate generalization of the ADF test.
\begin{equation}
  \label{eq:Joh-tests}
  \Delta y_t = \mu_t + \Pi y_{t-1} + \sum_{i=1}^p \Gamma_i \Delta
  y_{t-i} + \epsilon_t
\end{equation}
If the rank of $\Pi$ is 0, the processes are all I(1); If the rank of
$\Pi$ is full, the processes are all I(0); in between, $\Pi$ can be
written as $\alpha \beta'$ and you have cointegration.

The rank of $\Pi$ is investigated by computing the eigenvalues of a
closely related matrix (call it $M$) whose rank is the same as $\Pi$:
however, $M$ is by construction symmetric and positive semidefinite.
As a consequence, all its eigenvalues are real and non-negative; tests
on the rank of $\Pi$ can therefore be carried out by testing how many
eigenvalues of $M$ are 0.

If all the eigenvalues are significantly different from 0, then all the
processes are stationary. If, on the contrary, there is at least one
zero eigenvalue, then the $y_t$ process is integrated, although some
linear combination $\beta'y_t$ might be stationary. On the other
extreme, if no eigenvalues are significantly different from 0, then not
only the process $y_t$ is non-stationary, but the same holds for any
linear combination $\beta'y_t$; in other words, no cointegration
occurs.

The two Johansen tests are the ``$\lambda$-max'' test, for hypotheses
on individual eigenvalues, and the ``trace'' test, for joint
hypotheses.  The \app{gretl} command \cmd{coint2} performs these two
tests.  

As in the ADF test, the asymptotic distribution of the tests varies
with the deterministic kernel $\mu_t$ one includes in the VAR.
\app{gretl} provides the following options (for a short discussion of
the meaning of the five options, see section \ref{sec:johansen-test} below):
\begin{center}
  \begin{tabular}{cc}
    \hline
    $\mu_t$ & command option \\
    \hline
    0 & \verb|--nc| \\
    $\mu_0, \alpha_{\perp}'\mu_0 = 0 $ &  \verb|--rc| \\
    $\mu_0$ &  default \\
    $\mu_0 + \mu_1 t , \alpha_{\perp}'\mu_1 = 0$ &  \verb|--crt| \\
    $\mu_0 + \mu_1 t$ &  \verb|--ct| \\
    \hline
  \end{tabular}
\end{center}
Note that for this command the above options are mutually exclusive.
In addition, you have the option of using the \verb|--seasonal|
options, for augmenting $\mu_t$ with centered seasonal dummies.  In
each case, p-values are computed via the approximations by Doornik
(1998).

The following code uses the \cmd{denmark} database, supplied with
\app{gretl}, to replicate Johansen's example found in his 1995 book.
\begin{code}
  open denmark
  coint2 2 LRM LRY IBO IDE --rc --seasonal
\end{code}
In this case, the vector $y_t$ in equation (\ref{eq:Joh-tests})
comprises the four variables \cmd{LRM}, \cmd{LRY}, \cmd{IBO},
\cmd{IDE}. The number of lags equals $p$ in (\ref{eq:Joh-tests}) plus
one. Part of the output is reported below:
\begin{center}
\begin{code}
Johansen test:
Number of equations = 4
Lag order = 2
Estimation period: 1974:3 - 1987:3 (T = 53)

Case 2: Restricted constant
Rank Eigenvalue Trace test p-value   Lmax test  p-value
   0    0.43317     49.144 [0.1284]     30.087 [0.0286]
   1    0.17758     19.057 [0.7833]     10.362 [0.8017]
   2    0.11279     8.6950 [0.7645]     6.3427 [0.7483]
   3   0.043411     2.3522 [0.7088]     2.3522 [0.7076]

eigenvalue     0.43317      0.17758      0.11279     0.043411 
\end{code}
\end{center}
Since both the trace and $\lambda$-max accept the null hypothesis that
the smallest eigenvalue is in fact 0, we may conclude that the series
are in fact non-stationary. However, some linear combination may be
$I(0)$, as indicated by the rejection of the $\lambda$-max of the
hypothesis that the rank of $\Pi$ is 0 (the trace test gives less
clear-cut evidence for this).

\section{ARCH and GARCH}
\label{sec:arch-garch}

Heteroskedasticity means a non-constant variance of the error term in
a regression model.  Autoregressive Conditional Heteroskedasticity
(ARCH) is a phenomenon specific to time series models, whereby the
variance of the error displays autoregressive behavior; for instance,
the time series exhibits successive periods where the error variance
is relatively large, and successive periods where it is relatively
small.  This sort of behavior is reckoned to be quite common in asset
markets: an unsettling piece of news can lead to a period of increased
volatility in the market.

An ARCH error process of order $q$ can be represented as
\[
u_t = \sigma_t \varepsilon_t; \qquad
\sigma^2_t \equiv {\rm E}(u^2_t|\Omega_{t-1}) = 
\alpha_0 + \sum_{i=1}^q \alpha_i u^2_{t-i}
\]
where the $\varepsilon_t$s are independently and identically
distributed (iid) with mean zero and variance 1, and where $\sigma_t$
is taken to be the positive square root of $\sigma^2_t$.
$\Omega_{t-1}$ denotes the information set as of time $t-1$ and
$\sigma^2_t$ is the conditional variance: that is, the
variance conditional on information dated $t-1$ and earlier.

It is important to notice the difference between ARCH and an ordinary
autoregressive error process.  The simplest (first-order) case of the
latter can be written as
\[
u_t = \rho u_{t-1} + \varepsilon_t; \qquad -1 < \rho < 1
\]
where the $\varepsilon_t$s are iid with mean zero and constant
variance $\sigma^2$.  With an AR(1) error, if $\rho$ is positive then
a positive value of $u_t$ will tend to be followed, with probability
greater than 0.5, by a positive $u_{t+1}$.  With an ARCH error
process, a disturbance $u_t$ of large absolute value will tend to be
followed by further large absolute values, but with no presumption
that the successive values will be of the same sign.  ARCH in asset
prices is a ``stylized fact'' and is consistent with market
efficiency; on the other hand autoregressive behavior of asset prices
would violate market efficiency.

One can test for ARCH of order $q$ in the following
way:
\begin{enumerate}
\item Estimate the model of interest via OLS and save the squared
  residuals, $\hat{u}^2_t$.
\item Perform an auxiliary regression in which the current squared
  residual is regressed on a constant and $q$ lags of itself.
\item Find the $TR^2$ value (sample size times unadjusted $R^2$) for
  the auxiliary regression.
\item Refer the $TR^2$ value to the $\chi^2$ distribution with $q$
  degrees of freedom, and if the p-value is ``small enough'' reject
  the null hypothesis of homoskedasticity in favor of the alternative
  of ARCH($q$).
\end{enumerate}

This test is implemented in \app{gretl} via the \cmd{arch} command.
This command may be issued following the estimation of a time-series
model by OLS, or by selection from the ``Tests'' menu in the model
window (again, following OLS estimation).  The result of the test is
reported and if the $TR^2$ from the auxiliary regression has a p-value
less than 0.10, ARCH estimates are also reported.  These estimates
take the form of Generalized Least Squares (GLS), specifically
weighted least squares, using weights that are inversely proportional
to the predicted standard deviations of the disturbances,
$\hat{\sigma}_t$, derived from the auxiliary regression.

In addition, the ARCH test is available after estimating a vector
autoregression (VAR).  In this case, however, there is no provision to
re-estimate the model via GLS.

\subsection{GARCH}
\label{subsec:garch}

The simple ARCH($q$) process is useful for introducing the general
concept of conditional heteroskedasticity in time series, but it has
been found to be insufficient in empirical work.  The dynamics of the
error variance permitted by ARCH($q$) are not rich enough to represent 
the patterns found in financial data.  The generalized ARCH or GARCH
model is now more widely used.  

The representation of the variance of a process in the GARCH model is
somewhat (but not exactly) analogous to the ARMA representation of the
level of a time series.  The variance at time $t$ is allowed
to depend on both past values of the variance and past values of the
realized squared disturbance, as shown in the following system
of equations:
\begin{eqnarray}
  \label{eq:garch-meaneq}
  y_t &  = & X_t \beta + u_t \\
  \label{eq:garch-epseq}
  u_t &  = & \sigma_t \varepsilon_t \\
  \label{eq:garch-vareq}
  \sigma^2_t & = & \alpha_0 + \sum_{i=1}^q \alpha_i u^2_{t-i} +
	  \sum_{j=1}^p \delta_i \sigma^2_{t-j}
\end{eqnarray}
As above, $\varepsilon_t$ is an iid sequence with unit variance.
$X_t$ is a matrix of regressors (or in the simplest case,
just a vector of 1s allowing for a non-zero mean of $y_t$).  Note that
if $p=0$, GARCH collapses to ARCH($q$): the generalization is embodied
in the $\delta_i$ terms that multiply previous values of the error
variance.

In principle the underlying innovation, $\varepsilon_t$, could follow
any suitable probability distribution, and besides the obvious
candidate of the normal or Gaussian distribution the $t$ distribution
has been used in this context.  Currently \app{gretl} only handles the
case where $\varepsilon_t$ is assumed to be Gaussian.  However, when
the \verb|--robust| option to the \cmd{garch} command is given, the
estimator \app{gretl} uses for the covariance matrix can be considered
Quasi-Maximum Likelihood even with non-normal disturbances.  See below
for more on the options regarding the GARCH covariance matrix.

Example:
\begin{code}
  garch p q ; y const x
\end{code}
where \verb|p| $\ge 0$ and \verb|q| $>0$ denote the respective lag
orders as shown in equation (\ref{eq:garch-vareq}).  These values
can be supplied in numerical form or as the names of pre-defined
scalar variables.

\subsection{GARCH estimation}
\label{subsec:garch-est}

Estimation of the parameters of a GARCH model is by no means a
straightforward task.  (Consider equation~\ref{eq:garch-vareq}: the
conditional variance at any point in time, $\sigma^2_t$, depends on
the conditional variance in earlier periods, but $\sigma^2_t$ is not
observed, and must be inferred by some sort of Maximum Likelihood
procedure.)  \app{Gretl} uses the method proposed by Fiorentini,
Calzolari and Panattoni (1996),\footnote{The algorithm is based on
  Fortran code deposited in the archive of the \textit{Journal of
    Applied Econometrics} by the authors, and is used by kind
  permission of Professor Fiorentini.}  which was adopted as a
benchmark in the study of GARCH results by McCullough and Renfro
(1998).  It employs analytical first and second derivatives of the
log-likelihood, and uses a mixed-gradient algorithm, exploiting the
information matrix in the early iterations and then switching to the
Hessian in the neighborhood of the maximum likelihood.  (This progress
can be observed if you append the \verb|--verbose| option to
\app{gretl}'s \cmd{garch} command.)

Several options are available for computing the covariance matrix of
the parameter estimates in connection with the \cmd{garch} command.
At a first level, one can choose between a ``standard'' and a
``robust'' estimator.  By default, the Hessian is used unless the
\verb|--robust| option is given, in which case the QML estimator is
used.  A finer choice is available via the \cmd{set} command, as
shown in Table~\ref{tab:garch-vcv}.

\begin{table}[htbp]
\caption{Options for the GARCH covariance matrix}
\label{tab:garch-vcv}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\textit{command}} &
\multicolumn{1}{c}{\textit{effect}} \\ [4pt]
\texttt{set garch\_vcv hessian} & Use the Hessian \\
\texttt{set garch\_vcv im} & Use the Information Matrix \\
\texttt{set garch\_vcv op} & Use the Outer Product of the Gradient \\
\texttt{set garch\_vcv qml} & QML estimator \\
\texttt{set garch\_vcv bw} & Bollerslev--Wooldridge ``sandwich'' estimator
\end{tabular}
\end{center}
\end{table}

It is not uncommon, when one estimates a GARCH model for an arbitrary
time series, to find that the iterative calculation of the estimates
fails to converge.  For the GARCH model to make sense, there are
strong restrictions on the admissible parameter values, and it is not
always the case that there exists a set of values inside the
admissible parameter space for which the likelihood is maximized.  

The restrictions in question can be explained by reference to the
simplest (and much the most common) instance of the GARCH model, where
$p = q = 1$.  In the GARCH(1, 1) model the conditional variance is
\begin{equation}
\label{eq:condvar}
\sigma^2_t = \alpha_0 + \alpha_1 u^2_{t-1} + \delta_1 \sigma^2_{t-1}
\end{equation}
Taking the unconditional expectation of (\ref{eq:condvar}) we get
\[
\sigma^2 = \alpha_0 + \alpha_1 \sigma^2 + \delta_1 \sigma^2
\]
so that
\[
\sigma^2 = \frac{\alpha_0}{1 - \alpha_1 - \delta_1}
\]
For this unconditional variance to exist, we require that $\alpha_1 +
\delta_1 < 1$, and for it to be positive we require that $\alpha_0 > 0$.

A common reason for non-convergence of GARCH estimates (that is, a
common reason for the non-existence of $\alpha_i$ and $\delta_i$ values
that satisfy the above requirements and at the same time maximize the
likelihood of the data) is misspecification of the model.  It is
important to realize that GARCH, in itself, allows \textit{only} for
time-varying volatility in the data.  If the \textit{mean} of the
series in question is not constant, or if the error process is not
only heteroskedastic but also autoregressive, it is necessary to take
this into account when formulating an appropriate model.  For example,
it may be necessary to take the first difference of the variable in
question and/or to add suitable regressors, $X_t$, as in
(\ref{eq:garch-meaneq}).

\section{Cointegration and Vector Error Correction Models}
\label{vecm-explanation}

\subsection{The Johansen cointegration test}
\label{sec:johansen-test}

The Johansen test for cointegration has to take into account what
hypotheses one is willing to make on the deterministic terms, which
leads to the famous ``five cases.'' A full and general illustration of
the five cases requires a fair amount of matrix algebra, but an
intuitive understanding of the issue can be gained by means of a
simple example.
    
Consider a series $x_t$ which behaves as follows
%      
\[ x_t = m + x_{t-1} + \varepsilon_t \] 
%
where $m$ is a real number and $\varepsilon_t$ is a white noise
process. As is easy to show, $x_t$ is a random walk which fluctuates
around a deterministic trend with slope $m$. In the special case $m$ =
0, the deterministic trend disappears and $x_t$ is a pure random walk.
    
Consider now another process $y_t$, defined by
%      
\[ y_t = k + x_t + u_t \] 
%
where, again, $k$ is a real number and $u_t$ is a white noise process.
Since $u_t$ is stationary by definition, $x_t$ and $y_t$ cointegrate:
that is, their difference
%      
\[ z_t = y_t - x_t = k + u_t \]
%	
is a stationary process. For $k$ = 0, $z_t$ is simple zero-mean white
noise, whereas for $k$ $\ne$ 0 the process $z_t$ is white noise with a
non-zero mean.
  
After some simple substitutions, the two equations above can be
represented jointly as a VAR(1) system
%      
\[ \left[ \begin{array}{c} y_t \\ x_t \end{array} \right] = \left[
  \begin{array}{c} k + m \\ m \end{array} \right] + \left[
  \begin{array}{rr} 0 & 1 \\ 0 & 1 \end{array} \right] \left[
  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + \left[
  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array}
\right] \]
%	
or in VECM form
%      
\begin{eqnarray*}
  \left[  \begin{array}{c} \Delta y_t \\ \Delta x_t \end{array} \right]  & = & 
  \left[  \begin{array}{c} k + m \\ m \end{array} \right] +
  \left[  \begin{array}{rr} -1 & 1 \\ 0 & 0 \end{array} \right] 
  \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + 
  \left[  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array} \right] = \\
  & = & 
  \left[  \begin{array}{c} k + m \\ m \end{array} \right] +
  \left[  \begin{array}{r} -1 \\ 0 \end{array} \right]
  \left[  \begin{array}{rr} 1 & -1 \end{array} \right] 
  \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + 
  \left[  \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t \end{array} \right] = \\
  & = & 
  \mu_0 + \alpha \beta^{\prime} \left[  \begin{array}{c} y_{t-1} \\ x_{t-1} \end{array} \right] + \eta_t = 
  \mu_0 + \alpha z_{t-1} + \eta_t ,
\end{eqnarray*}
%	
where $\beta$ is the cointegration vector and $\alpha$ is the
``loadings'' or ``adjustments'' vector.
     
We are now in a position to consider three possible cases:
    
\begin{enumerate}
\item $m$ $\ne$ 0: In this case $x_t$ is trended, as we just saw; it
  follows that $y_t$ also follows a linear trend because on average it
  keeps at a distance $k$ from $x_t$. The vector
  $\mu_0$ is unrestricted.  This case is the default
  for gretl's \cmd{vecm} command.
	
\item $m$ = 0 and $k$ $\ne$ 0: In this case, $x_t$ is not trended and
  as a consequence neither is $y_t$. However, the mean distance
  between $y_t$ and $x_t$ is non-zero. The vector
  $\mu_0$ is given by
%	  
  \[
  \mu_0 = \left[ \begin{array}{c} k \\ 0 \end{array} \right]
  \]
%	    
  which is not null and therefore the VECM shown above does have a
  constant term. The constant, however, is subject to the restriction
  that its second element must be 0. More generally,
  $\mu_0$ is a multiple of the vector $\alpha$. Note
  that the VECM could also be written as
%	  
  \[
  \left[ \begin{array}{c} \Delta y_t \\ \Delta x_t \end{array} \right]
  = \left[ \begin{array}{r} -1 \\ 0 \end{array} \right] \left[
    \begin{array}{rrr} 1 & -1 & -k \end{array} \right] \left[
    \begin{array}{c} y_{t-1} \\ x_{t-1} \\ 1 \end{array} \right] +
  \left[ \begin{array}{c} u_t + \varepsilon_t \\ \varepsilon_t
    \end{array} \right]
  \]
%	   
  which incorporates the intercept into the cointegration vector. This
  is known as the ``restricted constant'' case; it may be specified in
  gretl's \cmd{vecm} command using the option flag \verb+--rc+.
	
\item $m$ = 0 and $k$ = 0: This case is the most restrictive: clearly,
  neither $x_t$ nor $y_t$ are trended, and the mean distance between
  them is zero. The vector $\mu_0$ is also 0, which
  explains why this case is referred to as ``no constant.''  This case
  is specified using the option flag \verb+--nc+ with \cmd{vecm}.
	
\end{enumerate}

In most cases, the choice between the three possibilities is based on
a mix of empirical observation and economic reasoning. If the
variables under consideration seem to follow a linear trend then we
should not place any restriction on the intercept. Otherwise, the
question arises of whether it makes sense to specify a cointegration
relationship which includes a non-zero intercept. One example where
this is appropriate is the relationship between two interest rates:
generally these are not trended, but the VAR might still have an
intercept because the difference between the two (the ``interest rate
spread'') might be stationary around a non-zero mean (for example,
because of a risk or liquidity premium).
    
The previous example can be generalized in three directions:
    
\begin{enumerate}
\item If a VAR of order greater than 1 is considered, the algebra gets
  more convoluted but the conclusions are identical.
\item If the VAR includes more than two endogenous variables the
  cointegration rank $r$ can be greater than 1. In this case, $\alpha$
  is a matrix with $r$ columns, and the case with restricted constant
  entails the restriction that $\mu_0$ should be some
  linear combination of the columns of $\alpha$.
\item If a linear trend is included in the model, the deterministic
  part of the VAR becomes $\mu_0 + \mu_1 t$. The reasoning is
  practically the same as above except that the focus now centers on
  $\mu_1$ rather than $\mu_0$.  The counterpart to the ``restricted
  constant'' case discussed above is a ``restricted trend'' case, such
  that the cointegration relationships include a trend but the first
  differences of the variables in question do not.  In the case of an
  unrestricted trend, the trend appears in both the cointegration
  relationships and the first differences, which corresponds to the
  presence of a quadratic trend in the variables themselves (in
  levels).  These two cases are specified by the option flags
  \verb+--crt+ and \verb+--ct+, respectively, with the \cmd{vecm}
  command.
\end{enumerate}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "gretl-guide"
%%% End: 

