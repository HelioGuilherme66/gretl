\chapter{Sub-sampling a dataset}
\label{sampling}

\section{Introduction}
\label{sample-intro}

Some subtle issues can arise here; this chapter attempts to explain
the issues.

A sub-sample may be defined in relation to a full dataset in two
different ways: we will refer to these as ``setting'' the sample and
``restricting'' the sample; these methods are discussed in
sections~\ref{sec:sample-set} and~\ref{sec:sample-restrict}
respectively. In addition section~\ref{sec:resampling} discusses
resampling with replacement, which is useful in the context of
bootstrapping test statistics.

The following discussion focuses on the command-line approach. But you
can also invoke the methods outlined here via the items under the
\textsf{Sample} menu in the GUI program.


\section{Setting the sample}
\label{sec:sample-set}

By ``setting'' the sample we mean defining a sub-sample simply by
means of adjusting the starting and/or ending point of the current
sample range.  This is likely to be most relevant for time-series
data.  For example, one has quarterly data from 1960:1 to 2003:4, and
one wants to run a regression using only data from the 1970s.  A
suitable command is then

\begin{code}
smpl 1970:1 1979:4
\end{code}

Or one wishes to set aside a block of observations at the end of the
data period for out-of-sample forecasting.  In that case one might do

\begin{code}
smpl ; 2000:4
\end{code}

where the semicolon is shorthand for ``leave the starting observation
unchanged''.  (The semicolon may also be used in place of the second
parameter, to mean that the ending observation should be unchanged.)
By ``unchanged'' here, we mean unchanged relative to the last
\verb+smpl+ setting, or relative to the full dataset if no sub-sample
has been defined up to this point. For example, after

\begin{code}
smpl 1970:1 2003:4
smpl ; 2000:4
\end{code}

the sample range will be 1970:1 to 2000:4.  

An incremental or relative form of setting the sample range is also
supported.  In this case a relative offset should be given, in the
form of a signed integer (or a semicolon to indicate no change), for
both the starting and ending point. For example

\begin{code}
smpl +1 ;
\end{code}

will advance the starting observation by one while preserving the
ending observation, and

\begin{code}
smpl +2 -1
\end{code}

will both advance the starting observation by two and retard the
ending observation by one.

An important feature of ``setting'' the sample as described above is
that it necessarily results in the selection of a subset of
observations that are contiguous in the full dataset. The structure of
the dataset is therefore unaffected (for example, if it is a quarterly
time series before setting the sample, it remains a quarterly time
series afterwards).

\section{Restricting the sample}
\label{sec:sample-restrict}

By ``restricting'' the sample we mean selecting observations on the
basis of some Boolean (logical) criterion, or by means of a random
number generator.  This is likely to be most relevant for
cross-sectional or panel data.

Suppose we have data on a cross-section of individuals, recording
their gender, income and other characteristics.  We wish to select for
analysis only the women.  If we have a \verb+gender+ dummy variable
with value 1 for men and 0 for women we could do
%      
\begin{code}
smpl gender==0 --restrict
\end{code}
%
to this effect.  Or suppose we want to restrict the sample to
respondents with incomes over \$50,000.  Then we could use
%
\begin{code}
smpl income>50000 --restrict
\end{code}

A question arises here.  If we issue the two commands above in
sequence, what do we end up with in our sub-sample: all cases with
income over 50000, or just women with income over 50000? By default,
the answer is the latter: women with income over 50000.  The second
restriction augments the first, or in other words the final
restriction is the logical product of the new restriction and any
restriction that is already in place.  If you want a new restriction
to replace any existing restrictions you can first recreate the full
dataset using
%
\begin{code}
smpl --full
\end{code}
%
Alternatively, you can add the \verb+replace+ option to the
\verb+smpl+ command:
%
\begin{code}
smpl income>50000 --restrict --replace
\end{code}

This option has the effect of automatically re-establishing the full
dataset before applying the new restriction.

Unlike a simple ``setting'' of the sample, ``restricting'' the sample
may result in selection of non-contiguous observations from the full
data set.  It may therefore change the structure of the data set.

This can be seen in the case of panel data.  Say we have a panel of
five firms (indexed by the variable \verb+firm+) observed in each of
several years (identified by the variable \verb+year+).  Then the
restriction
%
\begin{code}
smpl year=1995 --restrict
\end{code}
%
produces a dataset that is not a panel, but a cross-section for the
year 1995.  Similarly
%
\begin{code}
smpl firm=3 --restrict
\end{code}
%
produces a time-series dataset for firm number 3.

For these reasons (possible non-contiguity in the observations,
possible change in the structure of the data), gretl acts differently
when you ``restrict'' the sample as opposed to simply ``setting'' it.
In the case of setting, the program merely records the starting and
ending observations and uses these as parameters to the various
commands calling for the estimation of models, the computation of
statistics, and so on. In the case of restriction, the program makes a
reduced copy of the dataset and by default treats this reduced copy as
a simple, undated cross-section.\footnote{With one exception: if you
  start with a balanced panel dataset and the restriction is such that
  it preserves a balanced panel --- for example, it results in the
  deletion of all the observations for one cross-sectional unit ---
  then the reduced dataset is still, by default, treated as a panel.}

If you wish to re-impose a time-series or panel interpretation of the
reduced dataset you can do so using the \cmd{setobs} command, or the
GUI menu item ``Data, Dataset structure''.

The fact that ``restricting'' the sample results in the creation of a
reduced copy of the original dataset may raise an issue when the
dataset is very large.  With such a dataset in memory, the creation of
a copy may lead to a situation where the computer runs low on memory
for calculating regression results.  You can work around this as
follows:

\begin{enumerate}
\item Open the full data set, and impose the sample restriction.
\item Save a copy of the reduced data set to disk.
\item Close the full dataset and open the reduced one.
\item Proceed with your analysis.
\end{enumerate}

\subsection{Random sub-sampling}
\label{sample-random}

Besides restricting the sample on some deterministic criterion, it may
sometimes be useful (when working with very large datasets, or perhaps
to study the properties of an estimator) to draw a random sub-sample
from the full dataset.  This can be done using, for example,
%
\begin{code}
smpl 100 --random
\end{code}
%
to select 100 cases.  If you want the sample to be reproducible, you
should set the seed for the random number generator first, using the
\cmd{set} command.  This sort of sampling falls under the
``restriction'' category: a reduced copy of the dataset is made.

\section{Resampling and bootstrapping}
\label{sec:resampling}

Given an original data series \varname{x}, the command
%
\begin{code}
series xr = resample(x)
\end{code}
%
creates a new series each of whose elements is drawn at random from
the elements of \varname{x}.  If the original series has 100
observations, each element of \varname{x} is selected with probability
$1/100$ at each drawing.  Thus the effect is to ``shuffle'' the
elements of \varname{x}, with the twist that each element of
\varname{x} may appear more than once, or not at all, in \varname{xr}.

The primary use of this function is in the construction of bootstrap
confidence intervals or p-values.  Here is a simple example.  Suppose
we estimate a simple regression of $y$ on $x$ via OLS and find that
the slope coefficient has a reported $t$-ratio of 2.5 with 40 degrees
of freedom.  The two-tailed p-value for the null hypothesis that the
slope parameter equals zero is then 0.0166, using the $t(40)$
distribution.  Depending on the context, however, we may doubt whether
the ratio of coefficient to standard error truly follows the $t(40)$
distribution.  In that case we could derive a bootstrap p-value as
shown in Example~\ref{resampling-loop}.  

Under the null hypothesis that the slope with respect to $x$ is zero,
$y$ is simply equal to its mean plus an error term.  We simulate $y$
by resampling the residuals from the initial OLS and re-estimate the
model.  We repeat this procedure a large number of times, and record
the number of cases where the absolute value of the $t$-ratio is
greater than 2.5: the proportion of such cases is our bootstrap
p-value.  For a good discussion of simulation-based tests and
bootstrapping, see Davidson and MacKinnon
(\citeyear{davidson-mackinnon04}, chapter 4).

\begin{script}[htbp]
  \caption{Calculation of bootstrap p-value}
  \label{resampling-loop}
\begin{scode}
ols y 0 x
# save the residuals
genr ui = $uhat
scalar ybar = mean(y)
# number of replications for bootstrap
scalar replics = 10000
scalar tcount = 0
series ysim
loop replics --quiet
  # generate simulated y by resampling
  ysim = ybar + resample(ui)
  ols ysim 0 x
  scalar tsim = abs($coeff(x) / $stderr(x))
  tcount += (tsim > 2.5)
endloop      
printf "proportion of cases with |t| > 2.5 = %g\n", tcount / replics
\end{scode}
%$
\end{script}

    
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "gretl-guide"
%%% End: 

