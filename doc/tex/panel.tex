\chapter{Panel data}
\label{chap-panel}

\section{Panel data structure}
\label{panel-structure}

Panel data are inherently three dimensional --- the dimensions being
variable, cross-sectional unit, and time-period.  For representation
in a textual computer file (and also for gretl's internal
calculations) these three dimensions must somehow be flattened into
two.  This ``flattening'' involves taking layers of the data that
would naturally stack in a third dimension, and stacking them in the
vertical dimension.

\app{Gretl} always expects data to be arranged ``by observation'',
that is, such that each row represents an observation (and each
variable occupies one and only one column).  In this context the
flattening of a panel data set can be done in either of two ways:

\begin{itemize}
\item Stacked time series: the successive vertical blocks each
  comprise a time series for a given cross-sectional unit.
\item Stacked cross sections: the successive vertical blocks each
  comprise a cross-section for a given period.
\end{itemize}

You may input data in whichever arrangement is more convenient.
Internally, however, \app{gretl} always stores panel data in
the form of stacked time series.

When you import panel data into \app{gretl} from a spreadsheet or
comma separated format, the panel nature of the data will not be
recognized automatically (most likely the data will be treated as
``undated'').  A panel interpretation can be imposed on the data
using the graphical interface or via the \cmd{setobs} command.

In the graphical interface, use the menu item ``Sample, Dataset
structure''.  In the first dialog box that appears, select ``Panel''.
In the next dialog you have a three-way choice.  The first two
options, ``Stacked time series'' and ``Stacked cross sections'' are
applicable if the data set is already organized in one of these two
ways.  If you select either of these options, the next step is to
specify the number of cross-sectional units in the data set.  The
third option, ``Use index variables'', is applicable if the data set
contains two variables that index the cross-sectional units and the
time periods respectively; the next step is then to select those
variables.  For example, a data file might contain a country code
variable and a variable representing the year of the observation.  In
that case \app{gretl} can reconstruct the panel structure of the data
regardless of how the observation rows are organized.

The \cmd{setobs} command has options that parallel those in the
graphical interface.  If suitable index variables are available
you can do, for example
\begin{code}
            setobs unitvar timevar --panel-vars
\end{code}
where \texttt{unitvar} is a variable that indexes the units and
\texttt{timevar} is a variable indexing the periods.  Alternatively
you can use the form \verb+setobs+ \textsl{freq} \verb+1:1+
\textsl{structure}, where \textsl{freq} is replaced by the ``block
size'' of the data (that is, the number of periods in the case of
stacked time series, or the number of cross-sectional units in the
case of stacked cross-sections) and structure is either
\verb+--stacked-time-series+ or \verb+--stacked-cross-section+.  Two
examples are given below: the first is suitable for a panel in the
form of stacked time series with observations from 20 periods; the
second for stacked cross sections with 5 cross-sectional units.
\begin{code}
            setobs 20 1:1 --stacked-time-series
            setobs 5 1:1 --stacked-cross-section
\end{code}

\subsection{Panel data arranged by variable}

Publicly available panel data sometimes come arranged ``by variable.''
Suppose we have data on two variables, \varname{x1} and \varname{x2},
for each of 50 states in each of 5 years (giving a total of 250
observations per variable).  One textual representation of such a data
set would start with a block for \varname{x1}, with 50 rows
corresponding to the states and 5 columns corresponding to the years.
This would be followed, vertically, by a block with the same structure
for variable \varname{x2}.  A fragment of such a data file is shown
below, with quinquennial observations 1965--1985.  Imagine the table
continued for 48 more states, followed by another 50 rows for variable
\varname{x2}.

\begin{center}
  \begin{tabular}{rrrrrr}
  \varname{x1} \\
     & 1965 & 1970 & 1975 & 1980 & 1985 \\
  AR & 100.0 & 110.5 & 118.7 & 131.2 & 160.4\\
  AZ & 100.0 & 104.3 & 113.8 & 120.9 & 140.6\\
  \end{tabular}
\end{center}

If a datafile with this sort of structure is read into \app{gretl},
the program will interpret the columns as distinct variables, so the
data will not be usable ``as is.''  But there is a mechanism for
correcting the situation, namely the \cmd{stack} function within
the \cmd{genr} command.

Consider the first data column in the fragment above: the first 50 rows
of this column constitute a cross-section for the variable \varname{x1}
in the year 1965.  If we could create a new variable by stacking the
first 50 entries in the second column underneath the first 50 entries
in the first, we would be on the way to making a data set ``by
observation'' (in the first of the two forms mentioned above, stacked
cross-sections).  That is, we'd have a column comprising a
cross-section for \varname{x1} in 1965, followed by a cross-section for
the same variable in 1970.

The following gretl script illustrates how we can accomplish the
stacking, for both \varname{x1} and \varname{x2}.  We assume
that the original data file is called \texttt{panel.txt}, and that in
this file the columns are headed with ``variable names'' \varname{p1},
\varname{p2}, \dots, \varname{p5}.  (The columns are not really
variables, but in the first instance we ``pretend'' that they are.)

\begin{code}
    open panel.txt
    genr x1 = stack(p1..p5) --length=50
    genr x2 = stack(p1..p5) --offset=50 --length=50
    setobs 50 1:1 --stacked-cross-section
    store panel.gdt x1 x2
\end{code}

The second line illustrates the syntax of the \cmd{stack} function.
The double dots within the parentheses indicate a range of variables
to be stacked: here we want to stack all 5 columns (for all 5 years).
The full data set contains 100 rows; in the stacking of variable
\varname{x1} we wish to read only the first 50 rows from each column:
we achieve this by adding \verb+--length=50+.  Note that if you want
to stack a non-contiguous set of columns you can put a comma-separated
list within the parentheses, as in

\begin{code}
    genr x = stack(p1,p3,p5)
\end{code}

On line 3 we do the stacking for variable \varname{x2}.  Again we want
a \texttt{length} of 50 for the components of the stacked series, but
this time we want gretl to start reading from the 50th row of the
original data, and we specify \verb+--offset=50+.

Line 4 imposes a panel interpretation on the data, as explained in
section~\ref{panel-structure}.  Finally, we save the data in gretl
format, with the panel interpretation, discarding the original
``variables'' \varname{p1} through \varname{p5}.

The illustrative script above is appropriate when the number of
variable to be processed is small.  When then are many variables in
the data set it will be more efficient to use a command loop to
accomplish the stacking, as shown in the following script.  The setup
is presumed to be the same as in the previous section (50 units, 5
periods), but with 20 variables rather than 2.

\begin{code}
    open panel.txt
    loop for i=1..20
      genr k = ($i - 1) * 50
      genr x$i = stack(p1..p5) --offset=k --length=50
    endloop
    setobs 50 1.01 --stacked-cross-section
    store panel.gdt x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 \
      x11 x12 x13 x14 x15 x16 x17 x18 x19 x20
\end{code}

\section{Generated variables}
\label{panel-genr}

\subsection{Dummy variables}
\label{dummies}

In a panel study you may wish to construct dummy variables of one or
both of the following sorts: (a) dummies as unique identifiers for the
cross-sectional units, and (b) dummies as unique identifiers for the
time periods.  The former may be used to allow the intercept of the
regression to differ across the units, the latter to allow the
intercept to differ across periods.

Two special functions are available to create such dummies.  These are
found under the ``Add'' menu in the GUI, or under the \cmd{genr}
command in script mode or \app{gretlcli}.

\begin{enumerate}
\item ``unit dummies'' (script command \cmd{genr unitdum}).  This
  command creates a set of dummy variables identifying the
  cross-sectional units.  The variable \verb+du_1+ will have value 1
  in each row corresponding to a unit 1 observation, 0 otherwise;
  \verb+du_2+ will have value 1 in each row corresponding to a unit 2
  observation, 0 otherwise; and so on.
\item ``time dummies'' (script command \cmd{genr timedum}).  This
  command creates a set of dummy variables identifying the periods.
  The variable \verb+dt_1+ will have value 1 in each row
  corresponding to a period 1 observation, 0 otherwise; \verb+dt_2+
  will have value 1 in each row corresponding to a period 2
  observation, 0 otherwise; and so on.
\end{enumerate}

If a panel data set has the \verb+YEAR+ of the observation entered as
one of the variables you can create a periodic dummy to pick out a
particular year, e.g.\ \cmd{genr dum = (YEAR=1960)}.  You can also
create periodic dummy variables using the modulus operator,
\verb+%+.  For instance, to create a dummy with
value 1 for the first observation and every thirtieth observation
thereafter, 0 otherwise, do
\begin{code}
      genr index 
      genr dum = ((index-1)%30) = 0
\end{code}

\subsection{Lags and differences}
\label{panel-lagged}

If the time periods are evenly spaced you may want to use lagged
values of variables in a panel regression; you may also with to
construct first differences of variables of interest.

Once a dataset is properly identified as a panel, \app{gretl} will
handle the generation of such variables correctly.  For example the
command \verb+genr x1_1 = x1(-1)+ will create a variable that contains
the first lag of \verb+x1+ where available, and the missing value code
where the lag is not available.  When you run a regression using such
variables, the program will automatically skip the missing
observations.

\section{Estimation of panel models}

\subsection{Pooled Ordinary Least Squares}
\label{pooled-est}

The simplest estimator for panel data is pooled OLS.  In most cases
this is unlikely to be adequate, but it provides a baseline for
comparison with more complex estimators.

If you estimate a model on panel data using OLS an additional test
item becomes available.  In the GUI model window this is the item
``panel diagnostics'' under the \textsf{Tests} menu; the script
counterpart is the \cmd{hausman} command.

To take advantage of this test, you should specify a model without any
dummy variables representing cross-sectional units.  The test compares
pooled OLS against the principal alternatives, the fixed effects and
random effects models.  These alternatives are explained in the
following section.

\subsection{The fixed and random effects models}
\label{panel-est}

In \app{gretl} version 1.5.2 and higher, the fixed and random effects
models for panel data can be estimated in their own right.  In the
graphical interface these options are found under the menu item
``Model/Panel/Fixed and random effects''.  In the command-line
interface one uses the \cmd{panel} command, with or without the
\verb+--random-effects+ option.

This section explains the nature of these models and comments on their
estimation via \app{gretl}.

The pooled OLS specification may be written as 
\begin{equation}
\label{eq:pooled}
y_{it} = X_{it}\beta + u_{it}
\end{equation}
where $y_{it}$ is the observation on the dependent variable for
cross-sectional unit $i$ in period $t$, $X_{it}$ is a $1\times k$
vector of independent variables observed for unit $i$ in period $t$,
$\beta$ is a $k\times 1$ vector of parameters, and $u_{it}$ is an error
or disturbance term specific to unit $i$ in period $t$.

The fixed and random effects models have in common that they decompose
the unitary pooled error term, $u_{it}$.  For the \textsl{fixed effects}
model we write $u_{it} = \alpha_i + \varepsilon_{it}$, yielding
\begin{equation}
\label{eq:FE}
y_{it} = X_{it}\beta + \alpha_i + \varepsilon_{it}
\end{equation}
That is, we decompose $u_{it}$ into a unit-specific and time-invariant
component, $\alpha_i$, and an observation-specific error,
$\varepsilon_{it}$.\footnote{It is possible to break a third component
  out of $u_{it}$, namely $w_t$, a shock that is time-specific but
  common to all the units in a given period.  In the interest of
  simplicity we do not pursue that option here.}  The $\alpha_i$s are
then treated as fixed parameters (in effect, unit-specific
$y$-intercepts), which are to be estimated.  This can be done by
including a dummy variable for each cross-sectional unit (and
suppressing the global constant).  Alternatively, one can proceed by
subtracting the group or unit mean from each of variables and
estimating a model without a constant.  In the latter case the
dependent variable may be written as
\[
\tilde{y}_{it} = y_{it} - \bar{y}_i
\]
The ``group mean'', $\bar{y}_i$, is defined as
\[
\bar{y}_i = \frac{1}{T_i} \sum_{t=1}^{T_i} y_{it}
\]
where $T_i$ is the number of observations for unit $i$.  An exactly
analogous formulation applies to the independent variables.  Given
parameter estimates, $\hat{\beta}$, obtained using such de-meaned data
we can recover estimates of the $\alpha_i$s using
\[
\hat{\alpha}_i = \frac{1}{T_i} \sum_{t=1}^{T_i} 
   \left(y_{it} - X_{it}\hat{\beta}\right)
\]

These two methods (using dummy variables, and using de-meaned data) are
numerically equivalent, and \app{gretl} chooses between them based on
the number of cross-sectional units and the number of independent
variables (with the objective of economizing the use of computer memory).

The $\hat{\alpha}_i$ estimates are not printed as part of the standard
model output in \app{gretl} (there may be a large number of these, and
typically they are not of much inherent interest).  However you can
retrieve them after estimation of the fixed effects model if you
wish.  In the graphical interface, go to the ``Save'' menu in the
model window and select ``per-unit constants''.  In command-line mode,
you can do \texttt{genr} \textsl{newname} = \verb+$ahat+, where
\textsl{newname} is the name you want to give the series. 

For the \textsl{random effects} model we write $u_{it} = v_i +
\varepsilon_{it}$, so the model becomes
\begin{equation}
\label{eq:RE}
y_{it} = X_{it}\beta + v_i + \varepsilon_{it}
\end{equation}
In contrast to the fixed effects model, the $v_i$s are not treated as
fixed parameters, but as random drawings from a given probability
distribution.

The celebrated Gauss--Markov theorem, according to which OLS is the
best linear unbiased estimator (BLUE), depends on the assumption that
the error term is independently and identically distributed (IID).  In
the panel context, the IID assumption means that $E(u_{it}^2)$, in
relation to equation~\ref{eq:pooled}, equals a constant, $\sigma^2_u$,
for all $i$ and $t$, while the covariance $E(u_{it} u_{is})$ equals
zero for all $s \neq t$ and the covariance $E(u_{it} u_{jt})$ equals
zero for all $j \neq i$.

If these assumptions are not met --- and they are unlikely to be met
in the context of panel data --- OLS is not the most efficient
estimator.  Greater efficiency may be gained using generalized least
squares (GLS), taking into account the covariance structure of the
error term.  To implement this approach we need to obtain estimates
of the respective variances of $v_i$ and $\varepsilon_{it}$.  The
variance of $v_i$, namely $\sigma^2_v$, is sometimes called the
``between'' variance (since it refers to variation between the
cross-sectional units), and $\sigma^2_{\varepsilon}$ is called the
``within'' variance.

GLS is often implemented by first transforming the data in some way
and then applying OLS to the modified data.  This is the case with the
random effects model: it is typically implemented by running OLS on a
data set where $y$ and $X$ are replaced by ``quasi-demeaned''
counterparts.  To illustrate with reference to $y$, the transformation
is
\[
\tilde{y}_{it} = {y}_{it} - \theta_i \bar{y}_i
\]
where $\bar{y}_i$ is the mean for unit $i$, as above, and the
coefficient $\theta_i$ is a function of the between and within
variances, namely
\[
\theta_i = 1 - \sqrt{\frac{\sigma^2_\varepsilon}{T_i \sigma^2_v +
  \sigma^2_\varepsilon}}
\] 
Several means of estimating the required variances have been suggested
in the literature (see Baltagi, 1995); \app{gretl} uses the method of
Swamy and Arora (1972): $\sigma^2_\varepsilon$ is estimated by the
residual variance from the fixed effects model, and the sum
$T_i \sigma^2_v + \sigma^2_\varepsilon$ is estimated as $T_i$ times
the residual variance from the ``between'' estimator,
\[
\bar{y}_i = \bar{X}_i \beta + e_i
\]
The latter regression is implemented by constructing a data set
consisting of the group or unit means of all the relevant variables.


\subsection{Choice of estimator}
\label{panel-choice}

Which panel method should one use, fixed effects or random effects?

One way of answering this question is in relation to the nature of the
data set.  If the panel comprises observations on a fixed and
relatively small set of units of interest (say, the member states of
the European Union), there is a presumption in favor of fixed effects.
If it comprises observations on a large number of randomly selected
individuals (as in many epidemiological and other longitudinal
studies), there is a presumption in favor of random effects.

Besides this general heuristic, however, various statistical
issues must be taken into account.

\begin{enumerate}

\item Some panel data sets contain variables whose values are specific
  to the cross-sectional unit but which do not vary over time.  If you
  want to include such variables in the model, the fixed effects
  option is simply not available.  When the fixed effects approach is
  implemented using dummy variables, the problem is that the
  time-invariant variables are perfectly collinear with the per-unit
  dummies.  When using the approach of subtracting the group means,
  the issue is that after de-meaning these variables are nothing but
  zeros.
\item A somewhat analogous prohibition applies to the random effects
  model.  To appreciate this point it is necessary to note that the
  random effects estimator is a matrix-weighted average of the
  ``within'' estimator (that is, the fixed effects estimator, which
  sweeps out between-unit variation by including per-unit dummy
  variables or by subtracting the unit means) and the ``between''
  estimator.  The point is that if one has observations on $m$ units
  and $k$ independent variables of interest, then if $k>m$ the
  ``between'' estimator is undefined --- since we have only $m$
  effective observations --- and hence so is the random effects
  estimator.
\item If one does not fall foul of one or other of the prohibitions
  mentioned above, the choice between fixed effects and random effects
  may be expressed in terms of the two econometric
  \textit{desiderata}, efficiency and consistency.  The short story is
  that the random effects model is consistent only under a more
  stringent condition than the fixed effects model, but if this
  condition is satisfied the random effects model offers greater
  efficiency.  The condition in question is that the unit-specific
  error, $v_i$, must be unrelated to the independent variables, $X_i$,
  or more precisely that the expectation of $v_i$ conditional on
  $X_i$ equals zero.  This point is taken up in relation to the
  Hausman test in the following section.

\end{enumerate}

\subsection{Testing panel models}
\label{panel-tests}

If you estimate a fixed effects or random effects model in the
graphical interface, you may notice that the number of items available
under the ``Tests'' menu in the model window is relatively limited.
Panel models carry certain complications that make it difficult to
implement all of the tests one expects to see for models estimated on
straight time-series or cross-sectional data.  

Nonetheless, various panel-specific tests are printed along with the
parameter estimates as a matter of course, as follows.

\begin{itemize}
\item When you estimate a model using \textsl{fixed effects}, you
  automatically get an $F$-test for the null hypothesis that the
  cross-sectional units all have a common intercept.  That is to say
  that all the $\alpha_i$s are equal, in which case the pooled model
  (\ref{eq:pooled}), with a column of 1s included in the $X$ matrix,
  is adequate.
\item When you estimate using \textsl{random effects}, two relevant
  tests are presented automatically.  The Breusch--Pagan test is the
  counterpart to the $F$-test mentioned above.  The null hypothesis is
  that the variance of $v_i$ equals zero; if this hypothesis is not
  rejected, then again we conclude that the pooled model is adequate.
  The Hausman test probes the consistency of the GLS estimates.  The
  null hypothesis is that these estimates are consistent, that is,
  that the requirement of orthogonality of the $v_i$ and the $X_i$ is
  satisfied.  If the Hausman $\chi^2$ value is ``large'' this suggests
  that the random effects estimator is not consistent and fixed
  effects may be preferable.
\end{itemize}



\section{Illustration: the Penn World Table}
\label{PWT}

The Penn World Table (homepage at
\href{http://pwt.econ.upenn.edu/}{pwt.econ.upenn.edu}) is a rich
macroeconomic panel dataset, spanning 152 countries over the years
1950--1992.  The data are available in \app{gretl} format; please see
the \app{gretl}
\href{http://gretl.sourceforge.net/gretl_data.html}{data site} (this
is a free download, although it is not included in the main
\app{gretl} package).

Example \ref{examp-pwt} opens \verb+pwt56_60_89.gdt+, a subset
of the PWT containing data on 120 countries, 1960--89, for 20
variables, with no missing observations (the full data set, which is
also supplied in the pwt package for \app{gretl}, has many missing
observations). Total growth of real GDP, 1960--89, is calculated for
each country and regressed against the 1960 level of real GDP, to see
if there is evidence for ``convergence'' (i.e.\ faster growth on the
part of countries starting from a low base).

\begin{script}[htbp]
  \caption{Use of the Penn World Table}
  \label{examp-pwt}
\begin{code}
          open pwt56_60_89.gdt 
          # for 1989 (the last obs), lag 29 gives 1960, the first obs 
          genr gdp60 = RGDPL(-29) 
          # find total growth of real GDP over 30 years
          genr gdpgro = (RGDPL - gdp60)/gdp60
          # restrict the sample to a 1989 cross-section 
          smpl --restrict YEAR=1989 
          # convergence: did countries with a lower base grow faster?  
          ols gdpgro const gdp60 
          # result: No! Try an inverse relationship?
          genr gdp60inv = 1/gdp60 
          ols gdpgro const gdp60inv 
          # no again.  Try treating Africa as special? 
          genr afdum = (CCODE = 1)
          genr afslope = afdum * gdp60 
          ols gdpgro const afdum gdp60 afslope 
\end{code}
\end{script}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "gretl-guide"
%%% End: 

