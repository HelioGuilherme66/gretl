\chapter{Panel data}
\label{chap-panel}

\section{Panel data structure}
\label{panel-structure}

Panel data are inherently three dimensional --- the dimensions being
variable, cross-sectional unit, and time-period.  For example, a
particular number in a panel data set might be identified as the
observation on capital stock for General Motors in 1980.  (A note on
terminology: we use the terms ``cross-sectional unit'', ``unit'' and
``group'' interchangeably below to refer to the entities that compose
the cross-sectional dimension of the panel.  These might, for
instance, be firms, countries or persons.)

For representation in a textual computer file (and also for gretl's
internal calculations) the three dimensions must somehow be flattened
into two.  This ``flattening'' involves taking layers of the data that
would naturally stack in a third dimension, and stacking them in the
vertical dimension.

\app{Gretl} always expects data to be arranged ``by observation'',
that is, such that each row represents an observation (and each
variable occupies one and only one column).  In this context the
flattening of a panel data set can be done in either of two ways:

\begin{itemize}
\item Stacked time series: the successive vertical blocks each
  comprise a time series for a given unit.
\item Stacked cross sections: the successive vertical blocks each
  comprise a cross-section for a given period.
\end{itemize}

You may input data in whichever arrangement is more convenient.
Internally, however, \app{gretl} always stores panel data in
the form of stacked time series.

When you import panel data into \app{gretl} from a spreadsheet or
comma separated format, the panel nature of the data will not be
recognized automatically (most likely the data will be treated as
``undated'').  A panel interpretation can be imposed on the data
using the graphical interface or via the \cmd{setobs} command.

In the graphical interface, use the menu item ``Sample, Dataset
structure''.  In the first dialog box that appears, select ``Panel''.
In the next dialog you have a three-way choice.  The first two
options, ``Stacked time series'' and ``Stacked cross sections'' are
applicable if the data set is already organized in one of these two
ways.  If you select either of these options, the next step is to
specify the number of cross-sectional units in the data set.  The
third option, ``Use index variables'', is applicable if the data set
contains two variables that index the units and the time periods
respectively; the next step is then to select those variables.  For
example, a data file might contain a country code variable and a
variable representing the year of the observation.  In that case
\app{gretl} can reconstruct the panel structure of the data regardless
of how the observation rows are organized.

The \cmd{setobs} command has options that parallel those in the
graphical interface.  If suitable index variables are available
you can do, for example
\begin{code}
            setobs unitvar timevar --panel-vars
\end{code}
where \texttt{unitvar} is a variable that indexes the units and
\texttt{timevar} is a variable indexing the periods.  Alternatively
you can use the form \verb+setobs+ \textsl{freq} \verb+1:1+
\textsl{structure}, where \textsl{freq} is replaced by the ``block
size'' of the data (that is, the number of periods in the case of
stacked time series, or the number of units in the case of stacked
cross-sections) and structure is either \verb+--stacked-time-series+
or \verb+--stacked-cross-section+.  Two examples are given below: the
first is suitable for a panel in the form of stacked time series with
observations from 20 periods; the second for stacked cross sections
with 5 units.
\begin{code}
            setobs 20 1:1 --stacked-time-series
            setobs 5 1:1 --stacked-cross-section
\end{code}

\subsection{Panel data arranged by variable}

Publicly available panel data sometimes come arranged ``by variable.''
Suppose we have data on two variables, \varname{x1} and \varname{x2},
for each of 50 states in each of 5 years (giving a total of 250
observations per variable).  One textual representation of such a data
set would start with a block for \varname{x1}, with 50 rows
corresponding to the states and 5 columns corresponding to the years.
This would be followed, vertically, by a block with the same structure
for variable \varname{x2}.  A fragment of such a data file is shown
below, with quinquennial observations 1965--1985.  Imagine the table
continued for 48 more states, followed by another 50 rows for variable
\varname{x2}.

\begin{center}
  \begin{tabular}{rrrrrr}
  \varname{x1} \\
     & 1965 & 1970 & 1975 & 1980 & 1985 \\
  AR & 100.0 & 110.5 & 118.7 & 131.2 & 160.4\\
  AZ & 100.0 & 104.3 & 113.8 & 120.9 & 140.6\\
  \end{tabular}
\end{center}

If a datafile with this sort of structure is read into \app{gretl},
the program will interpret the columns as distinct variables, so the
data will not be usable ``as is.''  But there is a mechanism for
correcting the situation, namely the \cmd{stack} function within
the \cmd{genr} command.

Consider the first data column in the fragment above: the first 50 rows
of this column constitute a cross-section for the variable \varname{x1}
in the year 1965.  If we could create a new variable by stacking the
first 50 entries in the second column underneath the first 50 entries
in the first, we would be on the way to making a data set ``by
observation'' (in the first of the two forms mentioned above, stacked
cross-sections).  That is, we'd have a column comprising a
cross-section for \varname{x1} in 1965, followed by a cross-section for
the same variable in 1970.

The following gretl script illustrates how we can accomplish the
stacking, for both \varname{x1} and \varname{x2}.  We assume
that the original data file is called \texttt{panel.txt}, and that in
this file the columns are headed with ``variable names'' \varname{p1},
\varname{p2}, \dots, \varname{p5}.  (The columns are not really
variables, but in the first instance we ``pretend'' that they are.)

\begin{code}
    open panel.txt
    genr x1 = stack(p1..p5) --length=50
    genr x2 = stack(p1..p5) --offset=50 --length=50
    setobs 50 1:1 --stacked-cross-section
    store panel.gdt x1 x2
\end{code}

The second line illustrates the syntax of the \cmd{stack} function.
The double dots within the parentheses indicate a range of variables
to be stacked: here we want to stack all 5 columns (for all 5 years).
The full data set contains 100 rows; in the stacking of variable
\varname{x1} we wish to read only the first 50 rows from each column:
we achieve this by adding \verb+--length=50+.  Note that if you want
to stack a non-contiguous set of columns you can put a comma-separated
list within the parentheses, as in

\begin{code}
    genr x = stack(p1,p3,p5)
\end{code}

On line 3 we do the stacking for variable \varname{x2}.  Again we want
a \texttt{length} of 50 for the components of the stacked series, but
this time we want gretl to start reading from the 50th row of the
original data, and we specify \verb+--offset=50+.

Line 4 imposes a panel interpretation on the data, as explained in
section~\ref{panel-structure}.  Finally, we save the data in gretl
format, with the panel interpretation, discarding the original
``variables'' \varname{p1} through \varname{p5}.

The illustrative script above is appropriate when the number of
variable to be processed is small.  When then are many variables in
the data set it will be more efficient to use a command loop to
accomplish the stacking, as shown in the following script.  The setup
is presumed to be the same as in the previous section (50 units, 5
periods), but with 20 variables rather than 2.

\begin{code}
    open panel.txt
    loop for i=1..20
      genr k = ($i - 1) * 50
      genr x$i = stack(p1..p5) --offset=k --length=50
    endloop
    setobs 50 1.01 --stacked-cross-section
    store panel.gdt x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 \
      x11 x12 x13 x14 x15 x16 x17 x18 x19 x20
\end{code}

\section{Generated variables}
\label{panel-genr}

\subsection{Dummy variables}
\label{dummies}

In a panel study you may wish to construct dummy variables of one or
both of the following sorts: (a) dummies as unique identifiers for the
units or groups, and (b) dummies as unique identifiers for the time
periods.  The former may be used to allow the intercept of the
regression to differ across the units, the latter to allow the
intercept to differ across periods.

Two special functions are available to create such dummies.  These are
found under the ``Add'' menu in the GUI, or under the \cmd{genr}
command in script mode or \app{gretlcli}.

\begin{enumerate}
\item ``unit dummies'' (script command \cmd{genr unitdum}).  This
  command creates a set of dummy variables identifying the
  cross-sectional units.  The variable \verb+du_1+ will have value 1
  in each row corresponding to a unit 1 observation, 0 otherwise;
  \verb+du_2+ will have value 1 in each row corresponding to a unit 2
  observation, 0 otherwise; and so on.
\item ``time dummies'' (script command \cmd{genr timedum}).  This
  command creates a set of dummy variables identifying the periods.
  The variable \verb+dt_1+ will have value 1 in each row
  corresponding to a period 1 observation, 0 otherwise; \verb+dt_2+
  will have value 1 in each row corresponding to a period 2
  observation, 0 otherwise; and so on.
\end{enumerate}

If a panel data set has the \verb+YEAR+ of the observation entered as
one of the variables you can create a periodic dummy to pick out a
particular year, e.g.\ \cmd{genr dum = (YEAR=1960)}.  You can also
create periodic dummy variables using the modulus operator,
\verb+%+.  For instance, to create a dummy with
value 1 for the first observation and every thirtieth observation
thereafter, 0 otherwise, do
\begin{code}
      genr index 
      genr dum = ((index-1)%30) = 0
\end{code}

\subsection{Lags, differences, trends}
\label{panel-lagged}

If the time periods are evenly spaced you may want to use lagged
values of variables in a panel regression (but see
section~\ref{panel-dyn} below); you may also wish to construct first
differences of variables of interest.

Once a dataset is identified as a panel, \app{gretl} will handle the
generation of such variables correctly.  For example the command
\verb+genr x1_1 = x1(-1)+ will create a variable that contains the
first lag of \verb+x1+ where available, and the missing value code
where the lag is not available (e.g.\ at the start of the time series
for each group).  When you run a regression using such variables, the
program will automatically skip the missing observations.

When a panel data set has a fairly substantial time dimension, you may
wish to include a trend in the analysis.  The command \cmd{genr time} 
creates a variable named \varname{time} which runs from 1 to $T$ for
each unit, where $T$ is the length of the time-series dimension of the
panel.  If you want to create an index that runs consecutively from 1
to $m\times T$, where $m$ is the number of units in the panel, use
\cmd{genr index}.

\section{Estimation of panel models}

\subsection{Pooled Ordinary Least Squares}
\label{pooled-est}

The simplest estimator for panel data is pooled OLS.  In most cases
this is unlikely to be adequate, but it provides a baseline for
comparison with more complex estimators.

If you estimate a model on panel data using OLS an additional test
item becomes available.  In the GUI model window this is the item
``panel diagnostics'' under the \textsf{Tests} menu; the script
counterpart is the \cmd{hausman} command.

To take advantage of this test, you should specify a model without any
dummy variables representing cross-sectional units.  The test compares
pooled OLS against the principal alternatives, the fixed effects and
random effects models.  These alternatives are explained in the
following section.

\subsection{The fixed and random effects models}
\label{panel-est}

In \app{gretl} version 1.5.2 and higher, the fixed and random effects
models for panel data can be estimated in their own right.  In the
graphical interface these options are found under the menu item
``Model/Panel/Fixed and random effects''.  In the command-line
interface one uses the \cmd{panel} command, with or without the
\verb+--random-effects+ option.

This section explains the nature of these models and comments on their
estimation via \app{gretl}.

The pooled OLS specification may be written as 
\begin{equation}
\label{eq:pooled}
y_{it} = X_{it}\beta + u_{it}
\end{equation}
where $y_{it}$ is the observation on the dependent variable for
cross-sectional unit $i$ in period $t$, $X_{it}$ is a $1\times k$
vector of independent variables observed for unit $i$ in period $t$,
$\beta$ is a $k\times 1$ vector of parameters, and $u_{it}$ is an error
or disturbance term specific to unit $i$ in period $t$.

The fixed and random effects models have in common that they decompose
the unitary pooled error term, $u_{it}$.  For the \textsl{fixed effects}
model we write $u_{it} = \alpha_i + \varepsilon_{it}$, yielding
\begin{equation}
\label{eq:FE}
y_{it} = X_{it}\beta + \alpha_i + \varepsilon_{it}
\end{equation}
That is, we decompose $u_{it}$ into a unit-specific and time-invariant
component, $\alpha_i$, and an observation-specific error,
$\varepsilon_{it}$.\footnote{It is possible to break a third component
  out of $u_{it}$, namely $w_t$, a shock that is time-specific but
  common to all the units in a given period.  In the interest of
  simplicity we do not pursue that option here.}  The $\alpha_i$s are
then treated as fixed parameters (in effect, unit-specific
$y$-intercepts), which are to be estimated.  This can be done by
including a dummy variable for each cross-sectional unit (and
suppressing the global constant).  Alternatively, one can proceed by
subtracting the group or unit mean from each of variables and
estimating a model without a constant.  In the latter case the
dependent variable may be written as
\[
\tilde{y}_{it} = y_{it} - \bar{y}_i
\]
The ``group mean'', $\bar{y}_i$, is defined as
\[
\bar{y}_i = \frac{1}{T_i} \sum_{t=1}^{T_i} y_{it}
\]
where $T_i$ is the number of observations for unit $i$.  An exactly
analogous formulation applies to the independent variables.  Given
parameter estimates, $\hat{\beta}$, obtained using such de-meaned data
we can recover estimates of the $\alpha_i$s using
\[
\hat{\alpha}_i = \frac{1}{T_i} \sum_{t=1}^{T_i} 
   \left(y_{it} - X_{it}\hat{\beta}\right)
\]

These two methods (using dummy variables, and using de-meaned data) are
numerically equivalent, and \app{gretl} chooses between them based on
the number of cross-sectional units and the number of independent
variables (with the objective of economizing the use of computer memory).

The $\hat{\alpha}_i$ estimates are not printed as part of the standard
model output in \app{gretl} (there may be a large number of these, and
typically they are not of much inherent interest).  However you can
retrieve them after estimation of the fixed effects model if you
wish.  In the graphical interface, go to the ``Save'' menu in the
model window and select ``per-unit constants''.  In command-line mode,
you can do \texttt{genr} \textsl{newname} = \verb+$ahat+, where
\textsl{newname} is the name you want to give the series. 

For the \textsl{random effects} model we write $u_{it} = v_i +
\varepsilon_{it}$, so the model becomes
\begin{equation}
\label{eq:RE}
y_{it} = X_{it}\beta + v_i + \varepsilon_{it}
\end{equation}
In contrast to the fixed effects model, the $v_i$s are not treated as
fixed parameters, but as random drawings from a given probability
distribution.

The celebrated Gauss--Markov theorem, according to which OLS is the
best linear unbiased estimator (BLUE), depends on the assumption that
the error term is independently and identically distributed (IID).  In
the panel context, the IID assumption means that $E(u_{it}^2)$, in
relation to equation~\ref{eq:pooled}, equals a constant, $\sigma^2_u$,
for all $i$ and $t$, while the covariance $E(u_{it} u_{is})$ equals
zero for all $s \neq t$ and the covariance $E(u_{it} u_{jt})$ equals
zero for all $j \neq i$.

If these assumptions are not met --- and they are unlikely to be met
in the context of panel data --- OLS is not the most efficient
estimator.  Greater efficiency may be gained using generalized least
squares (GLS), taking into account the covariance structure of the
error term.  

Consider two observation on the same unit $i$ at two different times
$s$ and $t$. From the hypotheses above it can be worked out that
$\mbox{Var}(u_{is}) = \mbox{Var}(u_{it}) = \sigma^2_{v} +
\sigma^2_{\varepsilon}$, while the covariance between $u_{is}$ and
$u_{it}$ is given by $\mbox{Cov}(u_{is},u_{it}) = \sigma^2_{v}$.

In matrix notation, we may group all the $T_i$ observations for unit
$i$ into the vector $\mathbf{y}_i$ and write it as
\begin{equation}
\label{eq:REvec}
\mathbf{y}_{i} = \mathbf{X}_{i} \beta + \mathbf{u}_i
\end{equation}
The vector $\mathbf{u}_i$, which includes all the disturbances for
individual $i$, has a variance--covariance matrix given by
\begin{equation}
\label{eq:CovMatUnitI}
  \mbox{Var}(\mathbf{u}_i) = \Sigma_i = \sigma^2_{\varepsilon} I + \sigma^2_{v} J
\end{equation}
where $J$ is a square matrix with all elements equal to 1. It can be
shown that the matrix
\[
  K_i = I - \frac{\theta}{T_i} J,
\]
where $\theta = 1 -
\sqrt{\frac{\sigma^2_{\varepsilon}}{\sigma^2_{\varepsilon} + T_i
    \sigma^2_{v}}}$, has the property
\[
  K_i \Sigma K_i' = \sigma^2_{\varepsilon} I
\]
It follows that the transformed system
\begin{equation}
\label{eq:REGLS}
K_i \mathbf{y}_{i} = K_i \mathbf{X}_{i} \beta + K_i \mathbf{u}_i
\end{equation}
satisfies the Gauss--Markov conditions, and OLS estimation of
(\ref{eq:REGLS}) provides efficient inference. But since 
\[
  K_i \mathbf{y}_{i} = \mathbf{y}_{i} - \theta \bar{\mathbf{y}}_{i}
\]
GLS estimation is equivalent to OLS using ``quasi-demeaned''
variables; that is, variables from which we subtract a fraction
$\theta$ of their average. Notice that for $\sigma^2_{\varepsilon} \to
0$, $\theta \to 1$, while for $\sigma^2_{v} \to 0$, $\theta \to 0$.
This means that if all the variance is attributable to the individual
effects, then the fixed effects estimator is optimal; if, on the other
hand, individual effects are negligible, then pooled OLS turns out,
unsurprisingly, to be the optimal estimator.

To implement the GLS approach we need to calculate $\theta$, which in
turn requires estimates of the variances $\sigma^2_{\varepsilon}$ and
$\sigma^2_v$.  (These are often referred to as the ``within'' and
``between'' variances respectively, since the former refers to
variation within each cross-sectional unit and the latter to variation
between the units).  Several means of estimating these magnitudes have
been suggested in the literature (see Baltagi, 1995); \app{gretl} uses
the method of Swamy and Arora (1972): $\sigma^2_\varepsilon$ is
estimated by the residual variance from the fixed effects model, and
the sum $\sigma^2_\varepsilon + T_i \sigma^2_v$ is estimated as $T_i$
times the residual variance from the ``between'' estimator,
\[
\bar{y}_i = \bar{X}_i \beta + e_i
\]
The latter regression is implemented by constructing a data set
consisting of the group or unit means of all the relevant variables.


\subsection{Choice of estimator}
\label{panel-choice}

Which panel method should one use, fixed effects or random effects?

One way of answering this question is in relation to the nature of the
data set.  If the panel comprises observations on a fixed and
relatively small set of units of interest (say, the member states of
the European Union), there is a presumption in favor of fixed effects.
If it comprises observations on a large number of randomly selected
individuals (as in many epidemiological and other longitudinal
studies), there is a presumption in favor of random effects.

Besides this general heuristic, however, various statistical
issues must be taken into account.

\begin{enumerate}

\item Some panel data sets contain variables whose values are specific
  to the cross-sectional unit but which do not vary over time.  If you
  want to include such variables in the model, the fixed effects
  option is simply not available.  When the fixed effects approach is
  implemented using dummy variables, the problem is that the
  time-invariant variables are perfectly collinear with the per-unit
  dummies.  When using the approach of subtracting the group means,
  the issue is that after de-meaning these variables are nothing but
  zeros.
\item A somewhat analogous prohibition applies to the random effects
  model.  To appreciate this point it is necessary to note that the
  random effects estimator is a matrix-weighted average of the
  ``within'' estimator (that is, the fixed effects estimator, which
  sweeps out between-unit variation by including per-unit dummy
  variables or by subtracting the unit means) and the ``between''
  estimator.  The point is that if one has observations on $m$ units
  and $k$ independent variables of interest, then if $k>m$ the
  ``between'' estimator is undefined --- since we have only $m$
  effective observations --- and hence so is the random effects
  estimator.
\end{enumerate}

If one does not fall foul of one or other of the prohibitions
mentioned above, the choice between fixed effects and random effects
may be expressed in terms of the two econometric \textit{desiderata},
efficiency and consistency.  

From a purely statistical viewpoint, we could say that there is a
tradeoff between robustness and efficiency. In the fixed effects
approach, we do not make any hypotheses on the ``group effects'' (that
is, the time-invariant differences in mean between the groups) beyond
the fact that they exist --- and that can be tested; see below. As a
consequence, once these effects are swept out by taking deviations
from the group means, the remaining parameters can be estimated.

On the other hand, the random effects approach attempts to model the
group effects as drawings from a probability distribution instead of
removing them. This requires that individual effects are representable
as a legitimate part of the disturbance term, that is, zero-mean
random variables, uncorrelated with the regressors.

As a consequence, the fixed-effects estimator ``always works'', but at
the cost of not being able to estimate the effect of time-invariant
regressors, since any time-invariant effect is, by definition, an
group-specific feature and therefore ignored.  The richer hypothesis
set of the random-effects estimator ensures that parameters for
time-invariant regressors can be estimated and that estimation of the
parameters for time-varying regressors is carried out more
efficiently.  These advantages, though, are tied to the validity of
these additional hypotheses. If, for example, there is reason to think
that individual effects may be correlated with some of the explanatory
variables, then the random-effects estimator would be inconsistent,
while fixed-effects estimates would be perfectly valid.  It is
precisely on this principle that the Hausman test is built (see
below): if the fixed- and random-effects estimates agree, to within
the usual statistical margin of error, there is no reason to think the
additional hypotheses invalid, and as a consequence, no reason not to
use the more efficient RE estimator.

\subsection{Testing panel models}
\label{panel-tests}

If you estimate a fixed effects or random effects model in the
graphical interface, you may notice that the number of items available
under the ``Tests'' menu in the model window is relatively limited.
Panel models carry certain complications that make it difficult to
implement all of the tests one expects to see for models estimated on
straight time-series or cross-sectional data.  

Nonetheless, various panel-specific tests are printed along with the
parameter estimates as a matter of course, as follows.

\begin{itemize}
\item When you estimate a model using \textsl{fixed effects}, you
  automatically get an $F$-test for the null hypothesis that the
  cross-sectional units all have a common intercept.  That is to say
  that all the $\alpha_i$s are equal, in which case the pooled model
  (\ref{eq:pooled}), with a column of 1s included in the $X$ matrix,
  is adequate.
\item When you estimate using \textsl{random effects}, two relevant
  tests are presented automatically.  The Breusch--Pagan test is the
  counterpart to the $F$-test mentioned above.  The null hypothesis is
  that the variance of $v_i$ equals zero; if this hypothesis is not
  rejected, then again we conclude that the pooled model is adequate.
  The Hausman test probes the consistency of the GLS estimates.  The
  null hypothesis is that these estimates are consistent, that is,
  that the requirement of orthogonality of the $v_i$ and the $X_i$ is
  satisfied.  If the Hausman $\chi^2$ value is ``large'' this suggests
  that the random effects estimator is not consistent and fixed
  effects may be preferable.
\end{itemize}


\subsection{Robust standard errors}
\label{panel-robust}

For most estimators, \app{gretl} offers the option of computing an
estimate of the covariance matrix that is robust with respect to
heteroskedasticity and/or autocorrelation (and hence also robust
standard errors).  In the case of panel data, a robust covariance
matrix is available for the fixed effects model but not currently for
random effects.  The estimator used for fixed effects is
\[
\left(\tilde{X}^{\prime}\tilde{X}\right)^{-1}
\left[ \sum_{i=1}^m \tilde{X}_i^{\prime} \tilde{u}_i 
    \tilde{u}_i^{\prime} \tilde{X}_i \right]
\left(\tilde{X}^{\prime}\tilde{X}\right)^{-1}
\]
where $\tilde{X}$ is the matrix of regressors with the group means
subtracted, $\tilde{u}_i$ denotes the FE residuals for unit $i$, and $m$
is the number of cross-sectional units.

\section{Dynamic panel models}
\label{panel-dyn}

Special problems arise when a lag of the dependent variable is
included among the regressors in a panel model.  Consider a dynamic
variant of the pooled model (\ref{eq:pooled}):
\begin{equation}
\label{eq:pooled-dyn}
y_{it} = X_{it}\beta + \rho y_{it-1} + u_{it}
\end{equation}
First, if the error $u_{it}$ includes a group effect, $v_i$, then
$y_{it-1}$ is bound to be correlated with the error, since the value
of $v_i$ affects $y_i$ at all $t$.  That means that OLS applied to
(\ref{eq:pooled-dyn}) will be inconsistent as well as inefficient.
The fixed-effects model sweeps out the group effects and so overcomes
this particular problem, but a subtler issue remains, which applies to
both fixed and random effects estimation.  Consider the de-meaned
representation of fixed effects, as applied to the dynamic model,
\[
\tilde{y}_{it} = \tilde{X}_{it}\beta + \rho \tilde{y}_{i,t-1} 
  + \varepsilon_{it}
\]
where $\tilde{y}_{it} = y_{it} - \bar{y}_i$ and $\varepsilon_{it} =
u_{it} - \bar{u}_i$ (or $u_{it} - \alpha_i$, using the notation of
equation~\ref{eq:FE}).  The trouble is that $\tilde{y}_{i,t-1}$ will be
correlated with $\varepsilon_{it}$ via the group mean, $\bar{y}_i$.
The disturbance $\varepsilon_{it}$ influences $y_{it}$ directly, which
influences $\bar{y}_i$, which, by construction, affects the value of
$\tilde{y}_{it}$ for all $t$.  The same issue arises in relation to
the quasi-demeaning used for random effects.  Estimators which ignore
this correlation will be consistent only as $T \to \infty$ (in which
case the marginal effect of $\varepsilon_{it}$ on the group mean of 
$y$ tends to vanish).  

One strategy for handling this problem, and producing consistent
estimates of $\beta$ and $\rho$, was proposed by Anderson and Hsiao
(1981).  Instead of de-meaning the data, they suggest taking the first
difference of (\ref{eq:pooled-dyn}), an alternative tactic for
sweeping out the group effects:
\begin{equation}
\label{eq:fe-dyn}
\Delta y_{it} = \Delta X_{it}\beta + \rho \Delta y_{i,t-1} 
  + \eta_{it}
\end{equation}
where $\eta_{it} = \Delta u_{it} = \Delta(v_i + \varepsilon_{it}) =
\varepsilon_{it} - \varepsilon_{i,t-1}$.  We're not in the clear yet,
given the structure of the error $\eta_{it}$: the disturbance
$\varepsilon_{i,t-1}$ is an influence on both $\eta_{it}$ and $\Delta
y_{i,t-1} = y_{it} - y_{i,t-1}$.  The next step is then to find an
instrument for the ``contaminated'' $\Delta y_{i,t-1}$. Anderson and
Hsiao suggest using either $y_{i,t-2}$ or $\Delta y_{i,t-2}$, both of
which will be uncorrelated with $\eta_{it}$ provided that the
underlying errors $\varepsilon_{it}$ are not themselves serially
correlated.

The Anderson--Hsiao estimator is not provided as a built-in function
in \app{gretl}, but it can be programmed in script mode quite
easily --- see Example~\ref{anderson-hsiao}, which is based
on a study of country growth rates by Nerlove (1999).\footnote{Also
  see Clint Cummins' benchmarks page,
  \url{http://www.stanford.edu/~clint/bench/}.}  Note that after
running the second OLS regression it is necessary to correct the
residuals and standard errors: these must be based on the actual data
for $\Delta y_{i,t-1}$, not the fitted value obtained from the
first-stage regression.

Although the Anderson--Hsiao estimator is consistent, it is not most
efficient: it does not make the fullest use of the available
instruments for $\Delta y_{i,t-1}$, and neither does it take into
account the differenced structure of the error $\eta_{it}$.  It is
improved upon by the method of Arellano and Bond (1991).  The latter
method is not yet supported in \app{gretl}; it is likely to be added
in a future release of the program.

\begin{script}[htbp]
  \caption{The Anderson--Hsiao estimator for a dynamic panel model}
  \label{anderson-hsiao}
\begin{code}
  # Penn World Table data as used by Nerlove
  open penngrow.gdt
  # generate lagged dependent variable
  genr Y1 = Y(-1)
  # Fixed effects (for comparison)
  panel Y 0 Y1 X
  # Random effects (for comparison)
  panel Y 0 Y1 X --random-effects
  # Anderson-Hsiao, using Y(-2) as instrument
  # take differences of all variables
  diff Y X Y1
  # first stage: obtain fitted values for \Delta Y_{t-1} 
  ols d_Y1 0 Y(-2) d_X
  genr DY1 = $yhat
  # second stage: run OLS using the fitted DY1
  ols d_Y DY1 d_X --simple-print
  # calculate corrected residuals and related statistics
  genr yh = $coeff(DY1) * d_Y1 + $coeff(d_X) * d_X
  genr uh = d_Y - yh
  genr SSR = sum(uh * uh)
  genr s = sqrt(SSR / $df)
  genr se_1 = $stderr(DY1) * s/$sigma
  genr se_2 = $stderr(d_X) * s/$sigma
\end{code}
\end{script}


\section{Illustration: the Penn World Table}
\label{PWT}

The Penn World Table (homepage at
\href{http://pwt.econ.upenn.edu/}{pwt.econ.upenn.edu}) is a rich
macroeconomic panel dataset, spanning 152 countries over the years
1950--1992.  The data are available in \app{gretl} format; please see
the \app{gretl}
\href{http://gretl.sourceforge.net/gretl_data.html}{data site} (this
is a free download, although it is not included in the main
\app{gretl} package).

Example \ref{examp-pwt} opens \verb+pwt56_60_89.gdt+, a subset
of the PWT containing data on 120 countries, 1960--89, for 20
variables, with no missing observations (the full data set, which is
also supplied in the pwt package for \app{gretl}, has many missing
observations). Total growth of real GDP, 1960--89, is calculated for
each country and regressed against the 1960 level of real GDP, to see
if there is evidence for ``convergence'' (i.e.\ faster growth on the
part of countries starting from a low base).

\begin{script}[htbp]
  \caption{Use of the Penn World Table}
  \label{examp-pwt}
\begin{code}
  open pwt56_60_89.gdt 
  # for 1989 (the last obs), lag 29 gives 1960, the first obs 
  genr gdp60 = RGDPL(-29) 
  # find total growth of real GDP over 30 years
  genr gdpgro = (RGDPL - gdp60)/gdp60
  # restrict the sample to a 1989 cross-section 
  smpl --restrict YEAR=1989 
  # convergence: did countries with a lower base grow faster?  
  ols gdpgro const gdp60 
  # result: No! Try an inverse relationship?
  genr gdp60inv = 1/gdp60 
  ols gdpgro const gdp60inv 
  # no again.  Try treating Africa as special? 
  genr afdum = (CCODE = 1)
  genr afslope = afdum * gdp60 
  ols gdpgro const afdum gdp60 afslope 
\end{code}
\end{script}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "gretl-guide"
%%% End: 

