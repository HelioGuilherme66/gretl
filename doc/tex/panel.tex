\chapter{Panel data}
\label{chap-panel}

\section{Panel data structure}
\label{panel-structure}

Panel data are inherently three dimensional --- the dimensions being
variable, cross-sectional unit, and time-period.  For representation
in a textual computer file (and also for gretl's internal
calculations) these three dimensions must somehow be flattened into
two.  This ``flattening'' involves taking layers of the data that
would naturally stack in a third dimension, and stacking them in the
vertical dimension.

\app{Gretl} always expects data to be arranged ``by observation'',
that is, such that each row represents an observation (and each
variable occupies one and only one column).  In this context the
flattening of a panel data set can be done in either of two ways:

\begin{itemize}
\item Stacked cross-sections: the successive vertical blocks each
  comprise a cross-section for a given period.
\item Stacked time-series: the successive vertical blocks each
  comprise a time series for a given cross-sectional unit.
\end{itemize}

You may input data in whichever arrangement is more convenient.
Internally, however, \app{gretl} always stores panel data in
the form of stacked time series.

When you import panel data into \app{gretl} from a spreadsheet or
comma separated format, the panel nature of the data will not be
recognized automatically (most likely the data will be treated as
``undated'').  A panel interpretation can be imposed on the data in
either of two ways.

\begin{enumerate}
\item Use the GUI menu item ``Sample, Dataset structure''.  In the
  first dialog box that appears, select ``Panel''.  In the next
  dialog, make a selection between stacked time series or stacked
  cross sections depending on how your data are organized.  In the
  next, supply the number of cross-sectional units in the dataset.
  Finally, check the specification that is shown to you, and confirm
  the change if it looks OK.
\item Use the script command \cmd{setobs}.  For panel data this
  command takes the form \verb+setobs+ \textsl{freq} \verb+1:1+
  structure, where \textsl{freq} is replaced by the ``block size'' of
  the data (that is, the number of periods in the case of stacked time
  series, or the number of cross-sectional units in the case of
  stacked cross-sections) and structure is either
  \verb+--stacked-time-series+ or \verb+--stacked-cross-section+.  Two
  examples are given below: the first is suitable for a panel in the
  form of stacked time series with observations from 20 periods; the
  second for stacked cross sections with 5 cross-sectional units.
\begin{code}
            setobs 20 1:1 --stacked-time-series
            setobs 5 1:1 --stacked-cross-section
\end{code}
\end{enumerate}

\subsection{Panel data arranged by variable}

Publicly available panel data sometimes come arranged ``by variable.''
Suppose we have data on two variables, \varname{x1} and \varname{x2},
for each of 50 states in each of 5 years (giving a total of 250
observations per variable).  One textual representation of such a data
set would start with a block for \varname{x1}, with 50 rows
corresponding to the states and 5 columns corresponding to the years.
This would be followed, vertically, by a block with the same structure
for variable \varname{x2}.  A fragment of such a data file is shown
below, with quinquennial observations 1965--1985.  Imagine the table
continued for 48 more states, followed by another 50 rows for variable
\varname{x2}.

\begin{center}
  \begin{tabular}{rrrrrr}
  \varname{x1} \\
     & 1965 & 1970 & 1975 & 1980 & 1985 \\
  AR & 100.0 & 110.5 & 118.7 & 131.2 & 160.4\\
  AZ & 100.0 & 104.3 & 113.8 & 120.9 & 140.6\\
  \end{tabular}
\end{center}

If a datafile with this sort of structure is read into \app{gretl},
the program will interpret the columns as distinct variables, so the
data will not be usable ``as is.''  But there is a mechanism for
correcting the situation, namely the \cmd{stack} function within
the \cmd{genr} command.

Consider the first data column in the fragment above: the first 50 rows
of this column constitute a cross-section for the variable \varname{x1}
in the year 1965.  If we could create a new variable by stacking the
first 50 entries in the second column underneath the first 50 entries
in the first, we would be on the way to making a data set ``by
observation'' (in the first of the two forms mentioned above, stacked
cross-sections).  That is, we'd have a column comprising a
cross-section for \varname{x1} in 1965, followed by a cross-section for
the same variable in 1970.

The following gretl script illustrates how we can accomplish the
stacking, for both \varname{x1} and \varname{x2}.  We assume
that the original data file is called \texttt{panel.txt}, and that in
this file the columns are headed with ``variable names'' \varname{p1},
\varname{p2}, \dots, \varname{p5}.  (The columns are not really
variables, but in the first instance we ``pretend'' that they are.)

\begin{code}
    open panel.txt
    genr x1 = stack(p1..p5) --length=50
    genr x2 = stack(p1..p5) --offset=50 --length=50
    setobs 50 1.01 --stacked-cross-section
    store panel.gdt x1 x2
\end{code}

The second line illustrates the syntax of the \cmd{stack} function.
The double dots within the parentheses indicate a range of variables
to be stacked: here we want to stack all 5 columns (for all 5 years).
The full data set contains 100 rows; in the stacking of variable
\varname{x1} we wish to read only the first 50 rows from each column:
we achieve this by adding \verb+--length=50+.  Note that if you want
to stack a non-contiguous set of columns you can put a comma-separated
list within the parentheses, as in

\begin{code}
    genr x = stack(p1,p3,p5)
\end{code}

On line 3 we do the stacking for variable \varname{x2}.  Again we want
a \texttt{length} of 50 for the components of the stacked series, but
this time we want gretl to start reading from the 50th row of the
original data, and we specify \verb+--offset=50+.

Line 4 imposes a panel interpretation on the data, as explained in
section~\ref{panel-structure}.  Finally, we save the data in gretl
format, with the panel interpretation, discarding the original
``variables'' \varname{p1} through \varname{p5}.

The illustrative script above is appropriate when the number of
variable to be processed is small.  When then are many variables in
the data set it will be more efficient to use a command loop to
accomplish the stacking, as shown in the following script.  The setup
is presumed to be the same as in the previous section (50 units, 5
periods), but with 20 variables rather than 2.

\begin{code}
    open panel.txt
    loop for i=1..20
      genr k = ($i - 1) * 50
      genr x$i = stack(p1..p5) --offset=k --length=50
    endloop
    setobs 50 1.01 --stacked-cross-section
    store panel.gdt x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 \
      x11 x12 x13 x14 x15 x16 x17 x18 x19 x20
\end{code}

\section{Generated variables}
\label{panel-genr}

\subsection{Dummy variables}
\label{dummies}

In a panel study you may wish to construct dummy variables of one or
both of the following sorts: (a) dummies as unique identifiers for the
cross-sectional units, and (b) dummies as unique identifiers for the
time periods.  The former may be used to allow the intercept of the
regression to differ across the units, the latter to allow the
intercept to differ across periods.

Two special functions are available to create such dummies.  These
are found under the ``Data, Add variables'' menu in the GUI, or under
the \cmd{genr} command in script mode or \app{gretlcli}.

\begin{enumerate}
\item ``unit dummies'' (script command \cmd{genr unitdum}).  This
  command creates a set of dummy variables identifying the
  cross-sectional units.  The variable \verb+du_1+ will have value 1
  in each row corresponding to a unit 1 observation, 0 otherwise;
  \verb+du_2+ will have value 1 in each row corresponding to a unit 2
  observation, 0 otherwise; and so on.
\item ``time dummies'' (script command \cmd{genr timedum}).  This
  command creates a set of dummy variables identifying the periods.
  The variable \verb+dt_1+ will have value 1 in each row
  corresponding to a period 1 observation, 0 otherwise; \verb+dt_2+
  will have value 1 in each row corresponding to a period 2
  observation, 0 otherwise; and so on.
\end{enumerate}

If a panel data set has the \verb+YEAR+ of the observation entered as
one of the variables you can create a periodic dummy to pick out a
particular year, e.g.\ \cmd{genr dum = (YEAR=1960)}.  You can also
create periodic dummy variables using the modulus operator,
\verb+%+.  For instance, to create a dummy with
value 1 for the first observation and every thirtieth observation
thereafter, 0 otherwise, do
\begin{code}
      genr index 
      genr dum = ((index-1)%30) = 0
\end{code}

\subsection{Lags and differences}
\label{panel-lagged}

If the time periods are evenly spaced you may want to use lagged
values of variables in a panel regression; you may also with to
construct first differences of variables of interest.

Once a dataset is properly identified as a panel, \app{gretl} will
handle the generation of such variables correctly.  For example the
command \verb+genr x1_1 = x1(-1)+ will create a variable that contains
the first lag of \verb+x1+ where available, and the missing value code
where the lag is not available.  When you run a regression using such
variables, the program will automatically skip the missing
observations.

\section{Estimation of panel models}

\subsection{Pooled Ordinary Least Squares}
\label{pooled-est}

The simplest estimator for panel data is pooled OLS.  In most cases
this is unlikely to be adequate, but it provides a baseline for
comparison with more complex estimators.

If you estimate a model on panel data using OLS an additional test
item becomes available.  In the GUI model window this is the item
``panel diagnostics'' under the \textsf{Tests} menu; the script
counterpart is the \cmd{hausman} command.

To take advantage of this test, you should specify a model without any
dummy variables representing cross-sectional units.  The test compares
pooled OLS against the principal alternatives, the fixed effects and
random effects models.  These alternatives are explained in the
following section.

\subsection{The fixed and random effects models}
\label{panel-est}

In \app{gretl} version 1.5.2 and higher, the fixed and random effects
models for panel data can be estimated in their own right.  In the
graphical interface these options are found under the menu item
``Model/Panel/Fixed and random effects''.  In the command-line
interface one uses the \cmd{panel} command, with or without the
\verb+--random-effects+ option.

This section explains the nature of these models and comments on their
estimation via \app{gretl}.

The pooled OLS specification may be written as 
\begin{equation}
\label{eq:pooled}
y_{it} = X_{it}\beta + u_{it}
\end{equation}
where $y_{it}$ is the observation on the dependent variable for
cross-sectional unit $i$ in period $t$, $X_{it}$ is a $1\times k$
vector of independent variables observed for unit $i$ in period $t$,
$\beta$ is a $k\times 1$ vector of parameters, and $u_{it}$ is an error
or disturbance term specific to unit $i$ in period $t$.

The fixed and random effects models have in common that they decompose
the unitary pooled error $u_{it}$.  For the \textsl{fixed effects}
model we can write $u_{it} = \alpha_i + \varepsilon_{it}$, yielding
\begin{equation}
\label{eq:FE}
y_{it} = X_{it}\beta + \alpha_i + \varepsilon_{it}
\end{equation}
That is, we decompose $u_{it}$ into a unit-specific and time-invariant
component, $\alpha_i$, and an observation-specific error,
$\varepsilon_{it}$.  The $\alpha_i$s are then treated as fixed
parameters (in effect, unit-specific $y$-intercepts), which are to be
estimated.  This can be done by including a dummy variable for each
cross-sectional unit (and suppressing the global constant).
Alternatively, one can proceed by subtracting the group or unit mean
from each of variables and estimating a model without a constant.  In
the latter case the dependent variable may be written as
\[
\tilde{y}_{it} = y_{it} - \bar{y}_i
\]
The ``group mean'', $\bar{y}_i$, is defined as
\[
\bar{y}_i = \frac{1}{T_i} \sum_{t=1}^{T_i} y_{it}
\]
where $T_i$ is the number of observations for unit $i$.  An exactly
analogous formulation applies to the independent variables.  Given
parameter estimates, $\hat{\beta}$, obtained using such de-meaned data
we can recover estimates of the $\alpha_i$s using
\[
\hat{\alpha}_i = \frac{1}{T_i} \sum_{t=1}^{T_i} 
   \left(y_{it} - X_{it}\hat{\beta}\right)
\]

These two methods (using dummy variables, and using de-meaned data) are
numerically equivalent, and \app{gretl} chooses between them based on
the number of cross-sectional units and the number of independent
variables (with the objective of using the minimum computer memory).

For the \textsl{random effects} model we write $u_{it} = v_i +
\varepsilon_{it}$, so the model becomes
\begin{equation}
\label{eq:RE}
y_{it} = X_{it}\beta + v_i + \varepsilon_{it}
\end{equation}
In this context the $v_i$s are not treated as fixed parameters, but as
random drawings from a given probability distribution.  

The celebrated Gauss--Markov theorem, according to which OLS is the
best linear unbiased estimator (BLUE), depends on the assumption that
the error term is independently and identically distributed (IID).  In
the panel context, the IID assumption means that $E(u_{it}^2)$, in
relation to equation~\ref{eq:pooled}, equals a constant for all $i$
and $t$, while the covariance $E(u_{it}, u_{is})$ equals zero for all
$s \neq t$ and the covariance $E(u_{it}, u_{jt})$ equals zero for all
$j \neq i$.

If these assumptions are not met --- and they are unlikely to be met
in the context of panel data --- OLS is not the most efficient
estimator.  Greater efficiency may be gained using generalized least
squares (GLS), taking into account the covariance structure of the
error term.  To implement this approach we need to start by estimating
the respective variances of $v_i$ and $\varepsilon_{it}$.  The
variance of $v_i$, namely $\sigma^2_v$, is sometimes called the
``between'' variance (since it refers to variation between the
cross-sectional units), and $\sigma^2_{\varepsilon}$ is called the
``within'' variance.

TO BE CONTINUED

Which panel method should one use, fixed effects or random effects?

One way of answering this question is in relation to the nature of the
data set.  If the panel comprises observations on a fixed and
relatively small set of units of interest (say, the member states of
the European Union), there is a presumption in favor of fixed effects.
If it comprises observations on a large number of randomly selected
individuals (as in many epidemiological and other longitudinal
studies), there is a presumption in favor of random effects.

Besides this general heuristic, however, various statistical
issues must be taken into account.

\begin{enumerate}

\item Some panel data sets contain variables whose values are specific
  to the cross-sectional unit but which do not vary over time.  If you
  want to include such variables in the model, the fixed effects
  option is simply not available.  When the fixed effects approach is
  implemented using dummy variables, the symptom is that the
  time-invariant variables are perfectly collinear with the per-unit
  dummies.  When using the approach of subtracting the group means,
  the symptom is that after de-meaning these variables are nothing but
  zeros.
\item A somewhat analogous prohibition applies to the random effects
  model.  To appreciate this point it is necessary to note that the
  random effects estimator is a matrix-weighted average of the
  ``within'' estimator (that is, the fixed effects estimator, which
  sweeps out between-unit variation by including per-unit dummy
  variables or by subtracting the unit means) and the ``between''
  estimator.  The latter estimator, which is generally of little
  interest other than as a stepping-stone, is expressed in terms of
  the unit means:
\[
\bar{y}_i = \bar{X}_i \beta + e_i
\]
The point is that if one has observations on $m$ units and $k$
independent variables of interest, then if $k>m$ the ``between''
estimator is undefined --- since we have only $m$ effective
observations --- and hence so is the random effects estimator.

\item If one does not fall foul of one or other of the prohibitions
  mentioned above, the choice between fixed effects and random effects
  may be expressed in terms of the two econometric
  \textit{desiderata}, efficiency and consistency.  The short story is
  that the random effects model is consistent only under more
  stringent conditions than the fixed effects model, but if these
  conditions are satisfied the random effects model offers greater
  efficiency.

\end{enumerate}


\section{Illustration: the Penn World Table}
\label{PWT}

The Penn World Table (homepage at
\href{http://pwt.econ.upenn.edu/}{pwt.econ.upenn.edu}) is a rich
macroeconomic panel dataset, spanning 152 countries over the years
1950--1992.  The data are available in \app{gretl} format; please see
the \app{gretl}
\href{http://gretl.sourceforge.net/gretl_data.html}{data site} (this
is a free download, although it is not included in the main
\app{gretl} package).

Example \ref{examp-pwt} opens \verb+pwt56_60_89.gdt+, a subset
of the PWT containing data on 120 countries, 1960--89, for 20
variables, with no missing observations (the full data set, which is
also supplied in the pwt package for \app{gretl}, has many missing
observations). Total growth of real GDP, 1960--89, is calculated for
each country and regressed against the 1960 level of real GDP, to see
if there is evidence for ``convergence'' (i.e.\ faster growth on the
part of countries starting from a low base).

\begin{script}[htbp]
  \caption{Use of the Penn World Table}
  \label{examp-pwt}
\begin{code}
          open pwt56_60_89.gdt 
          # for 1989 (the last obs), lag 29 gives 1960, the first obs 
          genr gdp60 = RGDPL(-29) 
          # find total growth of real GDP over 30 years
          genr gdpgro = (RGDPL - gdp60)/gdp60
          # restrict the sample to a 1989 cross-section 
          smpl --restrict YEAR=1989 
          # convergence: did countries with a lower base grow faster?  
          ols gdpgro const gdp60 
          # result: No! Try an inverse relationship?
          genr gdp60inv = 1/gdp60 
          ols gdpgro const gdp60inv 
          # no again.  Try treating Africa as special? 
          genr afdum = (CCODE = 1)
          genr afslope = afdum * gdp60 
          ols gdpgro const afdum gdp60 afslope 
\end{code}
\end{script}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "gretl-guide"
%%% End: 

