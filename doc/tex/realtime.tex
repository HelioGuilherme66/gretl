\chapter{Realtime data}
\label{chap:realtime}

\section{Introduction}
\label{sec:realtime-intro}

As of \app{gretl} version 1.9.13 the \cmd{join} command (see chapter
\ref{chap:join}) has been enhanced to deal with so-called realtime
datasets in a straightforward manner.  Such datasets contain
information on when the observations in a time series were actually
published by statistical agencies and how they have been revised over
time. Probably the most popular sources of such data are the
``Alfred'' online database at the St. Louis Fed
(\url{http://alfred.stlouisfed.org/}) and the OECD's
\textsf{StatExtracts} site \url{http://stats.oecd.org/}.  The
description in this section covers the case of a typical file
downloaded from Alfred, but should be easy to adapt to files with a
slightly different format.

As already stated, \cmd{join} requires a column-oriented plain text
file, where the columns may be separated by commas, tabs, spaces or
semicolons. Alfred and the OECD provide the option to download
realtime data in this format (tab-delimited files from Alfred,
comma-delimited from the OECD). If you have a realtime dataset in a
spreadsheet file you must export it to a delimited text file before
using it with \cmd{join}.

Representing revision histories is more complex than just storing a
standard time series, because for each observation period you have in
general more than one published value over time, along with the
information on when each of these values were valid or
current. Sometimes this is represented in spreadsheets with two time
axes, one for the observation period and another one for the
publication time or ``vintage''. The filled cells then form something
resembling an upper triangle (or a ``guillotine'' blade shape with
four corners, if the publication dates do not reach back far enough to
complete the triangle).

\subsection{Atomic format for realtime data}

However, this format is not optimal for automatic processing, and
\app{gretl}'s \cmd{join} works best instead with files that are in
what we call the ``atomic format'', which is exactly the format used
by Alfred if you choose the option ``Observations by Real-Time
Period'', and the the OECD if (FIXME conditional?). A file in atomic
format contains one actual data-point per line, together with some
associated metadata. This is illustrated in Table~\ref{tab:atomic},
where we show the first three lines from an Alfred file and an OECD
file (slightly modified).\footnote{In the Alfred file we have used
  commas rather than tabs as the column delimiter; in the OECD example
  we have shortened the name in the \texttt{Variable} column.}

\begin{table}[htbp]
\begin{center}
Alfred: monthly US industrial production
\begin{code}
observation_date,INDPRO,realtime_start_date,realtime_end_date
1960-01-01,112.0000,1960-02-16,1960-03-15
1960-01-01,111.0000,1960-03-16,1961-10-15
\end{code}
OECD: monthly UK industrial production
\begin{code}
Country,Variable,Frequency,Time,Edition,Value,Flags
"United Kingdom","INDPRO","Monthly","Jan-1990","February 1999",100,
"United Kingdom","INDPRO","Monthly","Feb-1990","February 1999",99.3,
\end{code}
\end{center}
\caption{Variant atomic formats for realtime data}
\label{tab:atomic}
\end{table}

Consider the first data line in the Alfred file: in the
\verb|observation_date| column we find \texttt{1960-01-01}, indicating
that the data-point on this line, namely 112.0, is an observation or
measurement (in this case, of the US index of industrial production)
that refers to the period starting on January 1st 1960. The
\verb|realtime_start_date| value of \texttt{1960-02-16} tells us that
this value was published on February 16th 1960, and the
\verb|realtime_end_date| value says that this vintage remained current
until (and including) March 15th 1960. On the next day (as we can see
from the following line) the data-point was revised slightly downward
to 111.0.

Daily dates in Alfred files are given in a format that follows the
relevant ISO standard, \texttt{YYYY-MM-DD}, but below we describe how
to deal with differently formatted dates. Note that daily dates are
appropriate for the last two columns, which jointly record the
interval over which a given data vintage was current. Daily dates may
be, in a sense, overly precise for the first column, since the data
period may well be the year, quarter or month (as it is in fact
here). However, following Alfred's practice it is OK to specify the
first day in the period even for non-daily data.\footnote{Notice that
  this implies that in the Alfred example above it is not clear
  without further information whether the observation period is the
  first quarter of 1960, the month January 1960, or the day January
  1st 1960.  However, we assume that this information is always
  available in context.}

Compare the first data line of the OECD example. There's a greater
amount of leading metadata, which is left implicit in the Alfred
file. Here \texttt{Time} is the equivalent of Alfred's
\verb|observation_date|, and \texttt{Edition} the equivalent of
Alfred's \verb|realtime_start_date|. So we read that in February 1999
a value of 100 was current for the UK index of industrial production
for January 1990, and from the next line we see that in the same month
a value of 99.3 was current for industrial production in February
1990.

Besides the different names and ordering of the columns, there are a
few more substantive differences between Alfred and OECD files, most
of which are irrelevant for \texttt{join} but some of which are
(possibly) relevant.

The first (irrelevant) difference is the ordering of the lines. It
appears (though we're not sure how consistent this is) that in Alfred
files the lines are sorted by observation date first and then by
publication date---so that all revisions of a given observation are
grouped together---while OECD files are sorted first by revision date
(\texttt{Edition}) and then by observation date (\texttt{Time}). If we
want the next revision of UK industrial production for January 1990 in
the OECD file we have to scan down several lines until we find
\begin{code}
"United Kingdom","INDPRO","Monthly","Jan-1990","March 1999",100,
\end{code}
This difference is basically irrelevant because \texttt{join} can
handle the case where the lines appear in random order, although some
operations can be coded more conveniently if we're able to assume
chronological ordering (either on the Alfred or the OECD pattern, it
doesn't matter).

The second (also irrelevant) difference is that the OECD seems to
include periodic ``Edition'' lines even when there is no change from
the previous value (as illustrated above, where the UK industrial
production index for January 1990 is reported as 100.0 as of March
1999, the same value that we saw to be current in February 1999),
while Alfred reports a new value only when it differs from what was
previously current.

A third difference lies in the dating of the revisions or editions.
As we have seen, Alfred gives a specific daily date while (in the UK
industrial production file at any rate), the OECD just dates each
edition to a month. This is not necessarily relevant for
\texttt{join}, but it does raise the question of whether the OECD
might date revisions to a finer granularity in some of their files, in
which case one would have to be on the lookout for a different date
format.

The final difference is that Alfred supplies an ``end date'' for each
data vintage while the OECD supplies only a starting date. In relation
to OECD files it is safe to assume that each vintage remains valid
until it is superceded by the next, explicitly stated, value. With the
Alfred files, however, it seems possible in principle that a given
vintage could ``expire'' some time before the next vintage becomes
available.

What could this mean? Well, it is possible that a given data-point has
a definite value up to time $t$, but at $t+1$ it turns to
``missing''. This might be the consequence of a change in the
calculation method for the variable in question: the method has
changed but, over some interval, the value according to the new method
has not yet been prepared. We might expect that if this were to happen
a new revision with a value of \texttt{NA} would be published, and
recorded in the relevant realtime file. However, another way of
representing this situation might be to show one revision as ending at
$t$ and the next starting at $t+k$ (where $k$ is greater than one
day), leaving a ``hole'' in the realtime file to represent,
implicitly, a period during which the value is missing. We'll consider
the consequences of this below.

\section{Getting a certain data vintage }

Probably the most common real-world application of realtime data is to
``travel back in time'' and retrieve the data that were available as
of a certain day in the past. For example, suppose that your variable
is named \texttt{INDPRO}, the path to the source file is available in
the string variable \texttt{fname} (for example
\verb|fname = "C:/Users/yourname/Downloads/INDPRO.txt"|) and that you
want to check the status quo on June 15th 2011; this can be achieved
as follows.

First, we create a new empty dataset (taking care that our string
variable \texttt{fname} is not erased, by the \option{preserve}
option), and knowing that we are dealing with monthly data, we
structure the dataset accordingly. For example,
\begin{code}
nulldata 132 --preserve
setobs 12 2004:01
\end{code}

Alternatively, we could of course start from an existing dataset of
monthly frequency.

Then, we import the data vintage 2011-06-15 into our dataset
with \cmd{join}, arbitrarily choosing the self-explanatory series
identifier of \texttt{ip\_asof\_20110615}.

\begin{code}
join @fname ip_asof_20110615 --tkey=observation_date --data=INDPRO \
--timecols="realtime_start_date,realtime_end_date" \
--filter="realtime_start_date<=20110615 && \
(missing(realtime_end_date) || 20110615<=realtime_end_date)"
\end{code}

Here some detailed explanations of the various options are warranted: 
\begin{itemize}
\item The \option{tkey} option specifies the column which should be
  treated as holding the observation period identifiers that are going
  to be matched against the periods in the current \app{gretl}
  dataset.\footnote{Strictly speaking, using \option{tkey} is
    unnecessary in this example because we could just have relied on the
    default, which is to use the first column in the source file for
    the periods. However, being explicit is often a good idea.}  The
  more general form of this option is \option{tkey="colname,format"}
  (note the double quotes here), so if the dates do not come in 
  standard (e.g., ISO) format, we can instruct \app{gretl} how to
  parse them by using the appropriate format description strings as
  shown in Table \ref{tab:join-datefmt}.  For example, here we could
  also have written \option{tkey="observation\_date,\%Y-\%m-\%d"}.
\item Next, \option{data=INDPRO} tells \app{gretl} that we want to
  retrieve the entries stored in the column named \texttt{INDPRO}.
\item The \option{timecols} option can be used to select certain
  columns in the right-hand datafile for special treatment: strings
  representing dates in these columns will be converted, if possible,
  to numerical values: 8-digit numbers on the pattern
  \texttt{YYYYMMDD} (ISO 8601 ``basic'' format for daily dates). We'll
  need this for the next option, \option{filter}, since this
  transformation to numerical values makes it possible to perform some
  basic date arithmetic.

  By default---which applies to files from Alfred---it is assumed that
  the strings in the selected columns are in ISO 8601 ``extended''
  format, \texttt{YYYY-MM-DD}. If that is not the case you can supply
  a time-format string using the \option{timecol-fmt} option, which
  then applies to all the columns included in the \option{timecols}
  option. For example, you would specify the US-style daily date
  format as \option{timecol-fmt="\%m/\%d/\%Y"} while quarterly
  date-strings on the pattern of \texttt{2004q1} may be handled via
  \option{timecol-fmt="\%Yq\%q"}. Any entry in the specified columns
  that does not adhere to the specified date format will be converted
  to a missing value (NA).

  Note that on successful conversion, the output is always in
  daily-date form as stated above. If the user specifies a monthly or
  quarterly time format, the converted date is the first day of the
  month or quarter.
\item The \option{filter} option specification is the central piece of
  our data retrieval; notice how we use the date constant 20110615 in
  basic ISO form to do numerical comparisons. It would also have been
  possible to predefine a numerical variable, as in
 \begin{code}
   vintagedate = 20110615
 \end{code}
 and then use \texttt{vintagedate} in the \cmd{join} command instead.

  Here we tell \cmd{join} that we only want to extract those
  publications that (1) already appeared before June 15th 2011, and
  (2) were not yet obsolete on that day. The second condition is
  complicated by the fact that Alfred uses a convention by which, in
  the atomic file, the line containing the latest vintage contains a
  missing value for \texttt{realtime\_end\_date}. The syntax shown in
  the example handles the case when the relevant publication is the
  latest available one. In practice, the following simpler filtering
  plus aggregation operation that only involves the
  \verb|realtime_start_date| column will typically be equivalent for
  well-formed source files: 
\end{itemize}
\begin{code}
  --filter="realtime_start_date<=20110615" --aggr=max(realtime_start_date)
\end{code}
As a result, your dataset will now contain a time series named
\verb|ip_asof_20110615| with the values that a researcher would have
had available on June 15th 2011. Of course, all values for the
observations after June 2011 will be missing (and probably a few
before that, too), because they only have become available later on.

\section{Getting the $n$-th release for each observation period}

For some purposes it may also be useful to retrieve the $n$-th published
value of each observation, where $n$ is a fixed positive integer,
irrespective of \emph{when} each of these $n$-th releases was
published. Suppose we are interested in the third release, then the
relevant \cmd{join} command becomes:
\begin{code}
  join @fname ip_3rdpub --tkey=observation_date --data=INDPRO --aggr="seq:3"
\end{code}
Since we do not need the \verb|realtime_start_date| and
\verb|realtime_end_date| information for this retrieval, we have
dropped the \option{timecols} option here. Note that this formulation
assumes that the source file is ordered chronologically, otherwise
using the \option{aggr="seq:3"} option, i.e.\ to retrieve the third
entries in each sequence of matches could have yielded a result
different from the one intended. This assumption, however, holds for
Alfred files and is probably rather safe in general.

The values of the variable imported as \texttt{ip\_3rdpub} in this way
were published at different dates, so the variable is effectively a
mix of different vintages. Depending on the type of variable, this may
also imply drastic jumps in the values; for example, index numbers are
regularly rebased to different base periods. This problem also carries
over to inflation-adjusted economic variables, where the base period
of the price index changes over time. Mixing vintages in general also
means mixing different scales in the output, with which you would have
to deal appropriately.\footnote{Some user-contributed functions may be
  available that address this issue, but it is beyond the scope of the
  user guide here. Another even more complicated issue in the realtime
  context is that of ``benchmark revisions'' applied by statistical
  agencies, where the underlying definition or composition of a
  variable changes on some date, which goes beyond a mere
  rescaling. However, this type of structural change is not, in
  principle, a feature of realtime data alone, but applies to any
  time-series data.}


\section{Getting the values at a fixed lag after the observation
  period}

New data releases may (and in general, will) take place literally on
any day of the month. However, if you are working with, say, monthly
or quarterly data you may sometimes want to adjust the granularity of
your realtime axis to a monthly or quarterly frequency. For example,
in order to analyse the data revision process of monthly industrial
production you might be interested in the extent of revisions between
the data published two and three months after each observation period.

This is not trivial, however, and our solution here is to first create
various auxiliary variables in the \app{gretl} dataset, also using
information from the source file. This implies that several passes of
\cmd{join} will be needed. First we will need a variable which for
each observation period in the dataset describes the (daily) target
date $n$ periods after the observation period. This boils down to an
exercise in manipulating calendar date numbers, making use of the
\app{gretl} accessors \dollar{obsmajor}, \dollar{obsminor},
\dollar{obsmicro}, as well as the periodicity indicator \dollar{pd}.
To facilitate reuse we wrap this in the function
\texttt{getLagdate()}, which we treat as a blackbox here (but see the
code at the end of the chapter).

So for the example case when $n=2$ we will construct the first
auxiliary variable holding the target dates in basic ISO format
(numerical, without hyphens) by calling:
\begin{code}
series targetdate = getLagdate(2)
\end{code}
But because we cannot use variables from the current dataset directly
in a \cmd{join} filter we first have to import essentially all the realtime
information into our dataset. 

The first step is to determine the number of needed passes, that is,
what is the maximum number of available releases for a single observation
period. The following straightforward \cmd{join} helps to achieve this,

\begin{code}
join @fname counter --tkey=observation_date --aggr=count
\end{code}
followed by:
\begin{code}
scalar maxpubs = max(counter)
\end{code}

Next we load all the information into the dataset, which may be
somewhat inefficient but should not be a problem unless you are
dealing with truly huge data (which is unlikely with macroeconomic
data---the following loop with three joins and 27 iterations through
a file with roughly 24000 lines, 1MB, takes roughly 25 seconds on a
reasonably fast 2013 vintage laptop). As a bonus, we demonstrate how
to abbreviate reused option parts with string variables:
\begin{code}
  # recycled options (note the escaped quotes)
  sprintf baseopt "--tkey=observation_date \
    --timecols=\"realtime_start_date,realtime_end_date\"
  loop i=1..maxpubs -q
    # get the starting and end dates of the i-th publication          
    join @fname sd$i --data=realtime_start_date @baseopt --aggr=seq:$i
    join @fname ed$i --data=realtime_end_date @baseopt --aggr=seq:$i
    join @fname rel$i --data=INDPRO @baseopt --aggr=seq:$i
  endloop
\end{code}
For the remaining operations we can work purely within the \app{gretl}
dataset, which is rather fast. Looping again through the various
releases we check observation by observation (but using the
convenient and fast functions available for series) whether the target
date falls between the realtime start and end dates, i.e.\ whether the
respective release was current on the day $n$ periods after the
observation period.
\begin{code}
series after2 = NA # initialize
loop i=1..maxpubs -q
    # record whether the i-th release was already available
    series tmp_s = misszero((sd$i <= targetdate))

    # record whether the i-th release was not already obsolete
    series tmp_e = misszero((missing(ed$i) || (ed$i >= targetdate)))

    # whenever both criteria are met, copy the values
    series after2 = (tmp_s && tmp_e) ? rel$i : after2
endloop
\end{code}
A caveat to remember is that here, too, the problem of base period
changes is likely to appear, because again we are mixing vintages.

\section{Functions}
\begin{scode}
function series getLagdate(int n) 
    /* 
      Construct the series of relevant daily dates, for the first day 
      in the n-th period after the observation period; in principle 
      should cover yearly, quarterly, monthly, and daily frequencies 
      (not weekly).
    */

    if $pd==1                 # yearly
        # (trailing 0101 for January 1st)
        series lagdate = ($obsmajor + n)*10000 + 101
    elif ($pd==4 || $pd==12)  # quarterly or monthly
        # how many year numbers to add
        series tempminor = $obsminor + n
        series yrsplus = floor((tempminor - 1) / $pd)
        # and the resulting quarter/month number
        series pubminor = tempminor - (yrsplus * $pd) 
        # construct the corresp. month, different for quarterly
        series month = ($pd==4) ? (pubminor-1)*3 + 1 : pubminor
        # put everything together
        series lagdate = ($obsmajor+yrsplus)*10000 + month*100 + 1
    elif ($pd>=5 && $pd<=7)    # daily
        series lagdate = isodate(epochday($obsmajor, $obsminor, $obsmicro) + n)
    endif

return lagdate
end function
\end{scode}
