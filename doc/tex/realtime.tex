\chapter{Realtime data}
\label{chap:realtime}

\section{Introduction}
\label{sec:realtime-intro}

As of \app{gretl} version 1.9.13 the \cmd{join} command has been
enhanced to deal with so-called realtime datasets in a straightforward
manner.  Such datasets contain information on when the observations in
a time series were actually published by statistical agencies and how
they have been revised over time. Probably the most popular source of
such data is the ``Alfred'' online database at the St. Louis Fed
(\url{http://alfred.stlouisfed.org/}); there are similar offerings at
the OECD.  The description in this section covers the case of a
typical file downloaded from Alfred, but should be easy to adapt to
files with a slightly different format.

From the general description of \cmd{join} it is clear that it needs a
plain column-oriented text file, in which columns may be separated by
commas (the most common choice), tabs, spaces or semicolons. Alfred
provides the option to retrieve realtime data directly in such files
(which are tab-delimited); if for some reason you only have a
spreadsheet file of your dataset available you would need to export it
to a plain text file first.

Representing revision histories is more complex than just storing a
standard time series, because for each observation period you have in
general more than one published value over time, along with the
information on when each of these values were valid or
current. Sometimes this is represented in spreadsheets with two time
axes, one for the observation period and another one for the
publication time (``vintage''). The filled cells then form something
resembling an upper triangle (or a ``guillotine'' blade shape with
four corners, if the publication dates do not reach back far enough to
complete the triangle).

However, this format is not optimal for automatic processing, and
\app{gretl}'s \cmd{join} works best instead with files that are in
what we call the ``atomic format'', which is exactly Alfred's format
if you choose the option ``Observations by Real-Time Period''. A file
in atomic format contains one individual datum (one number) per line,
together with some meta-information about it; a typical line of such a
file along with the header line looks as follows, where \texttt{vname} is a
placeholder for the actual name of a variable (such as \texttt{INDPRO}
for industrial production):

\begin{code}
observation_date;vname;realtime_start_date;realtime_end_date
...

2010-04-01; 98.34; 2011-06-14; 2012-08-02
...
\end{code}

What does the data line tell us? It says that for the period that
started on April 1st 2010 the value for the variable in question that
was published on June 14th 2011 was 98.34.\footnote{\app{Gretl}
  supports the use of other decimal separators here, too, not just the
  dot. As always, this must not interfere with the choice of the
  column separator.} And this data vintage remained in effect until
(and including) August 2nd 2012, when it was somehow invalidated,
perhaps by a newly revised value which would be stored in another line
of the same file (most likely, further down).

The daily dates in the line are given in a format that follows the
relevant ISO standard, \texttt{YYYY-MM-DD}, but below we also describe
how to deal with differently formatted dates. Note that having daily
dates is required for the last two columns, which record the interval
over which that data vintage was current. Daily dates are not
mandatory for the first column defining the observation or reference
period (the period to which the data \textit{refer}), since the
frequency may well be annual, quarterly or monthly. However, following
Alfred's practice it is OK to specify the first day in the period even
for non-daily data. Notice that this implies that in the example above
it is not clear without further information whether the observation
period is the second quarter of 2010, the month April 2010, or the
day April 1st 2010.  However, we assume that in practice this meta
information is always available.

We will use Alfred's column names in our code examples, but of course
these exact column header names are not mandatory in general, as long
as you adapt your code accordingly. Also, it is perhaps noteworthy
that these files are essentially tables structured according to
relational-database concepts, so in principle they could be handled
with SQL-based tools instead of using \app{gretl}'s \cmd{join}
command.

\section{Getting a certain data vintage }

The most widespread real-world application of realtime data would be
to ``travel back in time'' and retrieve the published data that were
available on a certain day in the past. For example, suppose that your
variable is named \texttt{INDPRO}, the path to the source file is
available in the string variable \texttt{fname} (for example
\verb|fname = "C:/Users/yourname/Downloads/INDPRO.txt"|) and that you
want to check the status quo on June 15th 2011; this can be achieved
as follows.

First, we create a new empty dataset (taking care that our string
variable \texttt{fname} is not erased, by the \option{preserve}
option), and knowing that we are dealing with monthly data, we
structure the dataset accordingly. For example,
\begin{code}
nulldata 132 --preserve
setobs 12 2004:01
\end{code}

Alternatively, we could of course start from an existing dataset of
monthly frequency.

Then, we import the data vintage 2011-06-15 into our dataset
with \cmd{join}, arbitrarily choosing the self-explanatory series
identifier of \texttt{ip\_asof\_20110615}.

\begin{code}
join @fname ip_asof_20110615 --tkey=observation_date \
--data=INDPRO --timecols="realtime_start_date,realtime_end_date"
--filter="realtime_start_date<=20110615 && \
(missing(realtime_end_date) || 20110615<=realtime_end_date)"
\end{code}

Here some detailed explanations of the various options are warranted: 
\begin{itemize}
\item The \option{tkey} option specifies the column which should be
  treated as holding the observation period identifiers that are going
  to be matched against the periods in the current \app{gretl}
  dataset.\footnote{Strictly speaking, using \option{tkey} is
    unnecessary in this example because we could just have relied on the
    default, which is to use the first column in the source file for
    the periods. However, being explicit is often a good idea.}  The
  more general form of this option is \option{tkey="colname,format"}
  (note the double quotes here), so if the dates do not come in 
  standard (e.g., ISO) format, we can instruct \app{gretl} how to
  parse them by using the appropriate format description strings as
  shown in Table \ref{tab:join-datefmt}.  For example, here we could
  also have written \option{tkey="observation\_date,\%Y-\%m-\%d"}.
\item Next, \option{data=INDPRO} tells \app{gretl} that we want to
  retrieve the entries stored in the column named \texttt{INDPRO}.
\item The \option{timecols} option can be used to select certain
  columns in the right-hand datafile for special treatment: strings
  representing dates in these columns will be converted, if possible,
  to numerical values: 8-digit numbers on the pattern
  \texttt{YYYYMMDD} (ISO 8601 ``basic'' format for daily dates). We'll
  need this for the next option, \option{filter}, since this
  transformation to numerical values makes it possible to perform some
  basic date arithmetic.

  By default---which applies to files from Alfred---it is assumed that
  the strings in the selected columns are in ISO 8601 ``extended''
  format, \texttt{YYYY-MM-DD}. If that is not the case you can supply
  a time-format string using the \option{timecol-fmt} option, which
  then applies to all the columns included in the \option{timecols}
  option. For example, you would specify the US-style daily date
  format as \option{timecol-fmt="\%m/\%d/\%Y"} while quarterly
  date-strings on the pattern of \texttt{2004q1} may be handled via
  \option{timecol-fmt="\%Yq\%q"}. Any entry in the specified columns
  that does not adhere to the specified date format will be converted
  to a missing value (NA).

  Note that on successful conversion, the output is always in
  daily-date form as stated above. If the user specifies a monthly or
  quarterly time format, the converted date is the first day of the
  month or quarter.
\item The \option{filter} option specification is the central piece of
  our data retrieval; notice how we use the date constant 20110615 in
  basic ISO form to do numerical comparisons. It would also have been
  possible to predefine a numerical variable, as in
 \begin{code}
   vintagedate = 20110615
 \end{code}
 and then use \texttt{vintagedate} in the \cmd{join} command instead.

  Here we tell \cmd{join} that we only want to extract those
  publications that (1) already appeared before June 15th 2011, and
  (2) were not yet obsolete on that day. The second condition is
  complicated by the fact that Alfred uses a convention by which, in
  the atomic file, the line containing the latest vintage contains a
  missing value for \texttt{realtime\_end\_date}. The syntax shown in
  the example handles the case when the relevant publication is the
  latest available one. In practice, the following simpler filtering
  plus aggregation operation that only involves the
  \verb|realtime_start_date| column will typically be equivalent for
  well-formed source files: 
\end{itemize}
\begin{code}
  --filter="realtime_start_date<=20110615" \
  --aggr=max(realtime_start_date)
\end{code}
As a result, your dataset will now contain a time series named
\verb|ip_asof_20110615| with the values that a researcher would have
had available on June 15th 2011. Of course, all values for the
observations after June 2011 will be missing (and probably a few
before that, too), because they only have become available later on.

\section{Getting the $n$-th release for each observation period}

For some purposes it may also be useful to retrieve the $n$-th published
value of each observation, where $n$ is a fixed positive integer,
irrespective of \emph{when} each of these $n$-th releases was
published. Suppose we are interested in the third release, then the
relevant \cmd{join} command becomes:
\begin{code}
  join @fname ip_3rdpub --tkey=observation_date --data=INDPRO \
    --aggr="seq:3"
\end{code}
Since we do not need the \verb|realtime_start_date| and
\verb|realtime_end_date| information for this retrieval, we have
dropped the \option{timecols} option here. Note that this formulation
assumes that the source file is ordered chronologically, otherwise
using the \option{aggr="seq:3"} option, i.e. to retrieve the third
entries in each sequence of matches could have yielded a result
different from the one intended. This assumptions, however, holds for
Alfred files and is rather safe to assume generally.

As a result of this operation, the values of the variable imported as
\texttt{ip\_3rdpub} were published at different dates, so the variable
is effectively a mix of different vintages. Depending on the type of
variable, this may also imply drastic jumps in the values; for
example, index numbers are regularly rebased to different base
periods. This problem also carries over to inflation-adjusted economic
variables, where the base period of the price index changes over
time. Mixing vintages in general also means mixing different scales in
the output, with which you would have to deal
appropriately.\footnote{Some user-contributed functions may be
  available that address this issue, but it is beyond the scope of the
  user guide here. Another even more complicated issue in the realtime
  context is that of ``benchmark revisions'' applied by statistical
  agencies, where the underlying definition or composition of a
  variable changes on some date, which goes beyond a mere
  rescaling. However, this type of structural is not, in principle, a
  feature of real-time datsets, but applies to any timeseries data
  set.}


\section{Getting the values at a fixed lag after the observation
  period}

New data releases may (and in general, will) take place literally on
any day. However, if you are working with, say, monthly or quarterly
data, you will sometimes want to adjust also your realtime axis
granularity to a monthly or quarterly frequency. For example, in order
to analyse the data revision process of monthly industrial production
you might be interested in the extent of revisions between the data
published two and three months after each observation period.

This is not trivial, however, and our solution here is to first create
various auxiliary variables in the \app{gretl} dataset, also using
information from the source file. This implies that several passes of
\cmd{join} will be needed. First we will need a variable which for
each observation period in the dataset describes the (daily) target
date $n$ periods after the observation period. This boils down to an
exercise in manipulating calendar date numbers, making use of the
\app{gretl} accessors \dollar{obsmajor}, \dollar{obsminor},
\dollar{obsmicro}, as well as the periodicity indicator \dollar{pd}.
To facilitate reuse we wrap this in the function
\texttt{getLagdate()}, which we treat as a blackbox here (but see the
code at the end of the chapter).

So for the example case when $n=2$ we will construct the first
auxiliary variable holding the target dates in basic ISO format
(numerical, without hyphens) by calling:
\begin{code}
series targetdate = getLagdate(2)
\end{code}
But because we cannot use variables from the current dataset directly
in a \cmd{join} filter we first have to import essentially all the realtime
information into our dataset. 

The first step is to determine the number of needed passes, that is,
what is the maximum number of available releases for a single observation
period. The following straightforward \cmd{join} helps to achieve this,

\begin{code}
join @fname counter --tkey=observation_date --aggr=count
\end{code}
followed by:
\begin{code}
scalar maxpubs = max(counter)
\end{code}

Next we load all the information into the dataset, which may be
somewhat inefficient but should not be a problem unless you are
dealing with truly huge data (which is unlikely with macroeconomic
data --- the following loop with three joins and 27 iterations through
a file with roughly 24000 lines, 1MB, takes roughly 25 seconds on a
reasonably fast 2013 vintage laptop). As a bonus, we demonstrate how
to abbreviate reused option parts with string variables:
\begin{code}
  # recycled options (note the escaped quotes)
  sprintf baseopt "--tkey=observation_date \
    --timecols=\"realtime_start_date,realtime_end_date\"
  loop i=1..maxpubs -q
    # get the starting and end dates of the i-th publication          
    join @fname sd$i --data=realtime_start_date @baseopt --seq=$i
    join @fname ed$i --data=realtime_end_date @baseopt --seq=$i
    join @fname rel$i --data=INDPRO @baseopt --seq=$i
  endloop
\end{code}
For the remaining operations we can work purely within the \app{gretl}
dataset, which is rather fast. Looping again through the various
releases we check observation per observation (but using the
convenient and fast functions available for series) whether the target
date falls between the realtime start and end dates, i.e. whether the
respective release was current on the day $n$ periods after the
observation period.
\begin{code}
series after2 = NA # initialize
loop i=1..maxpubs -q
    # record whether the i-th release was already available
    series tmp_s = misszero((sd$i <= targetdate))

    # record whether the i-th release was not already obsolete
    series tmp_e = misszero((missing(ed$i) || (ed$i >= targetdate)))

    # whenever both criteria are met, copy the values
    series after2 = (tmp_s + tmp_e == 2) ? rel$i : after2
endloop
\end{code}
A caveat to remember is that here, too, the problem of base period
changes is likely to appear, because again we are mixing vintages.

\section{Functions}
\begin{scode}
function series getLagdate(int n) 
    /* 
      Construct the series of relevant daily dates, for the first day 
      in the n-th period after the observation period; in principle 
      should cover yearly, quarterly, monthly, and daily frequencies 
      (not weekly).
    */

    if $pd==1                 # yearly
        # (trailing 0101 for January 1st)
        series lagdate = ($obsmajor + n)*10000 + 101
    elif ($pd==4 || $pd==12)  # quarterly or monthly
        # how many year numbers to add
        series tempminor = $obsminor + n
        series yrsplus = floor((tempminor - 1) / $pd)
        # and the resulting quarter/month number
        series pubminor = tempminor - (yrsplus * $pd) 
        # construct the corresp. month, different for quarterly
        series month = ($pd==4) ? (pubminor-1)*3 + 1 : pubminor
        # put everything together
        series lagdate = ($obsmajor+yrsplus)*10000 + month*100 + 1
    elif ($pd>=5 && $pd<=7)    # daily
        series lagdate = isodate(epochday($obsmajor, $obsminor, $obsmicro) + n)
    endif

return lagdate
end function
\end{scode}
