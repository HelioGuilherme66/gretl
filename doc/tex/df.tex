\documentclass{article}
\begin{document}

\newcommand{\subml}{\textrm{\scriptsize ML}}

It's well known that given a sample, $\{x_i\}$, of size $n$ from a
normally distributed population, the Maximum Likelihood (ML) estimator
of the population variance, $\sigma^2$, is
%
\begin{equation}
\label{eq:sigma-hat}
\hat{\sigma}_{\subml}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\end{equation}
%
where $\bar{x}$ is the sample mean, $n^{-1} \sum_{i=1}^n x_i$.
It's also well known that $\hat{\sigma}_{\subml}^2$, while consistent,
is a biased estimator, and is commonly replaced by the ``sample
variance'', namely,
%
\begin{equation}
\label{eq:sample-variance}
s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
\end{equation}

The intuition behind the bias in (\ref{eq:sigma-hat}) is
straightforward.  First, the quantity we seek to estimate is defined
as
%
\[
\sigma^2 = \frac{1}{N} \sum_{i=1}^n (x_i - \mu)^2
\]
%
for a population of size $N$ with mean $\mu$.  Second, $\bar{x}$ is an
estimator of $\mu$; it is easily shown to be the least-squares
estimator, and also (assuming normality) the ML estimator.  It is
unbiased, but is of course subject to sampling error; in any given
sample it is highly unlikely that $\bar{x} = \mu$.  Given that
$\bar{x}$ is the least-squares estimator, the sum of squared
deviations of the $x_i$ from \textit{any} value other than $\bar{x}$
must be greater than the summation in (\ref{eq:sigma-hat}).  But since
$\mu$ is almost certainly not equal to $\bar{x}$, the sum of squared
deviations of the $x_i$ from $\mu$ will surely be greater than the sum
of squared deviations in (\ref{eq:sigma-hat}). It follows that
$\hat{\sigma}_{\subml}^2$ must underestimate the population variance. 

The proof that $s^2$ is indeed the unbiased estimator can be found in
any good statistics textbook, where we also learn that the magnitude
$n-1$ in (\ref{eq:sample-variance}) can be brought under a general
description as the ``degrees of freedom'' of the calculation at
hand. Given $\bar{x}$, the $n$ sample values add only $n-1$ pieces of
information since the $n$th value can always be deduced via the
formula for $\bar{x}$.

This argument generalizes to the calculation of the Standard Error of
the Regression.  Given $n$ residuals, $\hat{u}_i$, calculated from a
regression in which $k$ parameters are estimated, the degrees of
freedom are $n-k$, and the unbiased estimator of the standard
deviation of the disturbance term is
%
\[
\mbox{SER} = \left( \frac{1}{n-k} \sum_{i=1}^n \hat{u}_i^2 \right)^{1/2}
\]

This carries over into the usual calculation of standard errors in the
context of OLS regression.  Writing $\hat{\sigma}$ for SER (and noting
that we no longer mean $\hat{\sigma}_{\subml}$), the ``classical''
variance--covariance matrix of the OLS estimates (assuming that the
disturbances are independently and identically distributed) is given
by
%
\[
V = \hat{\sigma}^2 (X'X)^{-1}
\]
%
where $X$ is an $n\times k$ matrix of regressors. The standard errors
of the parameter estimates, being the square roots of the diagonal
elements of $V$, inherit a ``degrees of freedom correction'' or
df correction from $\hat{\sigma}$.

So far, so good.  Everyone expects df correction in OLS standard
errors, just as we expect division by $n-1$ in the sample variance.




\end{document}

