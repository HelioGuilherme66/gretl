\chapter{GMM estimation}
\label{chap:gmm}

\section{Introduction and terminology}
\label{sec:gmm-intro}

The Generalized Method of Moments (GMM) is a very powerful and general
estimation method, which encompasses practically all the parametric
estimation techniques used in econometrics. It was introduced in
Hansen (1982) and Hansen and Singleton (1982); an excellent and
thorough treatment is given in Davidson and MacKinnon (1993), chapter
17.

The basic principle on which GMM is built is rather straightforward.
Suppose we wish to estimate a scalar parameter $\theta$ based on
a sample $x_1, x_2, \ldots, x_T$.  Let $\theta_0$ indicate the ``true''
value of $\theta$. Theoretical considerations (either of statistical
or economic nature) may suggest that a relationship like the following
holds:
\begin{equation}
  \label{eq:simple}
  E \left[ x_t - g(\theta) \right] = 0 \Leftrightarrow \theta =
  \theta_0 ,
\end{equation}
with $g(\cdot)$ a continuous and invertible function. That is to say,
there exists a function of the data and the parameter, with the
property that it has expectation zero if and only if it is evaluated
at the true parameter value.  For example, economic models with
rational expectations lead to expressions like (\ref{eq:simple}) quite
naturally.

If the sampling model for the $x_t$s is such that some version of the
Law of Large Numbers holds, then
\[
  \bar{X} = \frac{1}{T} \sum_{t=1}^T x_t \convp g(\theta_0) ;
\]
hence, since $g(\cdot)$ is invertible, the statistic
\[
  \hat{\theta} = g^{-1}(\bar{X}) \convp \theta_0 ,
\]
so $\hat{\theta}$ is a consistent estimator of $\theta$. A different
way to obtain the same outcome is to choose, as an estimator of
$\theta$, the value that minimizes the objective function
\begin{equation}
  \label{eq:obj-simple}
  F(\theta) = \left[ \frac{1}{T} \sum_{t=1}^T (x_t  - g(\theta)) \right]^2 =
  \left[ \bar{X} - g(\theta) \right]^2 ;
\end{equation}
the minimum is trivially reached at $\hat{\theta} = g^{-1}(\bar{X})$,
since the expression in square brackets equals 0.

The above reasoning can be generalized as follows: suppose $\theta$ is
an $n$-vector and we have $m$ relations like
\begin{equation}
  \label{eq:general}
  E \left[ f_i(x_t, \theta) \right] = 0 \quad\textrm{for\ } i=1 \ldots
  m ,
\end{equation}
where $E[\cdot]$ is a conditional expectation on a set of $p$
variables $z_t$, called the \emph{instruments}. In the above simple
example, $m=1$ and $f(x_t, \theta) = x_t - g(\theta)$, and the only
instrument used is $z_t = 1$. Then, it must also be true that
\begin{equation}
  \label{eq:oc}
  E \left[ f_i(x_t, \theta) \cdot z_{j,t} \right] = E \left[ f_{i,j,t}(\theta) \right] = 
  0 \quad\textrm{for\ } i=1 \ldots
  m \quad\textrm{and \ } j=1 \ldots p;
\end{equation}
equation (\ref{eq:oc}) is known as an \emph{orthogonality condition},
or \emph{moment condition}. The GMM estimator is defined as the
minimum of the quadratic form
\begin{equation}
  \label{eq:obj-general}
  F(\theta, \Omega) = \bar{\mathbf{f}}' \Omega \bar{\mathbf{f}},
\end{equation}
where $\bar{\mathbf{f}}$ is a $(1 \times m\cdot p)$ vector holding the
average of the orthogonality conditions and $\Omega$ is some symmetric,
positive definite matrix, known as the \emph{weights} matrix. A
necessary condition for the minimum to exist is the order condition $n
\le m \cdot p$. If $n = m \cdot p$, the minimum is zero and the
solution is the same for any $\Omega$.

The statistic
\begin{equation}
  \label{eq:gmmestimator}
  \hat{\theta} = \stackunder{\theta}{\mathrm{Argmin}} F(\theta, \Omega)
\end{equation}
is a consistent estimator of $\theta$ whatever the choice of $\Omega$.
However, to achieve maximum asymptotic efficiency $\Omega$ must be
proportional to the inverse of the long-run covariance matrix of the
orthogonality conditions; if $\Omega$ is not known, a consistent estimator
will suffice.

These considerations lead to the following empirical strategy:
\begin{enumerate}
\item Choose a positive definite $\Omega$ and compute the
  \emph{one-step} GMM estimator $\hat{\theta}_1$. Customary choices
  for $\Omega$ are $I_{m\cdot p}$ or $I_{m} \otimes (Z'Z)^{-1}$.
\item Use $\hat{\theta}_1$ to estimate $V(f_{i,j,t}(\theta))$ and use its
  inverse as the weights matrix. The resulting estimator
  $\hat{\theta}_2$ is called the \emph{two-step} estimator.
\item Re-estimate $V(f_{i,j,t}(\theta))$ by means of $\hat{\theta}_2$ and
  obtain $\hat{\theta}_3$; iterate until convergence. Asymptotically,
  these extra steps are unnecessary, since the two-step estimator is
  consistent and efficient; however, the iterated estimator often has
  better small-sample properties and should be independent of the
  choice of $\Omega$ made at step 1. 
\end{enumerate}

In the special case when the number of parameters $n$ is equal to the
total number of orthogonality conditions $m \cdot p$, the GMM
estimator $\hat{\theta}$ is the same for any choice of the weights
matrix $\Omega$, so the first step is sufficient; in this case, the
objective function is 0 at the minimum. 

If, on the contrary, $n < m \cdot p$, the second step (or successive
iterations) is needed to achieve efficiency, and the estimator so
obtained can be very different, in finite samples, from the one-step
estimator. Moreover, the value of the objective function at the
minimum, suitably scaled by the number of observations, yields
\emph{Hansen's J statistic}; this statistic can be interpreted as a
test statistic that has a $\chi^2$ distribution with $m \cdot p -n $
degrees of freedom under the null hypothesis of correct specification.
See Davidson and MacKinnon (1993), section 17.6 for details.

In the following sections we will show how these ideas are
implemented in \app{gretl} through some examples.

\section{OLS as GMM}
\label{sec:gmm-ols}

It is instructive to start with a somewhat contrived example: consider
the linear model $y_t = x_t \beta + u_t$.  Although most of us
are used to read it as the sum of a hazily defined ``systematic part''
plus an equally hazy ``disturbance'', a more rigorous interpretation
of this familiar expression comes from the \emph{hypothesis} that the
conditional mean $E(y_t|x_t)$ is linear and the \emph{definition} of
$u_t$ as $y_t - E(y_t|x_t)$.

>From the definition of $u_t$, it follows that $E(u_t|x_t) = 0$. 
The following orthogonality condition is therefore available:
\begin{equation}
  \label{eq:oc-ols}
  E \left[ f(\beta) \right] = 0 ,
\end{equation}
where $f(\beta) = (y_t - x_t \beta) x_t$. The definitions given in the
previous section therefore specialize here to:
\begin{itemize}
\item $\theta$ is $\beta$;
\item the instrument is $x_t$;
\item $f_{i,j,t}(\theta)$ is $(y_t - x_t \beta) x_t = u_t
  x_t$; the orthogonality condition is interpretable as the
  requirement that the regressors should be uncorrelated with the
  disturbances;
\item $\Omega$ can be any symmetric positive definite matrix, since
  the number of parameters equals the number of orthogonality
  conditions. Let's say we choose $I$.
\item The function $F(\theta, \Omega)$ is in this case
  \[
    F(\theta, \Omega) = \left[ \frac{1}{T} \sum_{t=1}^T (\hat{u}_t x_t) \right]^2
  \]
  and it is easy to see why OLS and GMM coincide here: the GMM
  objective function has the same minimizer as the objective function
  of OLS, the residual sum of squares. Note, however, that the two
  functions are not equal to one another: at the minimum, $F(\theta,
  \Omega) = 0$ while the minimized sum of squared residuals is zero
  only in the special case of a perfect linear fit.  
\end{itemize}

The code snippet contained in Example~\ref{gmm-ols-ex} uses
\app{gretl}'s \texttt{gmm} command to make the above operational.

\begin{script}[htbp]
  \caption{OLS via GMM}
  \label{gmm-ols-ex}
\begin{code}
/* initialize stuff */
series e = 0
scalar beta = 0
matrix V = I(1)

/* proceed with estimation */
gmm 
  series e = y - x*beta
  orthog e ; x
  weights V
  params beta
end gmm
\end{code}
\end{script}

We feed \app{gretl} the necessary ingredients for GMM estimation
in a command block, starting with \texttt{gmm} and ending with
\texttt{end gmm}. Three elements are compulsory within a \texttt{gmm}
block:
\begin{enumerate}
\item one or more \texttt{orthog} statements
\item one \texttt{weights} statement
\item one \texttt{params} statement
\end{enumerate}
The three elements should be given in the stated order.

The \texttt{orthog} statements are used to specify the orthogonality
conditions.  They must follow the syntax
\begin{code}
  orthog x ; Z
\end{code}
where \texttt{x} must be a vector or a series and \texttt{Z} a series,
list or matrix. In example~\ref{gmm-ols-ex}, the series \texttt{e}
holds the ``residuals'' and the series \texttt{x} holds the regressor.
If \texttt{x} had been a list (a matrix), the \texttt{orthog}
statement would have generated one orthogonality conditon for each
element (column) of \texttt{x}.  Note the structure of the
orthogonality condition: it is assumed that the term to the left of the
semicolon represents a quantity that depends on the estimated
parameters (and so must be updated in the process of iterative
estimation), while the term on the right is a constant function of
the data.

The \texttt{weights} statement is used to specify the initial
weighting matrix and its syntax is straightforward.  The
\texttt{params} statement specifies the parameters with respect to
which the GMM criterion should be minimized; it follows the same logic
and rules as in the \texttt{mle} and \texttt{nls} commands.

The minimum is found through numerical minimization via BFGS (see
section~\ref{genr-numerical} and chapter~\ref{chap:mle}).  The
progress of the optimization procedure can be observed by appending
the \verb|--verbose| switch to the \texttt{end gmm} line.  (In this
example GMM estimation is clearly a rather silly thing to do, since a
closed form solution is easily given by OLS.)

\section{TSLS as GMM}
\label{sec:gmm-tsls}

Moving closer to the proper domain of GMM, we now consider two-stage
least squares (TSLS) as a case of GMM.  

TSLS is employed in the case where one wishes to estimate a linear
model of the form $y_t = X_t \beta + u_t$, but where one or more of
the variables in the matrix $X$ are potentially endogenous ---
correlated with the error term, $u$.  We proceed by identifying a set
of instruments, $Z_t$, which are explanatory for the endogenous
variables in $X$ but which are plausibly uncorrelated with $u$.  The
classic two-stage procedure is (1) regress the endogenous elements of
$X$ on $Z$; then (2) estimate the equation of interest, with the
endogenous elements of $X$ replaced by their fitted values from (1).

An alternative perspective is given by GMM.  We define the residual
$\hat{u}_t$ as $y_t - X_t \hat{\beta}$, as usual.  But instead of
relying on $E(u|X) = 0$ as in OLS, we base estimation on the condition
$E(u|Z) = 0$.  In this case it is natural to base the initial
weighting matrix on the covariance matrix of the instruments.
Example~\ref{gmm-tsls-ex} gives the skeleton of the GMM approach.
(FIXME make this example a complete working script?)

\begin{script}[htbp]
  \caption{TSLS via GMM}
  \label{gmm-tsls-ex}
\begin{code}
series e = 0
matrix beta = ... # initialize the parameter estimates
matrix X = ...    # form the matrix of regressors
matrix Z = ...    # form the matrix of instruments
matrix V = inv(Z'Z)

gmm 
  series e = y - X*beta
  orthog e ; Z
  weights V
  params beta
end gmm
\end{code}
\end{script}


\section{A real example: the Consumption Based Asset Pricing Model}
\label{sec:gmm-CBAPM}

To illustrate \app{gretl}'s implementation of GMM, we will replicate
the example given in Chapter 3 of Hall (2005). The model to estimate
is a classic application of GMM, and provides an example of a case
when orthogonality conditions do not stem from statistical
considerations, but rather from economic theory.

A rational individual who must allocate his income between consumption
and investment in a financial asset must in fact choose the consumption
path of his whole lifetime, since investment translates into future
consumption. It can be shown that an optimal consumption path should
satisfy the following condition:
\begin{equation}
  \label{eq:gmm-CBAPM}
  p U'(c_t) = \delta^k E\left[ r_{t+k} U'(c_{t+k}) | \mathcal{F}_t
  \right] ,
\end{equation}
where $p$ is the asset price, $U(\cdot)$ is the individual's utility
function, $\delta$ is the individual's subjective discount rate and
$r_{t+k}$ is the asset's rate of return between time $t$ and time
$t+k$. $\mathcal{F}_t$ is the \emph{information set} at time $t$;
equation (\ref{eq:gmm-CBAPM}) says that the utility ``lost'' at time
$t$ by purchasing the asset instead of consumption goods must be
matched by a corresponding increase in the (discounted) future utility
of the consumption financed by the asset's return. Since the future is
uncertain, the individual considers his expectation, conditional on
what is known at the time when the choice is made.

We have said nothing about the nature of the asset, so equation
(\ref{eq:gmm-CBAPM}) should hold whatever asset we consider; hence, it
is possible to build a system of equations like (\ref{eq:gmm-CBAPM})
for each asset whose price we observe.

\begin{script}[htbp]
  \caption{Estimation of the Consumption Based Asset Pricing Model}
  \label{gmm-CBAPM-script}
\begin{scode}
open hall.gdt

scalar alpha = 0.5
scalar delta = 0.5
series e = 0

set hac_lag 7
list inst = const consrat(-1) consrat(-2) ewr(-1) ewr(-2)

matrix V0 = 100000*I(nelem(inst))
matrix Z = { inst }
matrix V1 = $nobs*inv(Z'Z)

set force_hc on

gmm e = delta*ewr*consrat^(alpha-1) - 1
  orthog e ; inst
  weights V0
  params alpha delta
end gmm

gmm e = delta*ewr*consrat^(alpha-1) - 1
  orthog e ; inst
  weights V1
  params alpha delta
end gmm

gmm e = delta*ewr*consrat^(alpha-1) - 1
  orthog e ; inst
  weights V0
  params alpha delta
end gmm --iterate

gmm e = delta*ewr*consrat^(alpha-1) - 1
  orthog e ; inst
  weights V1
  params alpha delta
end gmm --iterate
\end{scode}
%$
\end{script}

If we are willing to believe that
\begin{itemize}
\item the economy as a whole can be represented as a single gigantic
  and immortal representative individual, and
\item the function $U(x) = \frac{x^{\alpha} - 1 }{\alpha}$ is a
  faithful representation of the representative individual's
  preferences,
\end{itemize}
then, setting $k=1$, equation (\ref{eq:gmm-CBAPM}) implies the
following for any asset $j$:
\begin{equation}
  \label{eq:gmm-CBAPM-est}
  E\left[ \delta \frac{r_{j,t+1}}{p_{j,t}} \left(\frac{C_{t+1}}{C_{t}}
    \right)^{\alpha - 1} \bigg| \mathcal{F}_t \right] = 1 ,
\end{equation}
where $C_t$ is aggregate consumption and $\alpha$ and $\delta$ are the
risk aversion and discount rate of the representative individual. In
this case, it is easy to see that the ``deep'' parameters $\alpha$ and
$\delta$ can be estimated via GMM by using
\[
  e_t = \delta \frac{r_{j,t+1}}{p_{j,t}} \left(\frac{C_{t+1}}{C_{t}}
    \right)^{\alpha - 1} - 1
\]
as the moment condition, while any variable known at time $t$ may serve as
an instrument.

\begin{script}[htbp]
  \caption{Estimation of the Consumption Based Asset Pricing Model ---
  output}
  \label{gmm-CBAPM-out}
\begin{scode}
Model 1: 1-step GMM estimates using the 465 observations 1959:04-1997:12
e = d*ewr*consrat^(alpha-1) - 1

      PARAMETER       ESTIMATE          STDERROR      T STAT   P-VALUE

  alpha                -3.14475          6.84439      -0.459   0.64590
  d                     0.999215         0.0121044    82.549  <0.00001 ***

  GMM criterion = 2778.08

Model 2: 1-step GMM estimates using the 465 observations 1959:04-1997:12
e = d*ewr*consrat^(alpha-1) - 1

      PARAMETER       ESTIMATE          STDERROR      T STAT   P-VALUE

  alpha                 0.398194         2.26359       0.176   0.86036
  d                     0.993180         0.00439367  226.048  <0.00001 ***

  GMM criterion = 14.247

Model 3: Iterated GMM estimates using the 465 observations 1959:04-1997:12
e = d*ewr*consrat^(alpha-1) - 1

      PARAMETER       ESTIMATE          STDERROR      T STAT   P-VALUE

  alpha                -0.344325         2.21458      -0.155   0.87644
  d                     0.991566         0.00423620  234.070  <0.00001 ***

  GMM criterion = 5491.78
  J test: Chi-square(3) = 11.8103 (p-value 0.0081)

Model 4: Iterated GMM estimates using the 465 observations 1959:04-1997:12
e = d*ewr*consrat^(alpha-1) - 1

      PARAMETER       ESTIMATE          STDERROR      T STAT   P-VALUE

  alpha                -0.344315         2.21359      -0.156   0.87639
  d                     0.991566         0.00423469  234.153  <0.00001 ***

  GMM criterion = 5491.78
  J test: Chi-square(3) = 11.8103 (p-value 0.0081)
\end{scode}
\end{script}

In the example code given in \ref{gmm-CBAPM-script}, we replicate
selected portions of table 3.7 in Hall (2005). Part of the
output is given in \ref{gmm-CBAPM-out}.

In this example, the variable \texttt{consrat} is defined as the ratio
of monthly consecutive real per capita consumption (services and
nondurables) for the US, and \texttt{ewr} is the return--price ratio
of a fictitious asset constructed by averaging all the stocks in the
NYSE.  The instrument set contains the constant and two lags of each
variable.  Since the number of orthogonality conditions (5) is greater
than the number of estimated parameters (2), we can see that the
choice of the weighting matrix does make a difference here in the
one-step estimates (Models 1 and 2), while the iterated estimator
converges to the same point (numerically) for both weights matrices
(Models 3 and 4). Note, however, that the $J$ test leads to a
rejection of the hypothesis of correct specification.

\section{Covariance matrix options}
\label{sec:gmm-vcv}

Talk about HC, HAC.


\section{Caveats}
\label{sec:gmm-caveat}

Warnings: possible non-robustness; dependence of parameter estimates
on the precise specification of the weights; non-trivial dependence of
estimates on re-scaling of the instruments.  Difficulty of replicating
various well-known published studies.  Need to carefully record all
details of estimation procedure.  



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "gretl-guide"
%%% End: 
