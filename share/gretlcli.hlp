#
add
@Tests
Usage:          add varlist 
Examples:       add 5 7 9	          add xx yy zz 

The variables in varlist are added to the previous model and the new model
estimated.  If more than one variable is added, then the F statistic for the
added variables is printed (for the OLS procedure only) along with the
pvalue for it.  A pvalue below 0.05 means that the coefficients are jointly
significant at the 5 percent level. 

#
addto
@Tests
Usage:          addto model_ID varlist 
Example:        addto 2 5 7 9
	
Works like the "add" command, except that you specify a previous model (using
its ID number, which is printed at the start of the model output) to take as
the base for adding variables.  The example above adds variables number 5, 7
and 9 to Model 2.

#
adf
@Tests
Usage:	        adf order varname
Example:        adf 2 x1

Computes statistics for two Dickey-Fuller tests.  In each case the null
hypothesis is that the variable in question exhibits a unit root.  

The first is a t-test based on the model (1 - L)x(t) = m + g * x(t-1) + e(t).
The null hypothesis is that g = 0.

The second (augmented) test proceeds by estimating an unrestricted regression
(with regressors a constant, a time trend, the first lag of the variable, and
"order" lags of the first difference) and a restricted version (dropping the
time trend and the first lag).  The test statistic is F, determined as

[(ESSr - ESSu)/2]/[ESSu/(T - k)] 

where T is the sample size and k the number of parameters in the unrestricted
model.  Note that the critical values for these statistics are not the usual
ones.  

#
ar
@Estimation
Usage:          ar lags ; depvar indepvars      or
                ar lags ; -o depvar indepvars     
Example:        ar 1 3 4 ; y 0 x1 x2 x3 

Computes the estimates of a model using the generalized Cochrane-Orcutt
iterative procedure (see Ramanathan, Section 9.5).  Iteration is terminated
when successive error sum of squares do not vary by more than 0.005 percent or
when 20 iterations have been done.  lags is a list of lags in the residuals,
terminated by a semicolon.  In the above example, the error term is specified
as

     u(t) = rho1 u(t-1) + rho3 u(t-3) + rho4 u(t-4) + et

depvar is the dependent variable and indepvars is the list of independent
variables separated by spaces.  Use the number zero for a constant term.  If
the -o flag is present, the covariance matrix of regression coefficients will
be printed.  Residuals of the transformed regression are stored under the name
uhat and fitted values under the name yhat.  These series can be retrieved
with the genr command.

#
arch
@Tests
Usage:          arch lag depvar indepvars    
Examples:       arch 4 1 0 2 4 6 7      or  arch 4 y 0 x1 x2 x3 x4 

This command tests the model for ARCH of the order specified in "lag", which
must be an integer.  If the LM test statistic has p-value below 0.10, then
ARCH estimation is also carried out.  If the predicted variance of any
observation in the auxiliary regression is not positive, then the
corresponding uhat square is used instead.  Weighted least square estimation
is then performed on the original model.

#
boxplot
@Graphs
Usage:         boxplot varname

This command is available in the gretl console, but not in the command-line
program.  Please see the gretl manual or the GUI help file for details.

Boxplots (after Tukey and Chambers) display the distribution of a variable.
The central box encloses the middle 50 percent of the data, i.e. it is bounded
by the first and third quartiles.  The "whiskers" extend to the minimum and
maximum values.  A line is drawn across the box at the median.

#
chow
@Tests
Usage:         chow obs
Examples:      chow 25
               chow 1988.1

Must follow an OLS regression.  Creates a dummy variable which equals 1 from
the split point specified by "obs" to the end of the sample, 0 otherwise, and
also creates interaction terms between this dummy and the original independent
variables.  An augmented regression is run including these terms and an F
statistic is calculated, taking the augmented regression as the unrestricted
and the original as restricted.  This statistic is appropriate for testing the
null hypothesis of no structural break at the given split point.

#
coeffsum
@Tests
Usage:         coeffsum indepvars
Examples:      coeffsum xt xt_1 xt_2

Must follow a regression.  Calculates the sum of the coefficients on the
variables in the "indepvars" list.  Prints this sum along with its standard
error and the p-value for the null hypothesis that the sum is zero.

#
coint
@Tests
Usage:	        coint order depvar indepvars
Examples:       coint 2 y x
                coint 4 y x1 x2

Carries out Augmented Dickey-Fuller tests on the null hypothesis that each of
the variables listed has a unit root, using the given lag order.  The
cointegrating regression is estimated, and an ADF test is run on the residuals
from this regression.  The Durbin-Watson statistic for the cointegrating
regression is also given.  Note that none of these test statistics can be
referred to the usual statistical tables.

#
coint2
@Tests
Usage:	        coint2 order depvar indepvars
Examples:       coint2 2 y x
                coint2 12 y x1 x2 -o

Carries out the Johansen trace test for cointegration among the listed
variables for the given order.  For details of this test see for instance
James D. Hamilton, Time Series Analysis, chapter 19.  If the -o flag is
given the results of the various auxiliary regressions are printed.

#
corc
@Estimation
Usage:          corc depvar indepvars      or   corc -o depvar indepvars 
Examples:       corc 1 0 2 4 6 7                corc -o 1 0 2 4 6 7 
                corc y 0 x1 x2 x3               corc -o y 0 x1 x2 x3 

Computes the estimates of a model using the Cochrane-Orcutt iterative
procedure (see Ramanathan, Section 9.4) with depvar as the dependent variable
and indepvars as the list of independent variables separated by spaces and
terminated by a ;.  Use the number zero for a constant term.  Iteration is
terminated when successive rho values do not differ by more than 0.001 or when
20 iterations have been done.  If the -o flag is present, the covariance
matrix of regression coefficients is printed.  The final transformed
regression is performed for the observation range stobs+1 endobs currently in
effect.  Residuals of this transformed regression are stored under the name
uhat, and fitted values under the name yhat.

#
corr
@Statistics
Usage:         corr 
               corr varlist

corr prints correlation coefficients for all pairs of variables in the data
set (missing values denoted by -999 are skipped).  corr varlist prints
correlation coefficients for the listed variables.

#
corrgm
@Statistics
Usage:          corrgm varname or varnumber 
                corrgm varname or varnumber maxlag

Prints the values of the autocorrelation function for the variable specified
(see Ramanathan, Section 11.7).  It is thus cor[u(t), u(t-s)], where u(t) is
the t-th observation of the variable u and s is the number of lags.  The
partial autocorrelations are also shown: these are net of the effects of
intervening lags.  The command also graphs the correlogram and prints the
Ljung-Box Q statistic for testing the null hypothesis that the series is
"white noise".  This is asymptotically distributed as chi-square with degrees
of freedom equal to the number of lags used.

If an integer maxlag value is supplied the length of the correlogram is
limited to at most that number of lags, otherwise the length is determined
automatically.

#
criteria
@Utilities
Usage:          criteria ess T k        e.g. criteria 23.45 45 8

Computes the model selection statistics (see Section 4.4 of Ramanathan), given
ess (error sum of squares), the number of observations (T), and the number of
coefficients (k).  T, k, and ess may be numerical values or names of
previously defined variables.

#
critical
@Utilities
Usage:          critical t df           e.g. critical t 20
                critical X df           e.g. critical X 5
                critical F dfn dfd      e.g. critical F 3 37
                critical d n            e.g. critical d 24

If the first parameter is t, X or F, prints out the critical values for the
student's t, chi-square or F distribution respectively, for the common
significance levels and using the specified degrees of freedom.  If the first
parameter is d, prints the upper and lower values of the Durbin-Watson
statistic at 5 percent significance, for the given n (number of observations),
and for the range of 1 to 5 explanatory variables.

#
cusum
@Tests
Usage:          cusum

Must follow the estimation of a model via OLS.  Performs the CUSUM test for
parameter stability.  A series of (scaled) one-step ahead forecast errors is
obtained by running a series of regressions: the first regression uses the
first k observations and is used to generate a prediction of the dependent
variable at observation at observation k + 1; the second uses the first k + 1
observations and generates a prediction for observation k + 2, and so on
(where k is the number of parameters in the original model).  The cumulated
sum of the scaled forecast errors is printed and graphed.  The null hypothesis
of parameter stability is rejected at the 5 percent significance level if the
cumulated sum strays outside of the 95 percent confidence band.

The Harvey-Collier t-statistic for testing the null hypothesis of parameter
stability is also quoted.  See Chapter 7 of Greene's Econometric Analysis for
details.

#
data
@Dataset
Usage           data varlist

Reads the variables in varlist from a database (gretl or RATS 4.0), which must
have been opened previously using the "open" command.  In addition, a data
frequency and sample range must be established using the "setobs" and "smpl"
commands prior to using this command.  Here is a full example:

open macrodat.rat
setobs 4 1959:1
smpl ; 1999:4
data GDP_JP GDP_UK

These commands open a database named "macrodat.rat", establish a quarterly
data set starting in the first quarter of 1959 and ending in the fourth
quarter of 1999, and then import the series named "GDP_JP" and "GDP_UK".

If the series to be read are of higher frequency than the working data set,
you must specify a compaction method as below:

data (compact=average) LHUR PUNEW

The four available compaction methods are "average" (takes the mean of the
high frequency observations), "last" (uses the last observation), "first" and
"sum".  

#
delete
@Dataset
Usage:          delete

Removes the last (highest numbered) variable from the current data set.  Use
with caution: no confirmation is asked.  Can be useful for getting rid of
temporary dummy variables.  There is no provision for deleting any but the
last variable.

#
diff
@Transformations
Usage:          diff varlist 

The first difference of each variable in varlist is obtained and the result
stored in a new variable with the prefix "d_".  Thus "diff x y" creates the
new variables d_x = x(t) - x(t-1) and d_y = y(t) - y(t-1).

#
end
@Programming

Ends a block of commands of some sort.  For example, "end system" terminates
an equation system.

#
endloop
@Programming

Terminates a simulation loop.  See "loop".

#
equation
@Estimation
Usage:          equation depvar indepvars
Example:        equation y x1 x2 x3 const

Specifies an equation within a system of equations (see the "system" command).
The sytax for specifying an equation is the same as that for, e.g., the
"ols" command.

# 
eqnprint
@Printing
Usage:          eqnprint
                eqnprint -o
                eqnprint -f filename

Must follow the estimation of a model via OLS.  Prints the estimated model in
the form of a LaTeX equation.  If a filename is specified using the "-f" flag
output goes to that file, otherwise it goes to a file with a name of the form
"equation_N.tex", where N is the number of models estimated to date in the
current session.  This can be incorporated in a LaTeX document.  See also the
tabprint command.

If the -o flag is given the LaTeX file is a complete document, ready for
processing; otherwise it must be included in a document.

#
fcast
@Prediction
Usage:          fcast startobs endobs newvarname
                fcast newvarname
Example:        fcast fitted 

Fitted values are retrieved from the last regression run and saved as
newvarname.  These values can be printed, graphed, or plotted.  The right-hand
side variables are those in the original model.  There is no provision to
substitute other variables.  If starting and ending observations are specified
the forecast is restricted to the given range.  If an autoregressive error
process is specified (for hilu, corc, and ar) the forecast is conditional one
step ahead and incorporates the error process.

#
fcasterr
@Prediction
Usage:          fcasterr startobs endobs
                fcasterr startobs endobs -o

After estimating an OLS model which includes a constant and at least one
independent variable (these restrictions may be relaxed at some point) you can
use this command to print out fitted values over the specified observation
range, along with the estimated standard errors of those predictions and 95
percent confidence intervals.  If the -o flag is given the results will also
be displayed using gnuplot.

#
fit
@Prediction
Usage:		fit

This command (which must follow an estimation command) is a shortcut for the
fcast command.  It generates fitted values, in a series called "autofit", for
the current sample, based on the last regression.  In the case of time-series
models, fit also pops up a gnuplot graph of fitted and actual values of the
dependent variable against time (if support for gnuplot was compiled in).

#
freq
@Statistics
Usage:          freq varname (or varno) 

Prints the frequency distribution for varname or varno; the results of a
Chi-square test for normality are also reported.  The statistic for the latter
is

  sample_size * [skewness^2/6 + (kurtosis - 3.0)^2/24.0] 

It has 2 degrees of freedom.

In interactive mode a gnuplot graph of the distribution is generated.


#
genr
@Dataset
Usage:          genr newvarname = formula

Creates new variables, usually through transformations of existing
variables. See also diff, logs, lags, ldiff, multiply and square for
shortcuts.

Supported arithmetical operators are, in order of precedence: ^
(exponentiation); *, / and % (modulus or remainder); + and -.  

Boolean operators (again in order of precedence) are ! (logical NOT), &
(logical AND), | (logical OR), >, <, = and the composite symbols != (not
equal), >= (greater than or equal) and <= (less than or equal).  The Boolean
operators can be used in constructing dummy variables: for instance (x > 10)
returns 1 if x(t) > 10, 0 otherwise.

Supported functions fall into these groups: 

Standard math functions: abs, cos, exp, int (integer part), ln (natural log:
log is a synonym), sin, sqrt.

Statistical functions: mean (arithmetic mean), median, var (variance), sd
(standard deviation), sum, cov (covariance), corr (correlation coefficient),
min (minimum) and max (maximum) and sst (sum of squared deviations from the
mean).

Time-series functions: diff (first difference), ldiff (log-difference, or
first difference of natural logs).  To generate lags of a variable x use the
syntax "x(-N)" where 'N' is replaced by the desired lag length.  To generate
leads, use "x(+N)".

Miscellaneous: cum (cumulate), sort, uniform, normal, misszero (replace the
missing observation code with zero), zeromiss (inverse operation of misszero),
pvalue (probability value for a given statistic against a specified
distribution) and mpow (raise a series to an integer power using multiple
precision arithmetic).

All of the above functions with the exception of cov, corr, uniform, normal
pvalue and mpow take as their single argument either the name of a variable
(note that you can't refer to variable by their ID numbers in a genr command)
or a composite expression that evaluates to a variable (e.g. ln((x1+x2)/2)).
cov and corr both require two arguments, and return respectively the
covariance and the correlation coefficient between two named variables.
uniform() and normal(), which do not take arguments, return pseudo-random
series drawn from the uniform (0-1) and standard normal distributions
respectively (see also the seed command).  The pvalue() function takes the
same arguments as the pvalue command (see below), but in this context commas
should be placed between the arguments.  The mpow function takes as its
arguments the name of a data series and a positive integer power to which the
series should be raised.

Besides the operators and functions just noted there are some special
uses of genr:

* genr time creates a time trend variable (1,2,3,...) called time.  
* genr index creates an index variable (1,2,3,...) called index.  
* genr dummy creates dummy variables up to the periodicity of the data.
  E.g. in the case of quarterly data (periodicity 4), the program creates
  dummy_1 = 1 for first quarter and 0 in other quarters, dummy_2 = 1 for the
  second quarter and 0 in other quarters, and so on.
* genr paneldum creates a set of special dummy variables for use with a panel
  data set (see the gretl manual for details)
* Various internal variables defined in the course of running a regression can
  be retrieved using genr, as follows:

  $ess         error sum of squares
  $rsq         unadjusted R-squared
  $T           number of observations used in the model
  $df          degrees of freedom
  $trsq        TR^2 (sample size times R-squared)
  $sigma       standard error of residuals
  $lnl         log-likelihood (logit and probit models)
  coeff(var)   estimated coefficient for var
  stderr(var)  estimated std. error for var
  rho(i)       ith order autoregressive coefficient for residuals
  vcv(xi,xj)   covariance between coefficients for vars xi and xj

The internal series uhat and yhat hold, respectively, the residuals and fitted
values from the last regression.

The internal variable $nobs holds the number of observations in the current
sample range, which may or may not equal the value of $T for the last model.

The internal variable $pd holds the periodicity or frequency of the data
(e.g. 4 for quarterly data, 12 for monthly).

The internal variable t references the observations, starting at 1.  Thus one
can do "genr dum15 = (t=15)" to generate a dummy variable with value 1 for
observation 15, 0 otherwise.

Examples of genr commands:

  genr y = x1^3          [x1 cubed]           
  genr y=ln((x1+x2)/x3)  [composite argument to ln function]   
  genr z=x>y             [sets z(t) to 1 if x(t) > y(t) else to 0]
  genr y=x(-2)           [x lagged 2 periods]     
  genr y=x(2)            [x led 2 periods]
  genr y = mean(x)       [arithmetic mean]    
  genr y = diff(x)       [y(t) = x(t) - x(t-1)]
  genr y = ldiff(x)      [y = ln(x(t)) - ln(x(t-1))]
                          ldiff(x) is the instantaneous rate of growth of x.
  genr y = sort(x)       [sort x in increasing order and store in y]
  genr y = - sort(-x)    [sort x in decreasing order]
  genr y = int(x)        [truncate x and store its integer value as y]
  genr y = abs(x)        [store the absolute values of x]
  genr y = sum(x)        [sum x values excluding missing -999 entries]
  genr y = cum(x)        [cumulate x: y(t) is the sum of x up to t]
  genr aa = $ess         [aa = Error Sum of Squares from last regression]
  genr x = coeff(sqft)   [grab sqft coefficient from last model]
  genr rho4 = rho(4)     [grab 4th-order autoregressive coeff. from last
                          model (presumes an ar model)]
  genr cv=vcv(x1, x2)    [covariance of x1 and x2 coeffs. in last model]
  genr x=uniform()       [uniform pseudo-random variable, range 0 to 1]
  genr x=3*normal()      [normal pseudo-random var, mean 0, std. dev. 3]
  genr x=pvalue(t,20,1.4) [p-value for 1.4, under the t distribution with
                           20 degrees of freedom]

Tips on dummy variables: 

* Suppose x is coded with values 1, 2, or 3 and you want three dummy
variables, d1 = 1 if x = 1, 0 otherwise, d2 = 1 if x = 2, and so on.  To
create these, use the commands: genr d1 = (x=1), genr d2 = (x=2), and 
genr d3 = (x=3).

* To get z = max(x,y) do genr d=x>y and genr z=(x*d)+(y*(1-d))

#
gnuplot
@Graphs
Usage:          gnuplot yvar1 xvar [ flag ]
                gnuplot yvar1 yvar2 xvar [ flag ]
		gnuplot yvar xvar dummy -z

In the first two cases the yvars are graphed against xvar.  The optional flag
may be any one of the following:

-o  use lines rather than points (points are the default)
-m  use impulses (vertical lines) rather than points
-s  suppress the addition of a fitted line in the case of an x, y scatterplot

In the third case yvar is graphed against xvar with the points shown in
different colors depending on whether the value of "dummy" is 1 or 0.

To make a time-series graph, do "gnuplot yvars time". If no variable named
"time" already exists, then it will be generated automatically.  Special dummy
variables will be created for plotting quarterly and monthly data.

In interactive mode the result is piped to gnuplot for display.  In batch mode
a gnuplot command file named gpttmpNN.plt is written out, where NN is a number
from 01 to 99.  The plots can be generated later using gnuplot.

#
graph
@Graphs
Usage:          graph var1 var2 
                graph -o var1 var2 
                graph var1 var2 var3
 
In the first two examples, variable var1 (which may be a name or a number) is
graphed (y-axis) against var2 (x-axis).  -o flag will graph with 40 rows and
60 columns.  Without it, the graph will be 20 by 60 (for screen output).  In
the third example, both var1 and var2 will be graphed (on y-axis) against
var3.  This is particularly useful to graph observed and predicted values
against time.

#
hausman
@Tests
Usage:          hausman

This test is available only after estimating a model using the "pooled"
command (see also the commands panel and setobs).  It tests the simple pooled
model against the principal alternatives, the fixed effects and random effects
models.

The fixed effects model adds a dummy variable for all but one of the
cross-sectional units, allowing the intercept of the regression to vary across
the units.  An F-test for the joint significance of these dummies is
presented.  The random effects model decomposes the residual variance into two
parts, one part specific to the cross-sectional unit and the other specific to
the particular observation.  (This estimator can be computed only if the
number of cross-sectional units in the data set exceeds the number of
parameters to be estimated.)  The Breusch-Pagan LM statistic tests the null
hypothesis (that the pooled OLS estimator is adequate) against the random
effects alternative.

The pooled OLS model may be rejected against both of the alternatives, fixed
effects and random effects. Provided the unit- or group-specific error is
uncorrelated with the independent variables, the random effects estimator is
more efficient than the fixed effects estimator; otherwise the random effects
estimator is inconsistent and the fixed effects estimator is to be preferred.
The null hypothesis for the Hausman test is that the group-specific error is
not so correlated (and therefore the random effects model is preferable).  A
low p-value for this test counts against the random effects model and in
favor of fixed effects.

#
hccm
@Estimation
Usage:          hccm depvar indepvars      
            or  hccm -o depvar indepvars
 
Presents OLS estimates with the heteroskedasticity consistent covariance
matrix estimates for the standard errors of regression coefficients using the
MacKinnon-White "jackknife" procedure.

#
help
@Utilities
help                 gives a list of gretl commands
help commandname     describes commandname (e.g. help smpl)

#
hilu
@Estimation
Usage:          hilu depvar indepvars      or   hilu -o depvar indepvars 
Examples:       hilu 1 0 2 4 6 7                hilu -o 1 0 2 4 6 7 
                hilu y 0 x1 x2 x3               hilu -o y 0 x1 x2 x3 

Computes the estimates of a model using the Hildreth-Lu search procedure (fine
tuned by the CORC procedure) with depvar as the dependent variable and
indepvars as the list of independent variables separated by spaces.  Use the
number zero for a constant term.  The error sum of squares of the transformed
model is graphed against the value of rho from -0.99 to 0.99.  If the -o flag
is present, the covariance matrix of regression coefficients will be printed.
The final transformed regression is performed for the observation range
stobs+1 endobs currently in effect.  Residuals of this transformed regression
are stored under the name uhat, and fitted values under the name yhat.  

#
hsk
@Estimation
Usage:          hsk depvar indepvars 	   
            or  hsk -o depvar indepvars
 
Prints heteroskedasticity corrected estimates and associated statistics.  The
auxiliary regression predicts the log of the square of residuals (using
squares of independent variables but not their cross products) from which
weighted least squares estimates are obtained.  If the -o flag is present, the
covariance matrix of regression coefficients will be printed.  

#
if
@Programming
Usage:          if boolean_condition
                  command1
                  command2 ...
                endif

The gretl commands within the "if ... endif" block are executed if and only if
the boolean condition evaluates as true (non-zero).  For the syntax of boolean
conditions in gretl, see the genr command.  Optionally, "endif" may be
preceded by "else" (on a line by itself, like the "if" condition and the
"endif" terminator), followed by a block of commands to execute if the
original boolean condition evaluates as false (zero).  The blocks between
"if", "else" and "endif" can contain as many commands as you like, and these
conditions can be nested.  Each "if" must be paired with an "endif".

#
import
@Dataset
Usage:          import csv_file
                import -o box_file

Without the -o flag, brings in data from a comma-separated values (CSV) format
file, such as can easily be written from a spreadsheet program.  With the -o
flag, reads a data file in BOX1 format, as can be obtained using the Data
Extraction Service of the US Bureau of the Census.  BOX1 files have a definite
fixed format; the remainder of this entry pertains to CSV files.

The file should have variable names on the first line and a rectangular data
matrix on the remaining lines.  Variables should be arranged "by observation"
(one column per variable; each row represents an observation).  The column
separator need not be a comma; spaces and tabs are also acceptable.

The first column of the file can contain identifiers for the observations --
either strings such as the names of countries, or dates.  In this case the
first column heading should be "obs" or "date", or may be left blank.  For
dates to be recognized as such they should adhere to one of the following
patterns:

annual:    dates represented by 4-digit years
quarterly: use "1997.1" or "1997:1" or "1997Q1" for 1997, first quarter
monthly:   use "1997.01" or "1997:01" for January, 1997

#
info
@Dataset
Usage:          info

Prints out any information contained in the header file corresponding to the
current datafile.  This information must be enclosed between "(*" and "*)",
these markers being placed on separate lines.

#
label
@Dataset
Usage:          label varname -d "Descriptive string" -n "display name"

Sets the descriptive label for the given variable (if the -d flag is given,
followed by a string in double quotes) and/or the display name for the
variable (if the -n flag is given, followed by a quoted string).  If a variable
has a display name, this is used when generating graphs.

#
labels
@Dataset
Usage:          labels

Prints out the informative labels for any variables that have been defined
using the "genr" command, or have had labels attached to them using the
"label" command.

#
lad
@Estimation
Usage:          lad depvar indepvars

Calculates a regression that minimizes the sum of the absolute deviations of
the observed from the fitted values of the dependent variable.  Coefficient
estimates are derived using the Barrodale-Roberts simplex algorithm; a
warning is printed if the solution is not unique.  Standard errors are
derived using the bootstrap procedure with 500 drawings.  

#
lags
@Transformations
Usage:          lags varlist
 
Creates new variables which are lagged values of each of the variables in 
varlist.  The number of lagged variables equals the periodicity.  For
example,  if the periodicity was 4 (quarterly), the command  lags x y ; will
create x_1  = x(-1), x_2 = x(-2), x_3 = x(-3) and x_4 = x(-4).  Similarly
for y.  These  variables must be referred to in the exact form, that is,
with the underscore.

#
ldiff
@Transformations
Usage:          ldiff varlist 

The first difference of the natural log of each variable in varlist is 
obtained and the result stored in a new variable with the prefix "ld_".  
Thus "ldiff x y" creates the new variables 

               ld_x = ln[x(t)] - ln[x(t-1)]
               ld_y = ln[y(t)] - ln[y(t-1)]

#
leverage
@Tests
Usage:          leverage

Must immediately follow an ols command.  Calculates the leverage (h) for each
data point in the sample on which the previous model was estimated.  Displays
the residual (u) for each observation along with its leverage and a measure of
its influence on the estimates, u * h / (1 - h).  "Leverage points" for which
the value of h exceeds 2k/n (where k is the number of parameters being
estmated and n is the sample size) are flagged with an asterisk.  For details
on the concepts of leverage and influence see Davidson and MacKinnon,
Estimation and Inference in Econometrics, chapter 2.

#
lmtest
@Tests
Usage:          lmtest
                lmtest -o 

Must immediately follow an ols command.  Prints the Lagrange multiplier test
statistics (and associated p-values) for nonlinearity and heteroskedasticity
(White's test) or, if the -o flag is present, for serial correlation up to the
periodicity.  The corresponding auxiliary regression coefficients are also
printed out.  See Chapters 7, 8, and 9 of Ramanathan for details.

Only the squared independent variables are used and not their cross products.
If the internal creation of squares causes exact multicollinearity, LM test
statistics cannot be obtained.

#
logit
@Estimation
Usage:          logit depvar indepvars

Logit regression: the dependent variable should be a binary variable.  Maximum
likelihood estimates of the coefficients on indepvars are obtained via
iterated least squares (the EM or Expectation-Maximization method).  As the
model is non-linear the slopes depend on the values of the independent
variables: the reported slopes are evaluated at the means of those variables.
The Chi-square statistic tests the null hypothesis that all coefficients are
zero apart from the constant.

#
logs
@Transformations
Usage:          logs varlist
 
The natural log of each of the variables in varlist is obtained and the 
result stored in a new variable with the prefix l_ which is "el" underscore.
Thus, "logs x y" creates the new variables l_x = ln(x) and l_y = ln(y).

#
loop
@Programming
Usage:          loop number_of_times
                loop while condition
		loop for i=start..end
Examples:       loop 1000
		loop while essdiff > .00001
		loop for i=1991..2000

This (script) command opens a special mode in which the program accepts
commands to be repeated either a specified number of times, or so long as a
specified condition holds true, or for successive integer values of the
(internal) index variable i.

Within a loop, only a subset of gretl commands can be used: genr, ols, print,
sim, smpl, store and summary (store can't be used in a "while" loop).  Within
a "number of times" loop some additional estimation commands are available,
namely hccm, hsk and lad.

You exit the mode of entering loop commands with "endloop": at this point the
stacked commands are executed.  Loops cannot be nested.

The ols command gives special output, depending on the sort of loop.  If a
number of times is specified the results from each individual regression are
not printed, but rather you get a printout of (a) the mean value of each
estimated coefficient across all the repetitions, (b) the standard deviation of
those coefficient estimates, (c) the mean value of the estimated standard error
for each coefficient, and (d) the standard deviation of the estimated standard
errors.  This makes sense only if there is some random input at each step.
The command is designed for Monte Carlo analysis.  If a "while" condition is
given, you get a printout of the specified model from the last time round the
loop: this is designed for iterated least squares.

The print command also behaves differently in the context of a "number of
times" loop.  It prints the mean and standard deviation of the variable,
across the repetitions of the loop.  It is intended for use with variables
that have a single value at each iteration, for example the ess from a
regression.  The print command behaves as usual with the other loop
constructions.

The store command (use only one of these per loop, and only in a "number
of times" loop) writes out the values of the specified variables, from
each time round the loop, to the specified file.  Thus it keeps a complete
record of the variables.  This data file can then be read into the program
and analysed.

Example of loop code (Monte Carlo):

   genr x = 100 * uniform()
   loop 100
   genr u = normal()
   genr y = (10*x) + (20*u)
   ols y const x
   genr r2 = $rsq
   print r2
   genr a = coeff(const)
   genr b = coeff(x)
   store foo.gdt a b
   endloop

#
meantest
@Tests
Usage:          meantest x1 x2
                meantest x1 x2 -o

Calculates the t statistic for the null hypothesis that the population means
are equal for the variables x1 and x2, and shows its p-value.  Without the -o
flag, the statistic is computed on the assumption that the variances are equal
for the two variables; with the -o flag the variances are assumed to be
unequal.  (The flag will make a difference only if there are different numbers
of non-missing observations for the two variables.)

#
mpols
@Estimation
Usage:          mpols depvar indepvars
Examples:       ols 1 0 2 4 6 7
                ols y 0 x1 x2 x3
  
Computes ordinary least squares estimates with depvar as the dependent
variable and indepvars as the list of independent variables, using
multiple-precision arithmetic.  The variables can be specified either by names
or by their number; use the number zero for a constant term.  This command is
available only if gretl is configured with support for GMP, the GNU Multiple
Precision library.

To estimate a polynomial fit, using multiple precision arithmetic to generate
the required powers of the independent variable, use the form, e.g.

mpols y 0 x ; 2 3 4

This does a regression of y on x, x squared, x cubed and x to the fourth
power.  That is, the numbers (which must be positive integers) to the right of
the semicolon specify the powers of x to be used.  If more than one
independent variable is specified, the last variable before the semicolon is
taken to be the one that should be raised to various powers.
   
#
multiply
@Transformations
Usage:          multiply x suffix vars
Examples:       multiply invpop pc 3 4 5 6
                multiply 1000 big x1 x2 x3

The variables in the list "vars" (referenced by name or number) are multiplied
by x, which may be either a numerical value or the name of a variable already
defined.  The products are named with the specified suffix (maximum 3
characters).  The original varnames are truncated first if need be.  For
instance, suppose you want to create per capita versions of certain variables,
and you have the variable "pop" (population).  A suitable set of commands is
then:

  genr invpop = 1/pop
  multiply invpop pc income expend 

which will create incomepc = income * invpop, expendpc = expend * invpop.

#
nls
@Estimation

Performs Nonlinear Least Squares (NLS) estimation using a modified version of
the Levenberg-Marquandt algorithm.  The user must supply a function
specification.  The parameters of this function must be declared and given
starting values (using the genr command) prior to estimation.  Optionally, the
user may specify the derivatives of the regression function with respect to
each of the parameters; if analytical derivatives are not supplied, a
numerical approximation to the Jacobian is computed.

It is easiest to show what is required by example.  The following is a
complete script to estimate the nonlinear consumption function set out in
William Greene's Econometric Analysis, in chapter 11 of the 4th edition, or
chapter 9 of the 5th.  (The numbers to the left of the lines are for reference
and are not part of the commands.)

 1  open greene11_3.gdt
 2  ols C 0 Y
 3  genr alpha = coeff(0)
 4  genr beta = coeff(Y)
 5  genr gamma = 1.0
 6  nls C = alpha + beta * Y^gamma
 7  deriv alpha = 1
 8  deriv beta = Y^gamma
 9  deriv gamma = beta * Y^gamma * log(Y)
10  end nls

It is often convenient to initialize the parameters by reference to a related
linear model; that is accomplished here on lines 2 to 5.  The parameters
alpha, beta and gamma could be set to any initial values (not necessarily
based on a model estimated with OLS), although convergence of the NLS
procedure is not guaranteed for an arbitrary starting point.

The actual NLS commands occupy lines 6 to 10. On line 6 the "nls" command is
given: a dependent variable is specified, followed by an equals sign, followed
by a function specification.  The syntax for the expression on the right is
the same as that for the genr command.  The next three lines specify the
derivatives of the regression function with respect to each of the parameters
in turn.  Each line begins with the keyword "deriv", gives the name of a
parameter, an equals sign, and an expression whereby the derivative can be
calculated (again, the syntax here is the same as for "genr").  These "deriv"
lines are optional, but it is recommended that you supply them if possible.
Line 10, "end nls", completes the command and calls for estimation.

For further details on NLS estimation please see the gretl manual.

#
noecho
@Programming
Usage:          noecho

Suppresses the normal echoing of the input commands when you run a gretl
script.  See also the print command (literal string variant).

#
nulldata
@Dataset
Usage:          nulldata series_length
Example:        nulldata 100
 
Establishes a "blank" data set, containing only a constant, with periodicity 1
and the specified number of observations.  This may be used for simulation
purposes: some of the genr commands (e.g. genr uniform(), genr normal(), genr
time) will generate dummy data from scratch to fill out the data set.  The
nulldata command may be useful in conjunction with "loop".

#
ols
@Estimation
Usage:          ols depvar indepvars      or     ols -o depvar indepvars 
Examples:       ols 1 0 2 4 6 7                  ols -o 1 0 2 4 6 7  
                ols y 0 x1 x2 x3                 ols -o y 0 x1 x2 x3
                ols -q 1 0 2 3
  
Computes ordinary least squares estimates with depvar as the dependent
variable and indepvars as the list of independent variables.  The -o flag will
print the covariance matrix of regression coefficients.  The variables can be
specified either by names or by their number.  Use the number zero for a
constant term.  The program prints the p-values for two-tailed t-statistics.
A p-value below 0.01 indicates significance at the 1 percent level and is
denoted by three *'s.  Two *'s indicate significance between 1 and 5 percent
and one * indicates significance between 5 and 10 percent levels.  If the
regression has more than one independent variable the F-statistic for
assessing the overall fit is printed, along with its p-value.  Model selection
statistics described in Ramanathan Section 4.4 are also printed.

If the -q ("quiet") flag is given the results will not be printed, which may
be appropriate for auxiliary regressions.

#
omit
@Tests
Usage:          omit varlist 
Examples:       omit 5 7 9           
		omit xx yy zz 

Must be invoked after estimating a model.  The variables in varlist will be
omitted from the previous model and the new model estimated.  If more than one
variable is omitted, the Wald F-statistic for the omitted variables will be
printed along with the pvalue for it (for the OLS procedure only).  A pvalue
below 0.05 means that the coefficients are jointly significant at the 5
percent level.  

#
omitfrom
@Tests
Usage:          omitfrom model_ID varlist 
Example:        omitfrom 2 5 7 9 
          
Works like the "omit" command, except that you specify a previous model (using
its ID number, which is printed at the start of the model output) to take as
the base for omitting variables.  The example above omits variables number 5,
7 and 9 from Model 2.

#
open
@Dataset
Usage:          open datafile

Opens a data file.  If a data file is already open, it is replaced by the
newly opened one.  The program will try to detect the format of the data file
("native", CSV or BOX1) and treat it accordingly.

This command can also be used to open a database (gretl or RATS 4.0) for
reading, in which case it should be followed by the "data" command to extract
particular series from the database.

#
outfile
@Printing
Usage:          outfile [flag] filename
Example:        outfile -w regress.txt
                ols y 0 x1 x2
                outfile -c

Divert output to the named text file, until further notice.  Use the flag "-a"
to append output to an existing file, or "-w" to start a new file (or
overwrite an existing one).  Only one file can be opened in this way at any
given time.

The "-c" flag is used to close an output file that was previously opened with
the "-w" or "-a" flag.  Output will then revert to the default stream.

In the example above, the results of one regression are written to the file
regress.txt, which is then closed.

#
panel
@Dataset
Usage:          panel
                panel -s
	        panel -c

Request that the current data set be interpreted as a panel (pooled cross
section and time series).  With no flag, or with the -s flag, the data set is
taken to be in the form of stacked time series (successive blocks of data
contain time series for each cross-sectional unit).  With the -c flag, the
data set is read as stacked cross-sections (successive blocks contain cross
sections for each time period).  Also see the setobs command.

#
pca
@Statistics
Usage           pca varlist [flag]

Rudimentary Principal Components Analysis.  Prints the eigenvalues of the
correlation matrix for the variables in varlist along with the proportion of
the joint variance accounted for by each component.  Also prints the
corresponding eigenvectors ("component loadings").  

If the -o flag is given, components with eigenvalues greater than 1.0 are
saved to the dataset as variables, with names "PC1", "PC2" and so on.  These
artificial variables are formed as the sum of (component loading) times
(standardized Xi), where Xi denotes the ith variable in varlist.

If the -a flag is given, all of the components are saved as described above.

#
pergm
@Statistics
Usage:          pergm varname 
                pergm varname -o

Computes and displays (and if not in batch mode, graphs) the spectrum of the
specified variable.  Without the -o flag the sample periodogram is given; with
the flag a Bartlett lag window of length 2 * sqrt(sample size) is used in
estimating the spectrum (see Chapter 18 of Greene's Econometric Analysis).
When the sample periodogram is printed, a t-test for fractional integration of
the series ("long memory") is also given: the null hypothesis is that the
integration order is zero.

#
plot
@Graphs
Usage:          plot x1       plot x1 x2 
                plot 3 7      plot -o x1 x2

Plots data values for specified variables, for the range of observations
currently in effect.  Each line stands for an observation and the values are
plotted horizontally.  If the flag -o is present, x1 and x2 are plotted in the
same scale, otherwise x1 and x2 are scaled appropriately.  The -o flag should
be used only if the variables have approximately the same range of values
(e.g. observed and predicted dependent variable).

#
pooled
@Estimation
Usage:         pooled depvar indepvars

Estimates a model via OLS (see the ols command for details on syntax), and
flags it as a pooled or panel model, so that the "panel diagnostics" test item
becomes available.  For an account of these diagnostics see the hausman
command.

#
print
@Printing

Prints the values of the specified variables for the current stobs-endobs
range, or prints a literal string.

print              prints the entire data set in tabular form
print 3 6          prints values of variable numbers 3 and 6
print x y z        prints values of variables named x, y and z

If the -o flag is given, the variables are printed in columns, otherwise they
are printed in consecutive blocks.  If the flag "-t" is given, the data are
printed to 10 significant figures.

print "Literal string" prints the given string.  The closing quote is not
required, but the opening quote is needed to trigger this behavior.

#
probit
@Estimation
Usage:          probit depvar indepvars

Probit regression: the dependent variable should be a binary variable.
Maximum likelihood estimates of the coefficients on indepvars are obtained via
iterated least squares (the EM or Expectation-Maximization method).  As the
model is non-linear the slopes depend on the values of the independent
variables: the reported slopes are evaluated at the means of those variables.
The Chi-square statistic tests the null hypothesis that all coefficients are
zero apart from the constant.

#
pvalue
@Utilities
Usage interactively:     pvalue 
Usage in batch mode (or gretl console mode):
     Normal distribution:     pvalue 1 xvalue
     t-distribution:          pvalue 2 d.f. xvalue
     Chi-square:              pvalue 3 d.f. xvalue
     F-distribution:          pvalue 4 dfn dfd xvalue
     Gamma distribution:      pvalue 5 mean variance xvalue

Computes the area to the right of xvalue in the specified distribution.  d.f.
is the degrees of freedom, dfn is the d.f. for the numerator, dfd is the d.f.
for the denominator.

#
quit
@Utilities

Exit from gretl. ('q' is a shortcut; 'x' exits without a prompt to save
output.)

#
reset
@Tests
Usage:          reset

Must immediately follow an ols command.  Carries out Ramsey's RESET test for
model specification (non-linearity) by adding the square and the cube of the
fitted values to the regression and calculating the F-statistic for the null
hypothesis that the parameters on the two added terms are zero.

#
rhodiff
@Transformations
Usage:         rhodiff rholist ; varlist
Examples:      rhodiff .65 ; 2 3 4
               rhodiff r1 r2 ; x1 x2 x3

Creates rho-differenced counterparts of the variables (given by number or by
name) in varlist and adds them to the data set, using the suffix # for the new
vars.  Given variable v1 in varlist, and entries r1 and r2 in rholist,

v1# = v1(t) - r1*v1(t-1) - r2*v1(t-2)

is created.  The rholist entries can be given as numerical values or as the
names of variables previously defined.

#
rmplot
@Graphs
Usage:          rmplot varname

This command creates a simple graph to help in deciding whether a time series,
y(t), has constant variance or not.  We take the full sample t=1,...,T and
divide it into small subsamples of arbitrary size k.  The first subsample is
formed by y(1),...,y(k), the second is y(k+1),...,y(2k), and so on.  For each
subsample we calculate the sample mean and range (= maximum-minimum), and we
construct a graph with the means on the horizontal axis and the ranges on the
vertical.  So each subsample is represented by a point in this plane.  If the
variance of the series is constant we would expect the subsample range to be
independent of the subsample mean; if we see the points approximate an
upward-sloping line this suggests the variance of the series is increasing in
its mean; and if the points approximate a downward sloping line this suggests
the variance is decreasing in the mean.

Besides the graph, gretl displays the means and ranges for each subsample,
along with the slope coefficient for an OLS regression of the range on the
mean and the p-value for the null hypothesis that this slope is zero.  If the
slope coefficient is significant at the 10 percent significance level then the
fitted line from the regression of range on mean is shown on the graph.

#
run
@Programming
Usage:          run inputfile

If the file "inputfile" contains gretl commands, this command (invoked from
within gretl) will execute them one by one.  This is a useful way of executing
batch commands within an interactive session.

# 
runs
@Tests 
Usage:          runs varname
 
Carries out the nonparametric runs test for randomness of the specified
variable.  If you want to test for randomness of deviations from the median,
for a variable named x1 with a non-zero median, you can do the following:

genr signx1 = x1 - median(x1)
runs signx1

#
scatters
@Graphs
Usage:          scatters yvar ; xvarlist    or
		scatters yvarlist ; xvar
Examples:       scatters 1 ; 2 3 4 5
                scatters 1 2 3 4 5 6 ; time

Plots (using gnuplot) pairwise scatters of yvar against all the variables in
xvarlist, or of all the variables in yvarlist against xvar.  The first example
above puts variable 1 on the y-axis and draws four graphs, the first having
variable 2 on the x-axis, the second variable 3 on the x-axis, and so on.
Scanning a set of such plots can be a useful step in exploratory data
analysis.  The maximum number of plots is six; any extra variable in the list
will be ignored.

#
seed
@Programming
Usage:          seed integer

Sets the seed for the pseudo-random number generator for the uniform() and
normal() functions (see the genr command).  By default the seed is set when
the program is started, using the system time.  If you want to obtain
repeatable sequences of pseudo-random numbers you will need to set the seed
manually.

#
setobs
@Dataset
Usage:          setobs periodicity startobs
Examples:       setobs 4 1990.1
                setobs 12 1978.03
                setobs 20 1.01

Use this command to force the program to interpret the current data set as
time series or panel, when the data have been read in as simple undated
series.  "periodicity" must be an integer; "startobs" is a string representing
the date or panel ID of the first observation.  Use one digit after the point
in "startobs" for data with periodicity less than 10, two digits (with a
leading zero if necessary) for periodicity from 10 to 99.

In the case of daily data a special form of the "startobs" string is required,
namely a date on the pattern YY/MM/DD, for example "55/02/15" if the data
start on the 15th of February, 1955.  If the YY part is less than 50 the year
is assumed to be in the 21st century, otherwise it is assumed to be in the
20th century.  (With daily data, periodicity 5 and 7 are both acceptable.)

#
setmiss
@Dataset
Usage:         setmiss -1
               setmiss 100 xvar

Use this command to get the program to interpret some specific numerical data
value (the first parameter to the command) as a code for "missing", in the
case of imported data.  If this value is the only parameter, as in the first
example above, the interpretation will be applied to all series in the data
set.  If the missing numeric value code is followed by a list of variables, by
name or number, the interpretation is confined to the specified variable(s).
Thus in the second example the value 100 is interpreted as "missing", but only
for the variable named xvar.

#
shell
@Utilities
Usage:		! [shell command]

A "!" at the beginning of an gretl command line is interpreted as an escape
to the user's shell.  Thus arbitrary shell commands can be executed from
within gretl.

#
sim
@Dataset
Usage:          sim stobs endobs y a0 a1 a2 ...

Simulates values for y for the periods stobs through endobs.  The variable y
must have been defined earlier with appropriate initial values.  Also stobs
and endobs must be consistent with the periodicity.  The formula used is

     y(t) = a0(t) + a1(t)*y(t-1) + a2(t)*y(t-2) + ...

ai(t) may either be constants or variable names previously defined. 

Examples:

  sim 1979.2 1983.1 y 0 0.9  [generates y(t) = 0.9*y(t-1)]
  sim 15 25 y 10 0.8 x       [generates y(t) = 10 + 0.8*y(t-1) + x(t)*y(t-2)]

#
smpl
@Dataset
Usage:         smpl startobs endobs
               smpl +i -j
               smpl -o dummyvar
               smpl -o
               smpl -r <boolean expression>
	       smpl full

Resets the sample range.  In the first form startobs and endobs must be
consistent with the periodicity of the data.  In the second form, the "+i" and
"-j" (where i and j are integers) are taken as offsets relative to the
existing sample range.  In the third form, dummyvar must be an indicator
variable with values 0 or 1: the sample will be restricted to those
observations where the indicator value is 1.  The fourth form, smpl -o, drops
all observations for which values of one or more variables are missing.  The
fifth form, using the -r flag, restricts the sample to cases satisfying the
given expression.  The last form, "smpl full", restores the full data range.

    smpl 3 10                for data with periodicity 1
    smpl 1950 1990           for annual data with periodicity 1	
    smpl 1960.2 1982.4       for quarterly data
    smpl 1960.04 1985.10     for monthly data
    smpl 1960.2 ;            keep endobs unchanged
    smpl ; 1984.3            keep startobs unchanged
    smpl +1 -1               advance the starting obs by one, move the
                             ending obs back by one
    smpl -o dum1             create a sample based on "dum1"
    smpl -r sqft>1400        sample cases in which the variable sqft has
                             a value greater than 1400

One point should be noted about the "-o" and "-r" forms of smpl: any
"structural" information in the data header file (regarding the time series or
panel nature of the data) is lost when this command is issued.  You may
reimpose structure with the "setobs" command.

#
spearman
@Statistics
Usage:          spearman x y
                spearman x y -o

Prints Spearman's rank correlation coefficient for the two variables x and y.
The variables do not have to be ranked manually in advance; the function takes
care of this.  If the -o flag is supplied, the original data and the ranked
data are printed out side by side.

The automatic ranking is from largest to smallest (i.e. the largest data value
gets rank 1).  If you need to invert this ranking, create a new variable which
is the negative of the original first.  For example:

  genr altx = -x
  spearman altx y

#
square
@Transformations
Usage:          square x y       or     square -o x y 

Generates new variables which are squares and cross products of selected
variables (-o will create the cross products).  For the above example, new
variables created will be sq_x = x^2, sq_y = y^2 and x_y = x * y.  If a
particular variable is a dummy variable it is not squared because we will get
the same variable.

#
store
@Dataset
Usages:          store filename flag              
                 store filename flag varlist 
Examples:        store mydata.gdt
                 store mydata.csv -c
                 store mydatabin.gdt -o 2 3 4

Filename is the name of the file in which the values should be stored.  If
varlist is absent, the values of all the data series will be stored, otherwise
only those variables specified will be written to file.  Note that scalar
variables will not be included in the data file unless you specifically
request this by including them in varlist.

Possible values of the optional "flag" are as follows:

  none: data are saved in gretl xml format
  -z  : as above, but using gzip data compression
  -o  : data saved as binary, in double precision
  -s  : data saved as binary, in single precision
  -c  : data saved in comma-separated values format, which can be read
        directly by spreadsheet programs
  -r  : data are stored in the native format of GNU R.  They can be loaded
        using the R command 'source()'.
  -m  : data are stored in the native format of GNU Octave.  The first 
        variable cited is taken to be the dependent variable, and is written 
        out as a column vector; the remaining data are written as a matrix 
        named 'X', with one variable per column.
  -t  : data saved in "traditional" gretl format, as in Ramanathan's ESL,
        with an ascii data file plus a "header" file.

#
summary
@Statistics
summary            prints summary statistics for all variables in the file
summary 3 7 9      prints summary statistics for variable numbers 3, 7, and 9
summary x y z      prints summary statistics for the variables x, y, and z

Output consists of the mean, standard deviation (sd), coefficient of variation
(CV = sd/mean), median, minimum, maximum, skewness coefficient, and excess
kurtosis.

#
system
@Estimation
Usage:          system type=systype save=savevars
Examples:       system type=sur 
                system type=sur save=resids
                system type=sur save=resids,fitted

Starts a system of equations.  At present the only type of system supported is
"sur" (Seemingly Unrelated Regressions).  In the optional "save=" field of the
command you can specify whether to save the residuals ("resids") and/or the
fitted values ("fitted").  The system must contain at least two equations
specified using the "equation" command, and it must be terminated with the
line "end system".

#
tabprint
@Printing
Usage:          tabprint
                tabprint -o
                tabprint -f filename

Must follow the estimation of a model via OLS.  Prints the estimated model in
the form of a LaTeX tabular environment.  If a filename is specified using the
"-f" flag output goes to that file, otherwise it goes to a file with a name of
the form "model_N.tex", where N is the number of models estimated to date in
the current session.  This can be incorporated in a LaTeX document.  See also
the eqnprint command.

If the -o flag is given the LaTeX file is a complete document, ready for
processing; otherwise it must be included in a document.

#
testuhat
@Tests
Usage:          testuhat

Must follow a model estimation command.  Gives the frequency distribution for
the residual from the model along with a chi-square test for normality.

#
tsls
@Estimation
Usage:          tsls depvar varlist1 ; varlist2       [-o is optional]
Example:        tsls y1 0 y2 y3 x1 x2 ; 0 x1 x2 x3 x4 x5 x6 

Computes two-stage least squares (tsls) estimates of parameters.  depvar is
the dependent variable, varlist1 is the list of independent variables
(including right-hand side endogenous variables) in the structural equation
for which tsls estimates are needed.  varlist2 is the combined list of
exogenous and predetermined variables in all the equations.  If varlist2 is
not at least as long as varlist1, the model is not identified.  The -o flag
will print the covariance matrix of the coefficients.  In the above example,
the y's are the endogenous variables and the x's are the exogenous and
predetermined variables.

#
var
@Estimation
Usage:          var order varlist ; deterministic terms
Examples:       var 4 x1 x2 x3 ; const time
                var 3 1 2 3 4 ; 0

Sets up and estimates (via OLS) a vector autoregression (VAR).  The first
argument specifies the lag order, then follows the setup for the first
equation.  Don't include lags among the elements of "varlist" -- they will be
added automatically.  The semi-colon separates the stochastic variables, for
which "order" lags will be included, from the deterministic terms, such as the
constant, a time trend, and dummy variables.

In fact, gretl is able to recognize the more common deterministic variables
(constant, time trend, dummy variables with no values other than 0 and 1) as
such, so these do not have to placed after the semi-colon.  More complex
deterministic variables (e.g. a time trend interacted with a dummy variable)
must be put after the semi-colon.

A separate regression will be run for each variable in varlist.  Output for
each equation includes F-tests for zero restrictions on all lags of each of
the variables and an F-test for the maximum lag.  

# 
varlist
@Dataset
Usage:          varlist

Prints a list of the variables currently defined. "list" and "ls" are
synonyms.

#
vartest
@Tests
Usage:          vartest x1 x2

Calculates the F statistic for the null hypothesis that the population
variances for the variables x1 and x2 are equal, and shows its p-value.
	
#
wls
@Estimation
Usage:          wls weightvar depvar indepvars           [-o optional]

Weighted least squares estimates are obtained using weightvar as the weight,
depvar as the dependent variable and indepvars as the list of independent
variables.  More specifically, ols regression is run on weightvar*depvar
against weight*indepvars.  If the weightvar is a dummy variable, this is
equivalent to eliminating all observations with the number zero for weightvar.
The flag -o will print the covariance matrix of coefficients.  A number of
internal variables may be retrieved using the genr command, provided genr is
invoked immediately after this command.  Type help genr for more details on
this.

