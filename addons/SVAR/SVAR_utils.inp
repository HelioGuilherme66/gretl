###########################
# If this helper function is kept, it should probably be moved to an extra file
# in the medium term, or even to extra.gfn.

function strings basename(string fn)
    string base = regsub(fn, "(.*)\.([^\.]*)", "\1")
    string ext = fn + (strlen(base) + 1)
    return defarray(base, ext)
end function 

###########################

function scalar range(scalar a, scalar n, scalar *n0, scalar *n1)
    # this is used in the plotting function to have
    # 0 as a synonym for "all"
    #
    # The flipping of the shock by passing a negative number 
    # is then not possible to specify; users have to do it explicitly
    # in their own loop then.
    
    if a != 0
	ret = 0
	n0 = a
	n1 = a
    else
	ret = 1
	n0 = 1
	n1 = n
    endif
    return ret
end function

###########################

function scalar base_est(bundle *SVARobj)

/*
   takes a SVAR object and adds the basic VAR estimates
   to the bundle; returns an errcode (virginity check).
  */
  
  scalar step = SVARobj.step
  err = (step>0)

  if err
    printf "Base estimation done already!\n"
  else
    matrix mY  = SVARobj.Y
    scalar p   = SVARobj.p 
    scalar n   = SVARobj.n
    scalar k   = SVARobj.k

    scalar T   = SVARobj.T

    matrix E
    matrix mreg = SVARobj.X ~ mlag(mY, seq(1,p))
    matrix olspar = mols(mY[p+1:T,], mreg[p+1:T,], &E)
    matrix Sigma = (E'E) ./ (T - (n*p + k)) # was:  / df

    matrix SVARobj.VARpar = transp(olspar[k+1:k+n*p,])
    matrix SVARobj.mu = (k>0) ? olspar[1:k,] : {} # was: d 
    matrix SVARobj.Sigma = Sigma
    matrix SVARobj.E = E
    SVARobj.step = 1
    SVARobj.LL0 = T * (n* log(2*$pi) - 0.5*log(det(Sigma)) - 0.5)
  endif

  return err

end function

function matrix vecm_det(int T, int dcase)
  # build the deterministic matrix for the VECM; if alpha is
  # empty, it will be estimated via ols later

  # deterministics
  # note that in the "even cases" (restr. const or trend)
  # the restricted term is _not_ included in x, since its
  # parameter will be recovered later via alpha*beta
    
  matrix mreg = (dcase < 3) ? {} : ones(T,1)
  if dcase == 5
    matrix mreg ~= seq(1,T)'
  endif

  return mreg
end function
    
    
function scalar vecm_est(bundle *SVARobj)

/*
   We can't afford to be too flexible here, and the intended usage
   is as follows.

   We assume the model has already been declared as a SVEC (type=4)
   model. We also assume that the cointegration properties (beta mat plus
   deterministics) have already been set up via the SVAR_coint() function,
   so that we already have beta and alpha available as "jbeta" and
   "jalpha", respectively. Finally, we take care of proper treatment of
   deterministics, via the scalar "jcase" (1 to 5).
  */

  # --- preliminary checks 
    
  err = (SVARobj.step > 0)
    
  if err
    printf "Base estimation done already!\n"
    return err
  endif

  err = (SVARobj.type != 4)

  if err
    printf "Wrong model type!\n"
    return err
  endif

  # --- grab stuff from the bundle 

  matrix mY     = SVARobj.Y
  matrix jbeta   = SVARobj.jbeta
  matrix jalpha  = SVARobj.jalpha
  scalar p	  = SVARobj.p 
  scalar n	  = SVARobj.n
  scalar k	  = SVARobj.k
  scalar r      = cols(jbeta)
  scalar dcase   = SVARobj.jcase
  scalar ols_al = rows(jalpha) == 0

  scalar T      = SVARobj.T

  # --- first thing: do we have a pre-set value for alpha?

  matrix dY = diff(mY)
  matrix dep = dY[p+1:T,]
  matrix E = {}

  # deterministics
  matrix mreg = vecm_det(T, dcase)
    
  # ECM terms
  matrix ECM = mlag(mY * jbeta[1:n,],1)
  if dcase == 2
    ECM = ECM .+ jbeta[n+1, ]
  elif dcase == 4
    ECM += seq(1,T)'jbeta[n+1, ]
  endif

  if ols_al
    # alpha must be estimated together with the rest of the VECM
    matrix mreg ~= SVARobj.X ~ ECM
  else
    matrix dep -= (ECM[p+1:T,] * jalpha')
    matrix mreg ~= SVARobj.X 
  endif
    
  # extra lags
  if p > 1
    mreg ~= mlag(dY, seq(1,p-1))
  endif

  # trim the first rows
  if rows(mreg)>0
    mreg = mreg[p+1:T,]
    # printf "%d rows\n", rows(dep)
    # printf "%8.3f\n", (dep[1:10,] ~ mreg[1:10,])
    matrix olspar = mols(dep, mreg, &E)
  else
    matrix olspar = {}
    E = dep
  endif

  df = T - (n*p + k - (n-r)) 
  matrix Sigma = (E'E)./T

  # print olspar Sigma
    
  # --- construct the various matrices required later

  rp = rows(olspar)

  # alpha first (should be before the gammas, if estimated)
  if ols_al
    ng = n*(p-1)
    jalpha = olspar[rp-ng-r+1:rp-ng,]'
  endif
    
  # exogenous

  if dcase == 1
    matrix mu = {}
    scalar nd = 0
  elif dcase == 2
    matrix mu = jbeta[n+1,] * jalpha'
    scalar nd = 0
  elif dcase == 3
    matrix mu = olspar[1,]
    scalar nd = 1
  elif dcase == 4
    matrix mu =  olspar[1,] |(jbeta[n+1,] * jalpha')
    scalar nd = 1
  elif dcase == 5
    matrix mu = olspar[1:2,]
    scalar nd = 2
  endif

  if k > 0
    matrix sel = nd + seq(0, k-1) 
    mu = mu ~ olspar[sel,]
  endif

  /*
     companion form in levels
  */
  Pi = jalpha * jbeta[1:n,]'
    
  if p>1
    if ols_al
      # the Gammas should be after the ECM terms
      ini = nd + r + 1
    else
      # the Gammas should be right after the deterministics
      ini = nd + 1
    endif
    fin = ini + (p-1) * n - 1
    matrix A = olspar[ini:fin,] | zeros(n,n)
    A += I(n) + Pi' | -olspar[ini:fin,]
  else
    matrix A = I(n) + Pi'
  endif
    
  matrix SVARobj.VARpar = A'
  matrix SVARobj.mu = mu
  matrix SVARobj.Sigma = Sigma
  matrix SVARobj.E = E
  matrix SVARobj.jalpha = jalpha
  SVARobj.step = 1
  SVARobj.LL0 = T*(n*log(2*$pi) - 0.5*log(det(Sigma)) - 0.5)

  return err

end function

function matrix N2_ify(matrix A)
  n2 = rows(A) 
  n = int(sqrt(n2))
  k = cols(A)
  matrix e = vec(transp(mshape(seq(1,n2),n,n)))
  return A + A[e,1:k] 
end function

function scalar has_unit_diag(matrix Rd)
  ret = 0
  n2 = cols(Rd) - 1
  n = sqrt(n2)
  matrix test = transp(seq(0,n2-1)) .= seq(0,n2-1,n+1)
  test |= ones(1,n)

  matrix e
  mols(test, Rd', &e)
  # ret = maxr(sumc(e.*e)) < 1.0e-12

  return maxr(sumc(e.*e)) < 1.0e-12 # ret
end function

function scalar has_free_diag(matrix Rd)
  n2 = cols(Rd) - 1
  n = sqrt(n2)
  matrix e = 1 + seq(0,n2-1,n+1)
  matrix test = Rd[,e]
  # ret = sumc(abs(test)) < 1.0e-12
  return ( sumc(abs(test)) < 1.0e-12 ) # ret
end function

function matrix mat_exp(matrix theta, matrix Ss, bool force_pos[0])
  # FIXME: force_pos switch is not used currently (commented out below)
  
  # we don't check for conformability, but 
  # cols(Ss) should be equal to rows(theta)+1

  n2 = rows(Ss) 
  n = int(sqrt(n2))
  k = cols(Ss) - 1
  matrix C = (k>0) ? ( Ss[,1:k]*theta + Ss[,k+1] ) : Ss

  C = mshape(C,n,n)

  /*
     if force_pos
     neg = (diag(C)') .< 0
     pos = !neg
     C = C .* (pos - neg)
     endif
  */
  return C 
end function

function void unscramble_dat(matrix *dat, matrix *Sigma, matrix *Ss)
  n2 = rows(dat) 
  n = int(sqrt(n2))
  k = cols(dat)

  Sigma = mshape(dat[,1],n,n)
  Ss = dat[,2:k]
end function

function scalar VARloglik(scalar T, matrix Sigma, matrix *C[null])
  # computes the (concentrated) loglikelihood for a VAR
  # the matrix C (such that in theory CC' = Sigma) may be null,
    # for just-identified models

  n = rows(Sigma)
  ll = n * 1.83787706640934548355 # ln(2*$pi)

  if !exists(C) # just-identified model
    ll += log(det(Sigma)) + n
  else
    matrix KK = invpd(C*C')
    ll += -log(det(KK)) + tr(KK*Sigma)
  endif

  return -(T/2) * ll
end function

function scalar loglik(matrix *theta, matrix *dat, scalar modeltype)
  # modeltype < 0->C; modeltype>=0: AB (contains free elements of a)
  matrix Sigma = {}
  matrix Ss = {}
  unscramble_dat(&dat, &Sigma, &Ss)
  if modeltype < 0
    matrix C = mat_exp(theta, Ss, 0)
  else
    p1 = modeltype
    p2 = rows(theta)
    matrix aSs = Ss[,1:p1+1]
    matrix bSs = Ss[,p1+2:p2+2]
    matrix A B
    ABmat_exp(theta, aSs, bSs, &A, &B)
    # was: matrix C = B/A
    matrix C = A\B
  endif

  matrix KK = invpd(C*C')
  ll = det(KK) # should always be positive
  ll = (ll<=0) ? NA : -0.5 * (tr(KK*Sigma) - log(ll))

  return ll
end function 

function matrix InfoMat(matrix CorB, matrix S, matrix *A[null])
/*
   merged from InfoMatC and InfoMatAB
   First case C model (A is null), latter AB model.
  */
  matrix tmp = !exists(A) ? I(rows(CorB)) : (A\CorB) | -I(rows(A))    
  tmp = S' (tmp ** inv(CorB'))    
  return tmp * N2_ify(tmp')
end function


function matrix coeffVCV(matrix S, matrix *rhs, matrix *lhs[null])
  # C or AB model
  matrix IM = !exists(lhs) ? InfoMat(rhs, S) : InfoMat(rhs, S, &lhs)
  # (should be correct with new InfoMat)
  
  
  # quick-dirty check for singularity
  if rcond(IM)>1.0e-10
    matrix iIM = invpd(IM)/$nobs
  else
    matrix evec
    l = eigensym(IM, &evec)
    printf "\n\nInformation matrix is not pd!!\n\n%12.5f\n", IM
    printf "Eigenvalues:\n%12.5f\n", l 
    matrix e = (l .< 1.0e-07)
    printf "Troublesome eigenvectors:\n%12.5f\n", selifc(evec, e')
    printf "S:\n%4.0f\n", S
    matrix iIM = zeros(rows(IM), cols(IM))
  endif

  return qform(S,iIM)
end function

function void maybe_flip_columns(matrix C, matrix *X)
/*
   the objective here is to make X as similar as possible to C
   by flipping the sign of the columns. Used for bootstrapping, 
   to have IRFs with comparable signs.
  */
  n = rows(C)
  matrix sel = seq(1,n)
  matrix plus = sumc((C + X).^2)
  matrix minus = sumc((C - X).^2)

  matrix flip = plus .< minus
  if sumr(flip) > 0
    sel = selifc(sel, flip)
    X[,sel] = -X[,sel] 
  endif
end function

function void printStrMat(matrix X, matrix V, string name)
  n = rows(X)
  matrix x = vec(X)

  matrix cfse = vec(X)
  matrix se = sqrt(diag(V))
  
  matrix numzero = selifr(seq(1,rows(se))', (se .< 1.0e-15))
  if rows(numzero) > 1
    se[numzero] = 0.0
  endif
    
  cfse ~= se
  string parnames = ""

  loop j=1..n --quiet
    loop i=1..n --quiet
      sprintf tmpstr "%s[%2d;%2d]", name, i, j
      parnames += tmpstr
      if (j<n) || (i<n)
        parnames += ","
      endif
    endloop
  endloop

  modprint cfse parnames
end function

function void SVAR_est_printout(bundle *mod)
  # scalar type = mod.type
     
  printf "Optimization method = " 
  strings os = defarray("BFGS (numerical)", "BFGS (analytical)", \
    "Newton-Raphson (numerical)", "Newton-Raphson (analytical score)", \
    "Scoring algorithm")
  printf "%s\n", os[mod.optmeth + 1]	# Because optmeth starts at 0 here 
     
  printf "Unconstrained Sigma:\n%12.5f\n", mod.Sigma
  
  if mod.type < 3 || mod.type == 4 # plain or C-model, or (Jan 2018) SVEC 
    # Standard C matrix
    printStrMat(mod.C, mod.vcv, "C")	# was mod.S1

    # Long-run matrix     
    if rows(mod.Rd1l) || mod.calc_lr || mod.type == 4	
      # either long-run restr., or user wish, or SVEC
      # (this could be much embellished, probably)
      printf "Estimated long-run matrix"
      if rows(mod.Rd1l)
        printf " (restricted)"
      endif
      printf "\n"
      
      matrix longrun = mod.lrmat
      # round to zero for printout (also do it in general?)
      longrun = (abs(longrun) .< 1e-15) ? 0.0 : longrun  
      print longrun
    endif

  elif mod.type == 3	# AB, new <Sven> 
    n2 = (mod.n)^2
    if mod.ka > 0
      printStrMat(mod.S1, mod.vcv[1:n2, 1:n2], "A")
    endif
	  
    if mod.kb > 0
      printStrMat(mod.S2, mod.vcv[n2+1 : 2*n2, n2+1 : 2*n2], "B")
    endif
  endif

  printf "  Log-likelihood = %g\n", mod.LL1
  if inbundle(mod, "LRoid")	# over-id test was done
    printf "  Overidentification LR test = %g (%d df, pval = %g)\n\n", \
      mod.LRoid[1], mod.LRoid[2], mod.LRoid[3]
  endif
  
end function

# --------------------------
/* This function was called smash_unstable_roots(), 
   renamed because its name was misleading (to Sven at least).
   The new name contains 'add' because the Psi input is added
   to the A input. 
   (Call changes needed only in SVAR_boot.inp.) 
*/
function scalar add_and_smash(matrix *A, const matrix Psi)

  matrix Ab = A + Psi
  # now check stationarity
  scalar maxmod = max_eval(Ab)

  h = 0.99
  H = 1
  maxiter = 1000
  iter = 0
  loop while (maxmod > 0.9999) && (iter < maxiter) --quiet
    iter++
    Ab = A + H .* Psi
    maxmod = max_eval(Ab)
    H *= h
    # printf "H = %g\n", H
  endloop
  A = Ab
  H = (iter >= maxiter) ? NA : H
  return H
end function
